{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"QMCPy: Quasi-Monte Carlo Community Software in Python","text":"<p>Quasi-Monte Carlo (QMC) methods are used to approximate multivariate integrals. They have four main components: a discrete distribution, a true measure of randomness, an integrand, and a stopping criterion. Information about the integrand is obtained as a sequence of values of the function sampled at the data-sites of the discrete distribution. The stopping criterion tells the algorithm when the user-specified error tolerance has been satisfied. We are developing a framework that allows collaborators in the QMC community to develop plug-and-play modules in an effort to produce more efficient and portable QMC software. Each of the above four components is an abstract class. Abstract classes specify the common properties and methods of all subclasses. The ways in which the four kinds of classes interact with each other are also specified. Subclasses then flesh out different integrands, sampling schemes, and stopping criteria. Besides providing developers a way to link their new ideas with those implemented by the rest of the QMC community, we also aim to provide practitioners with state-of-the-art QMC software for their applications.</p>"},{"location":"#resources","title":"Resources","text":"<p>The QMCPy documentation contains a detailed package reference documenting functions and classes including thorough doctests. A number of example notebook demos are also rendered into the documentation from <code>QMCSoftware/demos/</code>. We recommend the following resources to start learning more about QMCPy</p> <ul> <li>mathematical description of QMCPy software and components.</li> <li>Aleksei Sorokin's 2023 PyData Chicago video tutorial and corresponding notebook</li> <li>Fred Hickernell's 2020 MCQMC video tutorial and corresponding notebook</li> <li>The QMCPy introduction notebook and quickstart notebook</li> </ul>"},{"location":"#installation","title":"Installation","text":"<pre><code>pip install qmcpy\n</code></pre> <p>To install from source, please see the contributing guidelines.</p>"},{"location":"#citation","title":"Citation","text":"<p>If you find QMCPy helpful in your work, please support us by citing the following work, which is also available as a QMCPy BibTex citation</p> <pre><code>Sou-Cheng T. Choi, Fred J. Hickernell, Michael McCourt, Jagadeeswaran Rathinavel, Aleksei G. Sorokin,\nQMCPy: A Quasi-Monte Carlo Python Library. 2025.\nhttps://qmcsoftware.github.io/QMCSoftware/\n</code></pre> <p>We maintain a list of publications on the development and use of QMCPy as well as a list of select references upon which QMCPy was built.</p>"},{"location":"#development","title":"Development","text":"<p>Want to contribute to QMCPy? Please see our guidelines for contributors which includes instructions on installation for developers, running tests, and compiling documentation.</p> <p>This software would not be possible without the efforts of the QMCPy community including our steering council, collaborators, contributors, and sponsors.</p> <p>QMCPy is distributed under an Apache 2.0 license from the Illinois Institute of Technology.</p>"},{"location":"CONTRIBUTING/","title":"Contributing","text":"<p>Thank you for your interest in contributing to the QMCPy package!</p> <p>Please submit pull requests to the <code>develop</code> branch and issues using a template from <code>.github/ISSUE_TEMPLATE/</code></p> <p>If you develop a new component please consider writing a blog for qmcpy.org</p> <p>Join team communications by reaching out to us at qmc-software@googlegroups.com</p>"},{"location":"CONTRIBUTING/#installation","title":"Installation","text":"<p>In a git enabled terminal (e.g. bash for Windows) with miniconda installed and C compilers enabled (Windows users may need to install Microsoft C++ Build Tools), run</p> <pre><code>git clone https://github.com/QMCSoftware/QMCSoftware.git\ncd QMCSoftware\ngit checkout develop\nconda create --name qmcpy python=3.12\nconda activate qmcpy\npip install -e .[dev]\n</code></pre> <p>While <code>dev</code> contains the most complete set of install dependencies, a number of other install dependency groups can be found in our <code>pyproject.toml</code> file. If running in the <code>zsh</code> terminal you may need to use</p> <pre><code>pip install -e \".[dev]\"\n</code></pre>"},{"location":"CONTRIBUTING/#tests","title":"Tests","text":"<p>Doctests and unittests take a few minute to run with</p> <pre><code>make tests_no_docker\n</code></pre> <p>Optionally, you may install Docker and then run all tests with</p> <pre><code>make tests\n</code></pre> <p>Please see the targets in the makefile for more granular control over tests.</p>"},{"location":"CONTRIBUTING/#documentation","title":"Documentation","text":"<p>To compile documentation, run</p> <pre><code>make doc\n</code></pre> <p>To download PDF documentation, go to the \"Printable Docs\" header in the documentation, press Ctrl-p to print, and then choose to save the PDF to your preferred location.</p>"},{"location":"CONTRIBUTING/#demos","title":"Demos","text":"<p>Demos are Jupyter notebooks which may be launched using the command</p> <pre><code>jupyter-lab\n</code></pre>"},{"location":"CONTRIBUTING/#other-developer-tools","title":"Other Developer Tools","text":"<p>The Developers Tools page on qmcpy.org documents additional tools we have found helpful for mathematical software development and presentation.</p>"},{"location":"CONTRIBUTING/#vscode-tips","title":"VSCode Tips","text":"<p>VSCode (Visual Studio Code) is the IDE of choice for many of our developers. Here we compile some helpful notes regarding additional setup for VSCode.</p> <ul> <li>Run <code>CMD</code>+<code>p</code> then <code>&gt; Python: Select Interpreter</code> then select the <code>('qmcpy')</code> choice from the dropdown to link the qmcpy environment into your workspace. Now when you open a terminal, your command line should read <code>(qmcpy) username@...</code> which indicates the qmcpy environment has been automatically activated. Also, when debugging the qmcpy environment will be automatically used.</li> <li>Go to <code>File</code> and click <code>Save Workspace as...</code> to save a <code>qmcpy</code> workspace for future development.</li> </ul> <p>Some VSCode extension we found useful include</p> <ul> <li>Python</li> <li>Jupyter</li> <li>Markdown Preview Enhanced</li> <li>eps-preview, which requires<ul> <li>Postscript Language</li> <li>pdf2svg</li> </ul> </li> <li>Git Graph</li> <li>Code Spell Checker</li> </ul>"},{"location":"community/","title":"Community","text":"<p>This document outlines a few key roles within the QMCPy Community. The purpose of this document is to provide guidance and clarity to community members on their responsibilities and the roles they play in the development and maintenance of the software. The key roles are the Steering Council, Collaborators, and Contributors. By working together, the community can ensure that the software continues to evolve and meet the needs of the scientific community.</p>"},{"location":"community/#steering-council","title":"Steering Council","text":"<p>The Steering Council is responsible for the overall direction and governance of the software. This includes defining the vision and goals of the software project, establishing policies and procedures, and ensuring that the community operates in a transparent and democratic manner. Their responsibilities include the following:</p> <ul> <li>Provide leadership and guidance to the community</li> <li>Set priorities and allocate resources</li> <li>Ensure that the community operates in an ethical and transparent manner</li> <li>Foster collaboration and communication between community members</li> <li>Represent the community to external stakeholders</li> </ul> <p>The current council members are as following (listed in alphabetical order of last names):</p> <ul> <li>Sou-Cheng T. Choi</li> <li>Fred J. Hickernell</li> <li>Michael McCourt</li> <li>Jagadeeswaran Rathinavel</li> <li>Aleksei Sorokin</li> </ul>"},{"location":"community/#collaborators","title":"Collaborators","text":"<p>Collaborators play a key role in the development of QMCPy by contributing their expertise in scientific research. They provide valuable insights, support, and feedback on the software\u2019s functionality and ensure that it meets the needs of the scientific community. Their responsibilities include the following:</p> <ul> <li>Offer suggestions for new features and enhancements based on their research and knowledge domain</li> <li>Provide feedback on the software\u2019s functionality and design</li> <li>Act as a liaison between the software community and the academic community</li> </ul> <p>The following are our collaborators (listed in alphabetical order of last names):</p> <ul> <li>Yuhan Ding</li> <li>Adrian Ebert</li> <li>Mike Giles</li> <li>Marius Hofert</li> <li>Lan Jiang</li> <li>Sergei Kucherenko</li> <li>Pierre L\u2019Ecuyer</li> <li>Christiane Lemieux</li> <li>Dirk Nuyens</li> <li>Onyekachi Osisiogu</li> <li>Art Owen</li> <li>Pieterjan Robbe</li> <li>Xuan Zhou</li> </ul>"},{"location":"community/#contributors","title":"Contributors","text":"<p>Contributors are individuals who actively participate in the development of the software. They may contribute code, documentation, bug reports, and other forms of support.  Their responsibilities include the following:</p> <ul> <li>Participate in the development of the software</li> <li>Resolve bugs and suggest improvements</li> <li>Develop tests and documentation</li> <li>Provide support to other community members</li> <li>Participate in community discussions and decision-making processes</li> </ul> <p>The contributors to our GitHub are:</p> <p> </p> <p>For a list of contributors to QMCPY.org, please refer to https://qmcpy.org/contributors/.</p>"},{"location":"community/#sponsors","title":"Sponsors","text":"<ul> <li>Illinois Tech</li> </ul> <ul> <li>Kamakura Corporation, acquired by SAS Institute Inc. in June 2022</li> </ul> <ul> <li>SigOpt, Inc.</li> </ul>"},{"location":"community/#select-references","title":"Select References","text":"<ol> <li> <p>F. Y. Kuo and D. Nuyens. \"Application of quasi-Monte Carlo methods to elliptic PDEs with random diffusion coefficients - a survey of analysis and implementation,\" Foundations of Computational Mathematics, 16(6):1631-1696, 2016. https://link.springer.com/article/10.1007/s10208-016-9329-5. https://arxiv.org/abs/1606.06613.</p> </li> <li> <p>Fred J. Hickernell, Lan Jiang, Yuewei Liu, and Art B. Owen, \"Guaranteed conservative fixed width confidence intervals via Monte Carlo   sampling,\" Monte Carlo and Quasi-Monte Carlo Methods 2012 (J. Dick, F.Y. Kuo, G. W.  Peters, and I. H. Sloan, eds.), pp. 105-128, Springer-Verlag, Berlin, 2014. DOI: 10.1007/978-3-642-41095-6_5</p> </li> <li> <p>Sou-Cheng T. Choi, Yuhan Ding, Fred J. Hickernell, Lan Jiang, Lluis Antoni Jimenez Rugama, Da Li, Jagadeeswaran Rathinavel, Xin Tong, Kan Zhang, Yizhi Zhang, and Xuan Zhou, GAIL: Guaranteed Automatic Integration Library (Version 2.3.1) [MATLAB Software], 2020. http://gailgithub.github.io/GAIL_Dev/.</p> </li> <li> <p>Sou-Cheng T. Choi, \"MINRES-QLP Pack and Reliable Reproducible Research via Supportable Scientific Software,\" Journal of Open Research Software, Volume 2, Number 1, e22, pp. 1-7, 2014.</p> </li> <li> <p>Sou-Cheng T. Choi and Fred J. Hickernell, \"IIT MATH-573 Reliable Mathematical Software\" [Course Slides], Illinois Institute of Technology, Chicago, IL, 2013. Available from http://gailgithub.github.io/GAIL_Dev/.</p> </li> <li> <p>Daniel S. Katz, Sou-Cheng T. Choi, Hilmar Lapp, Ketan Maheshwari, Frank Loffler, Matthew Turk, Marcus D. Hanwell, Nancy Wilkins-Diehr, James Hetherington, James Howison, Shel Swenson, Gabrielle D. Allen, Anne C. Elster, Bruce Berriman, Colin Venters, \"Summary of the First Workshop On Sustainable Software for Science: Practice and Experiences (WSSSPE1),\" Journal of Open Research Software, Volume 2, Number 1, e6, pp. 1-21, 2014.</p> </li> <li> <p>Fang, K.-T., and Wang, Y. (1994). Number-theoretic Methods in Statistics. London, UK: CHAPMAN &amp; HALL</p> </li> <li> <p>Lan Jiang, Guaranteed Adaptive Monte Carlo Methods for Estimating Means of Random Variables, PhD Thesis, Illinois Institute of Technology, 2016.</p> </li> <li> <p>Lluis Antoni Jimenez Rugama and Fred J. Hickernell, \"Adaptive multidimensional integration based on rank-1 lattices,\" Monte Carlo and Quasi-Monte Carlo  Methods: MCQMC, Leuven, Belgium, April 2014 (R. Cools and D. Nuyens, eds.), Springer Proceedings in Mathematics and Statistics, vol. 163, Springer-Verlag, Berlin, 2016, arXiv:1411.1966, pp. 407-422.</p> </li> <li> <p>Kai-Tai Fang and Yuan Wang, Number-theoretic Methods in Statistics, Chapman &amp; Hall, London, 1994.</p> </li> <li> <p>Fred J. Hickernell and Lluis Antoni Jimenez Rugama, \"Reliable adaptive cubature using digital sequences,\" Monte Carlo and Quasi-Monte Carlo Methods: MCQMC, Leuven, Belgium, April 2014 (R. Cools and D. Nuyens, eds.), Springer Proceedings in Mathematics and Statistics, vol. 163, Springer-Verlag, Berlin, 2016, arXiv:1410.8615 [math.NA], pp. 367-383.</p> </li> <li> <p>Marius Hofert and Christiane Lemieux (2019). qrng: (Randomized) Quasi-Random Number Generators. R package version 0.0-7. https://CRAN.R-project.org/package=qrng.</p> </li> <li> <p>Faure, Henri, and Christiane Lemieux. \u201cImplementation of Irreducible Sobol\u2019 Sequences in Prime Power Bases,\u201d Mathematics and Computers in Simulation 161 (2019): 13\u201322. </p> </li> <li> <p>M. B. Giles. \"Multi-level Monte Carlo path simulation,\" Operations Research, 56(3):607-617, 2008. http://people.maths.ox.ac.uk/~gilesm/files/OPRE_2008.pdf.</p> </li> <li> <p>M. B. Giles. \"Improved multilevel Monte Carlo convergence using the Milstein scheme,\" 343-358, in Monte Carlo and Quasi-Monte Carlo Methods 2006, Springer, 2008. http://people.maths.ox.ac.uk/~gilesm/files/mcqmc06.pdf.</p> </li> <li> <p>M. B. Giles and B. J. Waterhouse. \"Multilevel quasi-Monte Carlo path simulation,\" pp.165-181 in Advanced Financial Modelling, in Radon Series on Computational and Applied Mathematics, de Gruyter, 2009. http://people.maths.ox.ac.uk/~gilesm/files/radon.pdf.</p> </li> <li> <p>Owen, A. B. \"A randomized Halton algorithm in R,\" 2017. arXiv:1706.02808 [stat.CO]</p> </li> <li> <p>B. D. Keister, Multidimensional Quadrature Algorithms,  'Computers in Physics', 10, pp. 119-122, 1996.</p> </li> <li> <p>L\u2019Ecuyer, Pierre &amp; Munger, David. (2015). LatticeBuilder: A General Software Tool for Constructing Rank-1 Lattice Rules. ACM Transactions on Mathematical Software. 42. 10.1145/2754929. </p> </li> <li> <p>Fischer, Gregory &amp; Carmon, Ziv &amp; Zauberman, Gal &amp; L\u2019Ecuyer, Pierre. (1999). Good Parameters and Implementations for Combined Multiple Recursive Random Number Generators. Operations Research. 47. 159-164. 10.1287/opre.47.1.159. </p> </li> <li> <p>I.M. Sobol', V.I. Turchaninov, Yu.L. Levitan, B.V. Shukhman: \"Quasi-Random Sequence Generators\" Keldysh Institute of Applied Mathematics, Russian Academy of Sciences, Moscow (1992).</p> </li> <li> <p>Sobol, Ilya &amp; Asotsky, Danil &amp; Kreinin, Alexander &amp; Kucherenko, Sergei. (2011). Construction and Comparison of High-Dimensional Sobol' Generators. Wilmott. 2011. 10.1002/wilm.10056. </p> </li> <li> <p>Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., \u2026 Chintala, S. (2019). PyTorch: An Imperative Style, High-Performance Deep Learning Library. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d extquotesingle Alch'e-Buc, E. Fox, &amp; R. Garnett (Eds.), Advances in Neural Information Processing Systems 32 (pp. 8024\u20138035). Curran Associates, Inc. Retrieved from http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf</p> </li> <li> <p>S. Joe and F. Y. Kuo, Constructing Sobol sequences with better two-dimensional projections, SIAM J. Sci. Comput. 30, 2635-2654 (2008).</p> </li> <li> <p>Paul Bratley and Bennett L. Fox. 1988. Algorithm 659: Implementing Sobol's quasirandom sequence generator. ACM Trans. Math. Softw. 14, 1 (March 1988), 88\u2013100. DOI:https://doi.org/10.1145/42288.214372</p> </li> <li> <p>P. L'Ecuyer, P. Marion, M. Godin, and F. Puchhammer, \"A Tool for Custom Construction of QMC and RQMC Point Sets,\" Monte Carlo and Quasi-Monte Carlo Methods 2020.</p> </li> <li> <p>P Kumaraswamy, A generalized probability density function for double-bounded random processes. J. Hydrol. 46, 79\u201388 (1980).</p> </li> <li> <p>D Li, Reliable quasi-Monte Carlo with control variates. Master\u2019s thesis, Illinois Institute of Technology (2016)</p> </li> <li> <p>D.H. Bailey, J.M. Borwein, R.E. Crandall, Box integrals, Journal of Computational and Applied Mathematics, Volume 206, Issue 1, 2007, Pages 196-208, ISSN 0377-0427, https://doi.org/10.1016/j.cam.2006.06.010.</p> </li> <li> <p>Art B. Owen.Monte Carlo theory, methods and examples. 2013.</p> </li> </ol>"},{"location":"components/","title":"Components of QMCPy: Quasi-Monte Carlo Software in Python","text":"<p>Monte Carlo (MC) methods approximate the true mean (expectation) \\(\\mu\\) of a random variable \\(g(\\boldsymbol{T})\\) by the sample mean \\(\\hat{\\mu}_n = \\frac{1}{n} \\sum_{i=0}^{n-1} g(\\boldsymbol{T}_i)\\) for some samples \\(\\boldsymbol{T}_0,\\dots,\\boldsymbol{T}_{n-1}\\). We call the \\(d\\)-dimensional vector random variable \\(\\boldsymbol{T}\\) the true measure and we call \\(g\\) the integrand. As most computer-generated random numbers are uniformly distributed, we use a transform \\(\\boldsymbol{\\psi}\\) to write \\(\\boldsymbol{T} \\sim \\boldsymbol{\\psi}(\\boldsymbol{X})\\) where \\(\\boldsymbol{X} \\sim \\mathcal{U}[0,1]^d\\). The resulting Monte Carlo approximation is written in terms of the transformed integrand \\(f(\\boldsymbol{x}) = g(\\boldsymbol{\\psi}(\\boldsymbol{x}))\\) as</p> \\[\\mu = \\mathbb{E}[f(\\boldsymbol{X})] = \\int_{[0,1]^d} f(\\boldsymbol{x}) \\mathrm{d} \\boldsymbol{x} \\approx \\int_{[0,1]^d} f(\\boldsymbol{x}) \\hat{\\lambda}_n(\\mathrm{d} \\boldsymbol{x}) = \\frac{1}{n} \\sum_{i=0}^{n-1} f(\\boldsymbol{x}_i) = \\hat{\\mu}, \\qquad \\boldsymbol{X} \\sim \\mathcal{U}[0,1]^d\\] <p>for some discrete distribution \\(\\hat{\\lambda}_n\\) defined by samples \\(\\boldsymbol{x}_0,\\dots,\\boldsymbol{x}_{n-1} \\in [0,1]^d\\) (formally \\(\\hat{\\lambda}_n(A)\\) measures the proportion of points \\((\\boldsymbol{x}_i)_{i=0}^{n-1}\\) which lie in some set \\(A\\)). The error of this approximation is</p> \\[E_n = \\lvert \\mu - \\hat{\\mu}_n \\rvert.\\] <p>Classic Monte Carlo methods choose IID (independent and identically distributed) samples \\(\\boldsymbol{x}_0,\\dots,\\boldsymbol{x}_{n-1} \\overset{\\mathrm{IID}}{\\sim} \\mathcal{U}[0,1]^d\\) and have error \\(E_n\\) like \\(\\mathcal{O}(n^{-1/2})\\). Quasi-Monte Carlo (QMC) methods achieve a significantly better error rate of \\(\\mathcal{O}(n^{-1})\\) by using low discrepancy (LD) sequences for \\((\\boldsymbol{x}_i)_{i=0}^{n-1}\\) which more evenly fill the unit cube than IID points.</p> The first \\(32\\) points of each sequence are shown as purple starts, the next \\(32\\) points are shown as green triangles, and the \\(64\\) points after that are shown as blue circles. Notice the gaps and clusters of IID points compared to the more uniform coverage of LD sequences. <p>Often practitioners would like to run their (Quasi-)Monte Carlo method until the error \\(E_n\\) is below a desired error tolerance \\(\\varepsilon\\) and/or until they have expired their sample budget \\(B\\). For example, one may wish to estimate the expected discounted payoff of a financial option to within a tolerance of one penny, \\(\\varepsilon = 0.01\\), or until \\(1\\) million option paths have been simulated, \\(B=10^6\\). Stopping criterion deploy (Quasi-)Monte Carlo methods under such constraints by utilizing adaptive sampling schemes and efficient error estimation procedures.  </p> <p><code>QMCPy</code> is organized into into the four main components below. Details for each of these classes are available in the linked guides and API docs.</p>"},{"location":"components/#discrete-distributions","title":"Discrete Distributions","text":"<p>These generates IID or LD points \\(\\boldsymbol{x}_0,\\boldsymbol{x}_1,\\dots\\). Supported LD sequences include</p> <ul> <li>Lattices with<ul> <li>extensible constructions</li> <li>random shifts</li> </ul> </li> <li>Digital Nets in base \\(b=2\\) with<ul> <li>extensible constructions</li> <li>digital shifts</li> <li>linear matrix scrambling</li> <li>nested uniform scrambling (also called Owen scrambling)</li> <li>higher order constructions via digital interlacing</li> </ul> </li> <li>Halton point sets with<ul> <li>extensible constructions</li> <li>digital shifts</li> <li>permutation scrambling</li> <li>linear matrix scrambling</li> <li>nested uniform scrambling</li> </ul> </li> </ul>"},{"location":"components/#true-measures","title":"True Measures","text":"<p>These define \\(\\boldsymbol{T}\\), for which <code>QMCPy</code> will automatically choose an appropriate transform \\(\\boldsymbol{\\psi}\\) so that \\(\\boldsymbol{T} \\sim \\boldsymbol{\\psi}(\\boldsymbol{X})\\) with \\(\\boldsymbol{X} \\sim \\mathcal{U}[0,1]^d\\). Some popular true measures are</p> <ul> <li>Uniform \\(\\boldsymbol{T} \\sim \\mathcal{U}[\\boldsymbol{l},\\boldsymbol{u}]\\) with elementwise \\(\\boldsymbol{l} \\leq \\boldsymbol{u}\\) for which \\(\\boldsymbol{\\psi}(\\boldsymbol{x}) = \\boldsymbol{l}+(\\boldsymbol{u}-\\boldsymbol{l}) \\odot \\boldsymbol{x}\\) with \\(\\odot\\) the Hadamard (elementwise) product.</li> <li>Gaussian \\(\\boldsymbol{T} \\sim \\mathcal{N}(\\boldsymbol{m},\\mathsf{\\Sigma})\\) for which \\(\\boldsymbol{\\psi}(\\boldsymbol{x}) = \\boldsymbol{m}+\\mathsf{A}\\boldsymbol{x}\\) where the covariance \\(\\mathsf{\\Sigma} = \\mathsf{A} \\mathsf{A}^T\\) may be decomposed using either<ul> <li>the Cholesky decomposition or</li> <li>the eigendecomposition.</li> </ul> </li> <li>Brownian Motion observed with an initial value \\(B_0\\), drift \\(\\gamma\\), and diffusion \\(\\sigma^2\\) at times \\(\\boldsymbol{t} := (t_1,\\dots,t_d)^T\\) satisfying \\(0 \\leq t_1 &lt; t_1 &lt; \\dots &lt; t_d\\) is a Gaussian with mean and covariance</li> </ul> \\[\\boldsymbol{m} = B_0 + \\gamma \\boldsymbol{t}\\] \\[\\mathsf{\\Sigma} = \\sigma^2 \\left(\\min\\{t_i,t_{i'}\\}\\right)_{i,i'=1}^{d}\\] <ul> <li>Independent Marginals have \\(\\boldsymbol{T} = (T_1,\\dots,T_d)^T\\) with \\(T_1,\\dots,T_d\\) independent. We support (continuous) marginal distributions from <code>scipy.stats</code>.</li> </ul>"},{"location":"components/#integrands","title":"Integrands","text":"<p>These define \\(g\\), which <code>QMCPy</code> will use to define \\(f = g \\circ \\boldsymbol{\\psi}\\). Some popular integrands are</p> <ul> <li>User Defined Integrands, where the user provides a function handle for \\(g\\)</li> <li>Financial Options, including the European option, Asian option, and Barrier option</li> <li><code>UM-Bridge</code> Functions. From their docs, <code>UM-Bridge</code> is a universal interface that makes any numerical model accessible from any programming language or higher-level software through the use of containerized environments. <code>UM-Bridge</code> also enables simulations to scale to supercomputers or the cloud with minimal effort.</li> </ul>"},{"location":"components/#stopping-criteria","title":"Stopping Criteria","text":"The cost of IID-Monte Carlo algorithms is \\(\\mathcal{O}(n^2)\\) in the number of samples \\(n\\) while Quasi-Monte Carlo algorithms only cost around \\(\\mathcal{O}(n)\\). Both IID-Monte Carlo and Quasi-Monte Carlo stopping criterion consistently determine approximations which meet the desired error tolerance. <p>These deploy (Quasi-)Monte Carlo methods under error tolerance and budgetary constraints by utilizing adaptive sampling schemes and efficient error estimation procedures. Common stopping criteria include</p> <ul> <li>Quasi-Monte Carlo via tracking the decay of coefficients in an orthogonal basis expansion. These methods are guaranteed for cones of functions whose coefficients decay in a regular manner.  Efficient procedures exist to estimate coefficients when<ul> <li>pairing lattices with the Fourier expansion or</li> <li>pairing digital nets with the Walsh expansion.</li> </ul> </li> <li>Quasi-Monte Carlo via efficient Bayesian cubature methods which assume \\(f\\) is a draw from a Gaussian process so the posterior expectation has an analytic expression. While classic Bayesian cubature would require \\(\\mathcal{O}(n^2)\\) storage and \\(\\mathcal{O}(n^3)\\) computations, when matching certain LD sequences to special kernels the Gram matrices become nicely structured to permit Bayesian cubature with only \\(\\mathcal{O}(n)\\) storage and \\(\\mathcal{O}(n \\log n)\\) computations. Specifically,<ul> <li>pairing lattices with shift-invariant kernels gives circulant Gram matrices which are diagonalizable by the Fast Fourier Transform (FFT), and</li> <li>pairing digital nets with digitally-shift-invariant kernels gives Gram matrices which are diagonalizable by the Fast Walsh-Hadamard Transform (FWHT).</li> </ul> </li> <li>Quasi-Monte Carlo via multiple independent randomizations of an LD point set and Student's \\(t\\) confidence intervals.</li> <li>IID Monte Carlo via a two step procedure using the Central Limit Theorem (CLT). Error estimates are not guaranteed as CLT is asymptotic in \\(n\\) is the variance must be estimated.</li> <li>IID Monte Carlo via a two step procedure using Berry-Esseen inequalities to account for finite sample sizes. Error estimates are guaranteed for functions with bounded Kurtosis.</li> <li>Multilevel IID Monte Carlo and Quasi-Monte Carlo which more efficiently integrate expensive functions by exploiting a telescoping sum over lower fidelity models.</li> </ul>"},{"location":"api/discrete_distributions/","title":"Discrete Distributions","text":""},{"location":"api/discrete_distributions/#uml-overview","title":"UML Overview","text":""},{"location":"api/discrete_distributions/#abstractdiscretedistribution","title":"<code>AbstractDiscreteDistribution</code>","text":"<p>               Bases: <code>object</code></p> Source code in <code>qmcpy/discrete_distribution/abstract_discrete_distribution.py</code> <pre><code>def __init__(self, dimension, replications, seed, d_limit, n_limit):\n    self.mimics = 'StdUniform'\n    if not hasattr(self,'parameters'):\n        self.parameters = []\n    self.d_limit = d_limit\n    self.n_limit = n_limit \n    if not (np.isscalar(self.d_limit) and self.d_limit&gt;0 and np.isscalar(self.n_limit) and self.n_limit&gt;0):\n        raise ParameterError(\"d_limit and n_limit must be greater than 0\")\n    self.d_limit = np.inf if self.d_limit==np.inf else int(self.d_limit)\n    self.n_limit = np.inf if self.n_limit==np.inf else int(self.n_limit)\n    self.no_replications = replications is None \n    self.replications = 1 if self.no_replications else int(replications)\n    if self.replications&lt;0:\n        raise ParameterError(\"replications must be None or a postive int\")\n    if isinstance(dimension,list) or isinstance(dimension,tuple) or isinstance(dimension,np.ndarray):\n        self.dvec = np.array(dimension,dtype=int)\n        self.d = len(self.dvec)\n        if not (self.dvec.ndim==1 and len(np.unique(self.dvec))==self.d):\n            raise ParameterError(\"dimension must be a 1d array of unique values\")\n    else:\n        self.d = int(dimension)\n        self.dvec = np.arange(self.d)\n    if any(self.dvec&gt;self.d_limit):\n        raise ParameterError('dimension greater than dimension limit %d'%self.d_limit)\n    self._base_seed = seed if isinstance(seed,np.random.SeedSequence) else np.random.SeedSequence(seed)\n    self.entropy = self._base_seed.entropy\n    self.spawn_key = self._base_seed.spawn_key\n    self.rng = np.random.Generator(np.random.SFC64(self._base_seed))\n</code></pre>"},{"location":"api/discrete_distributions/#qmcpy.discrete_distribution.abstract_discrete_distribution.AbstractDiscreteDistribution.__call__","title":"__call__","text":"<pre><code>__call__(n=None, n_min=None, n_max=None, return_binary=False, warn=True)\n</code></pre> <ul> <li>If just <code>n</code> is supplied, generate samples from the sequence at indices 0,...,<code>n</code>-1.</li> <li>If <code>n_min</code> and <code>n_max</code> are supplied, generate samples from the sequence at indices <code>n_min</code>,...,<code>n_max</code>-1.</li> <li>If <code>n</code> and <code>n_min</code> are supplied, then generate samples from the sequence at indices <code>n</code>,...,<code>n_min</code>-1.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>Union[None, int]</code> <p>Number of points to generate.</p> <code>None</code> <code>n_min</code> <code>Union[None, int]</code> <p>Starting index of sequence.</p> <code>None</code> <code>n_max</code> <code>Union[None, int]</code> <p>Final index of sequence.</p> <code>None</code> <code>return_binary</code> <code>bool</code> <p>Only used for <code>DigitalNetB2</code>. If <code>True</code>, only return the integer representation <code>x_integer</code> of base 2 digital net.  </p> <code>False</code> <code>warn</code> <code>bool</code> <p>If <code>False</code>, disable warnings when generating samples.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>x</code> <code>ndarray</code> <p>Samples from the sequence. </p> <ul> <li>If <code>replications</code> is <code>None</code> then this will be of size (<code>n_max</code>-<code>n_min</code>) \\(\\times\\) <code>dimension</code> </li> <li>If <code>replications</code> is a positive int, then <code>x</code> will be of size <code>replications</code> \\(\\times\\) (<code>n_max</code>-<code>n_min</code>) \\(\\times\\) <code>dimension</code> </li> </ul> <p>Note that if <code>return_binary=True</code> then <code>x</code> is returned where <code>x</code> are integer representations of the digital net points.</p> Source code in <code>qmcpy/discrete_distribution/abstract_discrete_distribution.py</code> <pre><code>def __call__(self, n=None, n_min=None, n_max=None, return_binary=False, warn=True):\n    r\"\"\"\n    - If just `n` is supplied, generate samples from the sequence at indices 0,...,`n`-1.\n    - If `n_min` and `n_max` are supplied, generate samples from the sequence at indices `n_min`,...,`n_max`-1.\n    - If `n` and `n_min` are supplied, then generate samples from the sequence at indices `n`,...,`n_min`-1.\n\n    Args:\n        n (Union[None,int]): Number of points to generate.\n        n_min (Union[None,int]): Starting index of sequence.\n        n_max (Union[None,int]): Final index of sequence.\n        return_binary (bool): Only used for `DigitalNetB2`.  \n            If `True`, *only* return the integer representation `x_integer` of base 2 digital net.  \n        warn (bool): If `False`, disable warnings when generating samples.\n\n    Returns:\n        x (np.ndarray): Samples from the sequence. \n\n            - If `replications` is `None` then this will be of size (`n_max`-`n_min`) $\\times$ `dimension` \n            - If `replications` is a positive int, then `x` will be of size `replications` $\\times$ (`n_max`-`n_min`) $\\times$ `dimension` \n\n            Note that if `return_binary=True` then `x` is returned where `x` are integer representations of the digital net points. \n    \"\"\"\n    return self.gen_samples(n=n,n_min=n_min,n_max=n_max,return_binary=return_binary,warn=warn)\n</code></pre>"},{"location":"api/discrete_distributions/#qmcpy.discrete_distribution.abstract_discrete_distribution.AbstractDiscreteDistribution.spawn","title":"spawn","text":"<pre><code>spawn(s=1, dimensions=None)\n</code></pre> <p>Spawn new instances of the current discrete distribution but with new seeds and dimensions. Used by multi-level QMC algorithms which require different seeds and dimensions on each level.</p> Note <p>Use <code>replications</code> instead of using <code>spawn</code> when possible, e.g., when spawning copies which all have the same dimension.</p> <p>Parameters:</p> Name Type Description Default <code>s</code> <code>int</code> <p>Number of copies to spawn</p> <code>1</code> <code>dimensions</code> <code>ndarray</code> <p>Length <code>s</code> array of dimensions for each copy. Defaults to the current dimension. </p> <code>None</code> <p>Returns:</p> Name Type Description <code>spawned_discrete_distribs</code> <code>list</code> <p>Discrete distributions with new seeds and dimensions.</p> Source code in <code>qmcpy/discrete_distribution/abstract_discrete_distribution.py</code> <pre><code>def spawn(self, s=1, dimensions=None):\n    r\"\"\"\n    Spawn new instances of the current discrete distribution but with new seeds and dimensions.\n    Used by multi-level QMC algorithms which require different seeds and dimensions on each level.\n\n    Note:\n        Use `replications` instead of using `spawn` when possible, e.g., when spawning copies which all have the same dimension.\n\n    Args:\n        s (int): Number of copies to spawn\n        dimensions (np.ndarray): Length `s` array of dimensions for each copy. Defaults to the current dimension. \n\n    Returns:\n        spawned_discrete_distribs (list): Discrete distributions with new seeds and dimensions.\n    \"\"\"\n    s = int(s)\n    if s&lt;=0:\n        raise ParameterError(\"Must spawn s&gt;0 instances\")\n    if dimensions is None: \n        dimensions = np.tile(self.d,s)\n    elif (isinstance(dimensions,list) or isinstance(dimensions,tuple) or isinstance(dimensions,np.ndarray)):\n        dimensions = np.array(dimensions,dtype=int)\n    else:\n        dimensions = np.tile(dimensions,s)\n    if not (dimensions.ndim==1 and len(dimensions)==s):\n        raise ParameterError(\"dimensions must be a length s np.ndarray\")\n    child_seeds = self._base_seed.spawn(s)\n    spawned_discrete_distribs = [self._spawn(child_seeds[i],int(dimensions[i])) for i in range(s)]\n    return spawned_discrete_distribs\n</code></pre>"},{"location":"api/discrete_distributions/#digitalnetb2","title":"<code>DigitalNetB2</code>","text":"<p>               Bases: <code>AbstractLDDiscreteDistribution</code></p> <p>Low discrepancy digital net in base 2.</p> Note <ul> <li>Digital net sample sizes should be powers of \\(2\\) e.g. \\(1\\), \\(2\\), \\(4\\), \\(8\\), \\(16\\), \\(\\dots\\).</li> <li>The first point of an unrandomized digital nets is the origin.</li> <li><code>Sobol</code> is an alias for <code>DigitalNetB2</code>.</li> <li> <p>To use higher order digital nets, either:</p> <ul> <li>Pass in <code>generating_matrices</code> without interlacing and supply <code>alpha</code>&gt;1 to apply interlacing, or </li> <li>Pass in <code>generating_matrices</code> with interlacing and set <code>alpha=1</code> to avoid additional interlacing  </li> </ul> <p>i.e. do not pass in interlaced <code>generating_matrices</code> and set <code>alpha&gt;1</code>, this will apply additional interlacing. </p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; discrete_distrib = DigitalNetB2(2,seed=7)\n&gt;&gt;&gt; discrete_distrib(4)\narray([[0.72162356, 0.914955  ],\n       [0.16345554, 0.42964856],\n       [0.98676255, 0.03436384],\n       [0.42956655, 0.55876342]])\n&gt;&gt;&gt; discrete_distrib(1) # first point in the sequence\narray([[0.72162356, 0.914955  ]])\n&gt;&gt;&gt; discrete_distrib\nDigitalNetB2 (AbstractLDDiscreteDistribution)\n    d               2^(1)\n    replications    1\n    randomize       LMS DS\n    gen_mats_source joe_kuo.6.21201.txt\n    order           RADICAL INVERSE\n    t               63\n    alpha           1\n    n_limit         2^(32)\n    entropy         7\n</code></pre> <p>Replications of independent randomizations</p> <pre><code>&gt;&gt;&gt; x = DigitalNetB2(dimension=3,seed=7,replications=2)(4)\n&gt;&gt;&gt; x.shape\n(2, 4, 3)\n&gt;&gt;&gt; x\narray([[[0.24653277, 0.1821862 , 0.74732591],\n        [0.68152903, 0.66169442, 0.42891961],\n        [0.48139855, 0.79818233, 0.08201287],\n        [0.91541325, 0.29520621, 0.77495809]],\n\n       [[0.44876891, 0.85899604, 0.50549679],\n        [0.53635924, 0.04353443, 0.33564946],\n        [0.23214143, 0.29281506, 0.06841036],\n        [0.75295715, 0.60241448, 0.76962976]]])\n</code></pre> <p>Different orderings (avoid warnings that the first point is the origin)</p> <pre><code>&gt;&gt;&gt; DigitalNetB2(dimension=2,randomize=False,order=\"GRAY\")(n_min=2,n_max=4,warn=False)\narray([[0.75, 0.25],\n       [0.25, 0.75]])\n&gt;&gt;&gt; DigitalNetB2(dimension=2,randomize=False,order=\"RADICAL INVERSE\")(n_min=2,n_max=4,warn=False)\narray([[0.25, 0.75],\n       [0.75, 0.25]])\n</code></pre> <p>Generating matrices from https://github.com/QMCSoftware/LDData/tree/main/dnet</p> <pre><code>&gt;&gt;&gt; DigitalNetB2(dimension=3,randomize=False,generating_matrices=\"mps.nx_s5_alpha2_m32.txt\")(8,warn=False)\narray([[0.        , 0.        , 0.        ],\n       [0.75841841, 0.45284834, 0.48844557],\n       [0.57679828, 0.13226272, 0.10061957],\n       [0.31858402, 0.32113875, 0.39369111],\n       [0.90278927, 0.45867532, 0.01803333],\n       [0.14542431, 0.02548793, 0.4749614 ],\n       [0.45587539, 0.33081476, 0.11474426],\n       [0.71318879, 0.15377192, 0.37629925]])\n</code></pre> <p>All randomizations </p> <pre><code>&gt;&gt;&gt; DigitalNetB2(dimension=3,randomize='LMS DS',seed=5)(8)\narray([[0.69346401, 0.20118185, 0.64779396],\n       [0.43998032, 0.90102467, 0.0936172 ],\n       [0.86663563, 0.60910036, 0.26043276],\n       [0.11327376, 0.30772653, 0.93959283],\n       [0.62102883, 0.79169756, 0.77051637],\n       [0.37451038, 0.1231324 , 0.46634012],\n       [0.94785596, 0.38577413, 0.13377215],\n       [0.20121617, 0.71843325, 0.56293458]])\n&gt;&gt;&gt; DigitalNetB2(dimension=3,randomize='LMS',seed=5)(8,warn=False)\narray([[0.        , 0.        , 0.        ],\n       [0.75446077, 0.83265937, 0.69584079],\n       [0.42329494, 0.65793842, 0.90427279],\n       [0.67763292, 0.48937304, 0.33344964],\n       [0.18550714, 0.97332905, 0.3772791 ],\n       [0.93104851, 0.17195496, 0.82311652],\n       [0.26221346, 0.31742386, 0.53093284],\n       [0.50787715, 0.5172669 , 0.2101083 ]])\n&gt;&gt;&gt; DigitalNetB2(dimension=3,randomize='DS',seed=5)(8)\narray([[0.68383949, 0.04047995, 0.42903182],\n       [0.18383949, 0.54047995, 0.92903182],\n       [0.93383949, 0.79047995, 0.67903182],\n       [0.43383949, 0.29047995, 0.17903182],\n       [0.55883949, 0.66547995, 0.05403182],\n       [0.05883949, 0.16547995, 0.55403182],\n       [0.80883949, 0.41547995, 0.80403182],\n       [0.30883949, 0.91547995, 0.30403182]])\n&gt;&gt;&gt; DigitalNetB2(dimension=3,randomize='OWEN',seed=5)(8)\narray([[0.33595486, 0.05834975, 0.30066401],\n       [0.89110875, 0.84905188, 0.81833285],\n       [0.06846074, 0.59997956, 0.67064205],\n       [0.6693703 , 0.25824002, 0.10469644],\n       [0.44586618, 0.99161977, 0.1873488 ],\n       [0.84245267, 0.16445553, 0.56544372],\n       [0.18546359, 0.44859876, 0.97389524],\n       [0.61215442, 0.64341386, 0.44529863]])\n</code></pre> <p>Higher order net without randomization </p> <pre><code>&gt;&gt;&gt; DigitalNetB2(dimension=3,randomize='FALSE',seed=7,alpha=2)(4,warn=False)\narray([[0.    , 0.    , 0.    ],\n       [0.75  , 0.75  , 0.75  ],\n       [0.4375, 0.9375, 0.1875],\n       [0.6875, 0.1875, 0.9375]])\n</code></pre> <p>Higher order nets with randomizations and replications </p> <pre><code>&gt;&gt;&gt; DigitalNetB2(dimension=3,randomize='LMS DS',seed=7,replications=2,alpha=2)(4,warn=False)\narray([[[0.42955149, 0.89149058, 0.43867111],\n        [0.68701828, 0.07601148, 0.51312447],\n        [0.10088033, 0.16293661, 0.25144138],\n        [0.85846252, 0.87103178, 0.70041789]],\n\n       [[0.27151905, 0.42406763, 0.21917369],\n        [0.55035224, 0.67864387, 0.90033876],\n        [0.19356758, 0.57589964, 0.00347701],\n        [0.97235125, 0.32168581, 0.86920948]]])\n&gt;&gt;&gt; DigitalNetB2(dimension=3,randomize='LMS',seed=7,replications=2,alpha=2)(4,warn=False)\narray([[[0.        , 0.        , 0.        ],\n        [0.75817062, 0.96603053, 0.94947625],\n        [0.45367986, 0.80295638, 0.18778553],\n        [0.71171791, 0.2295424 , 0.76175441]],\n\n       [[0.        , 0.        , 0.        ],\n        [0.78664636, 0.75470215, 0.86876474],\n        [0.45336727, 0.99953621, 0.22253579],\n        [0.73996397, 0.24544824, 0.9008679 ]]])\n&gt;&gt;&gt; DigitalNetB2(dimension=3,randomize='DS',seed=7,replications=2,alpha=2)(4)\narray([[[0.04386058, 0.58727432, 0.3691824 ],\n        [0.79386058, 0.33727432, 0.6191824 ],\n        [0.48136058, 0.39977432, 0.4316824 ],\n        [0.73136058, 0.64977432, 0.6816824 ]],\n\n       [[0.65212985, 0.69669968, 0.10605352],\n        [0.40212985, 0.44669968, 0.85605352],\n        [0.83962985, 0.25919968, 0.16855352],\n        [0.08962985, 0.50919968, 0.91855352]]])\n&gt;&gt;&gt; DigitalNetB2(dimension=3,randomize='OWEN',seed=7,replications=2,alpha=2)(4)\narray([[[0.46368517, 0.03964427, 0.62172094],\n        [0.7498683 , 0.76141348, 0.4243043 ],\n        [0.01729754, 0.97968459, 0.65963223],\n        [0.75365329, 0.1903774 , 0.34141493]],\n\n       [[0.52252547, 0.5679709 , 0.05949112],\n        [0.27248656, 0.36488289, 0.81844058],\n        [0.94219959, 0.39172304, 0.20285965],\n        [0.19716391, 0.64741585, 0.92494554]]])\n</code></pre> <p>References:</p> <ol> <li> <p>Marius Hofert and Christiane Lemieux.     qrng: (Randomized) Quasi-Random Number Generators (2019).     R package version 0.0-7. https://CRAN.R-project.org/package=qrng.</p> </li> <li> <p>Faure, Henri, and Christiane Lemieux.     Implementation of Irreducible Sobol' Sequences in Prime Power Bases.     Mathematics and Computers in Simulation 161 (2019): 13-22. Crossref. Web.</p> </li> <li> <p>F.Y. Kuo, D. Nuyens.     Application of quasi-Monte Carlo methods to elliptic PDEs with random diffusion coefficients - a survey of analysis and implementation.     Foundations of Computational Mathematics, 16(6):1631-1696, 2016. https://link.springer.com/article/10.1007/s10208-016-9329-5. </p> </li> <li> <p>D. Nuyens.     The Magic Point Shop of QMC point generators and generating vectors.     MATLAB and Python software, 2018. https://people.cs.kuleuven.be/~dirk.nuyens/.</p> </li> <li> <p>R. Cools, F.Y. Kuo, D. Nuyens.     Constructing embedded lattice rules for multivariate integration.     SIAM J. Sci. Comput., 28(6), 2162-2188.</p> </li> <li> <p>I.M. Sobol', V.I. Turchaninov, Yu.L. Levitan, B.V. Shukhman.     Quasi-Random Sequence Generators.     Keldysh Institute of Applied Mathematics.     Russian Academy of Sciences, Moscow (1992).</p> </li> <li> <p>Sobol, Ilya &amp; Asotsky, Danil &amp; Kreinin, Alexander &amp; Kucherenko, Sergei. (2011).     Construction and Comparison of High-Dimensional Sobol' Generators. Wilmott. 2011. 10.1002/wilm.10056.</p> </li> <li> <p>Paul Bratley and Bennett L. Fox.     Algorithm 659: Implementing Sobol's quasirandom sequence generator.     ACM Trans. Math. Softw. 14, 1 (March 1988), 88-100. 1988. https://doi.org/10.1145/42288.214372.</p> </li> </ol> <p>Parameters:</p> Name Type Description Default <code>dimension</code> <code>Union[int, ndarray]</code> <p>Dimension of the generator.</p> <ul> <li>If an <code>int</code> is passed in, use generating vector components at indices 0,...,<code>dimension</code>-1.</li> <li>If an <code>np.ndarray</code> is passed in, use generating vector components at these indices.</li> </ul> <code>1</code> <code>replications</code> <code>int</code> <p>Number of independent randomizations of a pointset.</p> <code>None</code> <code>seed</code> <code>Union[None,int,np.random.SeedSeq</code> <p>Seed the random number generator for reproducibility.</p> <code>None</code> <code>randomize</code> <code>str</code> <p>Options are</p> <ul> <li><code>'LMS DS'</code>: Linear matrix scramble with digital shift.</li> <li><code>'LMS'</code>: Linear matrix scramble only.</li> <li><code>'DS'</code>: Digital shift only.</li> <li><code>'NUS'</code>: Nested uniform scrambling. Also known as Owen scrambling. </li> <li><code>'FALSE'</code>: No randomization. In this case the first point will be the origin. </li> </ul> <code>'LMS DS'</code> <code>generating_matrices</code> <code>Union[str, ndarray, int]</code> <p>Specify the generating matrices.</p> <ul> <li>A <code>str</code> should be the name (or path) of a file from the LDData repo at https://github.com/QMCSoftware/LDData/tree/main/dnet.</li> <li>An <code>np.ndarray</code> of integers with shape \\((d,m_\\mathrm{max})\\) or \\((r,d,m_\\mathrm{max})\\) where \\(d\\) is the number of dimensions, \\(r\\) is the number of replications, and \\(2^{m_\\mathrm{max}}\\) is the maximum number of supported points. Setting <code>msb=False</code> will flip the bits of ints in the generating matrices.</li> </ul> <code>'joe_kuo.6.21201.txt'</code> <code>order</code> <code>str</code> <p><code>'RADICAL INVERSE'</code>, or <code>'GRAY'</code> ordering. See the doctest example above.</p> <code>'RADICAL INVERSE'</code> <code>t</code> <code>int</code> <p>Number of bits in integer represetation of points after randomization. The number of bits in the generating matrices is inferred based on the largest value.</p> <code>63</code> <code>alpha</code> <code>int</code> <p>Interlacing factor for higher order nets. When <code>alpha</code>&gt;1, interlacing is performed regardless of the generating matrices, i.e., for <code>alpha</code>&gt;1 do not pass in generating matrices which are already interlaced. The Note for this class contains more info.  </p> <code>1</code> <code>msb</code> <code>bool</code> <p>Flag for Most Significant Bit (MSB) vs Least Significant Bit (LSB) integer representations in generating matrices. If <code>msb=False</code> (LSB order), then integers in generating matrices will be bit-reversed. </p> <code>None</code> <code>_verbose</code> <code>bool</code> <p>If <code>True</code>, print linear matrix scrambling matrices.</p> <code>False</code> Source code in <code>qmcpy/discrete_distribution/digital_net_b2/digital_net_b2.py</code> <pre><code>def __init__(self,\n             dimension = 1,\n             replications = None,\n             seed = None,\n             randomize = 'LMS DS',\n             generating_matrices = \"joe_kuo.6.21201.txt\",\n             order = 'RADICAL INVERSE',\n             t = 63,\n             alpha = 1,\n             msb = None,\n             _verbose = False,\n             # deprecated\n             graycode = None,\n             t_max = None,\n             t_lms = None):\n    r\"\"\"\n    Args:\n        dimension (Union[int,np.ndarray]): Dimension of the generator.\n\n            - If an `int` is passed in, use generating vector components at indices 0,...,`dimension`-1.\n            - If an `np.ndarray` is passed in, use generating vector components at these indices.\n\n        replications (int): Number of independent randomizations of a pointset.\n        seed (Union[None,int,np.random.SeedSeq): Seed the random number generator for reproducibility.\n        randomize (str): Options are\n\n            - `'LMS DS'`: Linear matrix scramble with digital shift.\n            - `'LMS'`: Linear matrix scramble only.\n            - `'DS'`: Digital shift only.\n            - `'NUS'`: Nested uniform scrambling. Also known as Owen scrambling. \n            - `'FALSE'`: No randomization. In this case the first point will be the origin. \n\n        generating_matrices (Union[str,np.ndarray,int]: Specify the generating matrices.\n\n            - A `str` should be the name (or path) of a file from the LDData repo at [https://github.com/QMCSoftware/LDData/tree/main/dnet](https://github.com/QMCSoftware/LDData/tree/main/dnet).\n            - An `np.ndarray` of integers with shape $(d,m_\\mathrm{max})$ or $(r,d,m_\\mathrm{max})$ where $d$ is the number of dimensions, $r$ is the number of replications, and $2^{m_\\mathrm{max}}$ is the maximum number of supported points. Setting `msb=False` will flip the bits of ints in the generating matrices.\n\n        order (str): `'RADICAL INVERSE'`, or `'GRAY'` ordering. See the doctest example above.\n        t (int): Number of bits in integer represetation of points *after* randomization. The number of bits in the generating matrices is inferred based on the largest value.\n        alpha (int): Interlacing factor for higher order nets.  \n            When `alpha`&gt;1, interlacing is performed regardless of the generating matrices,  \n            i.e., for `alpha`&gt;1 do *not* pass in generating matrices which are already interlaced.  \n            The Note for this class contains more info.  \n        msb (bool): Flag for Most Significant Bit (MSB) vs Least Significant Bit (LSB) integer representations in generating matrices. If `msb=False` (LSB order), then integers in generating matrices will be bit-reversed. \n        _verbose (bool): If `True`, print linear matrix scrambling matrices. \n    \"\"\"\n    if graycode is not None:\n        order = 'GRAY' if graycode else 'RADICAL INVERSE'\n        warnings.warn(\"graycode argument deprecated, set order='GRAY' or order='RADICAL INVERSE' instead. Using order='%s'\"%order,ParameterWarning)\n    if t_lms is not None:\n        t = t_lms\n        warnings.warn(\"t_lms argument deprecated. Set t instead. Using t = %d\"%t,ParameterWarning)\n    if t_max is not None: \n        warnings.warn(\"t_max is deprecated as it can be inferred from the generating matrices. Set t to change the number of bits after randomization.\",ParameterWarning)\n    self.parameters = ['randomize','gen_mats_source','order','t','alpha','n_limit']\n    self.input_generating_matrices = deepcopy(generating_matrices)\n    self.input_t = deepcopy(t) \n    self.input_msb = deepcopy(msb)\n    if isinstance(generating_matrices,str) and generating_matrices==\"joe_kuo.6.21201.txt\":\n        self.gen_mats_source = generating_matrices\n        if np.isscalar(dimension) and dimension&lt;=1024 and alpha==1:\n            gen_mats = np.load(dirname(abspath(__file__))+'/generating_matrices/joe_kuo.6.1024.npy')[None,:]\n            d_limit = 1024\n        else:\n            gen_mats = np.load(dirname(abspath(__file__))+'/generating_matrices/joe_kuo.6.21201.npy')[None,:]\n            d_limit = 21201\n        msb = True\n        n_limit = 4294967296\n        self._t_curr = 32\n        compat_shift = self._t_curr-t if self._t_curr&gt;=t else 0\n        if compat_shift&gt;0: warnings.warn(\"Truncating ints in generating matrix to have t = %d bits.\"%t,ParameterWarning)\n        gen_mats = gen_mats&gt;&gt;compat_shift\n    elif isinstance(generating_matrices,str):\n        self.gen_mats_source = generating_matrices\n        assert generating_matrices[-4:]==\".txt\"\n        local_root = dirname(abspath(__file__))+'/generating_matrices/'\n        repos = DataSource()\n        if repos.exists(local_root+generating_matrices):\n            datafile = repos.open(local_root+generating_matrices)\n        elif repos.exists(\"https://raw.githubusercontent.com/QMCSoftware/LDData/refs/heads/main/dnet/\"+generating_matrices):\n            datafile = repos.open(\"https://raw.githubusercontent.com/QMCSoftware/LDData/refs/heads/main/dnet/\"+generating_matrices)\n        elif repos.exists(\"https://raw.githubusercontent.com/QMCSoftware/LDData/refs/heads/main/\"+generating_matrices):\n            datafile = repos.open(\"https://raw.githubusercontent.com/QMCSoftware/LDData/refs/heads/main/\"+generating_matrices)\n        elif repos.exists(\"https://raw.githubusercontent.com/QMCSoftware/LDData/refs/heads/main/dnet/\"+generating_matrices[7:]):\n            datafile = repos.open(\"https://raw.githubusercontent.com/QMCSoftware/LDData/refs/heads/main/dnet/\"+generating_matrices[7:])\n        elif repos.exists(\"https://raw.githubusercontent.com/QMCSoftware/\"+generating_matrices):\n            datafile = repos.open(\"https://raw.githubusercontent.com/QMCSoftware/\"+generating_matrices)\n        elif repos.exists(generating_matrices):\n            datafile = repos.open(generating_matrices)\n        else:\n            raise ParameterError(\"LDData path %s not found\"%generating_matrices)\n        contents = [line.rstrip('\\n').strip() for line in datafile.readlines()]\n        contents = [line.split(\"#\",1)[0] for line in contents if line[0]!=\"#\"]\n        datafile.close()\n        msb = True\n        assert int(contents[0])==2, \"DigitalNetB2 requires base=2 \" # base 2\n        d_limit = int(contents[1])\n        n_limit = int(contents[2])\n        self._t_curr = int(contents[3])\n        compat_shift = self._t_curr-t if self._t_curr&gt;=t else 0\n        if compat_shift&gt;0: warnings.warn(\"Truncating ints in generating matrix to have t = %d bits.\"%t,ParameterWarning)\n        gen_mats = np.array([[int(v)&gt;&gt;compat_shift for v in line.split(' ')] for line in contents[4:]],dtype=np.uint64)[None,:]\n    elif isinstance(generating_matrices,np.ndarray):\n        self.gen_mats_source = \"custom\"\n        assert generating_matrices.ndim==2 or generating_matrices.ndim==3\n        gen_mats = generating_matrices[None,:,:] if generating_matrices.ndim==2 else generating_matrices\n        assert isinstance(msb,bool), \"when generating_matrices is a np.ndarray you must set either msb=True (for most significant bit ordering) or msb=False (for least significant bit ordering which will require a bit reversal)\"\n        gen_mat_max = gen_mats.max() \n        assert gen_mat_max&gt;0, \"generating matrix must have positive ints\"\n        self._t_curr = int(np.ceil(np.log2(gen_mat_max+1)))\n        d_limit = gen_mats.shape[1]\n        n_limit = int(2**(gen_mats.shape[2]))\n    else:\n        raise ParameterError(\"invalid generating_matrices, must be a string or np.ndarray.\")\n    super(DigitalNetB2,self).__init__(dimension,replications,seed,d_limit,n_limit)\n    assert gen_mats.ndim==3 and gen_mats.shape[1]&gt;=self.d and (gen_mats.shape[0]==1 or gen_mats.shape[0]==self.replications) and gen_mats.shape[2]&gt;0, \"invalid gen_mats.shape = %s\"%str(gen_mats.shape)\n    self.m_max = int(gen_mats.shape[-1])\n    if isinstance(generating_matrices,np.ndarray) and msb:\n        qmctoolscl.dnb2_gmat_lsb_to_msb(np.uint64(gen_mats.shape[0]),np.uint64(self.d),np.uint64(self.m_max),np.tile(np.uint64(self._t_curr),int(gen_mats.shape[0])),gen_mats,gen_mats,backend=\"c\")\n    self.order = str(order).upper().strip().replace(\"_\",\" \")\n    if self.order==\"GRAY CODE\": self.order = \"GRAY\"\n    if self.order==\"NATURAL\": self.order = \"RADICAL INVERSE\"\n    assert self.order in ['RADICAL INVERSE','GRAY']\n    assert isinstance(t,int) and t&gt;0\n    assert self._t_curr&lt;=t&lt;=64, \"t must no more than 64 and no less than %d (the number of bits used to represent the generating matrices)\"%(self._t_curr)\n    assert isinstance(alpha,int) and alpha&gt;0\n    self.alpha = alpha\n    if self.alpha&gt;1:\n        assert (self.dvec==np.arange(self.d)).all(), \"digital interlacing requires dimension is an int\"\n        if self.m_max!=self._t_curr:\n            warnings.warn(\"Digital interlacing is often performed on matrices with the number of columns (m_max = %d) equal to the number of bits in each int (%d), but this is not the case. Ensure you are NOT setting alpha&gt;1 when generating matrices are already interlaced.\"%(self.m_max,self._t_curr),ParameterWarning)\n    self._verbose = _verbose\n    self.randomize = str(randomize).upper().strip().replace(\"_\",\" \")\n    if self.randomize==\"TRUE\": self.randomize = \"LMS DS\"\n    if self.randomize==\"OWEN\": self.randomize = \"NUS\"\n    if self.randomize==\"NONE\": self.randomize = \"FALSE\"\n    if self.randomize==\"NO\": self.randomize = \"FALSE\"\n    assert self.randomize in [\"LMS DS\",\"LMS\",\"DS\",\"NUS\",\"FALSE\"]\n    self.dtalpha = self.alpha*self.d\n    if self.randomize==\"FALSE\":\n        if self.alpha==1:\n            self.gen_mats = gen_mats[:,self.dvec,:]\n            self.t = self._t_curr\n        else: \n            t_alpha = min(self.alpha*self._t_curr,t)\n            gen_mat_ho = np.empty((gen_mats.shape[0],self.d,self.m_max),dtype=np.uint64)\n            qmctoolscl.dnb2_interlace(np.uint64(gen_mats.shape[0]),np.uint64(self.d),np.uint64(self.m_max),np.uint64(self.dtalpha),np.uint64(self._t_curr),np.uint64(t_alpha),np.uint64(self.alpha),gen_mats[:,:self.dtalpha,:].copy(),gen_mat_ho,backend=\"c\")\n            self.gen_mats = gen_mat_ho\n            self._t_curr = t_alpha\n            self.t = self._t_curr\n    elif self.randomize==\"DS\":\n        if self.alpha==1:\n            self.gen_mats = gen_mats[:,self.dvec,:]\n            self.t = t\n        else: \n            t_alpha = min(self.alpha*self._t_curr,t)\n            gen_mat_ho = np.empty((gen_mats.shape[0],self.d,self.m_max),dtype=np.uint64)\n            qmctoolscl.dnb2_interlace(np.uint64(gen_mats.shape[0]),np.uint64(self.d),np.uint64(self.m_max),np.uint64(self.dtalpha),np.uint64(self._t_curr),np.uint64(t_alpha),np.uint64(self.alpha),gen_mats[:,:self.dtalpha,:].copy(),gen_mat_ho,backend=\"c\")\n            self.gen_mats = gen_mat_ho\n            self._t_curr = t_alpha\n            self.t = t\n        self.rshift = qmctoolscl.random_tbit_uint64s(self.rng,self.t,(self.replications,self.d))\n    elif self.randomize in [\"LMS\",\"LMS DS\"]:\n        if self.alpha==1:\n            gen_mat_lms = np.empty((self.replications,self.d,self.m_max),dtype=np.uint64)\n            S = qmctoolscl.dnb2_get_linear_scramble_matrix(self.rng,np.uint64(self.replications),np.uint64(self.d),np.uint64(self._t_curr),np.uint64(t),np.uint64(self._verbose))\n            qmctoolscl.dnb2_linear_matrix_scramble(np.uint64(self.replications),np.uint64(self.d),np.uint64(self.m_max),np.uint64(gen_mats.shape[0]),np.uint64(t),S,gen_mats[:,self.dvec,:].copy(),gen_mat_lms,backend=\"c\")\n            self.gen_mats = gen_mat_lms\n            self._t_curr = t\n            self.t = self._t_curr\n        else:\n            gen_mat_lms = np.empty((self.replications,self.dtalpha,self.m_max),dtype=np.uint64)\n            S = qmctoolscl.dnb2_get_linear_scramble_matrix(self.rng,np.uint64(self.replications),np.uint64(self.dtalpha),np.uint64(self._t_curr),np.uint64(t),np.uint64(self._verbose))\n            qmctoolscl.dnb2_linear_matrix_scramble(np.uint64(self.replications),np.uint64(self.dtalpha),np.uint64(self.m_max),np.uint64(gen_mats.shape[0]),np.uint64(t),S,gen_mats[:,:self.dtalpha,:].copy(),gen_mat_lms,backend=\"c\")\n            gen_mat_lms_ho = np.empty((self.replications,self.d,self.m_max),dtype=np.uint64)\n            qmctoolscl.dnb2_interlace(np.uint64(self.replications),np.uint64(self.d),np.uint64(self.m_max),np.uint64(self.dtalpha),np.uint64(t),np.uint64(t),np.uint64(self.alpha),gen_mat_lms,gen_mat_lms_ho,backend=\"c\")\n            self.gen_mats = gen_mat_lms_ho\n            self._t_curr = t\n            self.t = self._t_curr\n        if self.randomize==\"LMS DS\":\n            self.rshift = qmctoolscl.random_tbit_uint64s(self.rng,self.t,(self.replications,self.d))\n    elif self.randomize==\"NUS\":\n        if alpha==1:\n            new_seeds = self._base_seed.spawn(self.replications*self.d)\n            self.rngs = np.array([np.random.Generator(np.random.SFC64(new_seeds[j])) for j in range(self.replications*self.d)]).reshape(self.replications,self.d)\n            self.root_nodes = np.array([qmctoolscl.NUSNode_dnb2() for i in range(self.replications*self.d)]).reshape(self.replications,self.d)\n            self.gen_mats = gen_mats[:,self.dvec,:].copy()\n            self.t = t\n        else:\n            self.dtalpha = self.alpha*self.d\n            new_seeds = self._base_seed.spawn(self.replications*self.dtalpha)\n            self.rngs = np.array([np.random.Generator(np.random.SFC64(new_seeds[j])) for j in range(self.replications*self.dtalpha)]).reshape(self.replications,self.dtalpha)\n            self.root_nodes = np.array([qmctoolscl.NUSNode_dnb2() for i in range(self.replications*self.dtalpha)]).reshape(self.replications,self.dtalpha)\n            self.gen_mats = gen_mats[:,:self.dtalpha,:].copy()\n            self.t = t\n    else:\n        raise ParameterError(\"self.randomize parsing error\")\n    self.gen_mats = np.ascontiguousarray(self.gen_mats)\n    gen_mat_max = self.gen_mats.max() \n    assert gen_mat_max&gt;0, \"generating matrix must have positive ints\"\n    assert self._t_curr==int(np.ceil(np.log2(gen_mat_max+1)))\n    assert 0&lt;self._t_curr&lt;=self.t&lt;=64, \"invalid 0 &lt;= self._t_curr (%d) &lt;= self.t (%d) &lt;= 64\"%(self._t_curr,self.t)\n    if self.randomize==\"FALSE\": assert self.gen_mats.shape[0]==self.replications, \"randomize='FALSE' but replications = %d does not equal the number of sets of generating matrices %d\"%(self.replications,self.gen_mats.shape[0])\n</code></pre>"},{"location":"api/discrete_distributions/#lattice","title":"<code>Lattice</code>","text":"<p>               Bases: <code>AbstractLDDiscreteDistribution</code></p> <p>Low discrepancy lattice sequence.</p> Note <ul> <li>Lattice sample sizes should be powers of \\(2\\) e.g. \\(1\\), \\(2\\), \\(4\\), \\(8\\), \\(16\\), \\(\\dots\\).</li> <li>The first point of an unrandomized lattice is the origin.</li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; discrete_distrib = Lattice(2,seed=7)\n&gt;&gt;&gt; discrete_distrib(4)\narray([[0.04386058, 0.58727432],\n       [0.54386058, 0.08727432],\n       [0.29386058, 0.33727432],\n       [0.79386058, 0.83727432]])\n&gt;&gt;&gt; discrete_distrib(1) # first point in the sequence\narray([[0.04386058, 0.58727432]])\n&gt;&gt;&gt; discrete_distrib\nLattice (AbstractLDDiscreteDistribution)\n    d               2^(1)\n    replications    1\n    randomize       SHIFT\n    gen_vec_source  kuo.lattice-33002-1024-1048576.9125.txt\n    order           RADICAL INVERSE\n    n_limit         2^(20)\n    entropy         7\n</code></pre> <p>Replications of independent randomizations</p> <pre><code>&gt;&gt;&gt; x = Lattice(3,seed=7,replications=2)(4)\n&gt;&gt;&gt; x.shape\n(2, 4, 3)\n&gt;&gt;&gt; x\narray([[[0.04386058, 0.58727432, 0.3691824 ],\n        [0.54386058, 0.08727432, 0.8691824 ],\n        [0.29386058, 0.33727432, 0.1191824 ],\n        [0.79386058, 0.83727432, 0.6191824 ]],\n\n       [[0.65212985, 0.69669968, 0.10605352],\n        [0.15212985, 0.19669968, 0.60605352],\n        [0.90212985, 0.44669968, 0.85605352],\n        [0.40212985, 0.94669968, 0.35605352]]])\n</code></pre> <p>Different orderings (avoid warnings that the first point is the origin).</p> <pre><code>&gt;&gt;&gt; Lattice(dimension=2,randomize=False,order='RADICAL INVERSE')(4,warn=False) \narray([[0.  , 0.  ],\n       [0.5 , 0.5 ],\n       [0.25, 0.75],\n       [0.75, 0.25]])\n&gt;&gt;&gt; Lattice(dimension=2,randomize=False,order='GRAY')(4,warn=False)\narray([[0.  , 0.  ],\n       [0.5 , 0.5 ],\n       [0.75, 0.25],\n       [0.25, 0.75]])\n&gt;&gt;&gt; Lattice(dimension=2,randomize=False,order='LINEAR')(4,warn=False)\narray([[0.  , 0.  ],\n       [0.25, 0.75],\n       [0.5 , 0.5 ],\n       [0.75, 0.25]])\n</code></pre> <p>Generating vector from https://github.com/QMCSoftware/LDData/tree/main/lattice</p> <pre><code>&gt;&gt;&gt; Lattice(dimension=3,randomize=False,generating_vector=\"mps.exod2_base2_m20_CKN.txt\")(8,warn=False)\narray([[0.   , 0.   , 0.   ],\n       [0.5  , 0.5  , 0.5  ],\n       [0.25 , 0.75 , 0.75 ],\n       [0.75 , 0.25 , 0.25 ],\n       [0.125, 0.375, 0.375],\n       [0.625, 0.875, 0.875],\n       [0.375, 0.125, 0.125],\n       [0.875, 0.625, 0.625]])\n</code></pre> <p>Random generating vector supporting \\(2^{25}\\) points </p> <pre><code>&gt;&gt;&gt; discrete_distrib = Lattice(3,generating_vector=25,seed=55,randomize=False)\n&gt;&gt;&gt; discrete_distrib.gen_vec\narray([[       1, 11961679, 12107519]], dtype=uint64)\n&gt;&gt;&gt; discrete_distrib(4,warn=False)\narray([[0.  , 0.  , 0.  ],\n       [0.5 , 0.5 , 0.5 ],\n       [0.25, 0.75, 0.75],\n       [0.75, 0.25, 0.25]])\n</code></pre> <p>Two random generating vectors both supporting \\(2^{25}\\) points along with independent random shifts</p> <pre><code>&gt;&gt;&gt; discrete_distrib = Lattice(3,seed=7,generating_vector=25,replications=2)\n&gt;&gt;&gt; discrete_distrib.gen_vec\narray([[       1, 32809149,  1471719],\n       [       1,   275319, 19705657]], dtype=uint64)\n&gt;&gt;&gt; discrete_distrib(4)\narray([[[0.3691824 , 0.65212985, 0.69669968],\n        [0.8691824 , 0.15212985, 0.19669968],\n        [0.6191824 , 0.90212985, 0.44669968],\n        [0.1191824 , 0.40212985, 0.94669968]],\n\n       [[0.10605352, 0.63025643, 0.13630282],\n        [0.60605352, 0.13025643, 0.63630282],\n        [0.35605352, 0.38025643, 0.38630282],\n        [0.85605352, 0.88025643, 0.88630282]]])\n</code></pre> <p>References</p> <ol> <li> <p>Sou-Cheng T. Choi, Yuhan Ding, Fred J. Hickernell, Lan Jiang, Lluis Antoni Jimenez Rugama, Da Li, Jagadeeswaran Rathinavel, Xin Tong, Kan Zhang, Yizhi Zhang, and Xuan Zhou.     GAIL: Guaranteed Automatic Integration Library (Version 2.3), MATLAB Software, 2019. http://gailgithub.github.io/GAIL_Dev/.  </p> </li> <li> <p>F.Y. Kuo, D. Nuyens.     Application of quasi-Monte Carlo methods to elliptic PDEs with random diffusion coefficients - a survey of analysis and implementation.     Foundations of Computational Mathematics, 16(6):1631-1696, 2016. https://link.springer.com/article/10.1007/s10208-016-9329-5.  </p> </li> <li> <p>D. Nuyens.     The Magic Point Shop of QMC point generators and generating vectors.     MATLAB and Python software, 2018. https://people.cs.kuleuven.be/~dirk.nuyens/.</p> </li> <li> <p>R. Cools, F.Y. Kuo, D. Nuyens.     Constructing embedded lattice rules for multivariate integration.     SIAM J. Sci. Comput., 28(6), 2162-2188.</p> </li> <li> <p>P. L'Ecuyer, D. Munger.     LatticeBuilder: A General Software Tool for Constructing Rank-1 Lattice Rules.     ACM Transactions on Mathematical Software. 42. (2015). 10.1145/2754929.</p> </li> </ol> <p>Parameters:</p> Name Type Description Default <code>dimension</code> <code>Union[int, ndarray]</code> <p>Dimension of the generator.</p> <ul> <li>If an <code>int</code> is passed in, use generating vector components at indices 0,...,<code>dimension</code>-1.</li> <li>If an <code>np.ndarray</code> is passed in, use generating vector components at these indices.</li> </ul> <code>1</code> <code>replications</code> <code>int</code> <p>Number of independent randomizations.</p> <code>None</code> <code>seed</code> <code>Union[None,int,np.random.SeedSeq</code> <p>Seed the random number generator for reproducibility.</p> <code>None</code> <code>randomize</code> <code>str</code> <p>Options are </p> <ul> <li><code>'SHIFT'</code>: Random shift.</li> <li><code>'FALSE'</code>: No randomization. In this case the first point will be the origin. </li> </ul> <code>'SHIFT'</code> <code>generating_vector</code> <code>Union[str, ndarray, int]</code> <p>Specify the generating vector.</p> <ul> <li>A <code>str</code> should be the name (or path) of a file from the LDData repo at https://github.com/QMCSoftware/LDData/tree/main/lattice.</li> <li>A <code>np.ndarray</code> of integers with shape \\((d,)\\) or \\((r,d)\\) where \\(d\\) is the number of dimensions and \\(r\\) is the number of replications.     Must supply <code>m_max</code> where \\(2^{m_\\mathrm{max}}\\) is the max number of supported samples. </li> <li>An <code>int</code>, call it \\(M\\),  gives the random generating vector \\((1,v_1,\\dots,v_{d-1})^T\\)  where \\(d\\) is the dimension and \\(v_i\\) are randomly selected from \\(\\{3,5,\\dots,2^M-1\\}\\) uniformly and independently. We require require \\(1 &lt; M &lt; 27\\). </li> </ul> <code>'kuo.lattice-33002-1024-1048576.9125.txt'</code> <code>order</code> <code>str</code> <p><code>'LINEAR'</code>, <code>'RADICAL INVERSE'</code>, or <code>'GRAY'</code> ordering. See the doctest example above.</p> <code>'RADICAL INVERSE'</code> <code>m_max</code> <code>int</code> <p>\\(2^{m_\\mathrm{max}}\\) is the maximum number of supported samples.</p> <code>None</code> Source code in <code>qmcpy/discrete_distribution/lattice/lattice.py</code> <pre><code>def __init__(self,\n             dimension = 1,\n             replications = None,\n             seed = None,\n             randomize = 'SHIFT',\n             generating_vector = \"kuo.lattice-33002-1024-1048576.9125.txt\",\n             order = 'RADICAL INVERSE',\n             m_max = None):\n    r\"\"\"\n    Args:\n        dimension (Union[int,np.ndarray]): Dimension of the generator.\n\n            - If an `int` is passed in, use generating vector components at indices 0,...,`dimension`-1.\n            - If an `np.ndarray` is passed in, use generating vector components at these indices.\n\n        replications (int): Number of independent randomizations.\n        seed (Union[None,int,np.random.SeedSeq): Seed the random number generator for reproducibility.\n        randomize (str): Options are \n\n            - `'SHIFT'`: Random shift.\n            - `'FALSE'`: No randomization. In this case the first point will be the origin. \n\n        generating_vector (Union[str,np.ndarray,int]: Specify the generating vector.\n\n            - A `str` should be the name (or path) of a file from the LDData repo at [https://github.com/QMCSoftware/LDData/tree/main/lattice](https://github.com/QMCSoftware/LDData/tree/main/lattice).\n            - A `np.ndarray` of integers with shape $(d,)$ or $(r,d)$ where $d$ is the number of dimensions and $r$ is the number of replications.  \n                Must supply `m_max` where $2^{m_\\mathrm{max}}$ is the max number of supported samples. \n            - An `int`, call it $M$, \n            gives the random generating vector $(1,v_1,\\dots,v_{d-1})^T$ \n            where $d$ is the dimension and $v_i$ are randomly selected from $\\{3,5,\\dots,2^M-1\\}$ uniformly and independently.  \n            We require require $1 &lt; M &lt; 27$. \n\n        order (str): `'LINEAR'`, `'RADICAL INVERSE'`, or `'GRAY'` ordering. See the doctest example above.\n        m_max (int): $2^{m_\\mathrm{max}}$ is the maximum number of supported samples.\n    \"\"\"\n    self.parameters = ['randomize','gen_vec_source','order','n_limit']\n    self.input_generating_vector = deepcopy(generating_vector)\n    self.input_m_max = deepcopy(m_max)\n    if isinstance(generating_vector,str) and generating_vector==\"kuo.lattice-33002-1024-1048576.9125.txt\":\n        self.gen_vec_source = generating_vector\n        gen_vec = np.load(dirname(abspath(__file__))+'/generating_vectors/kuo.lattice-33002-1024-1048576.9125.npy')[None,:]\n        d_limit = 9125\n        n_limit = 1048576\n    elif isinstance(generating_vector,str):\n        self.gen_vec_source = generating_vector\n        assert generating_vector[-4:]==\".txt\"\n        local_root = dirname(abspath(__file__))+'/generating_vectors/'\n        repos = DataSource()\n        if repos.exists(local_root+generating_vector):\n            datafile = repos.open(local_root+generating_vector)\n        elif repos.exists(\"https://raw.githubusercontent.com/QMCSoftware/LDData/refs/heads/main/lattice/\"+generating_vector):\n            datafile = repos.open(\"https://raw.githubusercontent.com/QMCSoftware/LDData/refs/heads/main/lattice/\"+generating_vector)\n        elif repos.exists(\"https://raw.githubusercontent.com/QMCSoftware/LDData/refs/heads/main/\"+generating_vector):\n            datafile = repos.open(\"https://raw.githubusercontent.com/QMCSoftware/LDData/refs/heads/main/\"+generating_vector)\n        elif repos.exists(\"https://raw.githubusercontent.com/QMCSoftware/LDData/refs/heads/main/lattice/\"+generating_vector[7:]):\n            datafile = repos.open(\"https://raw.githubusercontent.com/QMCSoftware/LDData/refs/heads/main/lattice/\"+generating_vector[7:])\n        elif repos.exists(\"https://raw.githubusercontent.com/QMCSoftware/\"+generating_vector):\n            datafile = repos.open(\"https://raw.githubusercontent.com/QMCSoftware/\"+generating_vector)\n        elif repos.exists(generating_vector):\n            datafile = repos.open(generating_vector)\n        else:\n            raise ParameterError(\"LDData path %s not found\"%generating_vector)\n        contents = [int(line.rstrip('\\n').strip().split(\"#\",1)[0]) for line in datafile.readlines() if line[0]!=\"#\"]\n        datafile.close()\n        d_limit = int(contents[0])\n        n_limit = int(contents[1])\n        gen_vec = np.array(contents[2:],dtype=np.uint64)[None,:]\n    elif isinstance(generating_vector,np.ndarray):\n        self.gen_vec_source = \"custom\"\n        gen_vec = generating_vector\n        if m_max is None:\n            raise ParameterError(\"m_max must be supplied when generating_vector is a np.ndarray\")\n        n_limit = int(2**m_max)\n        d_limit = int(gen_vec.shape[-1])\n    elif isinstance(generating_vector,int):\n        assert 1&lt;generating_vector&lt;27, \"int generating vector out of range\"\n        n_limit = 2**generating_vector\n        assert isinstance(dimension,int), \"random generating vector requires int dimension\"\n        d_limit = dimension\n    else:\n        raise ParameterError(\"invalid generating_vector, must be a string, numpy.ndarray, or int\")\n    super(Lattice,self).__init__(dimension,replications,seed,d_limit,n_limit)\n    if isinstance(generating_vector,int):\n        self.gen_vec_source = \"random\"\n        m_max = int(np.log2(self.n_limit))\n        gen_vec = np.hstack([np.ones((self.replications,1),dtype=np.uint64),2*self.rng.integers(1,2**(m_max-1),size=(self.replications,dimension-1),dtype=np.uint64)+1]).copy()\n    assert isinstance(gen_vec,np.ndarray)\n    gen_vec = np.atleast_2d(gen_vec) \n    assert gen_vec.ndim==2 and gen_vec.shape[1]&gt;=self.d and (gen_vec.shape[0]==1 or gen_vec.shape[0]==self.replications), \"invalid gen_vec.shape = %s\"%str(gen_vec.shape)\n    self.gen_vec = gen_vec[:,self.dvec].copy()\n    self.order = str(order).upper().strip().replace(\"_\",\" \")\n    if self.order==\"GRAY CODE\": self.order = \"GRAY\"\n    if self.order==\"NATURAL\": self.order = \"RADICAL INVERSE\"\n    assert self.order in ['LINEAR','RADICAL INVERSE','GRAY']\n    self.randomize = str(randomize).upper()\n    if self.randomize==\"TRUE\": self.randomize = \"SHIFT\"\n    if self.randomize==\"NONE\": self.randomize = \"FALSE\"\n    if self.randomize==\"NO\": self.randomize = \"FALSE\"\n    assert self.randomize in [\"SHIFT\",\"FALSE\"]\n    if self.randomize==\"SHIFT\":\n        self.shift = self.rng.uniform(size=(self.replications,self.d))\n    if self.randomize==\"FALSE\": assert self.gen_vec.shape[0]==self.replications, \"randomize='FALSE' but replications = %d does not equal the number of sets of generating vectors %d\"%(self.replications,self.gen_vec.shape[0])\n</code></pre>"},{"location":"api/discrete_distributions/#halton","title":"<code>Halton</code>","text":"<p>               Bases: <code>AbstractLDDiscreteDistribution</code></p> <p>Low discrepancy Halton points.</p> Note <ul> <li>The first point of an unrandomized Halton sequence is the origin.</li> <li>QRNG does not support multiple replications (independent randomizations).</li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; discrete_distrib = Halton(2,seed=7)\n&gt;&gt;&gt; discrete_distrib(4)\narray([[0.83790457, 0.89981478],\n       [0.00986102, 0.4610941 ],\n       [0.62236343, 0.02796307],\n       [0.29427505, 0.79909098]])\n&gt;&gt;&gt; discrete_distrib\nHalton (AbstractLDDiscreteDistribution)\n    d               2^(1)\n    replications    1\n    randomize       LMS DP\n    t               63\n    n_limit         2^(32)\n    entropy         7\n</code></pre> <p>Replications of independent randomizations </p> <pre><code>&gt;&gt;&gt; x = Halton(3,seed=7,replications=2)(4)\n&gt;&gt;&gt; x.shape\n(2, 4, 3)\n&gt;&gt;&gt; x\narray([[[0.70988236, 0.18180876, 0.54073621],\n        [0.38178158, 0.61168824, 0.64684354],\n        [0.98597752, 0.70650871, 0.31479029],\n        [0.15795399, 0.28162992, 0.98945647]],\n\n       [[0.620398  , 0.57025403, 0.46336542],\n        [0.44021889, 0.69926312, 0.60133428],\n        [0.89132308, 0.12030255, 0.35715804],\n        [0.04025218, 0.44304244, 0.10724799]]])\n</code></pre> <p>Unrandomized Halton </p> <pre><code>&gt;&gt;&gt; Halton(2,randomize=\"FALSE\",seed=7)(4,warn=False)\narray([[0.        , 0.        ],\n       [0.5       , 0.33333333],\n       [0.25      , 0.66666667],\n       [0.75      , 0.11111111]])\n</code></pre> <p>All randomizations </p> <pre><code>&gt;&gt;&gt; Halton(2,randomize=\"LMS DP\",seed=7)(4)\narray([[0.83790457, 0.89981478],\n       [0.00986102, 0.4610941 ],\n       [0.62236343, 0.02796307],\n       [0.29427505, 0.79909098]])\n&gt;&gt;&gt; Halton(2,randomize=\"LMS DS\",seed=7)(4)\narray([[0.82718745, 0.90603116],\n       [0.0303368 , 0.44704107],\n       [0.60182684, 0.03580544],\n       [0.30505343, 0.78367016]])\n&gt;&gt;&gt; Halton(2,randomize=\"LMS\",seed=7)(4,warn=False)\narray([[0.        , 0.        ],\n       [0.82822666, 0.92392942],\n       [0.28838899, 0.46493682],\n       [0.6165384 , 0.2493814 ]])\n&gt;&gt;&gt; Halton(2,randomize=\"DP\",seed=7)(4)\narray([[0.11593484, 0.99232505],\n       [0.61593484, 0.65899172],\n       [0.36593484, 0.32565839],\n       [0.86593484, 0.77010283]])\n&gt;&gt;&gt; Halton(2,randomize=\"DS\",seed=7)(4)\narray([[0.56793849, 0.04063513],\n       [0.06793849, 0.37396846],\n       [0.81793849, 0.7073018 ],\n       [0.31793849, 0.15174624]])\n&gt;&gt;&gt; Halton(2,randomize=\"NUS\",seed=7)(4)\narray([[0.141964  , 0.99285569],\n       [0.65536579, 0.51938353],\n       [0.46955206, 0.11342811],\n       [0.78505432, 0.87032345]])\n&gt;&gt;&gt; Halton(2,randomize=\"QRNG\",seed=7)(4)\narray([[0.35362988, 0.38733489],\n       [0.85362988, 0.72066823],\n       [0.10362988, 0.05400156],\n       [0.60362988, 0.498446  ]])\n</code></pre> <p>Replications of randomizations </p> <pre><code>&gt;&gt;&gt; Halton(3,randomize=\"LMS DP\",seed=7,replications=2)(4)\narray([[[0.70988236, 0.18180876, 0.54073621],\n        [0.38178158, 0.61168824, 0.64684354],\n        [0.98597752, 0.70650871, 0.31479029],\n        [0.15795399, 0.28162992, 0.98945647]],\n\n       [[0.620398  , 0.57025403, 0.46336542],\n        [0.44021889, 0.69926312, 0.60133428],\n        [0.89132308, 0.12030255, 0.35715804],\n        [0.04025218, 0.44304244, 0.10724799]]])\n&gt;&gt;&gt; Halton(3,randomize=\"LMS DS\",seed=7,replications=2)(4)\narray([[[4.57465163e-01, 5.75419751e-04, 7.47353067e-01],\n        [6.29314800e-01, 9.24349881e-01, 8.47915779e-01],\n        [2.37544271e-01, 4.63986168e-01, 1.78817056e-01],\n        [9.09318567e-01, 2.48566227e-01, 3.17475640e-01]],\n\n       [[6.04003127e-01, 9.92849835e-01, 4.21625151e-01],\n        [4.57027115e-01, 1.97310094e-01, 2.43670150e-01],\n        [8.76467351e-01, 4.22339232e-01, 1.05777101e-01],\n        [5.46933622e-02, 7.79075280e-01, 9.29409300e-01]]])\n&gt;&gt;&gt; Halton(3,randomize=\"LMS\",seed=7,replications=2)(4,warn=False)\narray([[[0.        , 0.        , 0.        ],\n        [0.82822666, 0.92392942, 0.34057871],\n        [0.28838899, 0.46493682, 0.47954399],\n        [0.6165384 , 0.2493814 , 0.77045601]],\n\n       [[0.        , 0.        , 0.        ],\n        [0.93115665, 0.57483093, 0.87170952],\n        [0.48046642, 0.8122114 , 0.69381851],\n        [0.58055977, 0.28006957, 0.55586147]]])\n&gt;&gt;&gt; Halton(3,randomize=\"DS\",seed=7,replications=2)(4)\narray([[[0.56793849, 0.04063513, 0.74276256],\n        [0.06793849, 0.37396846, 0.94276256],\n        [0.81793849, 0.7073018 , 0.14276256],\n        [0.31793849, 0.15174624, 0.34276256]],\n\n       [[0.98309816, 0.80260469, 0.17299622],\n        [0.48309816, 0.13593802, 0.37299622],\n        [0.73309816, 0.46927136, 0.57299622],\n        [0.23309816, 0.9137158 , 0.77299622]]])\n&gt;&gt;&gt; Halton(3,randomize=\"DP\",seed=7,replications=2)(4)\narray([[[0.11593484, 0.99232505, 0.6010751 ],\n        [0.61593484, 0.65899172, 0.0010751 ],\n        [0.36593484, 0.32565839, 0.4010751 ],\n        [0.86593484, 0.77010283, 0.8010751 ]],\n\n       [[0.26543198, 0.12273092, 0.20202896],\n        [0.76543198, 0.45606426, 0.60202896],\n        [0.01543198, 0.78939759, 0.40202896],\n        [0.51543198, 0.23384203, 0.00202896]]])\n&gt;&gt;&gt; Halton(3,randomize=\"NUS\",seed=7,replications=2)(4)\narray([[[0.141964  , 0.99285569, 0.77722918],\n        [0.65536579, 0.51938353, 0.22797442],\n        [0.46955206, 0.11342811, 0.9975298 ],\n        [0.78505432, 0.87032345, 0.57696123]],\n\n       [[0.04813634, 0.16158904, 0.56038465],\n        [0.89364888, 0.33578478, 0.36145822],\n        [0.34111023, 0.84596814, 0.0292313 ],\n        [0.71866903, 0.23852281, 0.80431142]]])\n</code></pre> <p>References:</p> <ol> <li> <p>Marius Hofert and Christiane Lemieux.     qrng: (Randomized) Quasi-Random Number Generators.     R package version 0.0-7. (2019). https://CRAN.R-project.org/package=qrng.</p> </li> <li> <p>A. B. Owen.     A randomized Halton algorithm in R. arXiv:1706.02808 [stat.CO]. 2017. </p> </li> <li> <p>A. B. Owen and Z. Pan.     Gain coefficients for scrambled Halton points. arXiv:2308.08035 [stat.CO]. 2023.</p> </li> </ol> <p>Parameters:</p> Name Type Description Default <code>dimension</code> <code>Union[int, ndarray]</code> <p>Dimension of the generator.</p> <ul> <li>If an <code>int</code> is passed in, use generating vector components at indices 0,...,<code>dimension</code>-1.</li> <li>If an <code>np.ndarray</code> is passed in, use generating vector components at these indices.</li> </ul> <code>1</code> <code>replications</code> <code>int</code> <p>Number of independent randomizations of a pointset.</p> <code>None</code> <code>seed</code> <code>Union[None,int,np.random.SeedSeq</code> <p>Seed the random number generator for reproducibility.</p> <code>None</code> <code>randomize</code> <code>str</code> <p>Options are</p> <ul> <li><code>'LMS DP'</code>: Linear matrix scramble with digital permutation.</li> <li><code>'LMS DS'</code>: Linear matrix scramble with digital shift.</li> <li><code>'LMS'</code>: Linear matrix scramble only.</li> <li><code>'DP'</code>: Digital permutation scramble only.</li> <li><code>'DS'</code>: Digital shift only.</li> <li><code>'NUS'</code>: Nested uniform scrambling.</li> <li><code>'QRNG'</code>: Deterministic permutation scramble and random digital shift from QRNG [1] (with <code>generalize=True</code>). Does not support replications&gt;1.</li> <li><code>None</code>: No randomization. In this case the first point will be the origin. </li> </ul> <code>'LMS DP'</code> <code>t</code> <code>int</code> <p>Number of bits in integer represetation of points after randomization. The number of bits in the generating matrices is inferred based on the largest value.</p> <code>63</code> <code>n_lim</code> <code>int</code> <p>Maximum number of compatible points, determines the number of rows in the generating matrices.</p> <code>2 ** 32</code> Source code in <code>qmcpy/discrete_distribution/halton.py</code> <pre><code>def __init__(self,\n             dimension = 1,\n             replications = None,\n             seed = None, \n             randomize = 'LMS DP',\n             t = 63,\n             n_lim = 2**32,\n             # deprecated\n             t_lms = None):\n    r\"\"\"\n    Args:\n        dimension (Union[int,np.ndarray]): Dimension of the generator.\n\n            - If an `int` is passed in, use generating vector components at indices 0,...,`dimension`-1.\n            - If an `np.ndarray` is passed in, use generating vector components at these indices.\n\n        replications (int): Number of independent randomizations of a pointset.\n        seed (Union[None,int,np.random.SeedSeq): Seed the random number generator for reproducibility.\n        randomize (str): Options are\n\n            - `'LMS DP'`: Linear matrix scramble with digital permutation.\n            - `'LMS DS'`: Linear matrix scramble with digital shift.\n            - `'LMS'`: Linear matrix scramble only.\n            - `'DP'`: Digital permutation scramble only.\n            - `'DS'`: Digital shift only.\n            - `'NUS'`: Nested uniform scrambling.\n            - `'QRNG'`: Deterministic permutation scramble and random digital shift from QRNG [1] (with `generalize=True`). Does *not* support replications&gt;1.\n            - `None`: No randomization. In this case the first point will be the origin. \n        t (int): Number of bits in integer represetation of points *after* randomization. The number of bits in the generating matrices is inferred based on the largest value.\n        n_lim (int): Maximum number of compatible points, determines the number of rows in the generating matrices. \n    \"\"\"\n    if t_lms is not None:\n        t = t_lms\n        warnings.warn(\"t_lms argument deprecated. Set t instead. Using t = %d\"%t,ParameterWarning)\n    self.parameters = ['randomize','t','n_limit']\n    self.all_primes = np.array([2, 3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, 47, 53, 59, 61, 67, 71, 73, 79, 83, 89, 97, 101, 103, 107, 109, 113, 127, 131, 137, 139, 149, 151, 157, 163, 167, 173, 179, 181, 191, 193, 197, 199, 211, 223, 227, 229, 233, 239, 241, 251, 257, 263, 269, 271, 277, 281, 283, 293, 307, 311, 313, 317, 331, 337, 347, 349, 353, 359, 367, 373, 379, 383, 389, 397, 401, 409, 419, 421, 431, 433, 439, 443, 449, 457, 461, 463, 467, 479, 487, 491, 499, 503, 509, 521, 523, 541, 547, 557, 563, 569, 571, 577, 587, 593, 599, 601, 607, 613, 617, 619, 631, 641, 643, 647, 653, 659, 661, 673, 677, 683, 691, 701, 709, 719, 727, 733, 739, 743, 751, 757, 761, 769, 773, 787, 797, 809, 811, 821, 823, 827, 829, 839, 853, 857, 859, 863, 877, 881, 883, 887, 907, 911, 919, 929, 937, 941, 947, 953, 967, 971, 977, 983, 991, 997, 1009, 1013, 1019, 1021, 1031, 1033, 1039, 1049, 1051, 1061, 1063, 1069, 1087, 1091, 1093, 1097, 1103, 1109, 1117, 1123, 1129, 1151, 1153, 1163, 1171, 1181, 1187, 1193, 1201, 1213, 1217, 1223, 1229, 1231, 1237, 1249, 1259, 1277, 1279, 1283, 1289, 1291, 1297, 1301, 1303, 1307, 1319, 1321, 1327, 1361, 1367, 1373, 1381, 1399, 1409, 1423, 1427, 1429, 1433, 1439, 1447, 1451, 1453, 1459, 1471, 1481, 1483, 1487, 1489, 1493, 1499, 1511, 1523, 1531, 1543, 1549, 1553, 1559, 1567, 1571, 1579, 1583, 1597, 1601, 1607, 1609, 1613, 1619, 1621, 1627, 1637, 1657, 1663, 1667, 1669, 1693, 1697, 1699, 1709, 1721, 1723, 1733, 1741, 1747, 1753, 1759, 1777, 1783, 1787, 1789, 1801, 1811, 1823, 1831, 1847, 1861, 1867, 1871, 1873, 1877, 1879, 1889, 1901, 1907, 1913, 1931, 1933, 1949, 1951, 1973, 1979, 1987, 1993, 1997, 1999, 2003, 2011, 2017, 2027, 2029, 2039, 2053, 2063, 2069, 2081, 2083, 2087, 2089, 2099, 2111, 2113, 2129, 2131, 2137, 2141, 2143, 2153, 2161, 2179, 2203, 2207, 2213, 2221, 2237, 2239, 2243, 2251, 2267, 2269, 2273, 2281, 2287, 2293, 2297, 2309, 2311, 2333, 2339, 2341, 2347, 2351, 2357, 2371, 2377, 2381, 2383, 2389, 2393, 2399, 2411, 2417, 2423, 2437, 2441, 2447, 2459, 2467, 2473, 2477, 2503, 2521, 2531, 2539, 2543, 2549, 2551, 2557, 2579, 2591, 2593, 2609, 2617, 2621, 2633, 2647, 2657, 2659, 2663, 2671, 2677, 2683, 2687, 2689, 2693, 2699, 2707, 2711, 2713, 2719, 2729, 2731, 2741, 2749, 2753, 2767, 2777, 2789, 2791, 2797, 2801, 2803, 2819, 2833, 2837, 2843, 2851, 2857, 2861, 2879, 2887, 2897, 2903, 2909, 2917, 2927, 2939, 2953, 2957, 2963, 2969, 2971, 2999, 3001, 3011, 3019, 3023, 3037, 3041, 3049, 3061, 3067, 3079, 3083, 3089, 3109, 3119, 3121, 3137, 3163, 3167, 3169, 3181, 3187, 3191, 3203, 3209, 3217, 3221, 3229, 3251, 3253, 3257, 3259, 3271, 3299, 3301, 3307, 3313, 3319, 3323, 3329, 3331, 3343, 3347, 3359, 3361, 3371, 3373, 3389, 3391, 3407, 3413, 3433, 3449, 3457, 3461, 3463, 3467, 3469, 3491, 3499, 3511, 3517, 3527, 3529, 3533, 3539, 3541, 3547, 3557, 3559, 3571, 3581, 3583, 3593, 3607, 3613, 3617, 3623, 3631, 3637, 3643, 3659, 3671, 3673, 3677, 3691, 3697, 3701, 3709, 3719, 3727, 3733, 3739, 3761, 3767, 3769, 3779, 3793, 3797, 3803, 3821, 3823, 3833, 3847, 3851, 3853, 3863, 3877, 3881, 3889, 3907, 3911, 3917, 3919, 3923, 3929, 3931, 3943, 3947, 3967, 3989, 4001, 4003, 4007, 4013, 4019, 4021, 4027, 4049, 4051, 4057, 4073, 4079, 4091, 4093, 4099, 4111, 4127, 4129, 4133, 4139, 4153, 4157, 4159, 4177, 4201, 4211, 4217, 4219, 4229, 4231, 4241, 4243, 4253, 4259, 4261, 4271, 4273, 4283, 4289, 4297, 4327, 4337, 4339, 4349, 4357, 4363, 4373, 4391, 4397, 4409, 4421, 4423, 4441, 4447, 4451, 4457, 4463, 4481, 4483, 4493, 4507, 4513, 4517, 4519, 4523, 4547, 4549, 4561, 4567, 4583, 4591, 4597, 4603, 4621, 4637, 4639, 4643, 4649, 4651, 4657, 4663, 4673, 4679, 4691, 4703, 4721, 4723, 4729, 4733, 4751, 4759, 4783, 4787, 4789, 4793, 4799, 4801, 4813, 4817, 4831, 4861, 4871, 4877, 4889, 4903, 4909, 4919, 4931, 4933, 4937, 4943, 4951, 4957, 4967, 4969, 4973, 4987, 4993, 4999, 5003, 5009, 5011, 5021, 5023, 5039, 5051, 5059, 5077, 5081, 5087, 5099, 5101, 5107, 5113, 5119, 5147, 5153, 5167, 5171, 5179, 5189, 5197, 5209, 5227, 5231, 5233, 5237, 5261, 5273, 5279, 5281, 5297, 5303, 5309, 5323, 5333, 5347, 5351, 5381, 5387, 5393, 5399, 5407, 5413, 5417, 5419, 5431, 5437, 5441, 5443, 5449, 5471, 5477, 5479, 5483, 5501, 5503, 5507, 5519, 5521, 5527, 5531, 5557, 5563, 5569, 5573, 5581, 5591, 5623, 5639, 5641, 5647, 5651, 5653, 5657, 5659, 5669, 5683, 5689, 5693, 5701, 5711, 5717, 5737, 5741, 5743, 5749, 5779, 5783, 5791, 5801, 5807, 5813, 5821, 5827, 5839, 5843, 5849, 5851, 5857, 5861, 5867, 5869, 5879, 5881, 5897, 5903, 5923, 5927, 5939, 5953, 5981, 5987, 6007, 6011, 6029, 6037, 6043, 6047, 6053, 6067, 6073, 6079, 6089, 6091, 6101, 6113, 6121, 6131, 6133, 6143, 6151, 6163, 6173, 6197, 6199, 6203, 6211, 6217, 6221, 6229, 6247, 6257, 6263, 6269, 6271, 6277, 6287, 6299, 6301, 6311, 6317, 6323, 6329, 6337, 6343, 6353, 6359, 6361, 6367, 6373, 6379, 6389, 6397, 6421, 6427, 6449, 6451, 6469, 6473, 6481, 6491, 6521, 6529, 6547, 6551, 6553, 6563, 6569, 6571, 6577, 6581, 6599, 6607, 6619, 6637, 6653, 6659, 6661, 6673, 6679, 6689, 6691, 6701, 6703, 6709, 6719, 6733, 6737, 6761, 6763, 6779, 6781, 6791, 6793, 6803, 6823, 6827, 6829, 6833, 6841, 6857, 6863, 6869, 6871, 6883, 6899, 6907, 6911, 6917, 6947, 6949, 6959, 6961, 6967, 6971, 6977, 6983, 6991, 6997, 7001, 7013, 7019, 7027, 7039, 7043, 7057, 7069, 7079, 7103, 7109, 7121, 7127, 7129, 7151, 7159, 7177, 7187, 7193, 7207, 7211, 7213, 7219, 7229, 7237, 7243, 7247, 7253, 7283, 7297, 7307, 7309, 7321, 7331, 7333, 7349, 7351, 7369, 7393, 7411, 7417, 7433, 7451, 7457, 7459, 7477, 7481, 7487, 7489, 7499, 7507, 7517, 7523, 7529, 7537, 7541, 7547, 7549, 7559, 7561, 7573, 7577, 7583, 7589, 7591, 7603, 7607, 7621, 7639, 7643, 7649, 7669, 7673, 7681, 7687, 7691, 7699, 7703, 7717, 7723, 7727, 7741, 7753, 7757, 7759, 7789, 7793, 7817, 7823, 7829, 7841, 7853, 7867, 7873, 7877, 7879, 7883, 7901, 7907, 7919],dtype=np.uint64)\n    d_limit = len(self.all_primes)\n    self.input_t = deepcopy(t) \n    super(Halton,self).__init__(dimension,replications,seed,d_limit,n_lim)\n    self.randomize = str(randomize).upper().strip().replace(\"_\",\" \")\n    if self.randomize==\"TRUE\": self.randomize = \"LMS DP\"\n    if self.randomize==\"LMS PERM\": self.randomize = \"LMS DP\"\n    if self.randomize==\"PERM\": self.randomize = \"DP\"\n    if self.randomize==\"OWEN\": self.randomize = \"NUS\"\n    if self.randomize==\"NONE\": self.randomize = \"FALSE\"\n    if self.randomize==\"NO\": self.randomize = \"FALSE\"\n    assert self.randomize in [\"LMS DP\",\"LMS DS\",\"LMS\",\"DP\",\"DS\",\"NUS\",\"QRNG\",\"FALSE\"]\n    if self.randomize==\"QRNG\":\n        from ._c_lib import _load_c_lib\n        assert self.replications==1, \"QRNG requires replications=1\"\n        self.randu_d_32 = self.rng.uniform(size=(self.d,32))\n        _c_lib = _load_c_lib()\n        import ctypes\n        self.halton_cf_qrng = _c_lib.halton_qrng\n        self.halton_cf_qrng.argtypes = [\n            ctypes.c_int,  # n\n            ctypes.c_int,  # d\n            ctypes.c_int, # n0\n            ctypes.c_int, # generalized\n            np.ctypeslib.ndpointer(ctypes.c_double, flags='C_CONTIGUOUS'), # res\n            np.ctypeslib.ndpointer(ctypes.c_double, flags='C_CONTIGUOUS'), # randu_d_32\n            np.ctypeslib.ndpointer(ctypes.c_int, flags='C_CONTIGUOUS')]  # dvec\n        self.halton_cf_qrng.restype = None\n    self.primes = self.all_primes[self.dvec]\n    self.m_max = int(np.ceil(np.log(self.n_limit)/np.log(self.primes.min())))\n    self._t_curr = self.m_max\n    self.t = self.m_max if self.m_max&gt;t else t\n    self.C = qmctoolscl.gdn_get_halton_generating_matrix(np.uint64(1),np.uint64(self.d),np.uint64(self._t_curr))\n    if \"LMS\" in self.randomize:\n        S = qmctoolscl.gdn_get_linear_scramble_matrix(self.rng,np.uint64(self.replications),np.uint64(self.d),np.uint64(self._t_curr),np.uint64(self.t),np.uint64(1),self.primes)\n        C_lms = np.empty((self.replications,self.d,self.m_max,self.t),dtype=np.uint64)\n        qmctoolscl.gdn_linear_matrix_scramble(np.uint64(self.replications),np.uint64(self.d),np.uint64(self.m_max),np.uint64(1),np.uint64(1),np.uint64(self._t_curr),np.uint64(self.t),self.primes,S,self.C,C_lms,backend=\"c\")\n        self.C = C_lms\n        self._t_curr = self.t\n    if \"DP\" in self.randomize:\n        self.perms = qmctoolscl.gdn_get_digital_permutations(self.rng,np.uint64(self.replications),np.uint64(self.d),self.t,np.uint64(1),self.primes)\n    if \"DS\" in self.randomize:\n        self.rshift = qmctoolscl.gdn_get_digital_shifts(self.rng,np.uint64(self.replications),np.uint64(self.d),self.t,np.uint64(1),self.primes)\n    if \"NUS\" in self.randomize:\n        new_seeds = self._base_seed.spawn(self.replications*self.d)\n        self.rngs = np.array([np.random.Generator(np.random.SFC64(new_seeds[j])) for j in range(self.replications*self.d)]).reshape(self.replications,self.d)\n        self.root_nodes = np.array([qmctoolscl.NUSNode_gdn() for i in range(self.replications*self.d)]).reshape(self.replications,self.d)\n    assert self.C.ndim==4 and (self.C.shape[0]==1 or self.C.shape[0]==self.replications) and self.C.shape[1]==self.d and self.C.shape[2]==self.m_max and self.C.shape[3]==self._t_curr\n    assert 0&lt;self._t_curr&lt;=self.t&lt;=64\n    if self.randomize==\"FALSE\": assert self.C.shape[0]==self.replications, \"randomize='FALSE' but replications = %d does not equal the number of sets of generating vectors %d\"%(self.replications,self.C.shape[0])\n</code></pre>"},{"location":"api/discrete_distributions/#iidstduniform","title":"<code>IIDStdUniform</code>","text":"<p>               Bases: <code>AbstractIIDDiscreteDistribution</code></p> <p>IID standard uniform points, a wrapper around <code>numpy.random.rand</code>. </p> Note <ul> <li>Unlike low discrepancy sequence, calling an <code>IIDStdUniform</code> instance gives new samples every time,     e.g., running the first doctest below with <code>dd = Lattice(dimension=2)</code> would give the same 4 points in both calls,     but since we are using an <code>IIDStdUniform</code> instance it gives different points every call. </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; discrete_distrib = IIDStdUniform(dimension=2,seed=7)\n&gt;&gt;&gt; discrete_distrib(4)\narray([[0.04386058, 0.58727432],\n       [0.3691824 , 0.65212985],\n       [0.69669968, 0.10605352],\n       [0.63025643, 0.13630282]])\n&gt;&gt;&gt; discrete_distrib(4) # gives new samples every time\narray([[0.5968363 , 0.0576251 ],\n       [0.2028797 , 0.22909681],\n       [0.1366783 , 0.75220658],\n       [0.84501765, 0.56269008]])\n&gt;&gt;&gt; discrete_distrib\nIIDStdUniform (AbstractIIDDiscreteDistribution)\n    d               2^(1)\n    replications    1\n    entropy         7\n</code></pre> <p>Replications (implemented for API consistency) </p> <pre><code>&gt;&gt;&gt; x = IIDStdUniform(dimension=3,replications=2,seed=7)(4)\n&gt;&gt;&gt; x.shape\n(2, 4, 3)\n&gt;&gt;&gt; x\narray([[[0.04386058, 0.58727432, 0.3691824 ],\n        [0.65212985, 0.69669968, 0.10605352],\n        [0.63025643, 0.13630282, 0.5968363 ],\n        [0.0576251 , 0.2028797 , 0.22909681]],\n\n       [[0.1366783 , 0.75220658, 0.84501765],\n        [0.56269008, 0.04826852, 0.71308655],\n        [0.80983568, 0.85383675, 0.80475135],\n        [0.6171181 , 0.1239209 , 0.16809479]]])\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>dimension</code> <code>int</code> <p>Dimension of the samples.</p> <code>1</code> <code>replications</code> <code>Union[None, int]</code> <p>Number of randomizations. This is implemented only for API consistency. Equivalent to reshaping samples. </p> <code>None</code> <code>seed</code> <code>Union[None,int,np.random.SeedSeq</code> <p>Seed the random number generator for reproducibility.</p> <code>None</code> Source code in <code>qmcpy/discrete_distribution/iid_std_uniform.py</code> <pre><code>def __init__(self,\n             dimension = 1, \n             replications = None, \n             seed = None):\n    r\"\"\"\n    Args:\n        dimension (int): Dimension of the samples.\n        replications (Union[None,int]): Number of randomizations. This is implemented only for API consistency. Equivalent to reshaping samples. \n        seed (Union[None,int,np.random.SeedSeq): Seed the random number generator for reproducibility.\n    \"\"\"\n    super(IIDStdUniform,self).__init__(int(dimension),replications,seed,d_limit=np.inf,n_limit=np.inf)\n    if not (self.dvec==np.arange(self.d)).all():\n        warnings.warn(\"IIDStdUniform does not accomodate dvec\",ParameterWarning)\n</code></pre>"},{"location":"api/discrete_distributions/#uml-specific","title":"UML Specific","text":""},{"location":"api/fast_transforms/","title":"Fast Transforms","text":""},{"location":"api/fast_transforms/#numpy-compatible","title":"<code>numpy</code> compatible","text":""},{"location":"api/fast_transforms/#fwht","title":"<code>fwht</code>","text":"<p>1 dimensional Fast Walsh Hadamard Transform (FWHT) along the last dimension. Requires the size of the last dimension is a power of 2. </p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; rng = np.random.Generator(np.random.SFC64(11))\n&gt;&gt;&gt; with np.printoptions(precision=2):\n...     fwht(rng.random(8))\narray([ 1.07, -0.29,  0.12,  0.08,  0.1 , -0.45,  0.1 , -0.03])\n&gt;&gt;&gt; fwht(rng.random((2,3,4,5,8))).shape\n(2, 3, 4, 5, 8)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ndarray</code> <p>Array of samples at which to run FWHT.</p> required <p>Returns:</p> Name Type Description <code>y</code> <code>ndarray</code> <p>FWHT values.</p> Source code in <code>qmcpy/fast_transform/ft.py</code> <pre><code>def fwht(x):\n    r\"\"\"\n    1 dimensional Fast Walsh Hadamard Transform (FWHT) along the last dimension.  \n    Requires the size of the last dimension is a power of 2. \n\n    Examples:\n        &gt;&gt;&gt; rng = np.random.Generator(np.random.SFC64(11))\n        &gt;&gt;&gt; with np.printoptions(precision=2):\n        ...     fwht(rng.random(8))\n        array([ 1.07, -0.29,  0.12,  0.08,  0.1 , -0.45,  0.1 , -0.03])\n        &gt;&gt;&gt; fwht(rng.random((2,3,4,5,8))).shape\n        (2, 3, 4, 5, 8)\n\n    Args:\n        x (np.ndarray): Array of samples at which to run FWHT.\n\n    Returns:\n        y (np.ndarray): FWHT values.\n    \"\"\"\n    y = x.copy()+0.\n    n = x.shape[-1]\n    if n&lt;=1: return y\n    assert n&amp;(n-1)==0 # require n is a power of 2\n    m = int(np.log2(n))\n    it = np.arange(n,dtype=np.int64).reshape([2]*m) # 2 x 2 x ... x 2 array (size 2^m)\n    idx0 = [slice(None)]*(m-1)+[0]\n    idx1 = [slice(None)]*(m-1)+[1]\n    for k in range(m):\n        eps0 = it[tuple(idx0[-(k+1):])].flatten()\n        eps1 = it[tuple(idx1[-(k+1):])].flatten()\n        y0,y1 = y[...,eps0],y[...,eps1]\n        y[...,eps0],y[...,eps1] = (y0+y1)/np.sqrt(2),(y0-y1)/np.sqrt(2)\n    return y\n</code></pre>"},{"location":"api/fast_transforms/#fftbr","title":"<code>fftbr</code>","text":"<p>1 dimensional Bit-Reversed-Order (BRO) Fast Fourier Transform (FFT) along the last dimension.  Requires the last dimension of x is already in BRO, so we can skip the first step of the decimation-in-time FFT.  Requires the size of the last dimension is a power of 2. </p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; rng = np.random.Generator(np.random.SFC64(11))\n&gt;&gt;&gt; x = rng.random(8)+1j*rng.random(8)\n&gt;&gt;&gt; with np.printoptions(precision=2):\n...     fftbr(x)\narray([ 1.07+1.14j, -0.32+0.26j, -0.27+0.22j, -0.27-0.34j,  0.1 +0.16j,\n       -0.15+0.17j,  0.5 +0.24j,  0.06-0.02j])\n&gt;&gt;&gt; fftbr(rng.random((2,3,4,5,8))+1j*rng.random((2,3,4,5,8))).shape\n(2, 3, 4, 5, 8)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ndarray</code> <p>Array of samples at which to run BRO-FFT.</p> required <p>Returns:</p> Name Type Description <code>y</code> <code>ndarray</code> <p>BRO-FFT values.</p> Source code in <code>qmcpy/fast_transform/ft.py</code> <pre><code>def fftbr(x):\n    r\"\"\"\n    1 dimensional Bit-Reversed-Order (BRO) Fast Fourier Transform (FFT) along the last dimension. \n    Requires the last dimension of x is already in BRO, so we can skip the first step of the decimation-in-time FFT. \n    Requires the size of the last dimension is a power of 2. \n\n    Examples:\n        &gt;&gt;&gt; rng = np.random.Generator(np.random.SFC64(11))\n        &gt;&gt;&gt; x = rng.random(8)+1j*rng.random(8)\n        &gt;&gt;&gt; with np.printoptions(precision=2):\n        ...     fftbr(x)\n        array([ 1.07+1.14j, -0.32+0.26j, -0.27+0.22j, -0.27-0.34j,  0.1 +0.16j,\n               -0.15+0.17j,  0.5 +0.24j,  0.06-0.02j])\n        &gt;&gt;&gt; fftbr(rng.random((2,3,4,5,8))+1j*rng.random((2,3,4,5,8))).shape\n        (2, 3, 4, 5, 8)\n\n    Args:\n        x (np.ndarray): Array of samples at which to run BRO-FFT.\n\n    Returns:\n        y (np.ndarray): BRO-FFT values.\n    \"\"\"\n    n = x.shape[-1]\n    assert n&amp;(n-1)==0 # require n is a power of 2\n    m = int(np.log2(n))\n    shape = list(x.shape)\n    ndim = x.ndim\n    twos = [2]*m\n    xs = x.reshape(shape[:-1]+twos)\n    pdims = tuple(itertools.chain(range(ndim-1),range(m+ndim-2,ndim-2,-1)))#[i for i in range(ndim-1)]+[i+ndim-1 for i in range(m-1,-1,-1)]\n    xrf = np.moveaxis(xs,np.arange(len(pdims)),pdims)\n    xr = np.ascontiguousarray(xrf).reshape(shape)\n    return scipy.fft.fft(xr,norm=\"ortho\")\n</code></pre>"},{"location":"api/fast_transforms/#ifftbr","title":"<code>ifftbr</code>","text":"<p>1 dimensional Bit-Reversed-Order (BRO) Inverse Fast Fourier Transform (IFFT) along the last dimension. Outputs an array in bit-reversed order, so we can skip the last step of the decimation-in-time IFFT. Requires the size of the last dimension is a power of 2. </p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; rng = np.random.Generator(np.random.SFC64(11))\n&gt;&gt;&gt; x = rng.random(8)+1j*rng.random(8)\n&gt;&gt;&gt; with np.printoptions(precision=2):\n...     ifftbr(x)\narray([ 1.07+1.14j, -0.29-0.22j,  0.3 +0.06j, -0.09+0.02j,  0.03+0.54j,\n       -0.04-0.33j, -0.19+0.26j, -0.08+0.36j])\n&gt;&gt;&gt; ifftbr(rng.random((2,3,4,5,8))+1j*rng.random((2,3,4,5,8))).shape\n(2, 3, 4, 5, 8)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ndarray</code> <p>Array of samples at which to run BRO-IFFT.</p> required <p>Returns:</p> Name Type Description <code>y</code> <code>ndarray</code> <p>BRO-IFFT values.</p> Source code in <code>qmcpy/fast_transform/ft.py</code> <pre><code>def ifftbr(x):\n    r\"\"\"\n    1 dimensional Bit-Reversed-Order (BRO) Inverse Fast Fourier Transform (IFFT) along the last dimension.  \n    Outputs an array in bit-reversed order, so we can skip the last step of the decimation-in-time IFFT.  \n    Requires the size of the last dimension is a power of 2. \n\n    Examples:\n        &gt;&gt;&gt; rng = np.random.Generator(np.random.SFC64(11))\n        &gt;&gt;&gt; x = rng.random(8)+1j*rng.random(8)\n        &gt;&gt;&gt; with np.printoptions(precision=2):\n        ...     ifftbr(x)\n        array([ 1.07+1.14j, -0.29-0.22j,  0.3 +0.06j, -0.09+0.02j,  0.03+0.54j,\n               -0.04-0.33j, -0.19+0.26j, -0.08+0.36j])\n        &gt;&gt;&gt; ifftbr(rng.random((2,3,4,5,8))+1j*rng.random((2,3,4,5,8))).shape\n        (2, 3, 4, 5, 8)\n\n    Args:\n        x (np.ndarray): Array of samples at which to run BRO-IFFT.\n\n    Returns:\n        y (np.ndarray): BRO-IFFT values.\n    \"\"\"\n    n = x.shape[-1]\n    assert n&amp;(n-1)==0 # require n is a power of 2\n    m = int(np.log2(n))\n    shape = list(x.shape)\n    ndim = x.ndim\n    twos = [2]*m\n    x = scipy.fft.ifft(x,norm=\"ortho\")\n    xs = x.reshape(shape[:-1]+twos)\n    pdims = tuple(itertools.chain(range(ndim-1),range(m+ndim-2,ndim-2,-1)))\n    xrf = np.moveaxis(xs,np.arange(len(pdims)),pdims)\n    xr = np.ascontiguousarray(xrf).reshape(shape)\n    return xr\n</code></pre>"},{"location":"api/fast_transforms/#omega_fwht","title":"<code>omega_fwht</code>","text":"<p>A useful when efficiently updating FWHT values after doubling the sample size.  </p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; rng = np.random.Generator(np.random.SFC64(11))\n&gt;&gt;&gt; m = 3\n&gt;&gt;&gt; x1 = rng.random((3,5,7,2**m))\n&gt;&gt;&gt; x2 = rng.random((3,5,7,2**m))\n&gt;&gt;&gt; x = np.concatenate([x1,x2],axis=-1)\n&gt;&gt;&gt; ytrue = fwht(x)\n&gt;&gt;&gt; omega = omega_fwht(m)\n&gt;&gt;&gt; y1 = fwht(x1) \n&gt;&gt;&gt; y2 = fwht(x2) \n&gt;&gt;&gt; y = np.concatenate([y1+omega*y2,y1-omega*y2],axis=-1)/np.sqrt(2)\n&gt;&gt;&gt; np.allclose(y,ytrue)\nTrue\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>m</code> <code>int</code> <p>Size \\(2^m\\) output. </p> required <p>Returns:</p> Name Type Description <code>y</code> <code>ndarray</code> <p>\\(\\left(1\\right)_{k=0}^{2^m}\\).</p> Source code in <code>qmcpy/fast_transform/ft.py</code> <pre><code>def omega_fwht(m):\n    r\"\"\"\n    A useful when efficiently updating FWHT values after doubling the sample size.  \n\n    Examples:\n        &gt;&gt;&gt; rng = np.random.Generator(np.random.SFC64(11))\n        &gt;&gt;&gt; m = 3\n        &gt;&gt;&gt; x1 = rng.random((3,5,7,2**m))\n        &gt;&gt;&gt; x2 = rng.random((3,5,7,2**m))\n        &gt;&gt;&gt; x = np.concatenate([x1,x2],axis=-1)\n        &gt;&gt;&gt; ytrue = fwht(x)\n        &gt;&gt;&gt; omega = omega_fwht(m)\n        &gt;&gt;&gt; y1 = fwht(x1) \n        &gt;&gt;&gt; y2 = fwht(x2) \n        &gt;&gt;&gt; y = np.concatenate([y1+omega*y2,y1-omega*y2],axis=-1)/np.sqrt(2)\n        &gt;&gt;&gt; np.allclose(y,ytrue)\n        True\n\n    Args:\n        m (int): Size $2^m$ output. \n\n    Returns:\n        y (np.ndarray): $\\left(1\\right)_{k=0}^{2^m}$. \n    \"\"\"\n    return np.ones(2**m)\n</code></pre>"},{"location":"api/fast_transforms/#omega_fftbr","title":"<code>omega_fftbr</code>","text":"<p>A useful when efficiently updating FFT values after doubling the sample size. </p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; rng = np.random.Generator(np.random.SFC64(11))\n&gt;&gt;&gt; m = 3\n&gt;&gt;&gt; x1 = rng.random((3,5,7,2**m))+1j*rng.random((3,5,7,2**m))\n&gt;&gt;&gt; x2 = rng.random((3,5,7,2**m))+1j*rng.random((3,5,7,2**m))\n&gt;&gt;&gt; x = np.concatenate([x1,x2],axis=-1)\n&gt;&gt;&gt; ytrue = fftbr(x)\n&gt;&gt;&gt; omega = omega_fftbr(m)\n&gt;&gt;&gt; y1 = fftbr(x1) \n&gt;&gt;&gt; y2 = fftbr(x2) \n&gt;&gt;&gt; y = np.concatenate([y1+omega*y2,y1-omega*y2],axis=-1)/np.sqrt(2)\n&gt;&gt;&gt; np.allclose(y,ytrue)\nTrue\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>m</code> <code>int</code> <p>Size \\(2^m\\) output. </p> required <p>Returns:</p> Name Type Description <code>y</code> <code>ndarray</code> <p>\\(\\left(e^{- \\pi \\mathrm{i} k / 2^m}\\right)_{k=0}^{2^m}\\).</p> Source code in <code>qmcpy/fast_transform/ft.py</code> <pre><code>def omega_fftbr(m):\n    r\"\"\"\n    A useful when efficiently updating FFT values after doubling the sample size. \n\n    Examples:\n        &gt;&gt;&gt; rng = np.random.Generator(np.random.SFC64(11))\n        &gt;&gt;&gt; m = 3\n        &gt;&gt;&gt; x1 = rng.random((3,5,7,2**m))+1j*rng.random((3,5,7,2**m))\n        &gt;&gt;&gt; x2 = rng.random((3,5,7,2**m))+1j*rng.random((3,5,7,2**m))\n        &gt;&gt;&gt; x = np.concatenate([x1,x2],axis=-1)\n        &gt;&gt;&gt; ytrue = fftbr(x)\n        &gt;&gt;&gt; omega = omega_fftbr(m)\n        &gt;&gt;&gt; y1 = fftbr(x1) \n        &gt;&gt;&gt; y2 = fftbr(x2) \n        &gt;&gt;&gt; y = np.concatenate([y1+omega*y2,y1-omega*y2],axis=-1)/np.sqrt(2)\n        &gt;&gt;&gt; np.allclose(y,ytrue)\n        True\n\n    Args:\n        m (int): Size $2^m$ output. \n\n    Returns:\n        y (np.ndarray): $\\left(e^{- \\pi \\mathrm{i} k / 2^m}\\right)_{k=0}^{2^m}$. \n    \"\"\"\n    return np.exp(-np.pi*1j*np.arange(2**m)/2**m)\n</code></pre>"},{"location":"api/fast_transforms/#torch-compatible","title":"<code>torch</code> compatible","text":""},{"location":"api/fast_transforms/#fwht_torch","title":"<code>fwht_torch</code>","text":"<p>Torch implementation of the 1 dimensional Fast Walsh Hadamard Transform (FWHT) along the last dimension. Requires the size of the last dimension is a power of 2. </p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; rng = np.random.Generator(np.random.SFC64(11))\n&gt;&gt;&gt; x = torch.from_numpy(rng.random(8)).float().requires_grad_()\n&gt;&gt;&gt; y = fwht_torch(x)\n&gt;&gt;&gt; with torch._tensor_str.printoptions(precision=2):\n...     y.detach()\ntensor([ 1.07, -0.29,  0.12,  0.08,  0.10, -0.45,  0.10, -0.03])\n&gt;&gt;&gt; v = torch.sum(y**2)\n&gt;&gt;&gt; dvdx = torch.autograd.grad(v,x)[0]\n&gt;&gt;&gt; with torch._tensor_str.printoptions(precision=2):\n...     x.detach()\ntensor([0.25, 0.73, 0.06, 0.61, 0.45, 0.26, 0.35, 0.32])\n&gt;&gt;&gt; print(\"%.4f\"%v.detach())\n1.4694\n&gt;&gt;&gt; with torch._tensor_str.printoptions(precision=2):\n...     dvdx.detach()\ntensor([0.50, 1.47, 0.11, 1.23, 0.90, 0.51, 0.70, 0.64])\n&gt;&gt;&gt; fwht_torch(torch.rand((2,3,4,5,8))).shape\ntorch.Size([2, 3, 4, 5, 8])\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Array of samples at which to run FWHT.</p> required <p>Returns:</p> Name Type Description <code>y</code> <code>Tensor</code> <p>FWHT values.</p> Source code in <code>qmcpy/fast_transform/ft_pytorch.py</code> <pre><code>def fwht_torch(x):\n    r\"\"\"\n    Torch implementation of the 1 dimensional Fast Walsh Hadamard Transform (FWHT) along the last dimension.  \n    Requires the size of the last dimension is a power of 2. \n\n    Examples:\n        &gt;&gt;&gt; rng = np.random.Generator(np.random.SFC64(11))\n        &gt;&gt;&gt; x = torch.from_numpy(rng.random(8)).float().requires_grad_()\n        &gt;&gt;&gt; y = fwht_torch(x)\n        &gt;&gt;&gt; with torch._tensor_str.printoptions(precision=2):\n        ...     y.detach()\n        tensor([ 1.07, -0.29,  0.12,  0.08,  0.10, -0.45,  0.10, -0.03])\n        &gt;&gt;&gt; v = torch.sum(y**2)\n        &gt;&gt;&gt; dvdx = torch.autograd.grad(v,x)[0]\n        &gt;&gt;&gt; with torch._tensor_str.printoptions(precision=2):\n        ...     x.detach()\n        tensor([0.25, 0.73, 0.06, 0.61, 0.45, 0.26, 0.35, 0.32])\n        &gt;&gt;&gt; print(\"%.4f\"%v.detach())\n        1.4694\n        &gt;&gt;&gt; with torch._tensor_str.printoptions(precision=2):\n        ...     dvdx.detach()\n        tensor([0.50, 1.47, 0.11, 1.23, 0.90, 0.51, 0.70, 0.64])\n        &gt;&gt;&gt; fwht_torch(torch.rand((2,3,4,5,8))).shape\n        torch.Size([2, 3, 4, 5, 8])\n\n    Args:\n        x (torch.Tensor): Array of samples at which to run FWHT.\n\n    Returns:\n        y (torch.Tensor): FWHT values.\n    \"\"\"\n    return _FWHTB2Ortho.apply(x)\n</code></pre>"},{"location":"api/fast_transforms/#fftbr_torch","title":"<code>fftbr_torch</code>","text":"<p>Torch implementation of the 1 dimensional Bit-Reversed-Order (BRO) Fast Fourier Transform (FFT) along the last dimension.  Requires the last dimension of x is already in BRO, so we can skip the first step of the decimation-in-time FFT.  Requires the size of the last dimension is a power of 2. </p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; rng = np.random.Generator(np.random.SFC64(11))\n&gt;&gt;&gt; x = torch.from_numpy(rng.random(8)+1j*rng.random(8)).to(torch.complex64).requires_grad_()\n&gt;&gt;&gt; y = fftbr_torch(x)\n&gt;&gt;&gt; with torch._tensor_str.printoptions(precision=2):\n...     y.detach()\ntensor([ 1.07+1.14j, -0.32+0.26j, -0.27+0.22j, -0.27-0.34j,  0.10+0.16j,\n        -0.15+0.17j,  0.50+0.24j,  0.06-0.02j])\n&gt;&gt;&gt; v = torch.abs(torch.sum(y**2))\n&gt;&gt;&gt; dvdx = torch.autograd.grad(v,x)[0]\n&gt;&gt;&gt; with torch._tensor_str.printoptions(precision=2):\n...     x.detach()\ntensor([0.25+0.65j, 0.73+0.60j, 0.06+0.20j, 0.61+0.39j, 0.45+0.06j, 0.26+0.09j,\n        0.35+0.39j, 0.32+0.85j])\n&gt;&gt;&gt; print(\"%.4f\"%v.detach())\n2.5584\n&gt;&gt;&gt; with torch._tensor_str.printoptions(precision=2):\n...     dvdx.detach()\ntensor([1.30+0.49j, 1.21+1.45j, 0.79+1.22j, 0.41+0.11j, 1.71+0.61j, 0.79+0.69j,\n        0.19+0.51j, 0.12+0.90j])\n&gt;&gt;&gt; fftbr_torch(torch.rand((2,3,4,5,8))+1j*torch.rand((2,3,4,5,8))).shape\ntorch.Size([2, 3, 4, 5, 8])\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Array of samples at which to run BRO-FFT.</p> required <p>Returns:</p> Name Type Description <code>y</code> <code>Tensor</code> <p>BRO-FFT values.</p> Source code in <code>qmcpy/fast_transform/ft_pytorch.py</code> <pre><code>def fftbr_torch(x):\n    r\"\"\"\n    Torch implementation of the 1 dimensional Bit-Reversed-Order (BRO) Fast Fourier Transform (FFT) along the last dimension. \n    Requires the last dimension of x is already in BRO, so we can skip the first step of the decimation-in-time FFT. \n    Requires the size of the last dimension is a power of 2. \n\n    Examples:\n        &gt;&gt;&gt; rng = np.random.Generator(np.random.SFC64(11))\n        &gt;&gt;&gt; x = torch.from_numpy(rng.random(8)+1j*rng.random(8)).to(torch.complex64).requires_grad_()\n        &gt;&gt;&gt; y = fftbr_torch(x)\n        &gt;&gt;&gt; with torch._tensor_str.printoptions(precision=2):\n        ...     y.detach()\n        tensor([ 1.07+1.14j, -0.32+0.26j, -0.27+0.22j, -0.27-0.34j,  0.10+0.16j,\n                -0.15+0.17j,  0.50+0.24j,  0.06-0.02j])\n        &gt;&gt;&gt; v = torch.abs(torch.sum(y**2))\n        &gt;&gt;&gt; dvdx = torch.autograd.grad(v,x)[0]\n        &gt;&gt;&gt; with torch._tensor_str.printoptions(precision=2):\n        ...     x.detach()\n        tensor([0.25+0.65j, 0.73+0.60j, 0.06+0.20j, 0.61+0.39j, 0.45+0.06j, 0.26+0.09j,\n                0.35+0.39j, 0.32+0.85j])\n        &gt;&gt;&gt; print(\"%.4f\"%v.detach())\n        2.5584\n        &gt;&gt;&gt; with torch._tensor_str.printoptions(precision=2):\n        ...     dvdx.detach()\n        tensor([1.30+0.49j, 1.21+1.45j, 0.79+1.22j, 0.41+0.11j, 1.71+0.61j, 0.79+0.69j,\n                0.19+0.51j, 0.12+0.90j])\n        &gt;&gt;&gt; fftbr_torch(torch.rand((2,3,4,5,8))+1j*torch.rand((2,3,4,5,8))).shape\n        torch.Size([2, 3, 4, 5, 8])\n\n    Args:\n        x (torch.Tensor): Array of samples at which to run BRO-FFT.\n\n    Returns:\n        y (torch.Tensor): BRO-FFT values.\n    \"\"\"\n    n = x.size(-1)\n    assert n&amp;(n-1)==0 # require n is a power of 2\n    m = int(np.log2(n))\n    shape = list(x.shape)\n    ndim = x.ndim\n    twos = [2]*m\n    xs = x.reshape(shape[:-1]+twos)\n    pdims = tuple(itertools.chain(range(ndim-1),range(m+ndim-2,ndim-2,-1)))#[i for i in range(ndim-1)]+[i+ndim-1 for i in range(m-1,-1,-1)]\n    xrf = torch.permute(xs,pdims)\n    xr = xrf.contiguous().view(shape)\n    return torch.fft.fft(xr,norm=\"ortho\")\n</code></pre>"},{"location":"api/fast_transforms/#ifftbr_torch","title":"<code>ifftbr_torch</code>","text":"<p>Torch implementation of the 1 dimensional Bit-Reversed-Order (BRO) Inverse Fast Fourier Transform (IFFT) along the last dimension. Outputs an array in bit-reversed order, so we can skip the last step of the decimation-in-time IFFT. Requires the size of the last dimension is a power of 2. </p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; rng = np.random.Generator(np.random.SFC64(11))\n&gt;&gt;&gt; x = torch.from_numpy(rng.random(8)+1j*rng.random(8)).to(torch.complex64).requires_grad_()\n&gt;&gt;&gt; y = ifftbr_torch(x)\n&gt;&gt;&gt; with torch._tensor_str.printoptions(precision=2):\n...     y.detach()\ntensor([ 1.07+1.14j, -0.29-0.22j,  0.30+0.06j, -0.09+0.02j,  0.03+0.54j,\n        -0.04-0.33j, -0.19+0.26j, -0.08+0.36j])\n&gt;&gt;&gt; v = torch.abs(torch.sum(y**2))\n&gt;&gt;&gt; dvdx = torch.autograd.grad(v,x)[0]\n&gt;&gt;&gt; with torch._tensor_str.printoptions(precision=2):\n...     x.detach()\ntensor([0.25+0.65j, 0.73+0.60j, 0.06+0.20j, 0.61+0.39j, 0.45+0.06j, 0.26+0.09j,\n        0.35+0.39j, 0.32+0.85j])\n&gt;&gt;&gt; print(\"%.4f\"%v.detach())\n2.5656\n&gt;&gt;&gt; with torch._tensor_str.printoptions(precision=2):\n...     dvdx.detach()\ntensor([ 1.15+0.79j,  1.51+1.01j,  0.60+0.86j,  0.06+0.54j, -0.10+0.90j,  0.47+1.37j,\n         0.37+0.20j,  0.83+1.70j])\n&gt;&gt;&gt; ifftbr_torch(torch.rand((2,3,4,5,8))+1j*torch.rand((2,3,4,5,8))).shape\ntorch.Size([2, 3, 4, 5, 8])\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Array of samples at which to run BRO-IFFT.</p> required <p>Returns:</p> Name Type Description <code>y</code> <code>Tensor</code> <p>BRO-IFFT values.</p> Source code in <code>qmcpy/fast_transform/ft_pytorch.py</code> <pre><code>def ifftbr_torch(x):\n    r\"\"\"\n    Torch implementation of the 1 dimensional Bit-Reversed-Order (BRO) Inverse Fast Fourier Transform (IFFT) along the last dimension.  \n    Outputs an array in bit-reversed order, so we can skip the last step of the decimation-in-time IFFT.  \n    Requires the size of the last dimension is a power of 2. \n\n    Examples:\n        &gt;&gt;&gt; rng = np.random.Generator(np.random.SFC64(11))\n        &gt;&gt;&gt; x = torch.from_numpy(rng.random(8)+1j*rng.random(8)).to(torch.complex64).requires_grad_()\n        &gt;&gt;&gt; y = ifftbr_torch(x)\n        &gt;&gt;&gt; with torch._tensor_str.printoptions(precision=2):\n        ...     y.detach()\n        tensor([ 1.07+1.14j, -0.29-0.22j,  0.30+0.06j, -0.09+0.02j,  0.03+0.54j,\n                -0.04-0.33j, -0.19+0.26j, -0.08+0.36j])\n        &gt;&gt;&gt; v = torch.abs(torch.sum(y**2))\n        &gt;&gt;&gt; dvdx = torch.autograd.grad(v,x)[0]\n        &gt;&gt;&gt; with torch._tensor_str.printoptions(precision=2):\n        ...     x.detach()\n        tensor([0.25+0.65j, 0.73+0.60j, 0.06+0.20j, 0.61+0.39j, 0.45+0.06j, 0.26+0.09j,\n                0.35+0.39j, 0.32+0.85j])\n        &gt;&gt;&gt; print(\"%.4f\"%v.detach())\n        2.5656\n        &gt;&gt;&gt; with torch._tensor_str.printoptions(precision=2):\n        ...     dvdx.detach()\n        tensor([ 1.15+0.79j,  1.51+1.01j,  0.60+0.86j,  0.06+0.54j, -0.10+0.90j,  0.47+1.37j,\n                 0.37+0.20j,  0.83+1.70j])\n        &gt;&gt;&gt; ifftbr_torch(torch.rand((2,3,4,5,8))+1j*torch.rand((2,3,4,5,8))).shape\n        torch.Size([2, 3, 4, 5, 8])\n\n    Args:\n        x (torch.Tensor): Array of samples at which to run BRO-IFFT.\n\n    Returns:\n        y (torch.Tensor): BRO-IFFT values.\n    \"\"\"\n    n = x.size(-1)\n    assert n&amp;(n-1)==0 # require n is a power of 2\n    m = int(np.log2(n))\n    shape = list(x.shape)\n    ndim = x.ndim\n    twos = [2]*m\n    x = torch.fft.ifft(x,norm=\"ortho\")\n    xs = x.reshape(shape[:-1]+twos)\n    pdims = tuple(itertools.chain(range(ndim-1),range(m+ndim-2,ndim-2,-1)))\n    xrf = torch.permute(xs,pdims)\n    xr = xrf.contiguous().view(shape)\n    return xr\n</code></pre>"},{"location":"api/fast_transforms/#omega_fwht_1","title":"<code>omega_fwht</code>","text":"<p>Torch implementation useful when efficiently updating FWHT values after doubling the sample size.  </p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; rng = np.random.Generator(np.random.SFC64(11))\n&gt;&gt;&gt; m = 3\n&gt;&gt;&gt; x1 = torch.from_numpy(rng.random((3,5,7,2**m)))\n&gt;&gt;&gt; x2 = torch.from_numpy(rng.random((3,5,7,2**m)))\n&gt;&gt;&gt; x = torch.cat([x1,x2],axis=-1)\n&gt;&gt;&gt; ytrue = fwht_torch(x)\n&gt;&gt;&gt; omega = omega_fwht_torch(m)\n&gt;&gt;&gt; y1 = fwht_torch(x1) \n&gt;&gt;&gt; y2 = fwht_torch(x2) \n&gt;&gt;&gt; y = torch.cat([y1+omega*y2,y1-omega*y2],axis=-1)/np.sqrt(2)\n&gt;&gt;&gt; np.allclose(y,ytrue)\nTrue\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>m</code> <code>int</code> <p>Size \\(2^m\\) output. </p> required <p>Returns:</p> Name Type Description <code>y</code> <code>ndarray</code> <p>\\(\\left(1\\right)_{k=0}^{2^m}\\).</p> Source code in <code>qmcpy/fast_transform/ft_pytorch.py</code> <pre><code>def omega_fwht_torch(m, device=None):\n    r\"\"\"\n    Torch implementation useful when efficiently updating FWHT values after doubling the sample size.  \n\n    Examples:\n        &gt;&gt;&gt; rng = np.random.Generator(np.random.SFC64(11))\n        &gt;&gt;&gt; m = 3\n        &gt;&gt;&gt; x1 = torch.from_numpy(rng.random((3,5,7,2**m)))\n        &gt;&gt;&gt; x2 = torch.from_numpy(rng.random((3,5,7,2**m)))\n        &gt;&gt;&gt; x = torch.cat([x1,x2],axis=-1)\n        &gt;&gt;&gt; ytrue = fwht_torch(x)\n        &gt;&gt;&gt; omega = omega_fwht_torch(m)\n        &gt;&gt;&gt; y1 = fwht_torch(x1) \n        &gt;&gt;&gt; y2 = fwht_torch(x2) \n        &gt;&gt;&gt; y = torch.cat([y1+omega*y2,y1-omega*y2],axis=-1)/np.sqrt(2)\n        &gt;&gt;&gt; np.allclose(y,ytrue)\n        True\n\n    Args:\n        m (int): Size $2^m$ output. \n\n    Returns:\n        y (np.ndarray): $\\left(1\\right)_{k=0}^{2^m}$. \n    \"\"\"\n    if device is None: device = \"cpu\"\n    return torch.ones(2**m,device=device)\n</code></pre>"},{"location":"api/fast_transforms/#omega_fftbr_torch","title":"<code>omega_fftbr_torch</code>","text":"<p>Torch implementation useful when efficiently updating FFT values after doubling the sample size. </p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; rng = np.random.Generator(np.random.SFC64(11))\n&gt;&gt;&gt; m = 3\n&gt;&gt;&gt; x1 = torch.from_numpy(rng.random((3,5,7,2**m))+1j*rng.random((3,5,7,2**m)))\n&gt;&gt;&gt; x2 = torch.from_numpy(rng.random((3,5,7,2**m))+1j*rng.random((3,5,7,2**m)))\n&gt;&gt;&gt; x = torch.cat([x1,x2],axis=-1)\n&gt;&gt;&gt; ytrue = fftbr_torch(x)\n&gt;&gt;&gt; omega = omega_fftbr_torch(m)\n&gt;&gt;&gt; y1 = fftbr_torch(x1) \n&gt;&gt;&gt; y2 = fftbr_torch(x2) \n&gt;&gt;&gt; y = torch.cat([y1+omega*y2,y1-omega*y2],axis=-1)/np.sqrt(2)\n&gt;&gt;&gt; np.allclose(y,ytrue)\nTrue\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>m</code> <code>int</code> <p>Size \\(2^m\\) output. </p> required <p>Returns:</p> Name Type Description <code>y</code> <code>ndarray</code> <p>\\(\\left(e^{- \\pi \\mathrm{i} k / 2^m}\\right)_{k=0}^{2^m}\\).</p> Source code in <code>qmcpy/fast_transform/ft_pytorch.py</code> <pre><code>def omega_fftbr_torch(m, device=None):\n    r\"\"\"\n    Torch implementation useful when efficiently updating FFT values after doubling the sample size. \n\n    Examples:\n        &gt;&gt;&gt; rng = np.random.Generator(np.random.SFC64(11))\n        &gt;&gt;&gt; m = 3\n        &gt;&gt;&gt; x1 = torch.from_numpy(rng.random((3,5,7,2**m))+1j*rng.random((3,5,7,2**m)))\n        &gt;&gt;&gt; x2 = torch.from_numpy(rng.random((3,5,7,2**m))+1j*rng.random((3,5,7,2**m)))\n        &gt;&gt;&gt; x = torch.cat([x1,x2],axis=-1)\n        &gt;&gt;&gt; ytrue = fftbr_torch(x)\n        &gt;&gt;&gt; omega = omega_fftbr_torch(m)\n        &gt;&gt;&gt; y1 = fftbr_torch(x1) \n        &gt;&gt;&gt; y2 = fftbr_torch(x2) \n        &gt;&gt;&gt; y = torch.cat([y1+omega*y2,y1-omega*y2],axis=-1)/np.sqrt(2)\n        &gt;&gt;&gt; np.allclose(y,ytrue)\n        True\n\n    Args:\n        m (int): Size $2^m$ output. \n\n    Returns:\n        y (np.ndarray): $\\left(e^{- \\pi \\mathrm{i} k / 2^m}\\right)_{k=0}^{2^m}$. \n    \"\"\"\n    if device is None: device = \"cpu\"\n    return torch.exp(-torch.pi*1j*torch.arange(2**m,device=device)/2**m)\n</code></pre>"},{"location":"api/integrands/","title":"Integrands","text":""},{"location":"api/integrands/#uml-overview","title":"UML Overview","text":""},{"location":"api/integrands/#abstractintegrand","title":"<code>AbstractIntegrand</code>","text":"<p>               Bases: <code>object</code></p> <p>Parameters:</p> Name Type Description Default <code>dimension_indv</code> <code>tuple</code> <p>Individual solution shape.</p> required <code>dimension_comb</code> <code>tuple</code> <p>Combined solution shape. </p> required <code>parallel</code> <code>int</code> <p>Parallelization flag. </p> <ul> <li>When <code>parallel = 0</code> or <code>parallel = 1</code> then function evaluation is done in serial fashion.</li> <li><code>parallel &gt; 1</code> specifies the number of processes used by <code>multiprocessing.Pool</code> or <code>multiprocessing.pool.ThreadPool</code>.</li> </ul> <p>Setting <code>parallel=True</code> is equivalent to <code>parallel = os.cpu_count()</code>.</p> required <code>threadpool</code> <code>bool</code> <p>When <code>parallel &gt; 1</code>: </p> <ul> <li>Setting <code>threadpool = True</code> will use <code>multiprocessing.pool.ThreadPool</code>.</li> <li>Setting <code>threadpool = False</code> will use <code>setting multiprocessing.Pool</code>.</li> </ul> <code>False</code> Source code in <code>qmcpy/integrand/abstract_integrand.py</code> <pre><code>def __init__(self, dimension_indv, dimension_comb, parallel, threadpool=False):\n    r\"\"\"\n    Args:\n        dimension_indv (tuple): Individual solution shape.\n        dimension_comb (tuple): Combined solution shape. \n        parallel (int): Parallelization flag. \n\n            - When `parallel = 0` or `parallel = 1` then function evaluation is done in serial fashion.\n            - `parallel &gt; 1` specifies the number of processes used by `multiprocessing.Pool` or `multiprocessing.pool.ThreadPool`.\n\n            Setting `parallel=True` is equivalent to `parallel = os.cpu_count()`.\n        threadpool (bool): When `parallel &gt; 1`: \n\n            - Setting `threadpool = True` will use `multiprocessing.pool.ThreadPool`.\n            - Setting `threadpool = False` will use `setting multiprocessing.Pool`.\n    \"\"\"\n    prefix = 'A concrete implementation of AbstractIntegrand must have '\n    self.d = self.true_measure.d\n    self.d_indv = (dimension_indv,) if isinstance(dimension_indv,int) else tuple(dimension_indv)\n    self.d_comb = (dimension_comb,) if isinstance(dimension_comb,int) else tuple(dimension_comb)\n    cpus = os.cpu_count()\n    self.parallel = cpus if parallel is True else int(parallel)\n    self.parallel = 0 if self.parallel==1 else self.parallel\n    self.threadpool = threadpool\n    if not (hasattr(self, 'sampler') and isinstance(self.sampler,(AbstractTrueMeasure,AbstractDiscreteDistribution))):\n        raise ParameterError(prefix + 'self.sampler, a AbstractTrueMeasure or AbstractDiscreteDistribution instance')\n    if not (hasattr(self, 'true_measure') and isinstance(self.true_measure,AbstractTrueMeasure)):\n        raise ParameterError(prefix + 'self.true_measure, a AbstractTrueMeasure instance')\n    if not hasattr(self,'parameters'):\n        self.parameters = []\n    if not hasattr(self,'multilevel'):\n        self.multilevel = False\n    assert isinstance(self.multilevel,bool)\n    if not hasattr(self,'max_level'):\n        self.max_level = np.inf\n    if not hasattr(self,'discrete_distrib'):\n        self.discrete_distrib = self.true_measure.discrete_distrib\n    if self.true_measure.transform!=self.true_measure and \\\n       not (self.true_measure.range==self.true_measure.transform.range).all():\n        raise ParameterError(\"The range of the composed transform is not compatible with this true measure\")\n    self.EPS = np.finfo(float).eps\n</code></pre>"},{"location":"api/integrands/#qmcpy.integrand.abstract_integrand.AbstractIntegrand.__call__","title":"__call__","text":"<pre><code>__call__(n=None, n_min=None, n_max=None, warn=True)\n</code></pre> <ul> <li>If just <code>n</code> is supplied, generate samples from the sequence at indices 0,...,<code>n</code>-1.</li> <li>If <code>n_min</code> and <code>n_max</code> are supplied, generate samples from the sequence at indices <code>n_min</code>,...,<code>n_max</code>-1.</li> <li>If <code>n</code> and <code>n_min</code> are supplied, then generate samples from the sequence at indices <code>n</code>,...,<code>n_min</code>-1.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>Union[None, int]</code> <p>Number of points to generate.</p> <code>None</code> <code>n_min</code> <code>Union[None, int]</code> <p>Starting index of sequence.</p> <code>None</code> <code>n_max</code> <code>Union[None, int]</code> <p>Final index of sequence.</p> <code>None</code> <code>warn</code> <code>bool</code> <p>If <code>False</code>, disable warnings when generating samples.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>t</code> <code>ndarray</code> <p>Samples from the sequence. </p> <ul> <li>If <code>replications</code> is <code>None</code> then this will be of size (<code>n_max</code>-<code>n_min</code>) \\(\\times\\) <code>dimension</code> </li> <li>If <code>replications</code> is a positive int, then <code>t</code> will be of size <code>replications</code> \\(\\times\\) (<code>n_max</code>-<code>n_min</code>) \\(\\times\\) <code>dimension</code> </li> </ul> <code>weights</code> <code>ndarray</code> <p>Only returned when <code>return_weights=True</code>. The Jacobian weights for the transformation</p> Source code in <code>qmcpy/integrand/abstract_integrand.py</code> <pre><code>def __call__(self, n=None, n_min=None, n_max=None, warn=True):\n    r\"\"\"\n    - If just `n` is supplied, generate samples from the sequence at indices 0,...,`n`-1.\n    - If `n_min` and `n_max` are supplied, generate samples from the sequence at indices `n_min`,...,`n_max`-1.\n    - If `n` and `n_min` are supplied, then generate samples from the sequence at indices `n`,...,`n_min`-1.\n\n    Args:\n        n (Union[None,int]): Number of points to generate.\n        n_min (Union[None,int]): Starting index of sequence.\n        n_max (Union[None,int]): Final index of sequence.\n        warn (bool): If `False`, disable warnings when generating samples.\n\n    Returns:\n        t (np.ndarray): Samples from the sequence. \n\n            - If `replications` is `None` then this will be of size (`n_max`-`n_min`) $\\times$ `dimension` \n            - If `replications` is a positive int, then `t` will be of size `replications` $\\times$ (`n_max`-`n_min`) $\\times$ `dimension` \n        weights (np.ndarray): Only returned when `return_weights=True`. The Jacobian weights for the transformation\n    \"\"\"\n    return self.gen_samples(n=n,n_min=n_min,n_max=n_max,warn=warn)\n</code></pre>"},{"location":"api/integrands/#qmcpy.integrand.abstract_integrand.AbstractIntegrand.g","title":"g","text":"<pre><code>g(t, *args, **kwargs)\n</code></pre> <p>Abstract method implementing the integrand as a function of the true measure.</p> <p>Parameters:</p> Name Type Description Default <code>t</code> <code>ndarray</code> <p>Inputs with shape <code>(*batch_shape, d)</code>.</p> required <code>args</code> <code>tuple</code> <p>positional arguments to <code>g</code>.</p> <code>()</code> <code>kwargs</code> <code>dict</code> <p>keyword arguments to <code>g</code>. </p> <p>Some algorithms will additionally try to pass in a <code>compute_flags</code> keyword argument.  This <code>np.ndarray</code> are flags indicating which outputs require evaluation. For example, if the vector function has 3 outputs and <code>compute_flags = [False, True, False]</code>,  then the function is only required to evaluate the second output and may leave the remaining outputs as <code>np.nan</code> values,  i.e., the outputs corresponding to <code>compute_flags</code> which are <code>False</code> will not be used in the computation.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>y</code> <code>ndarray</code> <p>function evaluations with shape <code>(*batch_shape, *dimension_indv)</code> where <code>dimension_indv</code> is the shape of the function outputs.</p> Source code in <code>qmcpy/integrand/abstract_integrand.py</code> <pre><code>def g(self, t, *args, **kwargs):\n    r\"\"\"\n    *Abstract method* implementing the integrand as a function of the true measure.\n\n    Args:\n        t (np.ndarray): Inputs with shape `(*batch_shape, d)`.\n        args (tuple): positional arguments to `g`.\n        kwargs (dict): keyword arguments to `g`. \n\n            Some algorithms will additionally try to pass in a `compute_flags` keyword argument. \n            This `np.ndarray` are flags indicating which outputs require evaluation.  \n            For example, if the vector function has 3 outputs and `compute_flags = [False, True, False]`, \n            then the function is only required to evaluate the second output and may leave the remaining outputs as `np.nan` values, \n            i.e., the outputs corresponding to `compute_flags` which are `False` will not be used in the computation.\n\n    Returns:\n        y (np.ndarray): function evaluations with shape `(*batch_shape, *dimension_indv)` where `dimension_indv` is the shape of the function outputs. \n    \"\"\"\n    raise MethodImplementationError(self, 'g')\n</code></pre>"},{"location":"api/integrands/#qmcpy.integrand.abstract_integrand.AbstractIntegrand.f","title":"f","text":"<pre><code>f(x, *args, **kwargs)\n</code></pre> <p>Function to evaluate the transformed integrand as a function of the discrete distribution. Automatically applies the transformation determined by the true measure. </p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ndarray</code> <p>Inputs with shape <code>(*batch_shape, d)</code>.</p> required <code>args</code> <code>tuple</code> <p>positional arguments to <code>g</code>.</p> <code>()</code> <code>kwargs</code> <code>dict</code> <p>keyword arguments to <code>g</code>. </p> <p>Some algorithms will additionally try to pass in a <code>compute_flags</code> keyword argument.  This <code>np.ndarray</code> are flags indicating which outputs require evaluation. For example, if the vector function has 3 outputs and <code>compute_flags = [False, True, False]</code>,  then the function is only required to evaluate the second output and may leave the remaining outputs as <code>np.nan</code> values,  i.e., the outputs corresponding to <code>compute_flags</code> which are <code>False</code> will not be used in the computation.</p> <p>The keyword argument <code>periodization_transform</code>, a string, specifies a periodization transform.  Options are: </p> <ul> <li><code>False</code>: No periodizing transform, \\(\\psi(x) = x\\). </li> <li><code>'BAKER'</code>: Baker tansform \\(\\psi(x) = 1-2\\lvert x-1/2 \\rvert\\).</li> <li><code>'C0'</code>: \\(C^0\\) transform \\(\\psi(x) = 3x^2-2x^3\\).</li> <li><code>'C1'</code>: \\(C^1\\) transform \\(\\psi(x) = x^3(10-15x+6x^2)\\).</li> <li><code>'C1SIN'</code>: Sidi \\(C^1\\) transform \\(\\psi(x) = x-\\sin(2 \\pi x)/(2 \\pi)\\). </li> <li><code>'C2SIN'</code>: Sidi \\(C^2\\) transform \\(\\psi(x) = (8-9 \\cos(\\pi x)+\\cos(3 \\pi x))/16\\).</li> <li><code>'C3SIN'</code>: Sidi \\(C^3\\) transform \\(\\psi(x) = (12\\pi x-8\\sin(2 \\pi x) + \\sin(4 \\pi x))/(12 \\pi)\\).</li> </ul> <code>{}</code> <p>Returns:</p> Name Type Description <code>y</code> <code>ndarray</code> <p>function evaluations with shape <code>(*batch_shape, *dimension_indv)</code> where <code>dimension_indv</code> is the shape of the function outputs.</p> Source code in <code>qmcpy/integrand/abstract_integrand.py</code> <pre><code>def f(self, x, *args, **kwargs):\n    r\"\"\"\n    Function to evaluate the transformed integrand as a function of the discrete distribution.  \n    Automatically applies the transformation determined by the true measure. \n\n    Args:\n        x (np.ndarray): Inputs with shape `(*batch_shape, d)`.\n        args (tuple): positional arguments to `g`.\n        kwargs (dict): keyword arguments to `g`. \n\n            Some algorithms will additionally try to pass in a `compute_flags` keyword argument. \n            This `np.ndarray` are flags indicating which outputs require evaluation.  \n            For example, if the vector function has 3 outputs and `compute_flags = [False, True, False]`, \n            then the function is only required to evaluate the second output and may leave the remaining outputs as `np.nan` values, \n            i.e., the outputs corresponding to `compute_flags` which are `False` will not be used in the computation.\n\n            The keyword argument `periodization_transform`, a string, specifies a periodization transform. \n            Options are: \n\n            - `False`: No periodizing transform, $\\psi(x) = x$. \n            - `'BAKER'`: Baker tansform $\\psi(x) = 1-2\\lvert x-1/2 \\rvert$.\n            - `'C0'`: $C^0$ transform $\\psi(x) = 3x^2-2x^3$.\n            - `'C1'`: $C^1$ transform $\\psi(x) = x^3(10-15x+6x^2)$.\n            - `'C1SIN'`: Sidi $C^1$ transform $\\psi(x) = x-\\sin(2 \\pi x)/(2 \\pi)$. \n            - `'C2SIN'`: Sidi $C^2$ transform $\\psi(x) = (8-9 \\cos(\\pi x)+\\cos(3 \\pi x))/16$.\n            - `'C3SIN'`: Sidi $C^3$ transform $\\psi(x) = (12\\pi x-8\\sin(2 \\pi x) + \\sin(4 \\pi x))/(12 \\pi)$.\n\n    Returns:\n        y (np.ndarray): function evaluations with shape `(*batch_shape, *dimension_indv)` where `dimension_indv` is the shape of the function outputs. \n    \"\"\"\n    if \"periodization_transform\" in kwargs:\n        periodization_transform = kwargs[\"periodization_transform\"]\n        del kwargs[\"periodization_transform\"]\n    else:\n        periodization_transform = \"None\"\n    periodization_transform = str(periodization_transform).upper()\n    batch_shape = tuple(x.shape[:-1])\n    d_indv_ndim = len(self.d_indv)\n    if periodization_transform==\"NONE\": periodization_transform = \"FALSE\"\n    if self.discrete_distrib.mimics != 'StdUniform' and periodization_transform!='NONE':\n        raise ParameterError('''\n            Applying a periodization transform currently requires a discrete distribution \n            that mimics a standard uniform measure.''')\n    if periodization_transform == 'FALSE':\n        xp = x\n        wp = np.ones(batch_shape,dtype=float)\n    elif periodization_transform == 'BAKER':\n        xp = 1-2*abs(x-1/2)\n        wp = np.ones(batch_shape,dtype=float)\n    elif periodization_transform == 'C0':\n        xp = 3*x**2-2*x**3\n        wp = np.prod(6*x*(1-x),-1)\n    elif periodization_transform == 'C1':\n        xp = x**3*(10-15*x+6*x**2)\n        wp = np.prod(30*x**2*(1-x)**2,-1)\n    elif periodization_transform == 'C1SIN':\n        xp = x - np.sin(2*np.pi*x)/(2*np.pi)\n        wp = np.prod(2*np.sin(np.pi*x)**2,-1)\n    elif periodization_transform == 'C2SIN':\n        xp = (8-9*np.cos(np.pi*x)+np.cos(3*np.pi*x))/16\n        wp = np.prod((9*np.sin(np.pi*x)*np.pi-np.sin(3*np.pi*x)*3*np.pi)/16,-1)\n    elif periodization_transform=='C3SIN':\n        xp = (12*np.pi*x-8*np.sin(2*np.pi*x)+np.sin(4*np.pi*x))/(12*np.pi)\n        wp = np.prod((12*np.pi-8*np.cos(2*np.pi*x)*2*np.pi+np.sin(4*np.pi*x)*4*np.pi)/(12*np.pi),-1)\n    else:\n        raise ParameterError(\"The %s periodization transform is not implemented\"%periodization_transform)\n    if periodization_transform in ['C1','C1SIN','C2SIN','C3SIN']:\n        xp[xp&lt;=0] = self.EPS\n        xp[xp&gt;=1] = 1-self.EPS\n    assert wp.shape==batch_shape\n    assert xp.shape==x.shape\n    # function evaluation with chain rule\n    i = (None,)*d_indv_ndim+(...,)\n    if self.true_measure==self.true_measure.transform:\n        # jacobian*weight/pdf will cancel so f(x) = g(\\Psi(x))\n        xtf = self.true_measure._jacobian_transform_r(xp,return_weights=False) # get transformed samples, equivalent to self.true_measure._transform_r(x)\n        assert xtf.shape==xp.shape\n        y = self._g(xtf,*args,**kwargs)\n    else: # using importance sampling --&gt; need to compute pdf, jacobian(s), and weight explicitly\n        pdf = self.discrete_distrib.pdf(xp) # pdf of samples\n        assert pdf.shape==batch_shape\n        xtf,jacobians = self.true_measure.transform._jacobian_transform_r(xp,return_weights=True) # compute recursive transform+jacobian\n        assert xtf.shape==xp.shape \n        assert jacobians.shape==batch_shape\n        weight = self.true_measure._weight(xtf) # weight based on the true measure\n        assert weight.shape==batch_shape\n        gvals = self._g(xtf,*args,**kwargs)\n        assert gvals.shape==(self.d_indv+batch_shape)\n        y = gvals*weight[i]/pdf[i]*jacobians[i]\n    assert y.shape==(self.d_indv+batch_shape)\n    # account for periodization weight\n    y = y*wp[i]\n    assert y.shape==(self.d_indv+batch_shape)\n    return y\n</code></pre>"},{"location":"api/integrands/#qmcpy.integrand.abstract_integrand.AbstractIntegrand.bound_fun","title":"bound_fun","text":"<pre><code>bound_fun(bound_low, bound_high)\n</code></pre> <p>Compute the bounds on the combined function based on bounds for the individual functions.  </p> <p>Defaults to the identity where we essentially do not combine integrands, but instead integrate each function individually.</p> <p>Parameters:</p> Name Type Description Default <code>bound_low</code> <code>ndarray</code> <p>Lower bounds on individual estimates with shape <code>integrand.d_indv</code>.</p> required <code>bound_high</code> <code>ndarray</code> <p>Upper bounds on individual estimates with shape <code>integrand.d_indv</code>.</p> required <p>Returns:</p> Name Type Description <code>comb_bound_low</code> <code>ndarray</code> <p>Lower bounds on combined estimates with shape <code>integrand.d_comb</code>.</p> <code>comb_bound_high</code> <code>ndarray</code> <p>Upper bounds on combined estimates with shape <code>integrand.d_comb</code>.</p> Source code in <code>qmcpy/integrand/abstract_integrand.py</code> <pre><code>def bound_fun(self, bound_low, bound_high):\n    \"\"\"\n    Compute the bounds on the combined function based on bounds for the\n    individual functions.  \n\n    Defaults to the identity where we essentially\n    do not combine integrands, but instead integrate each function\n    individually.\n\n    Args:\n        bound_low (np.ndarray): Lower bounds on individual estimates with shape `integrand.d_indv`.\n        bound_high (np.ndarray): Upper bounds on individual estimates with shape `integrand.d_indv`.\n\n    Returns:\n        comb_bound_low (np.ndarray): Lower bounds on combined estimates with shape `integrand.d_comb`.\n        comb_bound_high (np.ndarray): Upper bounds on combined estimates with shape `integrand.d_comb`.\n    \"\"\"\n    if self.d_indv!=self.d_comb:\n        raise ParameterError('''\n            Set bound_fun explicitly. \n            The default bound_fun is the identity map. \n            Since the individual solution dimensions d_indv = %s does not equal the combined solution dimensions d_comb = %d, \n            QMCPy cannot infer a reasonable bound function.'''%(str(self.d_indv),str(self.d_comb)))\n    return bound_low,bound_high\n</code></pre>"},{"location":"api/integrands/#qmcpy.integrand.abstract_integrand.AbstractIntegrand.dependency","title":"dependency","text":"<pre><code>dependency(comb_flags)\n</code></pre> <p>Takes a vector of indicators of weather of not the error bound is satisfied for combined integrands and returns flags for individual integrands.  </p> <p>For example, if we are taking the ratio of 2 individual integrands, then getting <code>comb_flags=True</code> means the ratio has not been approximated to within the tolerance, so the dependency function should return <code>indv_flags=[True,True]</code> indicating that both the numerator and denominator integrands need to be better approximated.</p> <p>Parameters:</p> Name Type Description Default <code>comb_flags</code> <code>ndarray</code> <p>Flags of shape <code>integrand.d_comb</code> indicating whether the combined outputs are insufficiently approximated.</p> required <p>Returns:</p> Name Type Description <code>indv_flags</code> <code>ndarray</code> <p>Flags of shape <code>integrand.d_indv</code> indicating whether the individual integrands require additional sampling.</p> Source code in <code>qmcpy/integrand/abstract_integrand.py</code> <pre><code>def dependency(self, comb_flags):\n    \"\"\"\n    Takes a vector of indicators of weather of not the error bound is satisfied for combined integrands and returns flags for individual integrands.  \n\n    For example, if we are taking the ratio of 2 individual integrands, then getting `comb_flags=True` means the ratio\n    has not been approximated to within the tolerance, so the dependency function should return `indv_flags=[True,True]`\n    indicating that both the numerator and denominator integrands need to be better approximated.\n\n    Args:\n        comb_flags (np.ndarray): Flags of shape `integrand.d_comb` indicating whether the combined outputs are insufficiently approximated.\n\n    Returns:\n        indv_flags (np.ndarray): Flags of shape `integrand.d_indv` indicating whether the individual integrands require additional sampling. \n    \"\"\"\n    return comb_flags if self.d_indv==self.d_comb else np.tile((comb_flags==False).any(),self.d_indv)\n</code></pre>"},{"location":"api/integrands/#qmcpy.integrand.abstract_integrand.AbstractIntegrand.spawn","title":"spawn","text":"<pre><code>spawn(levels)\n</code></pre> <p>Spawn new instances of the current integrand at different levels with new seeds. Used by multi-level QMC algorithms which require integrands at multiple levels.</p> Note <p>Use <code>replications</code> instead of using <code>spawn</code> when possible, e.g., when spawning copies which all have the same level.</p> <p>Parameters:</p> Name Type Description Default <code>levels</code> <code>ndarray</code> <p>Levels at which to spawn new integrands.</p> required <p>Returns:</p> Name Type Description <code>spawned_integrand</code> <code>list</code> <p>Integrands with new true measures and discrete distributions.</p> Source code in <code>qmcpy/integrand/abstract_integrand.py</code> <pre><code>def spawn(self, levels):\n    r\"\"\"\n    Spawn new instances of the current integrand at different levels with new seeds.\n    Used by multi-level QMC algorithms which require integrands at multiple levels.\n\n    Note:\n        Use `replications` instead of using `spawn` when possible, e.g., when spawning copies which all have the same level.\n\n    Args:\n        levels (np.ndarray): Levels at which to spawn new integrands.\n\n    Returns:\n        spawned_integrand (list): Integrands with new true measures and discrete distributions.\n    \"\"\"\n    levels = np.array([levels]) if np.isscalar(levels) else np.array(levels)\n    if (levels&gt;self.max_level).any():\n        raise ParameterError(\"requested spawn level exceeds max level\")\n    n_levels = len(levels)\n    new_dims = np.array([self.dimension_at_level(level) for level in levels])\n    tm_spawns = self.sampler.spawn(s=n_levels,dimensions=new_dims) \n    spawned_integrand = [None]*n_levels\n    for l,level in enumerate(levels):\n        spawned_integrand[l] = self._spawn(level,tm_spawns[l])\n    return spawned_integrand\n</code></pre>"},{"location":"api/integrands/#qmcpy.integrand.abstract_integrand.AbstractIntegrand.dimension_at_level","title":"dimension_at_level","text":"<pre><code>dimension_at_level(level)\n</code></pre> <p>Abstract method which returns the dimension of the generator required for a given level.</p> Note <p>Only used for multilevel problems.</p> <p>Parameters:</p> Name Type Description Default <code>level</code> <code>int</code> <p>Level at which to return the dimension.</p> required <p>Returns:</p> Name Type Description <code>d</code> <code>int</code> <p>Dimension at the given input level.</p> Source code in <code>qmcpy/integrand/abstract_integrand.py</code> <pre><code>def dimension_at_level(self, level):\n    \"\"\"\n    *Abstract method* which returns the dimension of the generator required for a given level.\n\n    Note:\n        Only used for multilevel problems.\n\n    Args:\n        level (int): Level at which to return the dimension.\n\n    Returns:\n        d (int): Dimension at the given input level. \n    \"\"\"\n    return self.d\n</code></pre>"},{"location":"api/integrands/#financialoption","title":"<code>FinancialOption</code>","text":"<p>               Bases: <code>AbstractIntegrand</code></p> <p>Financial options.</p> <ul> <li>Start price \\(S_0\\)</li> <li>Strike price \\(K\\)</li> <li>Interest rate \\(r\\) </li> <li>Volatility \\(\\sigma\\)</li> <li>Equidistant monitoring times \\(\\boldsymbol{\\tau} = (\\tau_1,\\dots,\\tau_d)^T\\) with \\(\\tau_d\\) the final (exercise) time and \\(\\tau_j = \\tau_d j/d\\). </li> </ul> <p>Define the geometric brownian motion as </p> \\[\\boldsymbol{S}(\\boldsymbol{t}) = S_0 e^{(r-\\sigma^2/2)\\boldsymbol{\\tau}+\\sigma\\boldsymbol{t}}, \\qquad \\boldsymbol{T} \\sim \\mathcal{N}(\\boldsymbol{0},\\mathsf{\\Sigma})\\] <p>where \\(\\boldsymbol{T}\\) is a standard Brownian motion so \\(\\mathsf{\\Sigma} = \\left(\\min\\{\\tau_j,\\tau_{j'}\\}\\right)_{j,j'=1}^d\\).</p> <p>The discounted payoff is </p> \\[g(\\boldsymbol{t}) = P(\\boldsymbol{S}(\\boldsymbol{t}))e^{-r \\tau_d}\\] <p>where the payoff function \\(P\\) will be defined depending on the option.</p> <p>Below we wil luse \\(S_{-1}\\) to denote the final element of \\(\\boldsymbol{S}\\), the value of the path at exercise time. </p>"},{"location":"api/integrands/#qmcpy.integrand.financial_option.FinancialOption--european-options","title":"European Options","text":"<p>European Call and Put Options have respective payoffs</p> \\[P(\\boldsymbol{S}) = \\max\\{S_{-1}-K,0\\}, \\qquad P(\\boldsymbol{S}) = \\max\\{K-S_{-1},0\\}.\\]"},{"location":"api/integrands/#qmcpy.integrand.financial_option.FinancialOption--asian-options","title":"Asian Options","text":"<p>An asian option considers the average value of an asset path across time. We use the trapezoidal rule to approximate either the arithmetic mean by </p> \\[A(\\boldsymbol{S}) = \\frac{1}{d}\\left[\\frac{1}{2} S_0 + \\sum_{j=1}^{d-1} S_j + \\frac{1}{2} S_{-1}\\right]\\] <p>or the geometric mean by </p> \\[A(\\boldsymbol{S}) = \\left[\\sqrt{S_0} \\prod_{j=1}^{d-1} S_j \\sqrt{S_{-1}}\\right]^{1/d}.\\] <p>Asian Call and Put Option have respective payoffs </p> \\[P(\\boldsymbol{S}) = \\max\\{A(\\boldsymbol{S})-K,0\\}, \\qquad P(\\boldsymbol{S}) = \\max\\{K-A(\\boldsymbol{S}),0\\}.\\]"},{"location":"api/integrands/#qmcpy.integrand.financial_option.FinancialOption--barrier-options","title":"Barrier Options","text":"<ul> <li>Barrier \\(B\\). </li> </ul> <p>In options are activate when the path crosses the barrier \\(B\\), while out options are activated only if the path never crosses the barrier \\(B\\). An up option satisfies \\(S_0&lt;B\\) while a down option satisfies \\(S_0&gt;B\\), both indicating the direction of the barrier from the start price. </p> <p>Barrier Up-In Call and Put Options have respective payoffs</p> \\[P(\\boldsymbol{S}) = \\begin{cases} \\max\\{S_{-1})-K,0\\}, &amp; \\text{any } \\boldsymbol{S} \\geq B \\\\ 0, &amp; \\mathrm{otherwise} \\end{cases}, \\qquad P(\\boldsymbol{S}) = \\begin{cases} \\max\\{K-S_{-1}),0\\}, &amp; \\text{any } \\boldsymbol{S} \\geq B \\\\ 0, &amp; \\mathrm{otherwise} \\end{cases}.\\] <p>Barrier Up-Out Call and Put Options have respective payoffs </p> \\[P(\\boldsymbol{S}) = \\begin{cases} \\max\\{S_{-1})-K,0\\}, &amp; \\text{all } \\boldsymbol{S} &lt; B \\\\ 0, &amp; \\mathrm{otherwise} \\end{cases}, \\qquad P(\\boldsymbol{S}) = \\begin{cases} \\max\\{K-S_{-1}),0\\}, &amp; \\text{all } \\boldsymbol{S} &lt; B \\\\ 0, &amp; \\mathrm{otherwise} \\end{cases}.\\] <p>Barrier Down-In Call and Put Options have respective payoffs </p> \\[P(\\boldsymbol{S}) = \\begin{cases} \\max\\{S_{-1})-K,0\\}, &amp; \\text{any } \\boldsymbol{S} \\leq B \\\\ 0, &amp; \\mathrm{otherwise} \\end{cases}, \\qquad P(\\boldsymbol{S}) = \\begin{cases} \\max\\{K-S_{-1}),0\\}, &amp; \\text{any } \\boldsymbol{S} \\leq B \\\\ 0, &amp; \\mathrm{otherwise} \\end{cases}.\\] <p>Barrier Down-Out Call and Put Options have respective payoffs </p> \\[P(\\boldsymbol{S}) = \\begin{cases} \\max\\{S_{-1})-K,0\\}, &amp; \\text{all } \\boldsymbol{S} &gt; B \\\\ 0, &amp; \\mathrm{otherwise} \\end{cases}, \\qquad P(\\boldsymbol{S}) = \\begin{cases} \\max\\{K-S_{-1}),0\\}, &amp; \\text{all } \\boldsymbol{S} &gt; B \\\\ 0, &amp; \\mathrm{otherwise} \\end{cases}.\\]"},{"location":"api/integrands/#qmcpy.integrand.financial_option.FinancialOption--lookback-options","title":"Lookback Options","text":"<p>Lookback Call and Put Options have respective payoffs</p> \\[P(\\boldsymbol{S}) = S_{-1}-\\min(\\boldsymbol{S}), \\qquad P(\\boldsymbol{S}) = \\max(\\boldsymbol{S})-S_{-1}.\\]"},{"location":"api/integrands/#qmcpy.integrand.financial_option.FinancialOption--digital-option","title":"Digital Option","text":"<ul> <li>Payout \\(\\rho\\). </li> </ul> <p>Digital Call and Put Options have respective payoffs</p> \\[P(\\boldsymbol{S}) = \\begin{cases} \\rho, &amp; S_{-1} \\geq K \\\\ 0, &amp; \\mathrm{otherwise} \\end{cases}, \\qquad P(\\boldsymbol{S}) =  \\begin{cases} \\rho, &amp; S_{-1} \\leq K \\\\ 0, &amp; \\mathrm{otherwise} \\end{cases}.\\]"},{"location":"api/integrands/#qmcpy.integrand.financial_option.FinancialOption--multilevel-options","title":"Multilevel Options","text":"<ul> <li>Initial level \\(\\ell_0 \\geq 0\\). </li> <li>Level \\(\\ell \\geq \\ell_0\\).</li> </ul> <p>Let \\(\\boldsymbol{S}_\\mathrm{fine}=\\boldsymbol{S}\\) be the fine full path. For \\(\\ell&gt;\\ell_0\\) write the coarse path as \\(\\boldsymbol{S}_\\mathrm{coarse} = (S_j)_{j \\text{ even}}\\) which only considers every other element of \\(\\boldsymbol{S}\\).  In this multilevel setting the payoff is</p> \\[P_\\ell(\\boldsymbol{S}) = \\begin{cases} P(\\boldsymbol{S}_\\mathrm{fine}), &amp; \\ell = \\ell_0, \\\\ P(\\boldsymbol{S}_\\mathrm{fine})-P(\\boldsymbol{S}_\\mathrm{coarse}), &amp; \\ell &gt; \\ell_0 \\end{cases}.\\] <p>Cancellations from the telescoping sum allow us to write </p> \\[\\lim_{\\ell \\to \\infty} P_\\ell = P_{\\ell_0} + \\sum_{\\ell=\\ell_0+1}^\\infty P_\\ell.\\] <p>Examples:</p> <pre><code>&gt;&gt;&gt; integrand = FinancialOption(DigitalNetB2(dimension=3,seed=7),option=\"EUROPEAN\")\n&gt;&gt;&gt; y = integrand(2**10)\n&gt;&gt;&gt; y.shape\n(1024,)\n&gt;&gt;&gt; print(\"%.4f\"%y.mean())\n4.2126\n&gt;&gt;&gt; print(\"%.4f\"%integrand.get_exact_value())\n4.2115\n&gt;&gt;&gt; integrand\nFinancialOption (AbstractIntegrand)\n    option          EUROPEAN\n    call_put        CALL\n    volatility      2^(-1)\n    start_price     30\n    strike_price    35\n    interest_rate   0\n    t_final         1\n&gt;&gt;&gt; integrand.true_measure\nBrownianMotion (AbstractTrueMeasure)\n    time_vec        [0.333 0.667 1.   ]\n    drift           0\n    mean            [0. 0. 0.]\n    covariance      [[0.333 0.333 0.333]\n                     [0.333 0.667 0.667]\n                     [0.333 0.667 1.   ]]\n    decomp_type     PCA\n</code></pre> <pre><code>&gt;&gt;&gt; integrand = FinancialOption(DigitalNetB2(dimension=64,seed=7),option=\"ASIAN\")\n&gt;&gt;&gt; y = integrand(2**10)\n&gt;&gt;&gt; y.shape\n(1024,)\n&gt;&gt;&gt; print(\"%.4f\"%y.mean())\n1.7782\n</code></pre> <p>With independent replications</p> <pre><code>&gt;&gt;&gt; integrand = FinancialOption(DigitalNetB2(dimension=64,seed=7,replications=2**4),option=\"ASIAN\")\n&gt;&gt;&gt; y = integrand(2**6)\n&gt;&gt;&gt; y.shape\n(16, 64)\n&gt;&gt;&gt; muhats = y.mean(-1) \n&gt;&gt;&gt; muhats.shape \n(16,)\n&gt;&gt;&gt; print(\"%.4f\"%muhats.mean())\n1.7765\n</code></pre> <p>Multi-level options </p> <pre><code>&gt;&gt;&gt; seed_seq = np.random.SeedSequence(7) \n&gt;&gt;&gt; d_coarsest = 8\n&gt;&gt;&gt; num_levels = 4\n&gt;&gt;&gt; ns = [2**11,2**10,2**9,2**8]\n&gt;&gt;&gt; integrands = [FinancialOption(DigitalNetB2(dimension=2**l*d_coarsest,seed=seed_seq.spawn(1)[0]),option=\"ASIAN\",level=l,d_coarsest=d_coarsest) for l in range(num_levels)]\n&gt;&gt;&gt; ys = [integrands[l](ns[l]) for l in range(num_levels)]\n&gt;&gt;&gt; for l in range(num_levels):\n...     print(\"ys[%d].shape = %s\"%(l,ys[l].shape))\nys[0].shape = (2, 2048)\nys[1].shape = (2, 1024)\nys[2].shape = (2, 512)\nys[3].shape = (2, 256)\n&gt;&gt;&gt; ymeans = np.stack([(ys[l][1]-ys[l][0]).mean(-1) for l in range(num_levels)])\n&gt;&gt;&gt; with np.printoptions(formatter={\"float\": lambda x: \"%.2e\"%x}):\n...     ymeans\narray([1.78e+00, 2.61e-03, 5.57e-04, 1.10e-03])\n&gt;&gt;&gt; print(\"%.4f\"%ymeans.sum())\n1.7850\n</code></pre> <p>Multi-level options with independent replications</p> <pre><code>&gt;&gt;&gt; seed_seq = np.random.SeedSequence(7) \n&gt;&gt;&gt; d_coarsest = 8\n&gt;&gt;&gt; num_levels = 4\n&gt;&gt;&gt; ns = [2**7,2**6,2**5,2**4]\n&gt;&gt;&gt; integrands = [FinancialOption(DigitalNetB2(dimension=2**l*d_coarsest,seed=seed_seq.spawn(1)[0],replications=2**4),option=\"ASIAN\",level=l,d_coarsest=d_coarsest) for l in range(num_levels)]\n&gt;&gt;&gt; ys = [integrands[l](ns[l]) for l in range(num_levels)]\n&gt;&gt;&gt; for l in range(num_levels):\n...     print(\"ys[%d].shape = %s\"%(l,ys[l].shape))\nys[0].shape = (2, 16, 128)\nys[1].shape = (2, 16, 64)\nys[2].shape = (2, 16, 32)\nys[3].shape = (2, 16, 16)\n&gt;&gt;&gt; muhats = np.stack([(ys[l][1]-ys[l][0]).mean(-1) for l in range(num_levels)])\n&gt;&gt;&gt; muhats.shape\n(4, 16)\n&gt;&gt;&gt; muhathat = muhats.mean(-1)\n&gt;&gt;&gt; with np.printoptions(formatter={\"float\": lambda x: \"%.2e\"%x}):\n...     muhathat\narray([1.80e+00, -3.08e-03, 2.64e-03, 1.14e-03])\n&gt;&gt;&gt; print(\"%.4f\"%muhathat.sum())\n1.7982\n</code></pre> <p>References:</p> <ol> <li>M.B. Giles.     Improved multilevel Monte Carlo convergence using the Milstein scheme.     343-358, in Monte Carlo and Quasi-Monte Carlo Methods 2006, Springer, 2008. http://people.maths.ox.ac.uk/~gilesm/files/mcqmc06.pdf.</li> </ol> <p>Parameters:</p> Name Type Description Default <code>sampler</code> <code>Union[AbstractDiscreteDistribution, AbstractTrueMeasure]</code> <p>Either  </p> <ul> <li>a discrete distribution from which to transform samples, or</li> <li>a true measure by which to compose a transform.</li> </ul> required <code>option</code> <code>str</code> <p>Option type in <code>['ASIAN', 'EUROPEAN', 'BARRIER', 'LOOKBACK', 'DIGITAL']</code></p> <code>'ASIAN'</code> <code>call_put</code> <code>str</code> <p>Either <code>'CALL'</code> or <code>'PUT'</code>. </p> <code>'CALL'</code> <code>volatility</code> <code>float</code> <p>\\(\\sigma\\).</p> <code>0.5</code> <code>start_price</code> <code>float</code> <p>\\(S_0\\).</p> <code>30</code> <code>strike_price</code> <code>float</code> <p>\\(K\\).</p> <code>35</code> <code>interest_rate</code> <code>float</code> <p>\\(r\\).</p> <code>0</code> <code>t_final</code> <code>float</code> <p>\\(\\tau_d\\).</p> <code>1</code> <code>decomp_type</code> <code>str</code> <p>Method for decomposition for covariance matrix. Options include</p> <ul> <li><code>'PCA'</code> for principal component analysis, or </li> <li><code>'Cholesky'</code> for cholesky decomposition.</li> </ul> <code>'PCA'</code> <code>level</code> <code>Union[None, int]</code> <p>Level for multilevel problems </p> <code>None</code> <code>d_coarsest</code> <code>Union[None, int]</code> <p>Dimension of the problem on the coarsest level.</p> <code>2</code> <code>asian_mean</code> <code>str</code> <p>Either <code>'ARITHMETIC'</code> or <code>'GEOMETRIC'</code>.</p> <code>'ARITHMETIC'</code> <code>asian_mean_quadrature_rule</code> <code>str</code> <p>Either 'TRAPEZOIDAL' or 'RIGHT'. </p> <code>'TRAPEZOIDAL'</code> <code>barrier_in_out</code> <code>str</code> <p>Either <code>'IN'</code> or <code>'OUT'</code>. </p> <code>'IN'</code> <code>barrier_price</code> <code>float</code> <p>\\(B\\). </p> <code>38</code> <code>digital_payout</code> <code>float</code> <p>\\(\\rho\\).</p> <code>10</code> Source code in <code>qmcpy/integrand/financial_option.py</code> <pre><code>def __init__(self, \n             sampler,\n             option = \"ASIAN\", \n             call_put = 'CALL',\n             volatility = 0.5,\n             start_price = 30,\n             strike_price = 35,\n             interest_rate = 0,\n             t_final = 1,\n             decomp_type = 'PCA',\n             level = None,\n             d_coarsest = 2,\n             asian_mean = \"ARITHMETIC\",\n             asian_mean_quadrature_rule = \"TRAPEZOIDAL\",\n             barrier_in_out = \"IN\",\n             barrier_price = 38,\n             digital_payout = 10):\n    r\"\"\"\n    Args:\n        sampler (Union[AbstractDiscreteDistribution,AbstractTrueMeasure]): Either  \n\n            - a discrete distribution from which to transform samples, or\n            - a true measure by which to compose a transform.\n        option (str): Option type in `['ASIAN', 'EUROPEAN', 'BARRIER', 'LOOKBACK', 'DIGITAL']`\n        call_put (str): Either `'CALL'` or `'PUT'`. \n        volatility (float): $\\sigma$.\n        start_price (float): $S_0$.\n        strike_price (float): $K$.\n        interest_rate (float): $r$.\n        t_final (float): $\\tau_d$.\n        decomp_type (str): Method for decomposition for covariance matrix. Options include\n\n            - `'PCA'` for principal component analysis, or \n            - `'Cholesky'` for cholesky decomposition.\n        level (Union[None,int]): Level for multilevel problems \n        d_coarsest (Union[None,int]): Dimension of the problem on the coarsest level.\n        asian_mean (str): Either `'ARITHMETIC'` or `'GEOMETRIC'`.\n        asian_mean_quadrature_rule (str): Either 'TRAPEZOIDAL' or 'RIGHT'. \n        barrier_in_out (str): Either `'IN'` or `'OUT'`. \n        barrier_price (float): $B$. \n        digital_payout (float): $\\rho$. \n    \"\"\"\n    self.parameters = ['option', 'call_put', 'volatility', 'start_price', 'strike_price', 'interest_rate', 't_final']\n    self.t_final = t_final\n    self.sampler = sampler\n    self.decomp_type = decomp_type\n    self.true_measure = BrownianMotion(self.sampler,t_final=self.t_final,decomp_type=self.decomp_type)\n    self.volatility = float(volatility)\n    self.start_price = float(start_price)\n    self.strike_price = float(strike_price)\n    self.interest_rate = float(interest_rate)\n    self.discount_factor = np.exp(-self.interest_rate*self.t_final)\n    self.level = level\n    self.d_coarsest = d_coarsest\n    if self.level is not None:\n        self.multilevel = True \n        self.parameters += ['level','d_coarsest']\n        assert np.isscalar(self.level) and self.level%1==0 \n        assert np.isscalar(self.d_coarsest) and self.d_coarsest%1==0 and d_coarsest&gt;0 and np.log2(d_coarsest)%1==0, \"d_coarsest must be an integer power of 2\"\n        self.level = int(self.level)\n        self.d_coarsest = int(self.d_coarsest) \n        assert self.sampler.d==self.d_coarsest*2**self.level, \"the dimension of the sampler must equal d_coarsest*2^level = %d\"%(d_coarsest*2**self.level)\n        self.cost = self.d_coarsest*2**self.level\n        dim_shape = (2,)\n    else:\n        self.multilevel = False\n        dim_shape = ()\n    self.call_put = str(call_put).upper()\n    assert self.call_put in ['CALL','PUT'], \"invalid call_put = %s\"%self.call_put\n    self.option = str(option).upper()\n    self.asian_mean = str(asian_mean).upper()\n    self.asian_mean_quadrature_rule = str(asian_mean_quadrature_rule).upper()\n    self.barrier_in_out = str(barrier_in_out).upper()\n    assert np.isscalar(barrier_price)\n    self.barrier_price = float(barrier_price)\n    assert np.isscalar(digital_payout) and digital_payout&gt;0\n    self.digital_payout = float(digital_payout)\n    if self.option==\"EUROPEAN\":\n        self.payoff = self.payoff_european_call if self.call_put=='CALL' else self.payoff_european_put\n    elif self.option==\"ASIAN\":\n        self.parameters += ['asian_mean']\n        assert self.asian_mean in ['ARITHMETIC','GEOMETRIC'], \"invalid asian_mean = %s\"%self.asian_mean\n        assert self.asian_mean_quadrature_rule in ['TRAPEZOIDAL','RIGHT'], \"invalid asian_mean_quadrature_rule = %s\"%self.asian_mean_quadrature_rule\n        if self.asian_mean==\"ARITHMETIC\":\n            if self.asian_mean_quadrature_rule==\"TRAPEZOIDAL\":\n                self.payoff = self.payoff_asian_arithmetic_trap_call if self.call_put=='CALL' else self.payoff_asian_arithmetic_trap_put\n            elif self.asian_mean_quadrature_rule==\"RIGHT\":\n                self.payoff = self.payoff_asian_arithmetic_right_call if self.call_put=='CALL' else self.payoff_asian_arithmetic_right_put\n        elif self.asian_mean==\"GEOMETRIC\":\n            if self.asian_mean_quadrature_rule==\"TRAPEZOIDAL\":\n                self.payoff = self.payoff_asian_geometric_trap_call if self.call_put=='CALL' else self.payoff_asian_geometric_trap_put\n            elif self.asian_mean_quadrature_rule==\"RIGHT\":\n                self.payoff = self.payoff_asian_geometric_right_call if self.call_put=='CALL' else self.payoff_asian_geometric_right_put\n    elif self.option==\"BARRIER\":\n        self.parameters += ['barrier_in_out','barrier_up_down','barrier_price']\n        self.barrier_up_down = \"UP\" if self.start_price&lt;self.barrier_price else \"DOWN\"\n        if self.barrier_in_out==\"IN\":\n            if self.barrier_up_down==\"UP\":\n                self.payoff = self.payoff_barrier_in_up_call if self.call_put=='CALL' else self.payoff_barrier_in_up_put\n            else:\n                self.payoff = self.payoff_barrier_in_down_call if self.call_put=='CALL' else self.payoff_barrier_in_down_put\n        elif self.barrier_in_out==\"OUT\":\n            if self.barrier_up_down==\"UP\":\n                self.payoff = self.payoff_barrier_out_up_call if self.call_put=='CALL' else self.payoff_barrier_out_up_put\n            else:\n                self.payoff = self.payoff_barrier_out_down_call if self.call_put=='CALL' else self.payoff_barrier_out_down_put\n        else:\n            raise ParameterError(\"invalid barrier_in_out = %s\"%self.barrier_in_out)\n    elif self.option==\"LOOKBACK\":\n        self.payoff = self.payoff_lookback_call if self.call_put=='CALL' else self.payoff_lookback_put\n    elif self.option==\"DIGITAL\":\n        self.payoff = self.payoff_digital_call if self.call_put=='CALL' else self.payoff_digital_put\n    else:\n        raise ParameterError(\"invalid option type %s\"%self.option)\n    super(FinancialOption,self).__init__(dimension_indv=dim_shape,dimension_comb=dim_shape,parallel=False)  \n</code></pre>"},{"location":"api/integrands/#qmcpy.integrand.financial_option.FinancialOption.get_exact_value","title":"get_exact_value","text":"<pre><code>get_exact_value()\n</code></pre> <p>Compute the exact analytic fair price of the option in finite dimensions. Supports </p> <ul> <li><code>option='EUROPEAN'</code></li> <li><code>option='ASIAN'</code> with <code>asian_mean='GEOMETRIC'</code> and <code>asian_mean_quadrature_rule='RIGHT'</code></li> </ul> <p>Returns:</p> Name Type Description <code>mean</code> <code>float</code> <p>Exact value of the integral.</p> Source code in <code>qmcpy/integrand/financial_option.py</code> <pre><code>def get_exact_value(self):\n    \"\"\"\n    Compute the exact analytic fair price of the option in finite dimensions. Supports \n\n    - `option='EUROPEAN'`\n    - `option='ASIAN'` with `asian_mean='GEOMETRIC'` and `asian_mean_quadrature_rule='RIGHT'`\n\n    Returns: \n        mean (float): Exact value of the integral. \n    \"\"\"\n    if self.option==\"EUROPEAN\":\n        denom = self.volatility*np.sqrt(self.t_final)\n        decay = self.strike_price*self.discount_factor\n        if self.call_put == 'CALL':\n            term1 = np.log(self.start_price/self.strike_price)+(self.interest_rate+self.volatility**2/2)*self.t_final\n            term2 = np.log(self.start_price/self.strike_price)+(self.interest_rate-self.volatility**2/2)*self.t_final\n            fp = self.start_price*norm.cdf(term1/denom)-decay*norm.cdf(term2/denom)\n        elif self.call_put == 'PUT':\n            term1 = np.log(self.strike_price/self.start_price)-(self.interest_rate-self.volatility**2/2)*self.t_final\n            term2 = np.log(self.strike_price/self.start_price)-(self.interest_rate+self.volatility**2/2)*self.t_final\n            fp = decay*norm.cdf(term1/denom)-self.start_price*norm.cdf(term2/denom)\n    elif self.option==\"ASIAN\":\n        assert self.asian_mean=='GEOMETRIC' and self.asian_mean_quadrature_rule=='RIGHT', \"exact value for Asian options only implemented for self.asian_mean=='GEOMETRIC' and self.asian_mean_quadrature_rule=='RIGHT'\"\n        Tbar = (1+1/self.d)*self.t_final/2\n        sigmabar = self.volatility*np.sqrt((2+1/self.d)/3)\n        rbar = self.interest_rate+(sigmabar**2-self.volatility**2)/2\n        gmeancall,gmeanput = _eurogbmprice(self.start_price,rbar,Tbar,sigmabar,self.strike_price)\n        if self.call_put=='CALL':\n            fp = gmeancall * np.exp(rbar*Tbar-self.interest_rate*self.t_final)\n        elif self.call_put=='PUT':\n            fp = gmeanput * np.exp(rbar*Tbar-self.interest_rate*self.t_final)\n        return fp\n    else:\n        raise ParameterError(\"exact value not supported for option = %s\"%self.option)\n    return fp\n</code></pre>"},{"location":"api/integrands/#qmcpy.integrand.financial_option.FinancialOption.get_exact_value_inf_dim","title":"get_exact_value_inf_dim","text":"<pre><code>get_exact_value_inf_dim()\n</code></pre> <p>Get the exact analytic fair price of the option in infinite dimensions. Supports </p> <ul> <li><code>option='ASIAN'</code> with <code>asian_mean='GEOMETRIC'</code></li> </ul> <p>Returns:</p> Name Type Description <code>mean</code> <code>float</code> <p>Exact value of the integral.</p> Source code in <code>qmcpy/integrand/financial_option.py</code> <pre><code>def get_exact_value_inf_dim(self):\n    r\"\"\"\n    Get the exact analytic fair price of the option in infinite dimensions. Supports \n\n    - `option='ASIAN'` with `asian_mean='GEOMETRIC'`\n\n    Returns: \n        mean (float): Exact value of the integral. \n    \"\"\"\n    if self.option=='ASIAN':\n        assert self.asian_mean=='GEOMETRIC' , \"get_exact_value_inf_dim for the Asian option only available for self.asian_mean=='GEOMETRIC'\"\n        sigma_g = self.volatility/np.sqrt(3) \n        b = 1/2*(self.interest_rate-1/2*sigma_g**2)\n        d1 = (np.log(self.start_price/self.strike_price)+(b+1/2*sigma_g**2)*self.t_final)/(sigma_g*np.sqrt(self.t_final))\n        d2 = d1-sigma_g*np.sqrt(self.t_final)\n        f1 = self.start_price*np.exp((b-self.interest_rate)*self.t_final)\n        f2 = self.strike_price*np.exp(-self.interest_rate*self.t_final)\n        if self.call_put==\"CALL\":\n            val = f1*norm.cdf(d1)-f2*norm.cdf(d2)\n        elif self.call_put==\"PUT\":\n            val = f2*norm.cdf(-d2)-f1*norm.cdf(-d1)\n    else:\n        raise Exception(\"get_exact_value_inf_dim not implemented for option = %s\"%self.option)\n    return val\n</code></pre>"},{"location":"api/integrands/#customfun","title":"<code>CustomFun</code>","text":"<p>               Bases: <code>AbstractIntegrand</code></p> <p>User supplied integrand \\(g\\). In the following example we implement </p> <p>Examples:</p> <p>First we will implement </p> \\[g(\\boldsymbol{t}) = t_1^2t_2, \\qquad \\boldsymbol{T}=(T_1,T_2) \\sim \\mathcal{N}((1,2)^T,\\mathsf{I}).\\] <pre><code>&gt;&gt;&gt; integrand = CustomFun(\n...     true_measure = Gaussian(DigitalNetB2(2,seed=7),mean=[1,2]),\n...     g = lambda t: t[...,0]**2*t[...,1])\n&gt;&gt;&gt; y = integrand(2**10)\n&gt;&gt;&gt; print(\"%.4f\"%y.mean())\n3.9991\n</code></pre> <p>With independent replications</p> <pre><code>&gt;&gt;&gt; integrand = CustomFun(\n...     true_measure = Gaussian(DigitalNetB2(2,seed=7,replications=2**4),mean=[1,2]),\n...     g = lambda t: t[...,0]**2*t[...,1])\n&gt;&gt;&gt; y = integrand(2**6)\n&gt;&gt;&gt; y.shape\n(16, 64)\n&gt;&gt;&gt; muhats = y.mean(1) \n&gt;&gt;&gt; muhats.shape \n(16,)\n&gt;&gt;&gt; print(\"%.4f\"%muhats.mean())\n3.9330\n</code></pre> <p>Next we will implement the multi-output function </p> \\[g(\\boldsymbol{t}) = \\begin{pmatrix} \\sin(t_1)\\cos(t_2) \\\\ \\cos(t_1)\\sin(t_2) \\\\ \\sin(t_1)+\\cos(t_2) \\\\ \\cos(t_1)+\\sin(t_2) \\end{pmatrix} \\qquad \\boldsymbol{T}=(T_1,T_2) \\sim \\mathcal{U}[0,2\\pi]^2.\\] <pre><code>&gt;&gt;&gt; def g(t):\n...     t1,t2 = t[...,0],t[...,1]\n...     sint1,cost1,sint2,cost2 = np.sin(t1),np.cos(t1),np.sin(t2),np.cos(t2)\n...     y1 = sint1*cost2\n...     y2 = cost1*sint2\n...     y3 = sint1+cost2\n...     y4 = cost1+sint2\n...     y = np.stack([y1,y2,y3,y4])\n...     return y\n&gt;&gt;&gt; integrand = CustomFun(\n...     true_measure = Uniform(DigitalNetB2(2,seed=7),lower_bound=0,upper_bound=2*np.pi),\n...     g = g, \n...     dimension_indv = (4,))\n&gt;&gt;&gt; x = integrand.discrete_distrib(2**10)\n&gt;&gt;&gt; y = integrand.f(x)\n&gt;&gt;&gt; y.shape\n(4, 1024)\n&gt;&gt;&gt; with np.printoptions(formatter={\"float\": lambda x: \"%.2e\"%x}):\n...     y.mean(-1)\narray([8.18e-04, 1.92e-06, -2.26e-10, 5.05e-07])\n</code></pre> <p>Stopping criterion which supporting vectorized outputs may pass in Boolean <code>compute_flags</code> with <code>dimension_indv</code> shape indicating which output need to evaluated,      i.e. where <code>compute_flags</code> is <code>False</code> we do not need to evaluate the integrand. We have not used this in inexpensive example above. </p> <p>With independent replications</p> <pre><code>&gt;&gt;&gt; integrand = CustomFun(\n...     true_measure = Uniform(DigitalNetB2(2,seed=7,replications=2**4),lower_bound=0,upper_bound=2*np.pi),\n...     g = g, \n...     dimension_indv = (4,))\n&gt;&gt;&gt; x = integrand.discrete_distrib(2**6)\n&gt;&gt;&gt; x.shape\n(16, 64, 2)\n&gt;&gt;&gt; y = integrand.f(x)\n&gt;&gt;&gt; y.shape\n(4, 16, 64)\n&gt;&gt;&gt; muhats = y.mean(-1) \n&gt;&gt;&gt; muhats.shape \n(4, 16)\n&gt;&gt;&gt; with np.printoptions(formatter={\"float\": lambda x: \"%.2e\"%x}):\n...     muhats.mean(-1)\narray([3.83e-03, -6.78e-03, -1.56e-03, -5.65e-04])\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>true_measure</code> <code>AbstractTrueMeasure</code> <p>The true measure. </p> required <code>g</code> <code>callable</code> <p>A function handle. </p> required <code>dimension_indv</code> <code>tuple</code> <p>Shape of individual solution outputs from <code>g</code>.</p> <code>()</code> <code>parallel</code> <code>int</code> <p>Parallelization flag. </p> <ul> <li>When <code>parallel = 0</code> or <code>parallel = 1</code> then function evaluation is done in serial fashion.</li> <li><code>parallel &gt; 1</code> specifies the number of processes used by <code>multiprocessing.Pool</code> or <code>multiprocessing.pool.ThreadPool</code>.</li> </ul> <p>Setting <code>parallel=True</code> is equivalent to <code>parallel = os.cpu_count()</code>.</p> <code>False</code> Note <p>For <code>parallel &gt; 1</code> do not set <code>g</code> to be anonymous function (i.e. a <code>lambda</code> function)</p> Source code in <code>qmcpy/integrand/custom_fun.py</code> <pre><code>def __init__(self, true_measure, g, dimension_indv=(), parallel=False):\n    \"\"\"\n    Args:\n        true_measure (AbstractTrueMeasure): The true measure. \n        g (callable): A function handle. \n        dimension_indv (tuple): Shape of individual solution outputs from `g`.\n        parallel (int): Parallelization flag. \n\n            - When `parallel = 0` or `parallel = 1` then function evaluation is done in serial fashion.\n            - `parallel &gt; 1` specifies the number of processes used by `multiprocessing.Pool` or `multiprocessing.pool.ThreadPool`.\n\n            Setting `parallel=True` is equivalent to `parallel = os.cpu_count()`.\n\n    Note:\n        For `parallel &gt; 1` do *not* set `g` to be anonymous function (i.e. a `lambda` function)\n    \"\"\"\n    self.parameters = []\n    self.true_measure = true_measure\n    self.sampler = self.true_measure\n    self.__g = g            \n    super(CustomFun,self).__init__(dimension_indv=dimension_indv,dimension_comb=dimension_indv,parallel=parallel)\n</code></pre>"},{"location":"api/integrands/#umbridgewrapper","title":"<code>UMBridgeWrapper</code>","text":"<p>               Bases: <code>AbstractIntegrand</code></p> <p>Wrapper around a <code>UM-Bridge</code> model. See also the <code>UM-Bridge</code> documentation for the QMCPy client.  Requires Docker is installed.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; _ = os.system('docker run --name muqbppytest -dit -p 4243:4243 linusseelinger/benchmark-muq-beam-propagation:latest &gt; /dev/null')\n&gt;&gt;&gt; import umbridge\n&gt;&gt;&gt; dnb2 = DigitalNetB2(dimension=3,seed=7)\n&gt;&gt;&gt; true_measure = Uniform(dnb2,lower_bound=1,upper_bound=1.05)\n&gt;&gt;&gt; um_bridge_model = umbridge.HTTPModel('http://localhost:4243','forward')\n&gt;&gt;&gt; um_bridge_config = {\"d\": dnb2.d}\n&gt;&gt;&gt; integrand = UMBridgeWrapper(true_measure,um_bridge_model,um_bridge_config,parallel=False)\n&gt;&gt;&gt; y = integrand(2**10)\n&gt;&gt;&gt; with np.printoptions(formatter={\"float\":lambda x: \"%.1e\"%x}):\n...     y.mean(-1)\narray([0.0e+00, 3.9e+00, 1.5e+01, 3.2e+01, 5.5e+01, 8.3e+01, 1.2e+02,\n       1.5e+02, 2.0e+02, 2.4e+02, 2.9e+02, 3.4e+02, 3.9e+02, 4.3e+02,\n       4.7e+02, 5.0e+02, 5.3e+02, 5.6e+02, 5.9e+02, 6.2e+02, 6.4e+02,\n       6.6e+02, 6.9e+02, 7.2e+02, 7.6e+02, 7.9e+02, 8.3e+02, 8.6e+02,\n       9.0e+02, 9.4e+02, 9.7e+02])\n&gt;&gt;&gt; _ = os.system('docker rm -f muqbppytest &gt; /dev/null')\n</code></pre> <p>Custom model with independent replications</p> <pre><code>&gt;&gt;&gt; class TestModel(umbridge.Model):\n...     def __init__(self):\n...         super().__init__(\"forward\")\n...     def get_input_sizes(self, config):\n...         return [1,2,3]\n...     def get_output_sizes(self, config):\n...         return [3,2,1]\n...     def __call__(self, parameters, config):\n...         out0 = [parameters[2][0],sum(parameters[2][:2]),sum(parameters[2])]\n...         out1 = [parameters[1][0],sum(parameters[1])]\n...         out2 = [parameters[0]]\n...         return [out0,out1,out2]\n...     def supports_evaluate(self):\n...         return True\n&gt;&gt;&gt; um_bridge_model = TestModel()\n&gt;&gt;&gt; um_bridge_config = {}\n&gt;&gt;&gt; d = sum(um_bridge_model.get_input_sizes(config=um_bridge_config))\n&gt;&gt;&gt; true_measure = Uniform(DigitalNetB2(dimension=d,seed=7,replications=15),lower_bound=-1,upper_bound=1)\n&gt;&gt;&gt; integrand = UMBridgeWrapper(true_measure,um_bridge_model)\n&gt;&gt;&gt; y = integrand(2**6)\n&gt;&gt;&gt; y.shape \n(6, 15, 64)\n&gt;&gt;&gt; muhats = y.mean(-1)\n&gt;&gt;&gt; muhats.shape \n(6, 15)\n&gt;&gt;&gt; muhats_aggregate = muhats.mean(-1)\n&gt;&gt;&gt; muhats_aggregate.shape \n(6,)\n&gt;&gt;&gt; muhats_agg_list_of_lists = integrand.to_umbridge_out_sizes(muhats_aggregate)\n&gt;&gt;&gt; [[\"%.2e\"%ii for ii in i] for i in muhats_agg_list_of_lists]\n[['-1.59e-08', '1.49e-04', '1.49e-04'], ['8.20e-06', '-1.38e-04'], ['-8.14e-06']]\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>true_measure</code> <code>AbstractTrueMeasure</code> <p>The true measure.  </p> required <code>model</code> <code>HTTPModel</code> <p>A <code>UM-Bridge</code> model. </p> required <code>config</code> <code>dict</code> <p>Configuration keyword argument to <code>umbridge.HTTPModel(url,name).__call__</code>.</p> <code>{}</code> <code>parallel</code> <code>int</code> <p>Parallelization flag. </p> <ul> <li>When <code>parallel = 0</code> or <code>parallel = 1</code> then function evaluation is done in serial fashion.</li> <li><code>parallel &gt; 1</code> specifies the number of processes used by <code>multiprocessing.Pool</code> or <code>multiprocessing.pool.ThreadPool</code>.</li> </ul> <p>Setting <code>parallel=True</code> is equivalent to <code>parallel = os.cpu_count()</code>.</p> <code>False</code> Source code in <code>qmcpy/integrand/umbridge_wrapper.py</code> <pre><code>def __init__(self, true_measure, model, config={}, parallel=False):\n    \"\"\"\n    Args:\n        true_measure (AbstractTrueMeasure): The true measure.  \n        model (umbridge.HTTPModel): A `UM-Bridge` model. \n        config (dict): Configuration keyword argument to `umbridge.HTTPModel(url,name).__call__`.\n        parallel (int): Parallelization flag. \n\n            - When `parallel = 0` or `parallel = 1` then function evaluation is done in serial fashion.\n            - `parallel &gt; 1` specifies the number of processes used by `multiprocessing.Pool` or `multiprocessing.pool.ThreadPool`.\n\n            Setting `parallel=True` is equivalent to `parallel = os.cpu_count()`.\n    \"\"\"\n    import umbridge\n    self.parameters = []\n    self.true_measure = true_measure\n    self.sampler = self.true_measure \n    self.model = model\n    if not self.model.supports_evaluate(): raise ParameterError(\"UMBridgeWrapper requires model supports evaluation.\")\n    self.config = config\n    self.parallel = parallel\n    self.d_in_umbridge = np.append(0,np.cumsum(self.model.get_input_sizes(self.config)))\n    self.n_d_in_umbridge = len(self.d_in_umbridge)-1\n    if self.true_measure.d!=self.d_in_umbridge[-1]:\n        raise ParameterError(\"sampler dimension (%d) must equal the sum of UMBridgeWrapper input sizes (%d).\"%(self.true_measure.d,self.d_in_umbridge[-1]))\n    self.d_out_umbridge = np.append(0,np.cumsum(self.model.get_output_sizes(self.config)))\n    self.n_d_out_umbridge = len(self.d_out_umbridge)-1\n    self.total_out_elements = int(self.d_out_umbridge[-1])\n    super(UMBridgeWrapper,self).__init__(\n        dimension_indv = () if self.total_out_elements==1 else (self.total_out_elements,),\n        dimension_comb = () if self.total_out_elements==1 else (self.total_out_elements,),\n        parallel = self.parallel,\n        threadpool = True)\n</code></pre>"},{"location":"api/integrands/#qmcpy.integrand.umbridge_wrapper.UMBridgeWrapper.to_umbridge_out_sizes","title":"to_umbridge_out_sizes","text":"<pre><code>to_umbridge_out_sizes(x)\n</code></pre> <p>Convert a data attribute to <code>UM-Bridge</code> output sized list of lists. </p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ndarray</code> <p>Array of length <code>sum(model.get_output_sizes(self.config))</code> where <code>model</code> is a <code>umbridge.HTTPModel</code>. </p> required <p>Returns:</p> Name Type Description <code>x_list_list</code> <code>list</code> <p>List of lists with sub-list lengths specified by <code>model.get_output_sizes(self.config)</code>.</p> Source code in <code>qmcpy/integrand/umbridge_wrapper.py</code> <pre><code>def to_umbridge_out_sizes(self, x):\n    \"\"\"\n    Convert a data attribute to `UM-Bridge` output sized list of lists. \n\n    Args:\n        x (np.ndarray): Array of length `sum(model.get_output_sizes(self.config))` where `model` is a `umbridge.HTTPModel`. \n\n    Returns:\n        x_list_list (list): List of lists with sub-list lengths specified by `model.get_output_sizes(self.config)`.\n    \"\"\"\n    return [x[...,self.d_out_umbridge[j]:self.d_out_umbridge[j+1]].tolist() for j in range(self.n_d_out_umbridge)]\n</code></pre>"},{"location":"api/integrands/#sensitivityindices","title":"<code>SensitivityIndices</code>","text":"<p>               Bases: <code>AbstractIntegrand</code></p> <p>Sensitivity indices i.e. normalized Sobol' Indices. </p> <p>Examples:</p> <p>Singleton indices</p> <pre><code>&gt;&gt;&gt; function = Keister(DigitalNetB2(dimension=4,seed=7))\n&gt;&gt;&gt; integrand = SensitivityIndices(function,indices='singletons')\n&gt;&gt;&gt; integrand.indices\narray([[ True, False, False, False],\n       [False,  True, False, False],\n       [False, False,  True, False],\n       [False, False, False,  True]])\n&gt;&gt;&gt; y = integrand(2**10)\n&gt;&gt;&gt; y.shape\n(2, 3, 4, 1024)\n&gt;&gt;&gt; ymean = y.mean(-1)\n&gt;&gt;&gt; ymean.shape\n(2, 3, 4)\n&gt;&gt;&gt; sigma_hat = ymean[:,2,:]-ymean[:,1,:]**2\n&gt;&gt;&gt; sigma_hat.shape \n(2, 4)\n&gt;&gt;&gt; sigma_hat\narray([[17.81421941, 17.81421941, 17.81421941, 17.81421941],\n       [17.81421941, 17.81421941, 17.81421941, 17.81421941]])\n&gt;&gt;&gt; closed_total_approx = ymean[:,0]/sigma_hat\n&gt;&gt;&gt; closed_total_approx.shape \n(2, 4)\n&gt;&gt;&gt; closed_total_approx\narray([[0.23756849, 0.2423556 , 0.24844127, 0.24771739],\n       [0.25090825, 0.25344588, 0.24531633, 0.26244379]])\n</code></pre> <p>Check what all indices look like for \\(d=3\\) </p> <pre><code>&gt;&gt;&gt; integrand = SensitivityIndices(Keister(DigitalNetB2(dimension=3,seed=7)),indices='all')\n&gt;&gt;&gt; integrand.indices\narray([[ True, False, False],\n       [False,  True, False],\n       [False, False,  True],\n       [ True,  True, False],\n       [ True, False,  True],\n       [False,  True,  True]])\n</code></pre> <p>Vectorized function for all singletons and pairs of dimensions</p> <pre><code>&gt;&gt;&gt; function = BoxIntegral(DigitalNetB2(dimension=4,seed=7,replications=2**4),s=np.arange(1,31).reshape((5,6)))\n&gt;&gt;&gt; indices = np.zeros((function.d,function.d,function.d),dtype=bool) \n&gt;&gt;&gt; r = np.arange(function.d) \n&gt;&gt;&gt; indices[r,:,r] = True \n&gt;&gt;&gt; indices[:,r,r] = True \n&gt;&gt;&gt; integrand = SensitivityIndices(function,indices=indices)\n&gt;&gt;&gt; integrand.indices.shape\n(4, 4, 4)\n&gt;&gt;&gt; integrand.indices\narray([[[ True, False, False, False],\n        [ True,  True, False, False],\n        [ True, False,  True, False],\n        [ True, False, False,  True]],\n\n       [[ True,  True, False, False],\n        [False,  True, False, False],\n        [False,  True,  True, False],\n        [False,  True, False,  True]],\n\n       [[ True, False,  True, False],\n        [False,  True,  True, False],\n        [False, False,  True, False],\n        [False, False,  True,  True]],\n\n       [[ True, False, False,  True],\n        [False,  True, False,  True],\n        [False, False,  True,  True],\n        [False, False, False,  True]]])\n&gt;&gt;&gt; y = integrand(2**10)\n&gt;&gt;&gt; y.shape \n(2, 3, 4, 4, 5, 6, 16, 1024)\n&gt;&gt;&gt; muhats = y.mean(-1) \n&gt;&gt;&gt; muhats.shape \n(2, 3, 4, 4, 5, 6, 16)\n&gt;&gt;&gt; muhathat = muhats.mean(-1) \n&gt;&gt;&gt; muhathat.shape \n(2, 3, 4, 4, 5, 6)\n&gt;&gt;&gt; sigma_hat = muhathat[:,2,:]-muhathat[:,1,:]**2\n&gt;&gt;&gt; sigma_hat.shape\n(2, 4, 4, 5, 6)\n&gt;&gt;&gt; closed_total_approx = muhathat[:,0]/sigma_hat\n&gt;&gt;&gt; closed_total_approx.shape\n(2, 4, 4, 5, 6)\n</code></pre> <p>References:</p> <ol> <li> <p>Aleksei G. Sorokin and Jagadeeswaran Rathinavel.     On Bounding and Approximating Functions of Multiple Expectations Using Quasi-Monte Carlo.     International Conference on Monte Carlo and Quasi-Monte Carlo Methods in Scientific Computing.     Cham: Springer International Publishing, 2022. https://link.springer.com/chapter/10.1007/978-3-031-59762-6_29. </p> </li> <li> <p>Art B. Owen.     Monte Carlo theory, methods and examples.     Appendix A. Equations (A.16) and (A.18). 2013.     https://artowen.su.domains/mc/A-anova.pdf.</p> </li> </ol> <p>Parameters:</p> Name Type Description Default <code>integrand</code> <code>AbstractIntegrand</code> <p>Integrand to find sensitivity indices of.</p> required <code>indices</code> <code>ndarray</code> <p>Bool array with shape \\((\\dots,d)\\) where each length \\(d\\) vector item indicates which dimensions are active in the subset.</p> <ul> <li>The default <code>indices='singletons'</code> sets <code>indices=np.eye(d,dtype=bool)</code>.</li> <li>Setting <code>incides='all'</code> sets <code>indices = np.array([[bool(int(b)) for b in np.binary_repr(i,width=d)] for i in range(1,2**d-1)],dtype=bool)</code></li> </ul> <code>'singletons'</code> Source code in <code>qmcpy/integrand/sensitivity_indices.py</code> <pre><code>def __init__(self, integrand, indices='singletons'):\n    r\"\"\"\n    Args:\n        integrand (AbstractIntegrand): Integrand to find sensitivity indices of.\n        indices (np.ndarray): Bool array with shape $(\\dots,d)$ where each length $d$ vector item indicates which dimensions are active in the subset.\n\n            - The default `indices='singletons'` sets `indices=np.eye(d,dtype=bool)`.\n            - Setting `incides='all'` sets `indices = np.array([[bool(int(b)) for b in np.binary_repr(i,width=d)] for i in range(1,2**d-1)],dtype=bool)`\n    \"\"\"\n    self.parameters = ['indices']\n    self.integrand = integrand\n    self.dtilde = self.integrand.d\n    assert self.dtilde&gt;1, \"SensitivityIndices does not make sense for d=1\"\n    self.indices = indices\n    if isinstance(self.indices,str) and self.indices=='singletons':\n        self.indices = np.eye(self.dtilde,dtype=bool)\n    elif isinstance(self.indices,str) and self.indices=='all':\n        self.indices = np.zeros((0,self.dtilde),dtype=bool)\n        for r in range(1,self.dtilde):\n            idxs_r = np.zeros((int(scipy.special.comb(self.dtilde,r)),self.dtilde),dtype=bool)\n            for i,comb in enumerate(combinations(range(self.dtilde),r)):\n                idxs_r[i,comb] = True \n            self.indices = np.vstack([self.indices,idxs_r])\n    self.indices = np.atleast_1d(self.indices)\n    assert self.indices.dtype==bool and self.indices.ndim&gt;=1 and self.indices.shape[-1]==self.dtilde \n    assert not (self.indices==self.indices[...,0,None]).all(-1).any(), \"indices cannot include the emptyset or the set of all dimensions\"\n    self.not_indices = ~self.indices\n    # sensitivity_index\n    self.true_measure = self.integrand.true_measure\n    self.discrete_distrib = self.integrand.discrete_distrib.spawn(s=1,dimensions=[2*self.dtilde])[0]\n    self.sampler = self.integrand.sampler\n    self.i_slice = (slice(None),)*len(self.integrand.d_indv)\n    super(SensitivityIndices,self).__init__(\n        dimension_indv = (2,3)+self.indices.shape[:-1]+self.integrand.d_indv,\n        dimension_comb = (2,)+self.indices.shape[:-1]+self.integrand.d_indv,\n        parallel = False)\n    self.d = 2*self.dtilde\n</code></pre>"},{"location":"api/integrands/#keister","title":"<code>Keister</code>","text":"<p>               Bases: <code>AbstractIntegrand</code></p> <p>Keister function from [1]. </p> \\[f(\\boldsymbol{t}) = \\pi^{d/2} \\cos(\\lVert \\boldsymbol{t} \\rVert_2) \\qquad \\boldsymbol{T} \\sim \\mathcal{N}(\\boldsymbol{0},\\mathsf{I}/2).\\] <p>Examples:</p> <pre><code>&gt;&gt;&gt; integrand = Keister(DigitalNetB2(2,seed=7))\n&gt;&gt;&gt; y = integrand(2**10)\n&gt;&gt;&gt; print(\"%.4f\"%y.mean())\n1.8080\n&gt;&gt;&gt; integrand.true_measure\nGaussian (AbstractTrueMeasure)\n    mean            0\n    covariance      2^(-1)\n    decomp_type     PCA\n</code></pre> <p>With independent replications</p> <pre><code>&gt;&gt;&gt; integrand = Keister(DigitalNetB2(2,seed=7,replications=2**4))\n&gt;&gt;&gt; y = integrand(2**6)\n&gt;&gt;&gt; y.shape\n(16, 64)\n&gt;&gt;&gt; muhats = y.mean(-1) \n&gt;&gt;&gt; muhats.shape \n(16,)\n&gt;&gt;&gt; print(\"%.4f\"%muhats.mean())\n1.8024\n</code></pre> <p>References:</p> <ol> <li>B. D. Keister.     Multidimensional Quadrature Algorithms.     Computers in Physics, 10, pp. 119-122, 1996.</li> </ol> <p>Parameters:</p> Name Type Description Default <code>sampler</code> <code>Union[AbstractDiscreteDistribution, AbstractTrueMeasure]</code> <p>Either  </p> <ul> <li>a discrete distribution from which to transform samples, or</li> <li>a true measure by which to compose a transform.</li> </ul> required Source code in <code>qmcpy/integrand/keister.py</code> <pre><code>def __init__(self, sampler):\n    r\"\"\"\n    Args:\n        sampler (Union[AbstractDiscreteDistribution,AbstractTrueMeasure]): Either  \n\n            - a discrete distribution from which to transform samples, or\n            - a true measure by which to compose a transform.\n    \"\"\"\n    self.sampler = sampler\n    self.true_measure = Gaussian(self.sampler,mean=0,covariance=1/2)\n    super(Keister,self).__init__(dimension_indv=(),dimension_comb=(),parallel=False)\n</code></pre>"},{"location":"api/integrands/#qmcpy.integrand.keister.Keister.get_exact_value","title":"get_exact_value  <code>classmethod</code>","text":"<pre><code>get_exact_value(d)\n</code></pre> <p>Compute the exact analytic value of the Keister integral with dimension \\(d\\). </p> <p>Parameters:</p> Name Type Description Default <code>d</code> <code>int</code> <p>Dimension. </p> required <p>Returns:</p> Name Type Description <code>mean</code> <code>float</code> <p>Exact value of the integral.</p> Source code in <code>qmcpy/integrand/keister.py</code> <pre><code>@classmethod\ndef get_exact_value(self, d):\n    \"\"\"\n    Compute the exact analytic value of the Keister integral with dimension $d$. \n\n    Args:\n        d (int): Dimension. \n\n    Returns: \n        mean (float): Exact value of the integral. \n    \"\"\"\n    cosinteg = np.zeros(shape=(d))\n    cosinteg[0] = np.sqrt(np.pi) / (2 * np.exp(1 / 4))\n    sininteg = np.zeros(shape=(d))\n    sininteg[0] = 4.244363835020225e-01\n    cosinteg[1] = (1 - sininteg[0]) / 2\n    sininteg[1] = cosinteg[0] / 2\n    for j in range(2, d):\n        cosinteg[j] = ((j-1)*cosinteg[j-2]-sininteg[j-1])/2\n        sininteg[j] = ((j-1)*sininteg[j-2]+cosinteg[j-1])/2\n    I = (2*(np.pi**(d/2))/gamma(d/2))*cosinteg[d-1]\n    return I\n</code></pre>"},{"location":"api/integrands/#genz","title":"<code>Genz</code>","text":"<p>               Bases: <code>AbstractIntegrand</code></p> <p>Genz function following the <code>DAKOTA</code> implementation. </p> \\[g_\\mathrm{oscillatory}(\\boldsymbol{t}) = \\cos\\left(-\\sum_{j=1}^d c_j t_j\\right)\\] <p>or </p> \\[g_\\mathrm{corner-peak}(\\boldsymbol{t}) = \\left(1+\\sum_{j=1}^d c_j t_j\\right)^{-(d+1)}\\] <p>where </p> \\[\\boldsymbol{T} \\sim \\mathcal{U}[0,1]^d\\] <p>and the coefficients \\(\\boldsymbol{c}\\) are have three kinds </p> \\[c_k^{(1)} = \\frac{k-1/2}{d}, \\qquad c_k^{(2)} = \\frac{1}{k^2}, \\qquad c_k^{(3)} = \\exp\\left(\\frac{k \\log(10^{-8})}{d}\\right), \\qquad k=1,\\dots,d.\\] <p>Examples:</p> <pre><code>&gt;&gt;&gt; for kind_func in ['OSCILLATORY','CORNER PEAK']:\n...     for kind_coeff in [1,2,3]:\n...         integrand = Genz(DigitalNetB2(2,seed=7),kind_func=kind_func,kind_coeff=kind_coeff)\n...         y = integrand(2**14)\n...         mu_hat = y.mean()\n...         print('%-15s %-3d %.3f'%(kind_func,kind_coeff,mu_hat))\nOSCILLATORY     1   -0.351\nOSCILLATORY     2   -0.329\nOSCILLATORY     3   -0.217\nCORNER PEAK     1   0.713\nCORNER PEAK     2   0.714\nCORNER PEAK     3   0.720\n</code></pre> <p>With independent replications</p> <pre><code>&gt;&gt;&gt; integrand = Genz(DigitalNetB2(2,seed=7,replications=2**4),kind_func=\"CORNER PEAK\",kind_coeff=3)\n&gt;&gt;&gt; y = integrand(2**6)\n&gt;&gt;&gt; y.shape\n(16, 64)\n&gt;&gt;&gt; muhats = y.mean(-1) \n&gt;&gt;&gt; muhats.shape \n(16,)\n&gt;&gt;&gt; print(\"%.4f\"%muhats.mean())\n0.7200\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>sampler</code> <code>Union[AbstractDiscreteDistribution, AbstractTrueMeasure]</code> <p>Either  </p> <ul> <li>a discrete distribution from which to transform samples, or</li> <li>a true measure by which to compose a transform.</li> </ul> required <code>kind_func</code> <code>str</code> <p>Either <code>'OSCILLATORY'</code> or <code>'CORNER PEAK'</code></p> <code>'OSCILLATORY'</code> <code>kind_coeff</code> <code>int</code> <p>1, 2, or 3 for choice of coefficients</p> <code>1</code> Source code in <code>qmcpy/integrand/genz.py</code> <pre><code>def __init__(self, sampler, kind_func='OSCILLATORY', kind_coeff=1):\n    \"\"\"\n    Args:\n        sampler (Union[AbstractDiscreteDistribution,AbstractTrueMeasure]): Either  \n\n            - a discrete distribution from which to transform samples, or\n            - a true measure by which to compose a transform.\n        kind_func (str): Either `'OSCILLATORY'` or `'CORNER PEAK'`\n        kind_coeff (int): 1, 2, or 3 for choice of coefficients \n    \"\"\"\n    self.kind_func = str(kind_func).upper().strip().replace(\"_\",\" \").replace(\"-\",\" \")\n    self.kind_coeff = kind_coeff\n    if (self.kind_func not in ['OSCILLATORY','CORNER PEAK']) or (self.kind_coeff not in [1,2,3]):\n        raise ParameterError('''\n            Genz expects \n                kind_func in ['OSCILLATORY','CORNER PEAK'] and \n                kind_coeffs in [1,2,3]''')\n    self.sampler = sampler\n    self.true_measure = Uniform(self.sampler)\n    self.d = self.true_measure.d\n    if self.kind_coeff==1: self.c = (np.arange(1,self.d+1)-.5)/self.d\n    elif self.kind_coeff==2: self.c = 1/np.arange(1,self.d+1)**2\n    elif self.kind_coeff==3: self.c = np.exp(np.arange(1,self.d+1)*np.log(10**(-8))/self.d)\n    if self.kind_func=='OSCILLATORY':\n        self.g = self.g_oscillatory\n        self.c = 4.5*self.c/self.c.sum()\n    elif self.kind_func=='CORNER PEAK':\n        self.g = self.g_corner_peak\n        self.c = 0.25*self.c/self.c.sum()\n    self.c = self.c[None,:]\n    self.parameters = ['kind_func','kind_coeff']\n    super(Genz,self).__init__(dimension_indv=(),dimension_comb=(),parallel=False)\n</code></pre>"},{"location":"api/integrands/#boxintegral","title":"<code>BoxIntegral</code>","text":"<p>               Bases: <code>AbstractIntegrand</code></p> <p>Box integral from [1], see also </p> \\[B_s(\\boldsymbol{t}) = \\left(\\sum_{j=1}^d t_j^2 \\right)^{s/2}, \\qquad \\boldsymbol{T} \\sim \\mathcal{U}[0,1]^d.\\] <p>Examples:</p> <p>Scalar <code>s</code> </p> <pre><code>&gt;&gt;&gt; integrand = BoxIntegral(DigitalNetB2(2,seed=7),s=7)\n&gt;&gt;&gt; y = integrand(2**10)\n&gt;&gt;&gt; y.shape\n(1024,)\n&gt;&gt;&gt; print(\"%.4f\"%y.mean(0))\n0.7519\n</code></pre> <p>With independent replications</p> <pre><code>&gt;&gt;&gt; integrand = BoxIntegral(DigitalNetB2(2,seed=7,replications=2**4),s=7)\n&gt;&gt;&gt; y = integrand(2**10)\n&gt;&gt;&gt; y.shape\n(16, 1024)\n&gt;&gt;&gt; muhats = y.mean(1) \n&gt;&gt;&gt; muhats.shape \n(16,)\n&gt;&gt;&gt; print(\"%.4f\"%muhats.mean(0))\n0.7518\n</code></pre> <p>Array <code>s</code></p> <pre><code>&gt;&gt;&gt; integrand = BoxIntegral(DigitalNetB2(5,seed=7),s=np.arange(6).reshape((2,3)))\n&gt;&gt;&gt; y = integrand(2**10)\n&gt;&gt;&gt; y.shape\n(2, 3, 1024)\n&gt;&gt;&gt; y.mean(-1)\narray([[1.        , 1.26234461, 1.66666661],\n       [2.28201516, 3.22195096, 4.67188113]])\n</code></pre> <p>With independent replications</p> <pre><code>&gt;&gt;&gt; integrand = BoxIntegral(DigitalNetB2(2,seed=7,replications=2**4),s=np.arange(6).reshape((2,3)))\n&gt;&gt;&gt; y = integrand(2**10)\n&gt;&gt;&gt; y.shape\n(2, 3, 16, 1024)\n&gt;&gt;&gt; muhats = y.mean(-1) \n&gt;&gt;&gt; muhats.shape \n(2, 3, 16)\n&gt;&gt;&gt; muhats.mean(-1)\narray([[1.        , 0.76519118, 0.66666666],\n       [0.62718785, 0.62224086, 0.64273341]])\n</code></pre> <p>References:</p> <ol> <li>D.H. Bailey, J.M. Borwein, R.E. Crandall, Box integrals.     Journal of Computational and Applied Mathematics, Volume 206, Issue 1, 2007, Pages 196-208, ISSN 0377-0427. https://doi.org/10.1016/j.cam.2006.06.010. https://www.sciencedirect.com/science/article/pii/S0377042706004250. https://www.davidhbailey.com/dhbpapers/boxintegrals.pdf</li> </ol> <p>Parameters:</p> Name Type Description Default <code>sampler</code> <code>Union[AbstractDiscreteDistribution, AbstractTrueMeasure]</code> <p>Either  </p> <ul> <li>a discrete distribution from which to transform samples, or</li> <li>a true measure by which to compose a transform.</li> </ul> required <code>s</code> <code>Union[float, ndarray]</code> <p><code>s</code> parameter or parameters. The output shape of <code>g</code> is the shape of <code>s</code>.</p> <code>1</code> Source code in <code>qmcpy/integrand/box_integral.py</code> <pre><code>def __init__(self, sampler, s=1):\n    r\"\"\"\n    Args:\n        sampler (Union[AbstractDiscreteDistribution,AbstractTrueMeasure]): Either  \n\n            - a discrete distribution from which to transform samples, or\n            - a true measure by which to compose a transform.\n        s (Union[float,np.ndarray]): `s` parameter or parameters. The output shape of `g` is the shape of `s`. \n    \"\"\"\n    self.parameters = ['s']\n    self.s = np.array(s)\n    assert self.s.size&gt;0\n    self.sampler = sampler\n    self.true_measure = Uniform(self.sampler)\n    self.s_over_2 = self.s/2\n    super(BoxIntegral,self).__init__(dimension_indv=self.s.shape,dimension_comb=self.s.shape,parallel=False)\n</code></pre>"},{"location":"api/integrands/#ishigami","title":"<code>Ishigami</code>","text":"<p>               Bases: <code>AbstractIntegrand</code></p> <p>Ishigami function in \\(d=3\\) dimensions from [1] and https://www.sfu.ca/~ssurjano/ishigami.html. </p> \\[g(\\boldsymbol{t}) = (1+bt_2^4)\\sin(t_0)+a\\sin^2(t_1), \\qquad \\boldsymbol{T} = (T_0,T_1,T_2) \\sim \\mathcal{U}(-\\pi,\\pi)^3.\\] <p>Examples:</p> <pre><code>&gt;&gt;&gt; integrand = Ishigami(DigitalNetB2(3,seed=7))\n&gt;&gt;&gt; y = integrand(2**10)\n&gt;&gt;&gt; y.shape \n(1024,)\n&gt;&gt;&gt; print(\"%.4f\"%y.mean())\n3.5000\n&gt;&gt;&gt; integrand.true_measure\nUniform (AbstractTrueMeasure)\n    lower_bound     -3.142\n    upper_bound     3.142\n</code></pre> <p>With independent replications</p> <pre><code>&gt;&gt;&gt; integrand = Ishigami(DigitalNetB2(3,seed=7,replications=2**4))\n&gt;&gt;&gt; y = integrand(2**6)\n&gt;&gt;&gt; y.shape\n(16, 64)\n&gt;&gt;&gt; muhats = y.mean(-1) \n&gt;&gt;&gt; muhats.shape \n(16,)\n&gt;&gt;&gt; print(\"%.4f\"%muhats.mean())\n3.4646\n</code></pre> <p>References:</p> <ol> <li>Ishigami, T., &amp; Homma, T.     An importance quantification technique in uncertainty analysis for computer models.     In Uncertainty Modeling and Analysis, 1990.     Proceedings, First International Symposium on (pp. 398-403). IEEE.</li> </ol> <p>Parameters:</p> Name Type Description Default <code>sampler</code> <code>Union[AbstractDiscreteDistribution, AbstractTrueMeasure]</code> <p>Either  </p> <ul> <li>a discrete distribution from which to transform samples, or</li> <li>a true measure by which to compose a transform.</li> </ul> required <code>a</code> <code>float</code> <p>First parameter \\(a\\).</p> <code>7</code> <code>b</code> <code>float</code> <p>Second parameter \\(b\\).</p> <code>0.1</code> Source code in <code>qmcpy/integrand/ishigami.py</code> <pre><code>def __init__(self,sampler, a=7, b=.1):\n    r\"\"\"\n    Args:\n        sampler (Union[AbstractDiscreteDistribution,AbstractTrueMeasure]): Either  \n\n            - a discrete distribution from which to transform samples, or\n            - a true measure by which to compose a transform.\n        a (float): First parameter $a$.\n        b (float): Second parameter $b$.\n    \"\"\"\n    self.sampler = sampler\n    if self.sampler.d != 3:\n        raise ParameterError(\"Ishigami integrand requires 3 dimensional sampler\")\n    self.a = a\n    self.b = b\n    self.true_measure = Uniform(self.sampler, lower_bound=-np.pi, upper_bound=np.pi)\n    super(Ishigami,self).__init__(dimension_indv=(),dimension_comb=(),parallel=False)\n</code></pre>"},{"location":"api/integrands/#multimodal2d","title":"<code>Multimodal2d</code>","text":"<p>               Bases: <code>AbstractIntegrand</code></p> <p>Multimodal function in \\(d=2\\) dimensions. </p> \\[g(\\boldsymbol{t}) = (t_0^2+4)(t_1-1)/20-\\sin(5t_0/2)-2 \\qquad \\boldsymbol{T} = (T_0,T_1) \\sim \\mathcal{U}([-4,7] \\times [-3,8]).\\] <p>Examples:</p> <pre><code>&gt;&gt;&gt; integrand = Multimodal2d(DigitalNetB2(2,seed=7))\n&gt;&gt;&gt; y = integrand(2**10)\n&gt;&gt;&gt; print(\"%.4f\"%y.mean())\n-0.7365\n&gt;&gt;&gt; integrand.true_measure\nUniform (AbstractTrueMeasure)\n    lower_bound     [-4 -3]\n    upper_bound     [7 8]\n</code></pre> <p>With independent replications</p> <pre><code>&gt;&gt;&gt; integrand = Multimodal2d(DigitalNetB2(2,seed=7,replications=2**4))\n&gt;&gt;&gt; y = integrand(2**6)\n&gt;&gt;&gt; y.shape\n(16, 64)\n&gt;&gt;&gt; muhats = y.mean(-1) \n&gt;&gt;&gt; muhats.shape \n(16,)\n&gt;&gt;&gt; print(\"%.4f\"%muhats.mean())\n-0.7366\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>sampler</code> <code>Union[AbstractDiscreteDistribution, AbstractTrueMeasure]</code> <p>Either  </p> <ul> <li>a discrete distribution from which to transform samples, or</li> <li>a true measure by which to compose a transform.</li> </ul> required Source code in <code>qmcpy/integrand/multimodal2d.py</code> <pre><code>def __init__(self, sampler):\n    r\"\"\"\n    Args:\n        sampler (Union[AbstractDiscreteDistribution,AbstractTrueMeasure]): Either  \n\n            - a discrete distribution from which to transform samples, or\n            - a true measure by which to compose a transform.\n    \"\"\"\n    self.sampler = sampler \n    assert self.sampler.d==2\n    self.true_measure = Uniform(self.sampler,lower_bound=[-4,-3],upper_bound=[7,8])\n    super(Multimodal2d,self).__init__(dimension_indv=(),dimension_comb=(),parallel=False)\n</code></pre>"},{"location":"api/integrands/#hartmann6d","title":"<code>Hartmann6d</code>","text":"<p>               Bases: <code>AbstractIntegrand</code></p> <p>Wrapper around <code>BoTorch</code>'s implementation of the Augmented Hartmann function in dimension \\(d=6\\). </p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; integrand = Hartmann6d(DigitalNetB2(6,seed=7))\n&gt;&gt;&gt; y = integrand(2**10)\n&gt;&gt;&gt; print(\"%.4f\"%y.mean())\n-0.2644\n&gt;&gt;&gt; integrand.true_measure\nUniform (AbstractTrueMeasure)\n    lower_bound     0\n    upper_bound     1\n</code></pre> <p>With independent replications</p> <pre><code>&gt;&gt;&gt; integrand = Hartmann6d(DigitalNetB2(6,seed=7,replications=2**4))\n&gt;&gt;&gt; y = integrand(2**6)\n&gt;&gt;&gt; y.shape\n(16, 64)\n&gt;&gt;&gt; muhats = y.mean(-1) \n&gt;&gt;&gt; muhats.shape \n(16,)\n&gt;&gt;&gt; print(\"%.4f\"%muhats.mean())\n-0.2599\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>sampler</code> <code>Union[AbstractDiscreteDistribution, AbstractTrueMeasure]</code> <p>Either  </p> <ul> <li>a discrete distribution from which to transform samples, or</li> <li>a true measure by which to compose a transform.</li> </ul> required Source code in <code>qmcpy/integrand/hartmann6d.py</code> <pre><code>def __init__(self, sampler):\n    r\"\"\"\n    Args:\n        sampler (Union[AbstractDiscreteDistribution,AbstractTrueMeasure]): Either  \n\n            - a discrete distribution from which to transform samples, or\n            - a true measure by which to compose a transform.\n    \"\"\"\n    self.sampler = sampler\n    assert self.sampler.d==6\n    self.true_measure = Uniform(self.sampler,lower_bound=0,upper_bound=1)\n    super(Hartmann6d,self).__init__(dimension_indv=(),dimension_comb=(),parallel=False)\n    from botorch.test_functions.multi_fidelity import AugmentedHartmann\n    self.ah = AugmentedHartmann(negate=False)\n</code></pre>"},{"location":"api/integrands/#fourbranch2d","title":"<code>FourBranch2d</code>","text":"<p>               Bases: <code>AbstractIntegrand</code></p> <p>Four Branch function in \\(d=2\\). </p> \\[g(\\boldsymbol{t}) = \\min \\begin{cases} 3+0.1(t_0-t_1)^2-\\frac{t_0-t_1}{\\sqrt{2}} \\\\ 3+0.1(t_0-t_1)^2+\\frac{t_0-t_1}{\\sqrt{2}} \\\\ t_0-t_1 + 7/\\sqrt{2} \\\\ t_1-t_0 + 7/\\sqrt{2}\\end{cases}, \\qquad \\boldsymbol{T}=(T_0,T_1) \\sim \\mathcal{U}[-8,8]^2.\\] <p>Examples:</p> <pre><code>&gt;&gt;&gt; integrand = FourBranch2d(DigitalNetB2(2,seed=7))\n&gt;&gt;&gt; y = integrand(2**10)\n&gt;&gt;&gt; print(\"%.4f\"%y.mean())\n-2.4995\n&gt;&gt;&gt; integrand.true_measure\nUniform (AbstractTrueMeasure)\n    lower_bound     -8\n    upper_bound     2^(3)\n</code></pre> <p>With independent replications</p> <pre><code>&gt;&gt;&gt; integrand = FourBranch2d(DigitalNetB2(2,seed=7,replications=2**4))\n&gt;&gt;&gt; y = integrand(2**6)\n&gt;&gt;&gt; y.shape\n(16, 64)\n&gt;&gt;&gt; muhats = y.mean(-1) \n&gt;&gt;&gt; muhats.shape \n(16,)\n&gt;&gt;&gt; print(\"%.4f\"%muhats.mean())\n-2.5042\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>sampler</code> <code>Union[AbstractDiscreteDistribution, AbstractTrueMeasure]</code> <p>Either  </p> <ul> <li>a discrete distribution from which to transform samples, or</li> <li>a true measure by which to compose a transform.</li> </ul> required Source code in <code>qmcpy/integrand/fourbranch2d.py</code> <pre><code>def __init__(self, sampler):\n    r\"\"\"\n    Args:\n        sampler (Union[AbstractDiscreteDistribution,AbstractTrueMeasure]): Either  \n\n            - a discrete distribution from which to transform samples, or\n            - a true measure by which to compose a transform.\n    \"\"\"\n    self.sampler = sampler\n    assert self.sampler.d==2\n    self.true_measure = Uniform(self.sampler,lower_bound=-8,upper_bound=8)\n    super(FourBranch2d,self).__init__(dimension_indv=(),dimension_comb=(),parallel=False)\n</code></pre>"},{"location":"api/integrands/#bayesianlrcoeffs","title":"<code>BayesianLRCoeffs</code>","text":"<p>               Bases: <code>AbstractIntegrand</code></p> <p>Logistic Regression Coefficients computed as the posterior mean in a Bayesian framework.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; integrand = BayesianLRCoeffs(DigitalNetB2(3,seed=7),feature_array=np.arange(8).reshape((4,2)),response_vector=[0,0,1,1])\n&gt;&gt;&gt; y = integrand(2**10)\n&gt;&gt;&gt; y.shape\n(2, 3, 1024)\n&gt;&gt;&gt; y.mean(-1)\narray([[ 0.04517466, -0.01103669, -0.06614381],\n       [ 0.02162049,  0.02162049,  0.02162049]])\n</code></pre> <p>With independent replications</p> <pre><code>&gt;&gt;&gt; integrand = BayesianLRCoeffs(DigitalNetB2(3,seed=7,replications=2**4),feature_array=np.arange(8).reshape((4,2)),response_vector=[0,0,1,1])\n&gt;&gt;&gt; y = integrand(2**6)\n&gt;&gt;&gt; y.shape\n(2, 3, 16, 64)\n&gt;&gt;&gt; muhats = y.mean(-1) \n&gt;&gt;&gt; muhats.shape \n(2, 3, 16)\n&gt;&gt;&gt; muhats.mean(-1)\narray([[ 0.0587368 , -0.01718134, -0.07203021],\n       [ 0.02498059,  0.02498059,  0.02498059]])\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>sampler</code> <code>Union[AbstractDiscreteDistribution, AbstractTrueMeasure]</code> <p>Either  </p> <ul> <li>a discrete distribution from which to transform samples, or</li> <li>a true measure by which to compose a transform.</li> </ul> required <code>feature_array</code> <code>ndarray</code> <p>Array of features with shape \\((N,d-1)\\) where \\(N\\) is the number of observations and \\(d\\) is the dimension.</p> required <code>response_vector</code> <code>ndarray</code> <p>Binary responses vector of length \\(N\\).</p> required <code>prior_mean</code> <code>ndarray</code> <p>Length \\(d\\) vector of prior means, one for each coefficient.</p> <ul> <li>The first \\(d-1\\) inputs correspond to the \\(d-1\\) features. </li> <li>The last input corresponds to the intercept coefficient.</li> </ul> <code>0</code> <code>prior_covariance</code> <code>ndarray</code> <p>Prior covariance array with shape \\((d,d)\\) d x d where indexing is consistent with the prior mean.</p> <code>10</code> Source code in <code>qmcpy/integrand/bayesian_lr_coeffs.py</code> <pre><code>def __init__(self, sampler, feature_array, response_vector, prior_mean=0, prior_covariance=10):\n    r\"\"\"\n    Args:\n        sampler (Union[AbstractDiscreteDistribution,AbstractTrueMeasure]): Either  \n\n            - a discrete distribution from which to transform samples, or\n            - a true measure by which to compose a transform.\n        feature_array (np.ndarray): Array of features with shape $(N,d-1)$ where $N$ is the number of observations and $d$ is the dimension.\n        response_vector (np.ndarray): Binary responses vector of length $N$.\n        prior_mean (np.ndarray): Length $d$ vector of prior means, one for each coefficient.\n\n            - The first $d-1$ inputs correspond to the $d-1$ features. \n            - The last input corresponds to the intercept coefficient.\n        prior_covariance (np.ndarray): Prior covariance array with shape $(d,d)$ d x d where indexing is consistent with the prior mean.\n    \"\"\"\n    self.prior_mean = prior_mean\n    self.prior_covariance = prior_covariance\n    self.sampler = sampler\n    self.true_measure = Gaussian(self.sampler, mean=self.prior_mean, covariance=self.prior_covariance)\n    self.feature_array = np.array(feature_array,dtype=float)\n    self.response_vector = np.array(response_vector,dtype=float)\n    obs,dm1 = self.feature_array.shape\n    self.num_coeffs = dm1+1\n    if self.num_coeffs!=self.true_measure.d:\n        ParameterError(\"sampler must have dimension one more than the number of features in the feature_array.\")\n    if self.response_vector.shape!=(obs,) or ((self.response_vector!=0)&amp;(self.response_vector!=1)).any():\n        ParameterError(\"response_vector must have the same length as feature_array and contain only 0 or 1 entries.\")\n    self.feature_array = np.column_stack((self.feature_array,np.ones((obs,1))))\n    super(BayesianLRCoeffs,self).__init__(dimension_indv=(2,self.num_coeffs),dimension_comb=self.num_coeffs,parallel=False)\n</code></pre>"},{"location":"api/integrands/#sin1d","title":"<code>Sin1d</code>","text":"<p>               Bases: <code>AbstractIntegrand</code></p> <p>Sine function in \\(d=1\\) dimension.  </p> \\[g(t) = \\sin(t), \\qquad t \\sim \\mathcal{U}[0,2\\pi k]\\] <p>Examples:</p> <pre><code>&gt;&gt;&gt; integrand = Sin1d(DigitalNetB2(1,seed=7),k=1)\n&gt;&gt;&gt; y = integrand(2**10)\n&gt;&gt;&gt; print(\"%.4e\"%y.mean())\n-1.3582e-10\n&gt;&gt;&gt; integrand.true_measure\nUniform (AbstractTrueMeasure)\n    lower_bound     0\n    upper_bound     6.283\n</code></pre> <p>With independent replications</p> <pre><code>&gt;&gt;&gt; integrand = Sin1d(DigitalNetB2(1,seed=7,replications=2**4),k=1)\n&gt;&gt;&gt; y = integrand(2**6)\n&gt;&gt;&gt; y.shape\n(16, 64)\n&gt;&gt;&gt; muhats = y.mean(-1) \n&gt;&gt;&gt; muhats.shape \n(16,)\n&gt;&gt;&gt; print(\"%.4e\"%muhats.mean())\n7.0800e-04\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>sampler</code> <code>Union[AbstractDiscreteDistribution, AbstractTrueMeasure]</code> <p>Either  </p> <ul> <li>a discrete distribution from which to transform samples, or</li> <li>a true measure by which to compose a transform.</li> </ul> required <code>k</code> <code>float</code> <p>The true measure will be uniform between \\(0\\) and \\(2 \\pi k\\).</p> <code>1</code> Source code in <code>qmcpy/integrand/sin1d.py</code> <pre><code>def __init__(self, sampler, k=1):\n    r\"\"\"\n    Args:\n        sampler (Union[AbstractDiscreteDistribution,AbstractTrueMeasure]): Either  \n\n            - a discrete distribution from which to transform samples, or\n            - a true measure by which to compose a transform.\n        k (float): The true measure will be uniform between $0$ and $2 \\pi k$.\n    \"\"\"\n    self.sampler = sampler\n    self.k = k\n    assert self.sampler.d==1\n    self.true_measure = Uniform(self.sampler,lower_bound=0,upper_bound=2*self.k*np.pi)\n    super(Sin1d,self).__init__(dimension_indv=(),dimension_comb=(),parallel=False)\n</code></pre>"},{"location":"api/integrands/#linear0","title":"<code>Linear0</code>","text":"<p>               Bases: <code>AbstractIntegrand</code></p> <p>Linear Function with analytic mean \\(0\\). </p> \\[g(\\boldsymbol{t}) = \\sum_{j=1}^d t_j \\qquad \\boldsymbol{T} \\sim \\mathcal{U}[0,1]^d.\\] <p>Examples:</p> <pre><code>&gt;&gt;&gt; integrand = Linear0(DigitalNetB2(100,seed=7))\n&gt;&gt;&gt; y = integrand(2**10)\n&gt;&gt;&gt; print(\"%.4e\"%y.mean())\n6.0560e-05\n</code></pre> <p>With independent replications</p> <pre><code>&gt;&gt;&gt; integrand = Linear0(DigitalNetB2(100,seed=7,replications=2**4))\n&gt;&gt;&gt; y = integrand(2**6)\n&gt;&gt;&gt; y.shape\n(16, 64)\n&gt;&gt;&gt; muhats = y.mean(-1) \n&gt;&gt;&gt; muhats.shape \n(16,)\n&gt;&gt;&gt; print(\"%.4e\"%muhats.mean())\n-9.8203e-05\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>sampler</code> <code>Union[AbstractDiscreteDistribution, AbstractTrueMeasure]</code> <p>Either  </p> <ul> <li>a discrete distribution from which to transform samples, or</li> <li>a true measure by which to compose a transform.</li> </ul> required Source code in <code>qmcpy/integrand/linear0.py</code> <pre><code>def __init__(self, sampler):\n    r\"\"\"\n    Args:\n        sampler (Union[AbstractDiscreteDistribution,AbstractTrueMeasure]): Either  \n\n            - a discrete distribution from which to transform samples, or\n            - a true measure by which to compose a transform.\n    \"\"\"\n    self.sampler = sampler\n    self.true_measure = Uniform(self.sampler, lower_bound=-.5, upper_bound=.5)\n    super(Linear0,self).__init__(dimension_indv=(),dimension_comb=(),parallel=False)\n</code></pre>"},{"location":"api/integrands/#uml-specific","title":"UML Specific","text":""},{"location":"api/kernels/","title":"Kernels","text":""},{"location":"api/kernels/#uml-overview","title":"UML Overview","text":""},{"location":"api/kernels/#abstractkernel","title":"<code>AbstractKernel</code>","text":"<p>               Bases: <code>object</code></p> Source code in <code>qmcpy/kernel/abstract_kernel.py</code> <pre><code>def __init__(self, d, torchify, device, compile_call, comiple_call_kwargs):\n    super().__init__()\n    # dimension \n    assert d%1==0 and d&gt;0, \"dimension d must be a positive int\"\n    self.d = d\n    # torchify\n    self.torchify = torchify \n    if self.torchify: \n        import torch \n        self.npt = torch\n        self.nptarray = torch.tensor\n        self.nptarraytype = torch.Tensor\n        self.device = torch.device(device) \n        self.nptkwargs = {\"device\":device}\n    else:\n        self.npt = np\n        self.nptarray = np.array\n        self.nptarraytype = np.ndarray\n        self.device = None\n        self.nptkwargs = {}\n    self.batch_param_names = []\n    if compile_call:\n        assert self.torchify, \"compile_call requires torchify is True\"\n        import torch\n        self.compiled_parsed___call__ = torch.compile(self.parsed___call__,**comiple_call_kwargs)\n    else:\n        self.compiled_parsed___call__ = self.parsed___call__\n</code></pre>"},{"location":"api/kernels/#qmcpy.kernel.abstract_kernel.AbstractKernel.__call__","title":"__call__","text":"<pre><code>__call__(x0, x1, beta0=None, beta1=None, c=None)\n</code></pre> <p>Evaluate the kernel with (optional) partial derivatives </p> \\[\\sum_{\\ell=1}^p c_{\\ell} \\partial_{\\boldsymbol{x}_0}^{\\boldsymbol{\\beta}_{\\ell 0}} \\partial_{\\boldsymbol{x}_1}^{\\boldsymbol{\\beta}_{\\ell 1}} K(\\boldsymbol{x}_0,\\boldsymbol{x}_1).\\] <p>Parameters:</p> Name Type Description Default <code>x0</code> <code>Union[ndarray, Tensor]</code> <p>Shape <code>x0.shape=(...,d)</code> first input to kernel with </p> required <code>x1</code> <code>Union[ndarray, Tensor]</code> <p>Shape <code>x1.shape=(...,d)</code> second input to kernel with </p> required <code>beta0</code> <code>Union[ndarray, Tensor]</code> <p>Shape <code>beta0.shape=(p,d)</code> derivative orders with respect to first inputs, \\(\\boldsymbol{\\beta}_0\\).</p> <code>None</code> <code>beta1</code> <code>Union[ndarray, Tensor]</code> <p>Shape <code>beta1.shape=(p,d)</code> derivative orders with respect to first inputs, \\(\\boldsymbol{\\beta}_1\\).</p> <code>None</code> <code>c</code> <code>Union[ndarray, Tensor]</code> <p>Shape <code>c.shape=(p,)</code> coefficients of derivatives.</p> <code>None</code> <p>Returns:     k (Union[np.ndarray,torch.Tensor]): Shape <code>y.shape=(x0+x1).shape[:-1]</code> kernel evaluations.</p> Source code in <code>qmcpy/kernel/abstract_kernel.py</code> <pre><code>def __call__(self, x0, x1, beta0=None, beta1=None, c=None):\n    r\"\"\"\n    Evaluate the kernel with (optional) partial derivatives \n\n    $$\\sum_{\\ell=1}^p c_{\\ell} \\partial_{\\boldsymbol{x}_0}^{\\boldsymbol{\\beta}_{\\ell 0}} \\partial_{\\boldsymbol{x}_1}^{\\boldsymbol{\\beta}_{\\ell 1}} K(\\boldsymbol{x}_0,\\boldsymbol{x}_1).$$\n\n    Args:\n        x0 (Union[np.ndarray,torch.Tensor]): Shape `x0.shape=(...,d)` first input to kernel with \n        x1 (Union[np.ndarray,torch.Tensor]): Shape `x1.shape=(...,d)` second input to kernel with \n        beta0 (Union[np.ndarray,torch.Tensor]): Shape `beta0.shape=(p,d)` derivative orders with respect to first inputs, $\\boldsymbol{\\beta}_0$.\n        beta1 (Union[np.ndarray,torch.Tensor]): Shape `beta1.shape=(p,d)` derivative orders with respect to first inputs, $\\boldsymbol{\\beta}_1$.\n        c (Union[np.ndarray,torch.Tensor]): Shape `c.shape=(p,)` coefficients of derivatives.\n    Returns:\n        k (Union[np.ndarray,torch.Tensor]): Shape `y.shape=(x0+x1).shape[:-1]` kernel evaluations. \n    \"\"\"\n    assert isinstance(x0,self.nptarraytype) \n    assert isinstance(x0,self.nptarraytype) \n    assert x0.shape[-1]==self.d, \"the size of the last dimension of x0 must equal d=%d, got x0.shape=%s\"%(self.d,str(tuple(x0.shape)))\n    assert x1.shape[-1]==self.d, \"the size of the last dimension of x1 must equal d=%d, got x1.shape=%s\"%(self.d,str(tuple(x1.shape)))\n    if beta0 is None:\n        beta0 = self.npt.zeros((1,self.d),dtype=int,**self.nptkwargs)\n    if beta1 is None:\n        beta1 = self.npt.zeros((1,self.d),dtype=int,**self.nptkwargs)\n    if not isinstance(beta0,self.nptarraytype):\n        beta0 = self.nptarray(beta0)\n    if not isinstance(beta1,self.nptarraytype):\n        beta1 = self.nptarray(beta1)\n    beta0 = self.npt.atleast_2d(beta0)\n    beta1 = self.npt.atleast_2d(beta1)\n    assert beta0.ndim==2 and beta1.ndim==2, \"beta0 and beta1 must both be 2 dimensional\"\n    p = beta0.shape[0]\n    assert beta0.shape==(p,self.d), \"expected beta0.shape=(%d,%d) but got beta0.shape=%s\"%(p,self.d,str(tuple(beta0.shape)))\n    assert beta1.shape==(p,self.d), \"expected beta1.shape=(%d,%d) but got beta1.shape=%s\"%(p,self.d,str(tuple(beta1.shape)))\n    assert (beta0%1==0).all() and (beta0&gt;=0).all(), \"require int beta0 &gt;= 0\"\n    assert (beta1%1==0).all() and (beta1&gt;=0).all(), \"require int beta1 &gt;= 0\"\n    if c is None:\n        c = self.npt.ones(p,**self.nptkwargs)\n    if not isinstance(c,self.nptarraytype):\n        c = self.nptarray(c) \n    c = self.npt.atleast_1d(c) \n    assert c.shape==(p,), \"expected c.shape=(%d,) but got c.shape=%s\"%(p,str(tuple(c.shape)))\n    if not self.AUTOGRADKERNEL:\n        batch_params = self.get_batch_params(max(x0.ndim-1,x1.ndim-1))\n        k = self.compiled_parsed___call__(x0,x1,beta0,beta1,c,batch_params)\n    else:\n        if (beta0==0).all() and (beta1==0).all():\n            batch_params = self.get_batch_params(max(x0.ndim-1,x1.ndim-1))\n            k = c.sum()*self.compiled_parsed___call__(x0,x1,batch_params)\n        else: # requires autograd, so self.npt=torch\n            assert self.torchify, \"autograd requires torchify=True\"\n            import torch\n            incoming_grad_enabled = torch.is_grad_enabled()\n            torch.set_grad_enabled(True)\n            incoming_grad_enabled_params = {pname: param.requires_grad for pname,param in self.named_parameters()}\n            if not incoming_grad_enabled:\n                for pname,param in self.named_parameters():\n                    param.requires_grad_(False)\n            if (beta0&gt;0).any():\n                tileshapex0 = tuple(self.npt.ceil(self.npt.tensor(x1.shape[:-1])/self.npt.tensor(x0.shape[:-1])).to(int))\n                x0gs = [self.npt.tile(x0[...,j].clone().requires_grad_(True),tileshapex0) for j in range(self.d)]\n                [x0gj.requires_grad_(True) for x0gj in x0gs]\n                x0g = self.npt.stack(x0gs,dim=-1)\n            else:\n                x0g = x0\n            if (beta1&gt;0).any():\n                tileshapex1 = tuple(self.npt.ceil(self.npt.tensor(x0.shape[:-1])/self.npt.tensor(x1.shape[:-1])).to(int))\n                x1gs = [self.npt.tile(x1[...,j].clone().requires_grad_(True),tileshapex1) for j in range(self.d)]\n                [x1gj.requires_grad_(True) for x1gj in x1gs]\n                x1g = self.npt.stack(x1gs,dim=-1)\n            else:\n                x1g = x1\n            batch_params = self.get_batch_params(max(x0.ndim-1,x1.ndim-1))\n            k = 0.\n            k_base = self.compiled_parsed___call__(x0g,x1g,batch_params)\n            for l in range(p):\n                if (beta0[l]&gt;0).any() or (beta1[l]&gt;0).any():\n                    k_part = k_base.clone()\n                    for j0 in range(self.d):\n                        for _ in range(beta0[l,j0]):\n                            k_part = torch.autograd.grad(k_part,x0gs[j0],grad_outputs=torch.ones_like(k_part,requires_grad=True),create_graph=True)[0]\n                    for j1 in range(self.d):\n                        for _ in range(beta1[l,j1]):\n                            k_part = torch.autograd.grad(k_part,x1gs[j1],grad_outputs=torch.ones_like(k_part,requires_grad=True),create_graph=True)[0]\n                else:\n                    k_part = k_base \n                k += c[l]*k_part\n            if not incoming_grad_enabled:\n                for pname,param in self.named_parameters():\n                    param.requires_grad_(incoming_grad_enabled_params[pname])\n            if (not incoming_grad_enabled) or ((not any(incoming_grad_enabled_params.values())) and (not x0.requires_grad) and (not x1.requires_grad)):\n                k = k.detach()\n            torch.set_grad_enabled(incoming_grad_enabled)\n    return k\n</code></pre>"},{"location":"api/kernels/#qmcpy.kernel.abstract_kernel.AbstractKernel.single_integral_01d","title":"single_integral_01d","text":"<pre><code>single_integral_01d(x)\n</code></pre> <p>Evaluate the integral of the kernel over the unit cube</p> \\[\\tilde{K}(\\boldsymbol{x}) = \\int_{[0,1]^d} K(\\boldsymbol{x},\\boldsymbol{z}) \\; \\mathrm{d} \\boldsymbol{z}.\\] <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Union[ndarray, Tensor]</code> <p>Shape <code>x0.shape=(...,d)</code> first input to kernel with </p> required <p>Returns:</p> Name Type Description <code>tildek</code> <code>Union[ndarray, Tensor]</code> <p>Shape <code>y.shape=x.shape[:-1]</code> integral kernel evaluations.</p> Source code in <code>qmcpy/kernel/abstract_kernel.py</code> <pre><code>def single_integral_01d(self, x):\n    r\"\"\"\n    Evaluate the integral of the kernel over the unit cube\n\n    $$\\tilde{K}(\\boldsymbol{x}) = \\int_{[0,1]^d} K(\\boldsymbol{x},\\boldsymbol{z}) \\; \\mathrm{d} \\boldsymbol{z}.$$\n\n    Args:\n        x (Union[np.ndarray,torch.Tensor]): Shape `x0.shape=(...,d)` first input to kernel with \n\n    Returns:\n        tildek (Union[np.ndarray,torch.Tensor]): Shape `y.shape=x.shape[:-1]` integral kernel evaluations. \n    \"\"\"\n    if self.npt==np:\n        assert isinstance(x,np.ndarray) \n    else: # self.npt==torch\n        assert isinstance(x,self.npt.Tensor)\n    assert x.shape[-1]==self.d, \"the size of the last dimension of x must equal d=%d, got x.shape=%s\"%(self.d,str(tuple(x.shape)))\n    batch_params = self.get_batch_params(x.ndim-1)\n    return self.parsed_single_integral_01d(x,batch_params)\n</code></pre>"},{"location":"api/kernels/#qmcpy.kernel.abstract_kernel.AbstractKernel.double_integral_01d","title":"double_integral_01d","text":"<pre><code>double_integral_01d()\n</code></pre> <p>Evaluate the integral of the kernel over the unit cube</p> \\[\\tilde{K} = \\int_{[0,1]^d} \\int_{[0,1]^d} K(\\boldsymbol{x},\\boldsymbol{z}) \\; \\mathrm{d} \\boldsymbol{x} \\; \\mathrm{d} \\boldsymbol{z}.\\] <p>Returns:</p> Name Type Description <code>tildek</code> <code>Union[ndarray, Tensor]</code> <p>Double integral kernel evaluations.</p> Source code in <code>qmcpy/kernel/abstract_kernel.py</code> <pre><code>def double_integral_01d(self):\n    r\"\"\"\n    Evaluate the integral of the kernel over the unit cube\n\n    $$\\tilde{K} = \\int_{[0,1]^d} \\int_{[0,1]^d} K(\\boldsymbol{x},\\boldsymbol{z}) \\; \\mathrm{d} \\boldsymbol{x} \\; \\mathrm{d} \\boldsymbol{z}.$$\n\n    Returns:\n        tildek (Union[np.ndarray,torch.Tensor]): Double integral kernel evaluations.\n    \"\"\"\n    raise MethodImplementationError(self, 'double_integral_01d')\n</code></pre>"},{"location":"api/kernels/#abstractkernelscalelengthscales","title":"<code>AbstractKernelScaleLengthscales</code>","text":"<p>               Bases: <code>AbstractKernel</code></p> <p>Parameters:</p> Name Type Description Default <code>d</code> <code>int</code> <p>Dimension. </p> required <code>scale</code> <code>Union[ndarray, Tensor]</code> <p>Scaling factor \\(S\\).</p> <code>1.0</code> <code>lengthscales</code> <code>Union[ndarray, Tensor]</code> <p>Lengthscales \\(\\boldsymbol{\\gamma}\\).</p> <code>1.0</code> <code>shape_scale</code> <code>list</code> <p>Shape of <code>scale</code> when <code>np.isscalar(scale)</code>. </p> <code>[1]</code> <code>shape_lengthscales</code> <code>list</code> <p>Shape of <code>lengthscales</code> when <code>np.isscalar(lengthscales)</code></p> <code>None</code> <code>tfs_scale</code> <code>Tuple[callable, callable]</code> <p>The first argument transforms to the raw value to be optimized; the second applies the inverse transform.</p> <code>(tf_exp_eps_inv, tf_exp_eps)</code> <code>tfs_lengthscales</code> <code>Tuple[callable, callable]</code> <p>The first argument transforms to the raw value to be optimized; the second applies the inverse transform.</p> <code>(tf_exp_eps_inv, tf_exp_eps)</code> <code>torchify</code> <code>bool</code> <p>If <code>True</code>, use the <code>torch</code> backend. Set to <code>True</code> if computing gradients with respect to inputs and/or hyperparameters.</p> <code>False</code> <code>requires_grad_scale</code> <code>bool</code> <p>If <code>True</code> and <code>torchify</code>, set <code>requires_grad=True</code> for <code>scale</code>.</p> <code>True</code> <code>requires_grad_lengthscales</code> <code>bool</code> <p>If <code>True</code> and <code>torchify</code>, set <code>requires_grad=True</code> for <code>lengthscales</code>.</p> <code>True</code> <code>device</code> <code>device</code> <p>If <code>torchify</code>, put things onto this device.</p> <code>'cpu'</code> <code>compile_call</code> <code>bool</code> <p>If <code>True</code>, <code>torch.compile</code> the <code>parsed___call__</code> method. </p> <code>False</code> <code>comiple_call_kwargs</code> <code>dict</code> <p>When <code>compile_call</code> is <code>True</code>, pass these keyword arguments to <code>torch.compile</code>.</p> <code>{}</code> Source code in <code>qmcpy/kernel/abstract_kernel.py</code> <pre><code>def __init__(self,\n        d, \n        scale = 1.,\n        lengthscales = 1.,\n        shape_scale = [1],\n        shape_lengthscales = None,\n        tfs_scale = (tf_exp_eps_inv,tf_exp_eps),\n        tfs_lengthscales = (tf_exp_eps_inv,tf_exp_eps),\n        torchify = False,\n        requires_grad_scale = True, \n        requires_grad_lengthscales = True, \n        device = \"cpu\",\n        compile_call = False,\n        comiple_call_kwargs = {},\n        ):\n    r\"\"\"\n    Args:\n        d (int): Dimension. \n        scale (Union[np.ndarray,torch.Tensor]): Scaling factor $S$.\n        lengthscales (Union[np.ndarray,torch.Tensor]): Lengthscales $\\boldsymbol{\\gamma}$.\n        shape_scale (list): Shape of `scale` when `np.isscalar(scale)`. \n        shape_lengthscales (list): Shape of `lengthscales` when `np.isscalar(lengthscales)`\n        tfs_scale (Tuple[callable,callable]): The first argument transforms to the raw value to be optimized; the second applies the inverse transform.\n        tfs_lengthscales (Tuple[callable,callable]): The first argument transforms to the raw value to be optimized; the second applies the inverse transform.\n        torchify (bool): If `True`, use the `torch` backend. Set to `True` if computing gradients with respect to inputs and/or hyperparameters.\n        requires_grad_scale (bool): If `True` and `torchify`, set `requires_grad=True` for `scale`.\n        requires_grad_lengthscales (bool): If `True` and `torchify`, set `requires_grad=True` for `lengthscales`.\n        device (torch.device): If `torchify`, put things onto this device.\n        compile_call (bool): If `True`, `torch.compile` the `parsed___call__` method. \n        comiple_call_kwargs (dict): When `compile_call` is `True`, pass these keyword arguments to `torch.compile`.\n    \"\"\"\n    super().__init__(d=d,torchify=torchify,device=device,compile_call=compile_call,comiple_call_kwargs=comiple_call_kwargs)\n    self.raw_scale,self.tf_scale = self.parse_assign_param(\n        pname = \"scale\",\n        param = scale, \n        shape_param = shape_scale,\n        requires_grad_param = requires_grad_scale,\n        tfs_param = tfs_scale,\n        endsize_ops = [1],\n        constraints = [\"POSITIVE\"])\n    self.batch_param_names.append(\"scale\")\n    self.raw_lengthscales,self.tf_lengthscales = self.parse_assign_param(\n        pname = \"lengthscales\",\n        param = lengthscales, \n        shape_param = [self.d] if shape_lengthscales is None else shape_lengthscales,\n        requires_grad_param = requires_grad_lengthscales,\n        tfs_param = tfs_lengthscales,\n        endsize_ops = [1,self.d],\n        constraints = [\"POSITIVE\"])\n    self.batch_param_names.append(\"lengthscales\")\n</code></pre>"},{"location":"api/kernels/#kernelshiftinvar","title":"<code>KernelShiftInvar</code>","text":"<p>               Bases: <code>AbstractSIDSIKernel</code></p> <p>Shift invariant kernel with  smoothness \\(\\boldsymbol{\\alpha}\\), product weights (lengthscales) \\(\\boldsymbol{\\gamma}\\), and scale \\(S\\):</p> \\[\\begin{aligned}     K(\\boldsymbol{x},\\boldsymbol{z}) &amp;= S \\prod_{j=1}^d \\left(1+ \\gamma_j \\tilde{K}_{\\alpha_j}((x_j - z_j) \\mod 1))\\right), \\\\      \\tilde{K}_\\alpha(x) &amp;= (-1)^{\\alpha+1}\\frac{(2 \\pi)^{2 \\alpha}}{(2\\alpha)!} B_{2\\alpha}(x) \\end{aligned}\\] <p>where \\(B_n\\) is the \\(n^\\text{th}\\) Bernoulli polynomial.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from qmcpy import Lattice, fftbr, ifftbr\n&gt;&gt;&gt; n = 8\n&gt;&gt;&gt; d = 4\n&gt;&gt;&gt; lat = Lattice(d,seed=11)\n&gt;&gt;&gt; x = lat(n)\n&gt;&gt;&gt; x.shape\n(8, 4)\n&gt;&gt;&gt; x.dtype\ndtype('float64')\n&gt;&gt;&gt; kernel = KernelShiftInvar(\n...     d = d, \n...     alpha = list(range(1,d+1)),\n...     scale = 10,\n...     lengthscales = [1/j**2 for j in range(1,d+1)])\n&gt;&gt;&gt; k00 = kernel(x[0],x[0])\n&gt;&gt;&gt; k00.item()\n91.23444453396341\n&gt;&gt;&gt; k0 = kernel(x,x[0])\n&gt;&gt;&gt; with np.printoptions(precision=2):\n...     print(k0)\n[91.23 -2.32  5.69  5.69 12.7  -4.78 -4.78 12.7 ]\n&gt;&gt;&gt; assert k0[0]==k00\n&gt;&gt;&gt; kmat = kernel(x[:,None,:],x[None,:,:])\n&gt;&gt;&gt; with np.printoptions(precision=2):\n...     print(kmat)\n[[91.23 -2.32  5.69  5.69 12.7  -4.78 -4.78 12.7 ]\n [-2.32 91.23  5.69  5.69 -4.78 12.7  12.7  -4.78]\n [ 5.69  5.69 91.23 -2.32 12.7  -4.78 12.7  -4.78]\n [ 5.69  5.69 -2.32 91.23 -4.78 12.7  -4.78 12.7 ]\n [12.7  -4.78 12.7  -4.78 91.23 -2.32  5.69  5.69]\n [-4.78 12.7  -4.78 12.7  -2.32 91.23  5.69  5.69]\n [-4.78 12.7  12.7  -4.78  5.69  5.69 91.23 -2.32]\n [12.7  -4.78 -4.78 12.7   5.69  5.69 -2.32 91.23]]\n&gt;&gt;&gt; assert (kmat[:,0]==k0).all()\n&gt;&gt;&gt; lam = np.sqrt(n)*fftbr(k0)\n&gt;&gt;&gt; y = np.random.Generator(np.random.PCG64(7)).uniform(low=0,high=1,size=(n))\n&gt;&gt;&gt; np.allclose(ifftbr(fftbr(y)*lam),kmat@y)\nTrue\n&gt;&gt;&gt; np.allclose(ifftbr(fftbr(y)/lam),np.linalg.solve(kmat,y))\nTrue\n&gt;&gt;&gt; import torch \n&gt;&gt;&gt; xtorch = torch.from_numpy(x)\n&gt;&gt;&gt; kernel_torch = KernelShiftInvar(\n...     d = d, \n...     alpha = list(range(1,d+1)),\n...     scale = 10,\n...     lengthscales = [1/j**2 for j in range(1,d+1)],\n...     torchify = True)\n&gt;&gt;&gt; kmat_torch = kernel_torch(xtorch[:,None,:],xtorch[None,:,:])\n&gt;&gt;&gt; np.allclose(kmat_torch.detach().numpy(),kmat)\nTrue\n&gt;&gt;&gt; kernel.single_integral_01d(x)\narray([10.])\n&gt;&gt;&gt; kernel_torch.single_integral_01d(xtorch)\ntensor([10.], grad_fn=&lt;SelectBackward0&gt;)\n</code></pre> <p>Batch Params </p> <pre><code>&gt;&gt;&gt; rng = np.random.Generator(np.random.PCG64(7))\n&gt;&gt;&gt; kernel = KernelShiftInvar(\n...     d = 2, \n...     shape_scale = [4,3,1],\n...     shape_lengthscales = [3,2])\n&gt;&gt;&gt; x = rng.uniform(low=0,high=1,size=(6,5,2))\n&gt;&gt;&gt; kernel(x,x).shape \n(4, 3, 6, 5)\n&gt;&gt;&gt; kernel(x[:,:,None,:],x[:,None,:,:]).shape\n(4, 3, 6, 5, 5)\n&gt;&gt;&gt; kernel(x[:,None,:,None,:],x[None,:,None,:,:]).shape\n(4, 3, 6, 6, 5, 5)\n</code></pre> <p>Derivatives </p> <pre><code>&gt;&gt;&gt; scale = rng.uniform(low=0,high=1,size=(1,))\n&gt;&gt;&gt; lengthscales = rng.uniform(low=0,high=1,size=(3,))\n&gt;&gt;&gt; kernel = KernelShiftInvar(\n...     d = 3,\n...     alpha = 3,\n...     torchify = True,\n...     scale = torch.from_numpy(scale),\n...     lengthscales = torch.from_numpy(lengthscales))\n&gt;&gt;&gt; x0 = torch.from_numpy(rng.uniform(low=0,high=1,size=(4,))).requires_grad_(True)\n&gt;&gt;&gt; x1 = torch.from_numpy(rng.uniform(low=0,high=1,size=(4,))).requires_grad_(True)\n&gt;&gt;&gt; x2 = torch.from_numpy(rng.uniform(low=0,high=1,size=(4,))).requires_grad_(True)\n&gt;&gt;&gt; x = torch.stack([x0,x1,x2],axis=-1)\n&gt;&gt;&gt; z0 = torch.from_numpy(rng.uniform(low=0,high=1,size=(4,))).requires_grad_(True)\n&gt;&gt;&gt; z1 = torch.from_numpy(rng.uniform(low=0,high=1,size=(4,))).requires_grad_(True)\n&gt;&gt;&gt; z2 = torch.from_numpy(rng.uniform(low=0,high=1,size=(4,))).requires_grad_(True)\n&gt;&gt;&gt; z = torch.stack([z0,z1,z2],axis=-1)\n&gt;&gt;&gt; c = torch.from_numpy(rng.uniform(low=0,high=1,size=(2,)))\n&gt;&gt;&gt; beta0 = torch.tensor([\n...     [1,0,0],\n...     [0,2,0]])\n&gt;&gt;&gt; beta1 = torch.tensor([\n...     [0,0,2],\n...     [2,1,0]])\n&gt;&gt;&gt; with torch.no_grad():\n...     y = kernel(x,z,beta0,beta1,c)\n&gt;&gt;&gt; y\ntensor([ 3003.7945, -1517.1556,   701.6692,  1470.4241], dtype=torch.float64)\n&gt;&gt;&gt; y_no_deriv = kernel(x,z)\n&gt;&gt;&gt; y_first = y_no_deriv.clone()\n&gt;&gt;&gt; y_first = torch.autograd.grad(y_first,x0,grad_outputs=torch.ones_like(y_first,requires_grad=True),create_graph=True)[0]\n&gt;&gt;&gt; y_first = torch.autograd.grad(y_first,z2,grad_outputs=torch.ones_like(y_first,requires_grad=True),create_graph=True)[0]\n&gt;&gt;&gt; y_first = torch.autograd.grad(y_first,z2,grad_outputs=torch.ones_like(y_first,requires_grad=True),create_graph=True)[0]\n&gt;&gt;&gt; y_second = y_no_deriv.clone()\n&gt;&gt;&gt; y_second = torch.autograd.grad(y_second,x1,grad_outputs=torch.ones_like(y_second,requires_grad=True),create_graph=True)[0]\n&gt;&gt;&gt; y_second = torch.autograd.grad(y_second,x1,grad_outputs=torch.ones_like(y_second,requires_grad=True),create_graph=True)[0]\n&gt;&gt;&gt; y_second = torch.autograd.grad(y_second,z0,grad_outputs=torch.ones_like(y_second,requires_grad=True),create_graph=True)[0]\n&gt;&gt;&gt; y_second = torch.autograd.grad(y_second,z0,grad_outputs=torch.ones_like(y_second,requires_grad=True),create_graph=True)[0]\n&gt;&gt;&gt; y_second = torch.autograd.grad(y_second,z1,grad_outputs=torch.ones_like(y_second,requires_grad=True),create_graph=True)[0]\n&gt;&gt;&gt; yhat = (y_first*c[0]+y_second*c[1]).detach()\n&gt;&gt;&gt; yhat\ntensor([ 3003.7946, -1517.1557,   701.6692,  1470.4241], dtype=torch.float64)\n&gt;&gt;&gt; torch.allclose(y,yhat)\nTrue\n&gt;&gt;&gt; kernel = KernelShiftInvar(\n...     d = 3,\n...     alpha = 3,\n...     scale = scale,\n...     lengthscales = lengthscales)\n&gt;&gt;&gt; ynp = kernel(x.detach().numpy(),z.detach().numpy(),beta0.numpy(),beta1.numpy(),c.numpy())\n&gt;&gt;&gt; ynp\narray([ 3003.79938927, -1517.15808005,   701.6700331 ,  1470.42580601])\n&gt;&gt;&gt; np.allclose(ynp,y.numpy())\nTrue\n</code></pre> <p>References: </p> <ol> <li>Kaarnioja, Vesa, Frances Y. Kuo, and Ian H. Sloan.     \"Lattice-based kernel approximation and serendipitous weights for parametric PDEs in very high dimensions.\"     International Conference on Monte Carlo and Quasi-Monte Carlo Methods in Scientific Computing. Cham: Springer International Publishing, 2022.</li> </ol> <p>Parameters:</p> Name Type Description Default <code>d</code> <code>int</code> <p>Dimension. </p> required <code>scale</code> <code>Union[ndarray, Tensor]</code> <p>Scaling factor \\(S\\).</p> <code>1.0</code> <code>lengthscales</code> <code>Union[ndarray, Tensor]</code> <p>Product weights \\((\\gamma_1,\\dots,\\gamma_d)\\).</p> <code>1.0</code> <code>alpha</code> <code>Union[ndarray, Tensor]</code> <p>Smoothness parameters \\((\\alpha_1,\\dots,\\alpha_d)\\) where \\(\\alpha_j \\geq 1\\) for \\(j=1,\\dots,d\\).</p> <code>2</code> <code>shape_scale</code> <code>list</code> <p>Shape of <code>scale</code> when <code>np.isscalar(scale)</code>. </p> <code>[1]</code> <code>shape_lengthscales</code> <code>list</code> <p>Shape of <code>lengthscales</code> when <code>np.isscalar(lengthscales)</code></p> <code>None</code> <code>tfs_scale</code> <code>Tuple[callable, callable]</code> <p>The first argument transforms to the raw value to be optimized; the second applies the inverse transform.</p> <code>(tf_exp_eps_inv, tf_exp_eps)</code> <code>tfs_lengthscales</code> <code>Tuple[callable, callable]</code> <p>The first argument transforms to the raw value to be optimized; the second applies the inverse transform.</p> <code>(tf_exp_eps_inv, tf_exp_eps)</code> <code>torchify</code> <code>bool</code> <p>If <code>True</code>, use the <code>torch</code> backend. Set to <code>True</code> if computing gradients with respect to inputs and/or hyperparameters.</p> <code>False</code> <code>requires_grad_scale</code> <code>bool</code> <p>If <code>True</code> and <code>torchify</code>, set <code>requires_grad=True</code> for <code>scale</code>.</p> <code>True</code> <code>requires_grad_lengthscales</code> <code>bool</code> <p>If <code>True</code> and <code>torchify</code>, set <code>requires_grad=True</code> for <code>lengthscales</code>.</p> <code>True</code> <code>device</code> <code>device</code> <p>If <code>torchify</code>, put things onto this device.</p> <code>'cpu'</code> <code>compile_call</code> <code>bool</code> <p>If <code>True</code>, <code>torch.compile</code> the <code>parsed___call__</code> method. </p> <code>False</code> <code>comiple_call_kwargs</code> <code>dict</code> <p>When <code>compile_call</code> is <code>True</code>, pass these keyword arguments to <code>torch.compile</code>.</p> <code>{}</code> Source code in <code>qmcpy/kernel/si_dsi_kernels.py</code> <pre><code>def __init__(self,\n        d, \n        scale = 1., \n        lengthscales = 1.,\n        alpha = 2,\n        shape_scale = [1],\n        shape_lengthscales = None, \n        tfs_scale = (tf_exp_eps_inv,tf_exp_eps),\n        tfs_lengthscales = (tf_exp_eps_inv,tf_exp_eps),\n        torchify = False, \n        requires_grad_scale = True, \n        requires_grad_lengthscales = True, \n        device = \"cpu\",\n        compile_call = False,\n        comiple_call_kwargs = {},\n        ):\n    r\"\"\"\n    Args:\n        d (int): Dimension. \n        scale (Union[np.ndarray,torch.Tensor]): Scaling factor $S$.\n        lengthscales (Union[np.ndarray,torch.Tensor]): Product weights $(\\gamma_1,\\dots,\\gamma_d)$.\n        alpha (Union[np.ndarray,torch.Tensor]): Smoothness parameters $(\\alpha_1,\\dots,\\alpha_d)$ where $\\alpha_j \\geq 1$ for $j=1,\\dots,d$.\n        shape_scale (list): Shape of `scale` when `np.isscalar(scale)`. \n        shape_lengthscales (list): Shape of `lengthscales` when `np.isscalar(lengthscales)`\n        tfs_scale (Tuple[callable,callable]): The first argument transforms to the raw value to be optimized; the second applies the inverse transform.\n        tfs_lengthscales (Tuple[callable,callable]): The first argument transforms to the raw value to be optimized; the second applies the inverse transform.\n        torchify (bool): If `True`, use the `torch` backend. Set to `True` if computing gradients with respect to inputs and/or hyperparameters.\n        requires_grad_scale (bool): If `True` and `torchify`, set `requires_grad=True` for `scale`.\n        requires_grad_lengthscales (bool): If `True` and `torchify`, set `requires_grad=True` for `lengthscales`.\n        device (torch.device): If `torchify`, put things onto this device.\n        compile_call (bool): If `True`, `torch.compile` the `parsed___call__` method. \n        comiple_call_kwargs (dict): When `compile_call` is `True`, pass these keyword arguments to `torch.compile`.\n    \"\"\"\n    super().__init__(\n        d = d, \n        scale = scale, \n        lengthscales = lengthscales,\n        alpha = alpha, \n        shape_scale = shape_scale,\n        shape_lengthscales = shape_lengthscales, \n        tfs_scale = tfs_scale, \n        tfs_lengthscales = tfs_lengthscales, \n        torchify = torchify,\n        requires_grad_scale = requires_grad_scale,\n        requires_grad_lengthscales = requires_grad_lengthscales,\n        device = device,\n        compile_call = compile_call,\n        comiple_call_kwargs = comiple_call_kwargs,\n    )\n    assert all(int(alphaj) in BERNOULLIPOLYSDICT for alphaj in self.alpha)\n    if self.torchify:\n        import torch \n        self.lgamma = torch.lgamma \n    else:\n        self.lgamma = scipy.special.loggamma\n</code></pre>"},{"location":"api/kernels/#kerneldigshiftinvar","title":"<code>KernelDigShiftInvar</code>","text":"<p>               Bases: <code>AbstractSIDSIKernel</code></p> <p>Digitally shift invariant kernel in base \\(b=2\\) with  smoothness \\(\\boldsymbol{\\alpha}\\), product weights \\(\\boldsymbol{\\gamma}\\), and scale \\(S\\): </p> \\[\\begin{aligned}     K(\\boldsymbol{x},\\boldsymbol{z}) &amp;= S \\prod_{j=1}^d \\left(1+ \\gamma_j \\tilde{K}_{\\alpha_j}(x_j \\oplus z_j)\\right), \\qquad\\mathrm{where} \\\\     \\tilde{K}_1(x) &amp;= \\sum_{k \\in \\mathbb{N}} \\frac{\\mathrm{wal}_k(x)}{2^{2 \\lfloor \\log_2(x) \\rfloor}} = 6 \\left(\\frac{1}{6} - 2^{\\lfloor \\log_2(x) \\rfloor -1}\\right), \\\\     \\tilde{K}_2(x) &amp;= \\sum_{k \\in \\mathbb{N}} \\frac{\\mathrm{wal}_k(x)}{2^{\\mu_2(k)}} = -\\beta(x) x + \\frac{5}{2}\\left[1-t_1(x)\\right]-1, \\\\     \\tilde{K}_3(x) &amp;= \\sum_{k \\in \\mathbb{N}} \\frac{\\mathrm{wal}_k(x)}{2^{\\mu_3(k)}} = \\beta(x)x^2-5\\left[1-t_1(x)\\right]x+\\frac{43}{18}\\left[1-t_2(x)\\right]-1, \\\\     \\tilde{K}_4(x) &amp;= \\sum_{k \\in \\mathbb{N}} \\frac{\\mathrm{wal}_k(x)}{2^{\\mu_4(k)}} = - \\frac{2}{3}\\beta(x)x^3+5\\left[1-t_1(x)\\right]x^2 - \\frac{43}{9}\\left[1-t_2(x)\\right]x +\\frac{701}{294}\\left[1-t_3(x)\\right]+\\beta(x)\\left[\\frac{1}{48}\\sum_{a=0}^\\infty \\frac{\\mathrm{wal}_{2^a}(x)}{2^{3a}} - \\frac{1}{42}\\right] - 1. \\end{aligned}\\] <p>where </p> <ul> <li>\\(x \\oplus z\\) is XOR between bits, </li> <li>\\(\\mathrm{wal}_k\\) is the \\(k^\\text{th}\\) Walsh function, </li> <li>\\(\\beta(x) = - \\lfloor \\log_2(x) \\rfloor\\) and \\(t_\\nu(x) = 2^{-\\nu \\beta(x)}\\) where \\(\\beta(0)=t_\\nu(0) = 0\\), and </li> <li>and \\(\\mu_\\alpha\\) is the Dick weight function which sums the first \\(\\alpha\\) largest indices of \\(1\\) bits in the binary expansion of \\(k\\)  e.g. \\(k=13=1101_2\\) has 1-bit indexes \\((4,3,1)\\) so </li> </ul> \\[\\mu_1(k) = 4, \\mu_2(k) = 4+3, \\mu_3(k) = 4+3+1 = \\mu_4(k) = \\mu_5(k) = \\dots.\\] <p>Examples:</p> <pre><code>&gt;&gt;&gt; from qmcpy import DigitalNetB2, fwht\n&gt;&gt;&gt; n = 8\n&gt;&gt;&gt; d = 4\n&gt;&gt;&gt; dnb2 = DigitalNetB2(d,seed=11)\n&gt;&gt;&gt; x = dnb2(n,return_binary=True)\n&gt;&gt;&gt; x.shape\n(8, 4)\n&gt;&gt;&gt; x.dtype\ndtype('uint64')\n&gt;&gt;&gt; kernel = KernelDigShiftInvar(\n...     d = d, \n...     t = dnb2.t,\n...     alpha = list(range(1,d+1)),\n...     scale = 10,\n...     lengthscales = [1/j**2 for j in range(1,d+1)])\n&gt;&gt;&gt; k00 = kernel(x[0],x[0])\n&gt;&gt;&gt; k00.item()\n34.490370029184525\n&gt;&gt;&gt; k0 = kernel(x,x[0])\n&gt;&gt;&gt; with np.printoptions(precision=2):\n...     print(k0)\n[34.49  4.15  9.59  4.98 15.42  5.45 11.99  4.51]\n&gt;&gt;&gt; assert k0[0]==k00\n&gt;&gt;&gt; kmat = kernel(x[:,None,:],x[None,:,:])\n&gt;&gt;&gt; with np.printoptions(precision=2):\n...     print(kmat)\n[[34.49  4.15  9.59  4.98 15.42  5.45 11.99  4.51]\n [ 4.15 34.49  4.98  9.59  5.45 15.42  4.51 11.99]\n [ 9.59  4.98 34.49  4.15 11.99  4.51 15.42  5.45]\n [ 4.98  9.59  4.15 34.49  4.51 11.99  5.45 15.42]\n [15.42  5.45 11.99  4.51 34.49  4.15  9.59  4.98]\n [ 5.45 15.42  4.51 11.99  4.15 34.49  4.98  9.59]\n [11.99  4.51 15.42  5.45  9.59  4.98 34.49  4.15]\n [ 4.51 11.99  5.45 15.42  4.98  9.59  4.15 34.49]]\n&gt;&gt;&gt; assert (kmat[:,0]==k0).all()\n&gt;&gt;&gt; lam = np.sqrt(n)*fwht(k0)\n&gt;&gt;&gt; y = np.random.Generator(np.random.PCG64(7)).uniform(low=0,high=1,size=(n))\n&gt;&gt;&gt; np.allclose(fwht(fwht(y)*lam),kmat@y)\nTrue\n&gt;&gt;&gt; np.allclose(fwht(fwht(y)/lam),np.linalg.solve(kmat,y))\nTrue\n&gt;&gt;&gt; import torch \n&gt;&gt;&gt; xtorch = bin_from_numpy_to_torch(x)\n&gt;&gt;&gt; kernel_torch = KernelDigShiftInvar(\n...     d = d, \n...     t = dnb2.t,\n...     alpha = list(range(1,d+1)),\n...     scale = 10,\n...     lengthscales = [1/j**2 for j in range(1,d+1)],\n...     torchify = True)\n&gt;&gt;&gt; kmat_torch = kernel_torch(xtorch[:,None,:],xtorch[None,:,:])\n&gt;&gt;&gt; np.allclose(kmat_torch.detach().numpy(),kmat)\nTrue\n&gt;&gt;&gt; xf = to_float(x,dnb2.t)\n&gt;&gt;&gt; kmat_from_floats = kernel(xf[:,None,:],xf[None,:,:])\n&gt;&gt;&gt; np.allclose(kmat,kmat_from_floats)\nTrue\n&gt;&gt;&gt; xftorch = to_float(xtorch,dnb2.t)\n&gt;&gt;&gt; xftorch.dtype\ntorch.float32\n&gt;&gt;&gt; kmat_torch_from_floats = kernel_torch(xftorch[:,None,:],xftorch[None,:,:])\n&gt;&gt;&gt; torch.allclose(kmat_torch_from_floats,kmat_torch)\nTrue\n&gt;&gt;&gt; kernel.single_integral_01d(x)\narray([10.])\n&gt;&gt;&gt; kernel_torch.single_integral_01d(xtorch)\ntensor([10.], grad_fn=&lt;SelectBackward0&gt;)\n</code></pre> <p>Batch Params </p> <pre><code>&gt;&gt;&gt; rng = np.random.Generator(np.random.PCG64(7))\n&gt;&gt;&gt; kernel = KernelDigShiftInvar(\n...     d = 2, \n...     t = 10,\n...     shape_scale = [4,3,1],\n...     shape_lengthscales = [3,2])\n&gt;&gt;&gt; x = rng.uniform(low=0,high=1,size=(6,5,2))\n&gt;&gt;&gt; kernel(x,x).shape \n(4, 3, 6, 5)\n&gt;&gt;&gt; kernel(x[:,:,None,:],x[:,None,:,:]).shape\n(4, 3, 6, 5, 5)\n&gt;&gt;&gt; kernel(x[:,None,:,None,:],x[None,:,None,:,:]).shape\n(4, 3, 6, 6, 5, 5)\n</code></pre> <p>References:</p> <ol> <li> <p>Dick, Josef.     \"Walsh spaces containing smooth functions and quasi\u2013Monte Carlo rules of arbitrary high order.\"     SIAM Journal on Numerical Analysis 46.3 (2008): 1519-1553.</p> </li> <li> <p>Dick, Josef.     \"The decay of the Walsh coefficients of smooth functions.\"     Bulletin of the Australian Mathematical Society 80.3 (2009): 430-453.  </p> </li> <li> <p>Dick, Josef, and Friedrich Pillichshammer.     \"Multivariate integration in weighted Hilbert spaces based on Walsh functions and weighted Sobolev spaces.\"     Journal of Complexity 21.2 (2005): 149-195.</p> </li> <li> <p>Jagadeeswaran, Rathinavel, and Fred J. Hickernell.     \"Fast automatic Bayesian cubature using Sobol\u2019sampling.\"     Advances in Modeling and Simulation: Festschrift for Pierre L'Ecuyer. Cham: Springer International Publishing, 2022. 301-318.</p> </li> <li> <p>Rathinavel, Jagadeeswaran.     Fast automatic Bayesian cubature using matching kernels and designs.     Illinois Institute of Technology, 2019.</p> </li> <li> <p>Sorokin, Aleksei.     \"A Unified Implementation of Quasi-Monte Carlo Generators, Randomization Routines, and Fast Kernel Methods.\"     arXiv preprint arXiv:2502.14256 (2025).</p> </li> </ol> <p>Parameters:</p> Name Type Description Default <code>d</code> <code>int</code> <p>Dimension. </p> required <code>t</code> <code>int</code> <p>number of bits in binary represtnations. Typically <code>dnb2.t</code> where <code>isinstance(dnb2,DigitalNetB2)</code>.</p> <code>None</code> <code>scale</code> <code>Union[ndarray, Tensor]</code> <p>Scaling factor \\(S\\).</p> <code>1.0</code> <code>lengthscales</code> <code>Union[ndarray, Tensor]</code> <p>Product weights \\((\\gamma_1,\\dots,\\gamma_d)\\).</p> <code>1.0</code> <code>alpha</code> <code>Union[ndarray, Tensor]</code> <p>Smoothness parameters \\((\\alpha_1,\\dots,\\alpha_d)\\) where \\(\\alpha_j \\geq 1\\) for \\(j=1,\\dots,d\\).</p> <code>2</code> <code>shape_scale</code> <code>list</code> <p>Shape of <code>scale</code> when <code>np.isscalar(scale)</code>. </p> <code>[1]</code> <code>shape_lengthscales</code> <code>list</code> <p>Shape of <code>lengthscales</code> when <code>np.isscalar(lengthscales)</code></p> <code>None</code> <code>tfs_scale</code> <code>Tuple[callable, callable]</code> <p>The first argument transforms to the raw value to be optimized; the second applies the inverse transform.</p> <code>(tf_exp_eps_inv, tf_exp_eps)</code> <code>tfs_lengthscales</code> <code>Tuple[callable, callable]</code> <p>The first argument transforms to the raw value to be optimized; the second applies the inverse transform.</p> <code>(tf_exp_eps_inv, tf_exp_eps)</code> <code>torchify</code> <code>bool</code> <p>If <code>True</code>, use the <code>torch</code> backend. Set to <code>True</code> if computing gradients with respect to inputs and/or hyperparameters.</p> <code>False</code> <code>requires_grad_scale</code> <code>bool</code> <p>If <code>True</code> and <code>torchify</code>, set <code>requires_grad=True</code> for <code>scale</code>.</p> <code>True</code> <code>requires_grad_lengthscales</code> <code>bool</code> <p>If <code>True</code> and <code>torchify</code>, set <code>requires_grad=True</code> for <code>lengthscales</code>.</p> <code>True</code> <code>device</code> <code>device</code> <p>If <code>torchify</code>, put things onto this device.</p> <code>'cpu'</code> <code>compile_call</code> <code>bool</code> <p>If <code>True</code>, <code>torch.compile</code> the <code>parsed___call__</code> method. </p> <code>False</code> <code>comiple_call_kwargs</code> <code>dict</code> <p>When <code>compile_call</code> is <code>True</code>, pass these keyword arguments to <code>torch.compile</code>.</p> <code>{}</code> Source code in <code>qmcpy/kernel/si_dsi_kernels.py</code> <pre><code>def __init__(self,\n        d,\n        t=None,\n        scale = 1., \n        lengthscales = 1.,\n        alpha = 2,\n        shape_scale = [1],\n        shape_lengthscales = None, \n        tfs_scale = (tf_exp_eps_inv,tf_exp_eps),\n        tfs_lengthscales = (tf_exp_eps_inv,tf_exp_eps),\n        torchify = False, \n        requires_grad_scale = True, \n        requires_grad_lengthscales = True, \n        device = \"cpu\",\n        compile_call = False,\n        comiple_call_kwargs = {},\n        ):\n    r\"\"\"\n    Args:\n        d (int): Dimension. \n        t (int): number of bits in binary represtnations. Typically `dnb2.t` where `isinstance(dnb2,DigitalNetB2)`.\n        scale (Union[np.ndarray,torch.Tensor]): Scaling factor $S$.\n        lengthscales (Union[np.ndarray,torch.Tensor]): Product weights $(\\gamma_1,\\dots,\\gamma_d)$.\n        alpha (Union[np.ndarray,torch.Tensor]): Smoothness parameters $(\\alpha_1,\\dots,\\alpha_d)$ where $\\alpha_j \\geq 1$ for $j=1,\\dots,d$.\n        shape_scale (list): Shape of `scale` when `np.isscalar(scale)`. \n        shape_lengthscales (list): Shape of `lengthscales` when `np.isscalar(lengthscales)`\n        tfs_scale (Tuple[callable,callable]): The first argument transforms to the raw value to be optimized; the second applies the inverse transform.\n        tfs_lengthscales (Tuple[callable,callable]): The first argument transforms to the raw value to be optimized; the second applies the inverse transform.\n        torchify (bool): If `True`, use the `torch` backend. Set to `True` if computing gradients with respect to inputs and/or hyperparameters.\n        requires_grad_scale (bool): If `True` and `torchify`, set `requires_grad=True` for `scale`.\n        requires_grad_lengthscales (bool): If `True` and `torchify`, set `requires_grad=True` for `lengthscales`.\n        device (torch.device): If `torchify`, put things onto this device.\n        compile_call (bool): If `True`, `torch.compile` the `parsed___call__` method. \n        comiple_call_kwargs (dict): When `compile_call` is `True`, pass these keyword arguments to `torch.compile`.\n    \"\"\"\n    super().__init__(\n        d = d, \n        scale = scale, \n        lengthscales = lengthscales,\n        alpha = alpha, \n        shape_scale = shape_scale,\n        shape_lengthscales = shape_lengthscales, \n        tfs_scale = tfs_scale, \n        tfs_lengthscales = tfs_lengthscales, \n        torchify = torchify,\n        requires_grad_scale = requires_grad_scale,\n        requires_grad_lengthscales = requires_grad_lengthscales,\n        device = device,\n        compile_call = compile_call,\n        comiple_call_kwargs = comiple_call_kwargs,\n    )\n    self.set_t(t)\n    assert all(1&lt;=int(alphaj)&lt;=4 for alphaj in self.alpha)\n</code></pre>"},{"location":"api/kernels/#kernelgaussian","title":"<code>KernelGaussian</code>","text":"<p>               Bases: <code>AbstractKernelGaussianSE</code></p> <p>Gaussian / Squared Exponential kernel implemented using the product of exponentials. </p> \\[K(\\boldsymbol{x},\\boldsymbol{z}) = S \\prod_{j=1}^d \\exp\\left(-\\left(\\frac{x_j-z_j}{\\sqrt{2} \\gamma_j}\\right)^2\\right)\\] <p>Examples:</p> <pre><code>&gt;&gt;&gt; rng = np.random.Generator(np.random.PCG64(7))\n&gt;&gt;&gt; kernel = KernelGaussian(d=2)\n&gt;&gt;&gt; x = rng.uniform(low=0,high=1,size=(4,2))\n&gt;&gt;&gt; kernel(x[0],x[0]).item()\n1.0\n&gt;&gt;&gt; kernel(x,x)\narray([1., 1., 1., 1.])\n&gt;&gt;&gt; kernel(x[:,None,:],x[None,:,:])\narray([[1.        , 0.78888466, 0.9483142 , 0.8228498 ],\n       [0.78888466, 1.        , 0.72380317, 0.62226176],\n       [0.9483142 , 0.72380317, 1.        , 0.95613874],\n       [0.8228498 , 0.62226176, 0.95613874, 1.        ]])\n</code></pre> <p>Multiple randomizations </p> <pre><code>&gt;&gt;&gt; x = rng.uniform(low=0,high=1,size=(6,5,2))\n&gt;&gt;&gt; kernel(x,x).shape \n(6, 5)\n&gt;&gt;&gt; kernel(x[:,:,None,:],x[:,None,:,:]).shape\n(6, 5, 5)\n&gt;&gt;&gt; kernel(x[:,None,:,None,:],x[None,:,None,:,:]).shape\n(6, 6, 5, 5)\n</code></pre> <p>Batch hyperparameters </p> <pre><code>&gt;&gt;&gt; kernel = KernelGaussian(\n...     d = 2, \n...     shape_scale = [4,3,1],\n...     shape_lengthscales = [3,1])\n&gt;&gt;&gt; kernel(x,x).shape \n(4, 3, 6, 5)\n&gt;&gt;&gt; kernel(x[:,:,None,:],x[:,None,:,:]).shape\n(4, 3, 6, 5, 5)\n&gt;&gt;&gt; kernel(x[:,None,:,None,:],x[None,:,None,:,:]).shape\n(4, 3, 6, 6, 5, 5)\n</code></pre> <p>Integrals </p> <pre><code>&gt;&gt;&gt; kernel = KernelGaussian(\n...     d = 2,\n...     scale = rng.uniform(low=0,high=1,size=(3,1,)),\n...     lengthscales = rng.uniform(low=0,high=1,size=(1,2)))\n&gt;&gt;&gt; kintint = kernel.double_integral_01d()\n&gt;&gt;&gt; kintint\narray([0.50079567, 0.11125229, 0.34760005])\n&gt;&gt;&gt; x_qmc_4d = DigitalNetB2(4,seed=7)(2**16)\n&gt;&gt;&gt; kintint_qmc = kernel(x_qmc_4d[:,:2],x_qmc_4d[:,2:]).mean(1)\n&gt;&gt;&gt; kintint_qmc\narray([0.5007959 , 0.11125234, 0.34760021])\n&gt;&gt;&gt; with np.printoptions(formatter={\"float\":lambda x: \"%.1e\"%x}):\n...     np.abs(kintint-kintint_qmc)\narray([2.3e-07, 5.1e-08, 1.6e-07])\n&gt;&gt;&gt; x = rng.uniform(low=0,high=1,size=(4,2))\n&gt;&gt;&gt; kint = kernel.single_integral_01d(x)\n&gt;&gt;&gt; kint\narray([[0.54610372, 0.4272801 , 0.43001936, 0.44688778],\n       [0.12131753, 0.09492073, 0.09552926, 0.0992766 ],\n       [0.37904817, 0.29657323, 0.29847453, 0.31018283]])\n&gt;&gt;&gt; x_qmc_2d = DigitalNetB2(2,seed=7)(2**16)\n&gt;&gt;&gt; kint_qmc = kernel(x[:,None,:],x_qmc_2d).mean(-1)\n&gt;&gt;&gt; kint_qmc\narray([[0.54610372, 0.4272801 , 0.43001936, 0.44688778],\n       [0.12131753, 0.09492073, 0.09552926, 0.0992766 ],\n       [0.37904817, 0.29657323, 0.29847453, 0.31018283]])\n&gt;&gt;&gt; with np.printoptions(formatter={\"float\":lambda x: \"%.1e\"%x}):\n...     np.abs(kint-kint_qmc)\narray([[2.2e-13, 4.0e-13, 4.2e-12, 3.9e-12],\n       [4.8e-14, 8.9e-14, 9.4e-13, 8.8e-13],\n       [1.5e-13, 2.8e-13, 2.9e-12, 2.7e-12]])\n&gt;&gt;&gt; k_1l = KernelGaussian(d=2,lengthscales=[.5])\n&gt;&gt;&gt; k_2l = KernelGaussian(d=2,lengthscales=[.5,.5])\n&gt;&gt;&gt; print(\"%.5f\"%k_2l.double_integral_01d())\n0.58363\n&gt;&gt;&gt; print(\"%.5f\"%k_1l.double_integral_01d())\n0.58363\n&gt;&gt;&gt; k_1l.single_integral_01d(x)\narray([0.58119655, 0.46559577, 0.57603494, 0.53494393])\n&gt;&gt;&gt; k_2l.single_integral_01d(x)\narray([0.58119655, 0.46559577, 0.57603494, 0.53494393])\n</code></pre> <p>PyTorch</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; rng = np.random.Generator(np.random.PCG64(7))\n&gt;&gt;&gt; kernel = KernelGaussian(d=2,torchify=True)\n&gt;&gt;&gt; x = torch.from_numpy(rng.uniform(low=0,high=1,size=(4,2)))\n&gt;&gt;&gt; kernel(x[0],x[0]).item()\n1.0\n&gt;&gt;&gt; kernel(x,x)\ntensor([1., 1., 1., 1.], dtype=torch.float64, grad_fn=&lt;MulBackward0&gt;)\n&gt;&gt;&gt; kernel(x[:,None,:],x[None,:,:])\ntensor([[1.0000, 0.7889, 0.9483, 0.8228],\n        [0.7889, 1.0000, 0.7238, 0.6223],\n        [0.9483, 0.7238, 1.0000, 0.9561],\n        [0.8228, 0.6223, 0.9561, 1.0000]], dtype=torch.float64,\n       grad_fn=&lt;MulBackward0&gt;)\n</code></pre> <p>Multiple randomizations </p> <pre><code>&gt;&gt;&gt; x = torch.from_numpy(rng.uniform(low=0,high=1,size=(6,5,2)))\n&gt;&gt;&gt; kernel(x,x).shape \ntorch.Size([6, 5])\n&gt;&gt;&gt; kernel(x[:,:,None,:],x[:,None,:,:]).shape\ntorch.Size([6, 5, 5])\n&gt;&gt;&gt; kernel(x[:,None,:,None,:],x[None,:,None,:,:]).shape\ntorch.Size([6, 6, 5, 5])\n</code></pre> <p>Batch hyperparameters </p> <pre><code>&gt;&gt;&gt; kernel = KernelGaussian(\n...     d = 2, \n...     shape_scale = [4,3,1],\n...     shape_lengthscales = [3,1],\n...     torchify = True)\n&gt;&gt;&gt; kernel(x,x).shape \ntorch.Size([4, 3, 6, 5])\n&gt;&gt;&gt; kernel(x[:,:,None,:],x[:,None,:,:]).shape\ntorch.Size([4, 3, 6, 5, 5])\n&gt;&gt;&gt; kernel(x[:,None,:,None,:],x[None,:,None,:,:]).shape\ntorch.Size([4, 3, 6, 6, 5, 5])\n</code></pre> <p>Integrals </p> <pre><code>&gt;&gt;&gt; kernel = KernelGaussian(\n...     d = 2,\n...     torchify = True,\n...     scale = torch.from_numpy(rng.uniform(low=0,high=1,size=(3,1,))),\n...     lengthscales = torch.from_numpy(rng.uniform(low=0,high=1,size=(1,2))),\n...     requires_grad_scale = False, \n...     requires_grad_lengthscales = False)\n&gt;&gt;&gt; kintint = kernel.double_integral_01d()\n&gt;&gt;&gt; kintint\ntensor([0.5008, 0.1113, 0.3476], dtype=torch.float64)\n&gt;&gt;&gt; x_qmc_4d = torch.from_numpy(DigitalNetB2(4,seed=7)(2**16))\n&gt;&gt;&gt; kintint_qmc = kernel(x_qmc_4d[:,:2],x_qmc_4d[:,2:]).mean(1)\n&gt;&gt;&gt; kintint_qmc\ntensor([0.5008, 0.1113, 0.3476], dtype=torch.float64)\n&gt;&gt;&gt; with np.printoptions(formatter={\"float\":lambda x: \"%.1e\"%x}):\n...     torch.abs(kintint-kintint_qmc).numpy()\narray([2.3e-07, 5.1e-08, 1.6e-07])\n&gt;&gt;&gt; x = torch.from_numpy(rng.uniform(low=0,high=1,size=(4,2)))\n&gt;&gt;&gt; kint = kernel.single_integral_01d(x)\n&gt;&gt;&gt; kint\ntensor([[0.5461, 0.4273, 0.4300, 0.4469],\n        [0.1213, 0.0949, 0.0955, 0.0993],\n        [0.3790, 0.2966, 0.2985, 0.3102]], dtype=torch.float64)\n&gt;&gt;&gt; x_qmc_2d = torch.from_numpy(DigitalNetB2(2,seed=7)(2**16))\n&gt;&gt;&gt; kint_qmc = kernel(x[:,None,:],x_qmc_2d).mean(-1)\n&gt;&gt;&gt; kint_qmc\ntensor([[0.5461, 0.4273, 0.4300, 0.4469],\n        [0.1213, 0.0949, 0.0955, 0.0993],\n        [0.3790, 0.2966, 0.2985, 0.3102]], dtype=torch.float64)\n&gt;&gt;&gt; with np.printoptions(formatter={\"float\":lambda x: \"%.1e\"%x}):\n...     torch.abs(kint-kint_qmc).numpy()\narray([[2.2e-13, 4.0e-13, 4.2e-12, 3.9e-12],\n       [4.8e-14, 8.9e-14, 9.4e-13, 8.8e-13],\n       [1.5e-13, 2.8e-13, 2.9e-12, 2.7e-12]])\n&gt;&gt;&gt; k_1l = KernelGaussian(d=2,lengthscales=[.5],torchify=True)\n&gt;&gt;&gt; k_2l = KernelGaussian(d=2,lengthscales=[.5,.5],torchify=True)\n&gt;&gt;&gt; k_2l.double_integral_01d()\ntensor(0.5836, grad_fn=&lt;MulBackward0&gt;)\n&gt;&gt;&gt; k_1l.double_integral_01d()\ntensor(0.5836, grad_fn=&lt;MulBackward0&gt;)\n&gt;&gt;&gt; k_1l.single_integral_01d(x)\ntensor([0.5812, 0.4656, 0.5760, 0.5349], dtype=torch.float64,\n       grad_fn=&lt;MulBackward0&gt;)\n&gt;&gt;&gt; k_2l.single_integral_01d(x)\ntensor([0.5812, 0.4656, 0.5760, 0.5349], dtype=torch.float64,\n       grad_fn=&lt;MulBackward0&gt;)\n</code></pre> <p>Derivatives </p> <pre><code>&gt;&gt;&gt; kernel = KernelGaussian(\n...     d = 3,\n...     torchify = True,\n...     scale = torch.from_numpy(rng.uniform(low=0,high=1,size=(1,))),\n...     lengthscales = torch.from_numpy(rng.uniform(low=0,high=1,size=(3,))))\n&gt;&gt;&gt; x0 = torch.from_numpy(rng.uniform(low=0,high=1,size=(4,))).requires_grad_(True)\n&gt;&gt;&gt; x1 = torch.from_numpy(rng.uniform(low=0,high=1,size=(4,))).requires_grad_(True)\n&gt;&gt;&gt; x2 = torch.from_numpy(rng.uniform(low=0,high=1,size=(4,))).requires_grad_(True)\n&gt;&gt;&gt; x = torch.stack([x0,x1,x2],axis=-1)\n&gt;&gt;&gt; z0 = torch.from_numpy(rng.uniform(low=0,high=1,size=(4,))).requires_grad_(True)\n&gt;&gt;&gt; z1 = torch.from_numpy(rng.uniform(low=0,high=1,size=(4,))).requires_grad_(True)\n&gt;&gt;&gt; z2 = torch.from_numpy(rng.uniform(low=0,high=1,size=(4,))).requires_grad_(True)\n&gt;&gt;&gt; z = torch.stack([z0,z1,z2],axis=-1)\n&gt;&gt;&gt; c = torch.from_numpy(rng.uniform(low=0,high=1,size=(2,)))\n&gt;&gt;&gt; beta0 = torch.tensor([\n...     [1,0,0],\n...     [0,2,0]])\n&gt;&gt;&gt; beta1 = torch.tensor([\n...     [0,0,2],\n...     [2,1,0]])\n&gt;&gt;&gt; with torch.no_grad():\n...     y = kernel(x,z,beta0,beta1,c)\n&gt;&gt;&gt; y\ntensor([ 97.6657,   3.8621, -65.9329,  -1.1932], dtype=torch.float64)\n&gt;&gt;&gt; y_no_deriv = kernel(x,z)\n&gt;&gt;&gt; y_first = y_no_deriv.clone()\n&gt;&gt;&gt; y_first = torch.autograd.grad(y_first,x0,grad_outputs=torch.ones_like(y_first,requires_grad=True),create_graph=True)[0]\n&gt;&gt;&gt; y_first = torch.autograd.grad(y_first,z2,grad_outputs=torch.ones_like(y_first,requires_grad=True),create_graph=True)[0]\n&gt;&gt;&gt; y_first = torch.autograd.grad(y_first,z2,grad_outputs=torch.ones_like(y_first,requires_grad=True),create_graph=True)[0]\n&gt;&gt;&gt; y_second = y_no_deriv.clone()\n&gt;&gt;&gt; y_second = torch.autograd.grad(y_second,x1,grad_outputs=torch.ones_like(y_second,requires_grad=True),create_graph=True)[0]\n&gt;&gt;&gt; y_second = torch.autograd.grad(y_second,x1,grad_outputs=torch.ones_like(y_second,requires_grad=True),create_graph=True)[0]\n&gt;&gt;&gt; y_second = torch.autograd.grad(y_second,z0,grad_outputs=torch.ones_like(y_second,requires_grad=True),create_graph=True)[0]\n&gt;&gt;&gt; y_second = torch.autograd.grad(y_second,z0,grad_outputs=torch.ones_like(y_second,requires_grad=True),create_graph=True)[0]\n&gt;&gt;&gt; y_second = torch.autograd.grad(y_second,z1,grad_outputs=torch.ones_like(y_second,requires_grad=True),create_graph=True)[0]\n&gt;&gt;&gt; yhat = (y_first*c[0]+y_second*c[1]).detach()\n&gt;&gt;&gt; yhat\ntensor([ 97.6657,   3.8621, -65.9329,  -1.1932], dtype=torch.float64)\n&gt;&gt;&gt; torch.allclose(y,yhat)\nTrue\n</code></pre> Source code in <code>qmcpy/kernel/abstract_kernel.py</code> <pre><code>def __init__(self,\n        d, \n        scale = 1.,\n        lengthscales = 1.,\n        shape_scale = [1],\n        shape_lengthscales = None,\n        tfs_scale = (tf_exp_eps_inv,tf_exp_eps),\n        tfs_lengthscales = (tf_exp_eps_inv,tf_exp_eps),\n        torchify = False,\n        requires_grad_scale = True, \n        requires_grad_lengthscales = True, \n        device = \"cpu\",\n        compile_call = False,\n        comiple_call_kwargs = {},\n        ):\n    r\"\"\"\n    Args:\n        d (int): Dimension. \n        scale (Union[np.ndarray,torch.Tensor]): Scaling factor $S$.\n        lengthscales (Union[np.ndarray,torch.Tensor]): Lengthscales $\\boldsymbol{\\gamma}$.\n        shape_scale (list): Shape of `scale` when `np.isscalar(scale)`. \n        shape_lengthscales (list): Shape of `lengthscales` when `np.isscalar(lengthscales)`\n        tfs_scale (Tuple[callable,callable]): The first argument transforms to the raw value to be optimized; the second applies the inverse transform.\n        tfs_lengthscales (Tuple[callable,callable]): The first argument transforms to the raw value to be optimized; the second applies the inverse transform.\n        torchify (bool): If `True`, use the `torch` backend. Set to `True` if computing gradients with respect to inputs and/or hyperparameters.\n        requires_grad_scale (bool): If `True` and `torchify`, set `requires_grad=True` for `scale`.\n        requires_grad_lengthscales (bool): If `True` and `torchify`, set `requires_grad=True` for `lengthscales`.\n        device (torch.device): If `torchify`, put things onto this device.\n        compile_call (bool): If `True`, `torch.compile` the `parsed___call__` method. \n        comiple_call_kwargs (dict): When `compile_call` is `True`, pass these keyword arguments to `torch.compile`.\n    \"\"\"\n    super().__init__(d=d,torchify=torchify,device=device,compile_call=compile_call,comiple_call_kwargs=comiple_call_kwargs)\n    self.raw_scale,self.tf_scale = self.parse_assign_param(\n        pname = \"scale\",\n        param = scale, \n        shape_param = shape_scale,\n        requires_grad_param = requires_grad_scale,\n        tfs_param = tfs_scale,\n        endsize_ops = [1],\n        constraints = [\"POSITIVE\"])\n    self.batch_param_names.append(\"scale\")\n    self.raw_lengthscales,self.tf_lengthscales = self.parse_assign_param(\n        pname = \"lengthscales\",\n        param = lengthscales, \n        shape_param = [self.d] if shape_lengthscales is None else shape_lengthscales,\n        requires_grad_param = requires_grad_lengthscales,\n        tfs_param = tfs_lengthscales,\n        endsize_ops = [1,self.d],\n        constraints = [\"POSITIVE\"])\n    self.batch_param_names.append(\"lengthscales\")\n</code></pre>"},{"location":"api/kernels/#kernelsquaredexponential","title":"<code>KernelSquaredExponential</code>","text":"<p>               Bases: <code>AbstractKernelGaussianSE</code></p> <p>Gaussian / Squared Exponential kernel implemented using the pairwise distance function. Please use <code>KernelGaussian</code> when using derivative information.</p> \\[K(\\boldsymbol{x},\\boldsymbol{z}) = S \\exp\\left(-d_{\\boldsymbol{\\gamma}}^2(\\boldsymbol{x},\\boldsymbol{z})\\right), \\qquad d_{\\boldsymbol{\\gamma}}(\\boldsymbol{x},\\boldsymbol{z}) = \\left\\lVert\\frac{\\boldsymbol{x}-\\boldsymbol{z}}{\\sqrt{2}\\boldsymbol{\\gamma}}\\right\\rVert_2.\\] <p>Examples:</p> <pre><code>&gt;&gt;&gt; rng = np.random.Generator(np.random.PCG64(7))\n&gt;&gt;&gt; kernel = KernelSquaredExponential(d=2)\n&gt;&gt;&gt; x = rng.uniform(low=0,high=1,size=(4,2))\n&gt;&gt;&gt; kernel(x[0],x[0]).item()\n1.0\n&gt;&gt;&gt; kernel(x,x)\narray([1., 1., 1., 1.])\n&gt;&gt;&gt; kernel(x[:,None,:],x[None,:,:])\narray([[1.        , 0.78888466, 0.9483142 , 0.8228498 ],\n       [0.78888466, 1.        , 0.72380317, 0.62226176],\n       [0.9483142 , 0.72380317, 1.        , 0.95613874],\n       [0.8228498 , 0.62226176, 0.95613874, 1.        ]])\n</code></pre> <p>Multiple randomizations </p> <pre><code>&gt;&gt;&gt; x = rng.uniform(low=0,high=1,size=(6,5,2))\n&gt;&gt;&gt; kernel(x,x).shape \n(6, 5)\n&gt;&gt;&gt; kernel(x[:,:,None,:],x[:,None,:,:]).shape\n(6, 5, 5)\n&gt;&gt;&gt; kernel(x[:,None,:,None,:],x[None,:,None,:,:]).shape\n(6, 6, 5, 5)\n</code></pre> <p>Batch hyperparameters </p> <pre><code>&gt;&gt;&gt; kernel = KernelSquaredExponential(\n...     d = 2, \n...     shape_scale = [4,3,1],\n...     shape_lengthscales = [3,1])\n&gt;&gt;&gt; kernel(x,x).shape \n(4, 3, 6, 5)\n&gt;&gt;&gt; kernel(x[:,:,None,:],x[:,None,:,:]).shape\n(4, 3, 6, 5, 5)\n&gt;&gt;&gt; kernel(x[:,None,:,None,:],x[None,:,None,:,:]).shape\n(4, 3, 6, 6, 5, 5)\n</code></pre> <p>PyTorch</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; rng = np.random.Generator(np.random.PCG64(7))\n&gt;&gt;&gt; kernel = KernelSquaredExponential(d=2,torchify=True)\n&gt;&gt;&gt; x = torch.from_numpy(rng.uniform(low=0,high=1,size=(4,2)))\n&gt;&gt;&gt; kernel(x[0],x[0]).item()\n1.0\n&gt;&gt;&gt; kernel(x,x)\ntensor([1., 1., 1., 1.], dtype=torch.float64, grad_fn=&lt;MulBackward0&gt;)\n&gt;&gt;&gt; kernel(x[:,None,:],x[None,:,:])\ntensor([[1.0000, 0.7889, 0.9483, 0.8228],\n        [0.7889, 1.0000, 0.7238, 0.6223],\n        [0.9483, 0.7238, 1.0000, 0.9561],\n        [0.8228, 0.6223, 0.9561, 1.0000]], dtype=torch.float64,\n       grad_fn=&lt;MulBackward0&gt;)\n</code></pre> <p>Multiple randomizations </p> <pre><code>&gt;&gt;&gt; x = torch.from_numpy(rng.uniform(low=0,high=1,size=(6,5,2)))\n&gt;&gt;&gt; kernel(x,x).shape \ntorch.Size([6, 5])\n&gt;&gt;&gt; kernel(x[:,:,None,:],x[:,None,:,:]).shape\ntorch.Size([6, 5, 5])\n&gt;&gt;&gt; kernel(x[:,None,:,None,:],x[None,:,None,:,:]).shape\ntorch.Size([6, 6, 5, 5])\n</code></pre> <p>Batch hyperparameters </p> <pre><code>&gt;&gt;&gt; kernel = KernelSquaredExponential(\n...     d = 2, \n...     shape_scale = [4,3,1],\n...     shape_lengthscales = [3,1],\n...     torchify = True)\n&gt;&gt;&gt; kernel(x,x).shape \ntorch.Size([4, 3, 6, 5])\n&gt;&gt;&gt; kernel(x[:,:,None,:],x[:,None,:,:]).shape\ntorch.Size([4, 3, 6, 5, 5])\n&gt;&gt;&gt; kernel(x[:,None,:,None,:],x[None,:,None,:,:]).shape\ntorch.Size([4, 3, 6, 6, 5, 5])\n</code></pre> Source code in <code>qmcpy/kernel/abstract_kernel.py</code> <pre><code>def __init__(self,\n        d, \n        scale = 1.,\n        lengthscales = 1.,\n        shape_scale = [1],\n        shape_lengthscales = None,\n        tfs_scale = (tf_exp_eps_inv,tf_exp_eps),\n        tfs_lengthscales = (tf_exp_eps_inv,tf_exp_eps),\n        torchify = False,\n        requires_grad_scale = True, \n        requires_grad_lengthscales = True, \n        device = \"cpu\",\n        compile_call = False,\n        comiple_call_kwargs = {},\n        ):\n    r\"\"\"\n    Args:\n        d (int): Dimension. \n        scale (Union[np.ndarray,torch.Tensor]): Scaling factor $S$.\n        lengthscales (Union[np.ndarray,torch.Tensor]): Lengthscales $\\boldsymbol{\\gamma}$.\n        shape_scale (list): Shape of `scale` when `np.isscalar(scale)`. \n        shape_lengthscales (list): Shape of `lengthscales` when `np.isscalar(lengthscales)`\n        tfs_scale (Tuple[callable,callable]): The first argument transforms to the raw value to be optimized; the second applies the inverse transform.\n        tfs_lengthscales (Tuple[callable,callable]): The first argument transforms to the raw value to be optimized; the second applies the inverse transform.\n        torchify (bool): If `True`, use the `torch` backend. Set to `True` if computing gradients with respect to inputs and/or hyperparameters.\n        requires_grad_scale (bool): If `True` and `torchify`, set `requires_grad=True` for `scale`.\n        requires_grad_lengthscales (bool): If `True` and `torchify`, set `requires_grad=True` for `lengthscales`.\n        device (torch.device): If `torchify`, put things onto this device.\n        compile_call (bool): If `True`, `torch.compile` the `parsed___call__` method. \n        comiple_call_kwargs (dict): When `compile_call` is `True`, pass these keyword arguments to `torch.compile`.\n    \"\"\"\n    super().__init__(d=d,torchify=torchify,device=device,compile_call=compile_call,comiple_call_kwargs=comiple_call_kwargs)\n    self.raw_scale,self.tf_scale = self.parse_assign_param(\n        pname = \"scale\",\n        param = scale, \n        shape_param = shape_scale,\n        requires_grad_param = requires_grad_scale,\n        tfs_param = tfs_scale,\n        endsize_ops = [1],\n        constraints = [\"POSITIVE\"])\n    self.batch_param_names.append(\"scale\")\n    self.raw_lengthscales,self.tf_lengthscales = self.parse_assign_param(\n        pname = \"lengthscales\",\n        param = lengthscales, \n        shape_param = [self.d] if shape_lengthscales is None else shape_lengthscales,\n        requires_grad_param = requires_grad_lengthscales,\n        tfs_param = tfs_lengthscales,\n        endsize_ops = [1,self.d],\n        constraints = [\"POSITIVE\"])\n    self.batch_param_names.append(\"lengthscales\")\n</code></pre>"},{"location":"api/kernels/#kernelrationalquadratic","title":"<code>KernelRationalQuadratic</code>","text":"<p>               Bases: <code>AbstractKernelScaleLengthscales</code></p> <p>Rational Quadratic kernel </p> \\[K(\\boldsymbol{x},\\boldsymbol{z}) = S \\left(1+\\frac{d_{\\boldsymbol{\\gamma}}^2(\\boldsymbol{x},\\boldsymbol{z})}{\\alpha}\\left\\lVert\\frac{\\boldsymbol{x}-\\boldsymbol{z}}{\\sqrt{2}\\boldsymbol{\\gamma}}\\right\\rVert_2^2\\right)^{-\\alpha}, \\qquad d_{\\boldsymbol{\\gamma}}(\\boldsymbol{x},\\boldsymbol{z}) = \\left\\lVert\\frac{\\boldsymbol{x}-\\boldsymbol{z}}{\\sqrt{2}\\boldsymbol{\\gamma}}\\right\\rVert_2.\\] <p>Examples:</p> <pre><code>&gt;&gt;&gt; rng = np.random.Generator(np.random.PCG64(7))\n&gt;&gt;&gt; kernel = KernelRationalQuadratic(d=2)\n&gt;&gt;&gt; x = rng.uniform(low=0,high=1,size=(4,2))\n&gt;&gt;&gt; kernel(x[0],x[0]).item()\n1.0\n&gt;&gt;&gt; kernel(x,x)\narray([1., 1., 1., 1.])\n&gt;&gt;&gt; kernel(x[:,None,:],x[None,:,:])\narray([[1.        , 0.80831912, 0.94960504, 0.83683297],\n       [0.80831912, 1.        , 0.75572321, 0.67824456],\n       [0.94960504, 0.75572321, 1.        , 0.95707312],\n       [0.83683297, 0.67824456, 0.95707312, 1.        ]])\n</code></pre> <p>Multiple randomizations </p> <pre><code>&gt;&gt;&gt; x = rng.uniform(low=0,high=1,size=(6,5,2))\n&gt;&gt;&gt; kernel(x,x).shape \n(6, 5)\n&gt;&gt;&gt; kernel(x[:,:,None,:],x[:,None,:,:]).shape\n(6, 5, 5)\n&gt;&gt;&gt; kernel(x[:,None,:,None,:],x[None,:,None,:,:]).shape\n(6, 6, 5, 5)\n</code></pre> <p>Batch hyperparameters </p> <pre><code>&gt;&gt;&gt; kernel = KernelRationalQuadratic(\n...     d = 2, \n...     shape_scale = [4,3,1],\n...     shape_lengthscales = [3,1])\n&gt;&gt;&gt; kernel(x,x).shape \n(4, 3, 6, 5)\n&gt;&gt;&gt; kernel(x[:,:,None,:],x[:,None,:,:]).shape\n(4, 3, 6, 5, 5)\n&gt;&gt;&gt; kernel(x[:,None,:,None,:],x[None,:,None,:,:]).shape\n(4, 3, 6, 6, 5, 5)\n</code></pre> <p>PyTorch</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; rng = np.random.Generator(np.random.PCG64(7))\n&gt;&gt;&gt; kernel = KernelRationalQuadratic(d=2,torchify=True)\n&gt;&gt;&gt; x = torch.from_numpy(rng.uniform(low=0,high=1,size=(4,2)))\n&gt;&gt;&gt; kernel(x[0],x[0]).item()\n1.0\n&gt;&gt;&gt; kernel(x,x)\ntensor([1., 1., 1., 1.], dtype=torch.float64, grad_fn=&lt;MulBackward0&gt;)\n&gt;&gt;&gt; kernel(x[:,None,:],x[None,:,:])\ntensor([[1.0000, 0.8083, 0.9496, 0.8368],\n        [0.8083, 1.0000, 0.7557, 0.6782],\n        [0.9496, 0.7557, 1.0000, 0.9571],\n        [0.8368, 0.6782, 0.9571, 1.0000]], dtype=torch.float64,\n       grad_fn=&lt;MulBackward0&gt;)\n</code></pre> <p>Multiple randomizations </p> <pre><code>&gt;&gt;&gt; x = torch.from_numpy(rng.uniform(low=0,high=1,size=(6,5,2)))\n&gt;&gt;&gt; kernel(x,x).shape \ntorch.Size([6, 5])\n&gt;&gt;&gt; kernel(x[:,:,None,:],x[:,None,:,:]).shape\ntorch.Size([6, 5, 5])\n&gt;&gt;&gt; kernel(x[:,None,:,None,:],x[None,:,None,:,:]).shape\ntorch.Size([6, 6, 5, 5])\n</code></pre> <p>Batch hyperparameters </p> <pre><code>&gt;&gt;&gt; kernel = KernelRationalQuadratic(\n...     d = 2, \n...     shape_alpha = [4,3,1],\n...     shape_lengthscales = [3,1],\n...     torchify = True)\n&gt;&gt;&gt; kernel(x,x).shape \ntorch.Size([4, 3, 6, 5])\n&gt;&gt;&gt; kernel(x[:,:,None,:],x[:,None,:,:]).shape\ntorch.Size([4, 3, 6, 5, 5])\n&gt;&gt;&gt; kernel(x[:,None,:,None,:],x[None,:,None,:,:]).shape\ntorch.Size([4, 3, 6, 6, 5, 5])\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>d</code> <code>int</code> <p>Dimension. </p> required <code>scale</code> <code>Union[ndarray, Tensor]</code> <p>Scaling factor \\(S\\).</p> <code>1.0</code> <code>lengthscales</code> <code>Union[ndarray, Tensor]</code> <p>Lengthscales \\(\\boldsymbol{\\gamma}\\).</p> <code>1.0</code> <code>alpha</code> <code>Union[ndarray, Tensor]</code> <p>Scale mixture parameter \\(\\alpha\\).</p> <code>1.0</code> <code>shape_scale</code> <code>list</code> <p>Shape of <code>scale</code> when <code>np.isscalar(scale)</code>. </p> <code>[1]</code> <code>shape_lengthscales</code> <code>list</code> <p>Shape of <code>lengthscales</code> when <code>np.isscalar(lengthscales)</code></p> <code>None</code> <code>shape_alpha</code> <code>list</code> <p>Shape of <code>alpha</code> when <code>np.isscalar(alpha)</code></p> <code>[1]</code> <code>tfs_scale</code> <code>Tuple[callable, callable]</code> <p>The first argument transforms to the raw value to be optimized; the second applies the inverse transform.</p> <code>(tf_exp_eps_inv, tf_exp_eps)</code> <code>tfs_lengthscales</code> <code>Tuple[callable, callable]</code> <p>The first argument transforms to the raw value to be optimized; the second applies the inverse transform.</p> <code>(tf_exp_eps_inv, tf_exp_eps)</code> <code>tfs_alpha</code> <code>Tuple[callable, callable]</code> <p>The first argument transforms to the raw value to be optimized; the second applies the inverse transform.</p> <code>(tf_exp_eps_inv, tf_exp_eps)</code> <code>torchify</code> <code>bool</code> <p>If <code>True</code>, use the <code>torch</code> backend. Set to <code>True</code> if computing gradients with respect to inputs and/or hyperparameters.</p> <code>False</code> <code>requires_grad_scale</code> <code>bool</code> <p>If <code>True</code> and <code>torchify</code>, set <code>requires_grad=True</code> for <code>scale</code>.</p> <code>True</code> <code>requires_grad_lengthscales</code> <code>bool</code> <p>If <code>True</code> and <code>torchify</code>, set <code>requires_grad=True</code> for <code>lengthscales</code>.</p> <code>True</code> <code>requires_grad_alpha</code> <code>bool</code> <p>If <code>True</code> and <code>torchify</code>, set <code>requires_grad=True</code> for <code>alpha</code>.</p> <code>True</code> <code>device</code> <code>device</code> <p>If <code>torchify</code>, put things onto this device.</p> <code>'cpu'</code> <code>compile_call</code> <code>bool</code> <p>If <code>True</code>, <code>torch.compile</code> the <code>parsed___call__</code> method. </p> <code>False</code> <code>comiple_call_kwargs</code> <code>dict</code> <p>When <code>compile_call</code> is <code>True</code>, pass these keyword arguments to <code>torch.compile</code>.</p> <code>{}</code> Source code in <code>qmcpy/kernel/common_kernels.py</code> <pre><code>def __init__(self,\n        d, \n        scale = 1., \n        lengthscales = 1.,\n        alpha = 1.,\n        shape_scale = [1],\n        shape_lengthscales = None, \n        shape_alpha = [1],\n        tfs_scale = (tf_exp_eps_inv,tf_exp_eps),\n        tfs_lengthscales = (tf_exp_eps_inv,tf_exp_eps),\n        tfs_alpha = (tf_exp_eps_inv,tf_exp_eps),\n        torchify = False, \n        requires_grad_scale = True, \n        requires_grad_lengthscales = True, \n        requires_grad_alpha = True, \n        device = \"cpu\",\n        compile_call = False,\n        comiple_call_kwargs = {},\n        ):\n    r\"\"\"\n    Args:\n        d (int): Dimension. \n        scale (Union[np.ndarray,torch.Tensor]): Scaling factor $S$.\n        lengthscales (Union[np.ndarray,torch.Tensor]): Lengthscales $\\boldsymbol{\\gamma}$.\n        alpha (Union[np.ndarray,torch.Tensor]): Scale mixture parameter $\\alpha$.\n        shape_scale (list): Shape of `scale` when `np.isscalar(scale)`. \n        shape_lengthscales (list): Shape of `lengthscales` when `np.isscalar(lengthscales)`\n        shape_alpha (list): Shape of `alpha` when `np.isscalar(alpha)`\n        tfs_scale (Tuple[callable,callable]): The first argument transforms to the raw value to be optimized; the second applies the inverse transform.\n        tfs_lengthscales (Tuple[callable,callable]): The first argument transforms to the raw value to be optimized; the second applies the inverse transform.\n        tfs_alpha (Tuple[callable,callable]): The first argument transforms to the raw value to be optimized; the second applies the inverse transform.\n        torchify (bool): If `True`, use the `torch` backend. Set to `True` if computing gradients with respect to inputs and/or hyperparameters.\n        requires_grad_scale (bool): If `True` and `torchify`, set `requires_grad=True` for `scale`.\n        requires_grad_lengthscales (bool): If `True` and `torchify`, set `requires_grad=True` for `lengthscales`.\n        requires_grad_alpha (bool): If `True` and `torchify`, set `requires_grad=True` for `alpha`.\n        device (torch.device): If `torchify`, put things onto this device.\n        compile_call (bool): If `True`, `torch.compile` the `parsed___call__` method. \n        comiple_call_kwargs (dict): When `compile_call` is `True`, pass these keyword arguments to `torch.compile`.\n    \"\"\"\n    super().__init__(\n        d = d, \n        scale = scale, \n        lengthscales = lengthscales,\n        shape_scale = shape_scale,\n        shape_lengthscales = shape_lengthscales, \n        tfs_scale = tfs_scale,\n        tfs_lengthscales = tfs_lengthscales,\n        torchify = torchify, \n        requires_grad_scale = requires_grad_scale, \n        requires_grad_lengthscales = requires_grad_lengthscales, \n        device = device,\n        compile_call = compile_call,\n        comiple_call_kwargs = comiple_call_kwargs,\n    )\n    self.raw_alpha,self.tf_alpha = self.parse_assign_param(\n        pname = \"alpha\",\n        param = alpha, \n        shape_param = shape_alpha,\n        requires_grad_param = requires_grad_alpha,\n        tfs_param = tfs_alpha,\n        endsize_ops = [1],\n        constraints = [\"POSITIVE\"])\n    self.batch_param_names.append(\"alpha\")\n</code></pre>"},{"location":"api/kernels/#kernelmatern12","title":"<code>KernelMatern12</code>","text":"<p>               Bases: <code>AbstractKernelScaleLengthscales</code></p> <p>Matern kernel with \\(\\alpha=1/2\\). </p> \\[K(\\boldsymbol{x},\\boldsymbol{z}) = S \\exp\\left(-d_{\\boldsymbol{\\gamma}}(\\boldsymbol{x},\\boldsymbol{z})\\right), \\qquad d_{\\boldsymbol{\\gamma}}(\\boldsymbol{x},\\boldsymbol{z}) = \\left\\lVert\\frac{\\boldsymbol{x}-\\boldsymbol{z}}{\\sqrt{2}\\boldsymbol{\\gamma}}\\right\\rVert_2.\\] <p>Examples:</p> <pre><code>&gt;&gt;&gt; rng = np.random.Generator(np.random.PCG64(7))\n&gt;&gt;&gt; kernel = KernelMatern12(d=2)\n&gt;&gt;&gt; x = rng.uniform(low=0,high=1,size=(4,2))\n&gt;&gt;&gt; kernel(x[0],x[0]).item()\n1.0\n&gt;&gt;&gt; kernel(x,x)\narray([1., 1., 1., 1.])\n&gt;&gt;&gt; kernel(x[:,None,:],x[None,:,:])\narray([[1.        , 0.61448839, 0.79424131, 0.64302787],\n       [0.61448839, 1.        , 0.56635268, 0.50219691],\n       [0.79424131, 0.56635268, 1.        , 0.80913986],\n       [0.64302787, 0.50219691, 0.80913986, 1.        ]])\n</code></pre> <p>Multiple randomizations </p> <pre><code>&gt;&gt;&gt; x = rng.uniform(low=0,high=1,size=(6,5,2))\n&gt;&gt;&gt; kernel(x,x).shape \n(6, 5)\n&gt;&gt;&gt; kernel(x[:,:,None,:],x[:,None,:,:]).shape\n(6, 5, 5)\n&gt;&gt;&gt; kernel(x[:,None,:,None,:],x[None,:,None,:,:]).shape\n(6, 6, 5, 5)\n</code></pre> <p>Batch hyperparameters </p> <pre><code>&gt;&gt;&gt; kernel = KernelMatern12(\n...     d = 2, \n...     shape_scale = [4,3,1],\n...     shape_lengthscales = [3,1])\n&gt;&gt;&gt; kernel(x,x).shape \n(4, 3, 6, 5)\n&gt;&gt;&gt; kernel(x[:,:,None,:],x[:,None,:,:]).shape\n(4, 3, 6, 5, 5)\n&gt;&gt;&gt; kernel(x[:,None,:,None,:],x[None,:,None,:,:]).shape\n(4, 3, 6, 6, 5, 5)\n</code></pre> <p>PyTorch</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; rng = np.random.Generator(np.random.PCG64(7))\n&gt;&gt;&gt; kernel = KernelMatern12(d=2,torchify=True)\n&gt;&gt;&gt; x = torch.from_numpy(rng.uniform(low=0,high=1,size=(4,2)))\n&gt;&gt;&gt; kernel(x[0],x[0]).item()\n1.0\n&gt;&gt;&gt; kernel(x,x)\ntensor([1., 1., 1., 1.], dtype=torch.float64, grad_fn=&lt;MulBackward0&gt;)\n&gt;&gt;&gt; kernel(x[:,None,:],x[None,:,:])\ntensor([[1.0000, 0.6145, 0.7942, 0.6430],\n        [0.6145, 1.0000, 0.5664, 0.5022],\n        [0.7942, 0.5664, 1.0000, 0.8091],\n        [0.6430, 0.5022, 0.8091, 1.0000]], dtype=torch.float64,\n       grad_fn=&lt;MulBackward0&gt;)\n</code></pre> <p>Multiple randomizations </p> <pre><code>&gt;&gt;&gt; x = torch.from_numpy(rng.uniform(low=0,high=1,size=(6,5,2)))\n&gt;&gt;&gt; kernel(x,x).shape \ntorch.Size([6, 5])\n&gt;&gt;&gt; kernel(x[:,:,None,:],x[:,None,:,:]).shape\ntorch.Size([6, 5, 5])\n&gt;&gt;&gt; kernel(x[:,None,:,None,:],x[None,:,None,:,:]).shape\ntorch.Size([6, 6, 5, 5])\n</code></pre> <p>Batch hyperparameters </p> <pre><code>&gt;&gt;&gt; kernel = KernelMatern12(\n...     d = 2, \n...     shape_scale = [4,3,1],\n...     shape_lengthscales = [3,1],\n...     torchify = True)\n&gt;&gt;&gt; kernel(x,x).shape \ntorch.Size([4, 3, 6, 5])\n&gt;&gt;&gt; kernel(x[:,:,None,:],x[:,None,:,:]).shape\ntorch.Size([4, 3, 6, 5, 5])\n&gt;&gt;&gt; kernel(x[:,None,:,None,:],x[None,:,None,:,:]).shape\ntorch.Size([4, 3, 6, 6, 5, 5])\n</code></pre> Source code in <code>qmcpy/kernel/abstract_kernel.py</code> <pre><code>def __init__(self,\n        d, \n        scale = 1.,\n        lengthscales = 1.,\n        shape_scale = [1],\n        shape_lengthscales = None,\n        tfs_scale = (tf_exp_eps_inv,tf_exp_eps),\n        tfs_lengthscales = (tf_exp_eps_inv,tf_exp_eps),\n        torchify = False,\n        requires_grad_scale = True, \n        requires_grad_lengthscales = True, \n        device = \"cpu\",\n        compile_call = False,\n        comiple_call_kwargs = {},\n        ):\n    r\"\"\"\n    Args:\n        d (int): Dimension. \n        scale (Union[np.ndarray,torch.Tensor]): Scaling factor $S$.\n        lengthscales (Union[np.ndarray,torch.Tensor]): Lengthscales $\\boldsymbol{\\gamma}$.\n        shape_scale (list): Shape of `scale` when `np.isscalar(scale)`. \n        shape_lengthscales (list): Shape of `lengthscales` when `np.isscalar(lengthscales)`\n        tfs_scale (Tuple[callable,callable]): The first argument transforms to the raw value to be optimized; the second applies the inverse transform.\n        tfs_lengthscales (Tuple[callable,callable]): The first argument transforms to the raw value to be optimized; the second applies the inverse transform.\n        torchify (bool): If `True`, use the `torch` backend. Set to `True` if computing gradients with respect to inputs and/or hyperparameters.\n        requires_grad_scale (bool): If `True` and `torchify`, set `requires_grad=True` for `scale`.\n        requires_grad_lengthscales (bool): If `True` and `torchify`, set `requires_grad=True` for `lengthscales`.\n        device (torch.device): If `torchify`, put things onto this device.\n        compile_call (bool): If `True`, `torch.compile` the `parsed___call__` method. \n        comiple_call_kwargs (dict): When `compile_call` is `True`, pass these keyword arguments to `torch.compile`.\n    \"\"\"\n    super().__init__(d=d,torchify=torchify,device=device,compile_call=compile_call,comiple_call_kwargs=comiple_call_kwargs)\n    self.raw_scale,self.tf_scale = self.parse_assign_param(\n        pname = \"scale\",\n        param = scale, \n        shape_param = shape_scale,\n        requires_grad_param = requires_grad_scale,\n        tfs_param = tfs_scale,\n        endsize_ops = [1],\n        constraints = [\"POSITIVE\"])\n    self.batch_param_names.append(\"scale\")\n    self.raw_lengthscales,self.tf_lengthscales = self.parse_assign_param(\n        pname = \"lengthscales\",\n        param = lengthscales, \n        shape_param = [self.d] if shape_lengthscales is None else shape_lengthscales,\n        requires_grad_param = requires_grad_lengthscales,\n        tfs_param = tfs_lengthscales,\n        endsize_ops = [1,self.d],\n        constraints = [\"POSITIVE\"])\n    self.batch_param_names.append(\"lengthscales\")\n</code></pre>"},{"location":"api/kernels/#kernelmatern32","title":"<code>KernelMatern32</code>","text":"<p>               Bases: <code>AbstractKernelScaleLengthscales</code></p> <p>Matern kernel with \\(\\alpha=3/2\\). </p> \\[K(\\boldsymbol{x},\\boldsymbol{z}) = S \\left(1+\\sqrt{3} d_{\\boldsymbol{\\gamma}}(\\boldsymbol{x},\\boldsymbol{z})\\right)\\exp\\left(-\\sqrt{3}d_{\\boldsymbol{\\gamma}}(\\boldsymbol{x},\\boldsymbol{z})\\right), \\qquad d_{\\boldsymbol{\\gamma}}(\\boldsymbol{x},\\boldsymbol{z}) = \\left\\lVert\\frac{\\boldsymbol{x}-\\boldsymbol{z}}{\\sqrt{2}\\boldsymbol{\\gamma}}\\right\\rVert_2.\\] <p>Examples:</p> <pre><code>&gt;&gt;&gt; rng = np.random.Generator(np.random.PCG64(7))\n&gt;&gt;&gt; kernel = KernelMatern32(d=2)\n&gt;&gt;&gt; x = rng.uniform(low=0,high=1,size=(4,2))\n&gt;&gt;&gt; kernel(x[0],x[0]).item()\n1.0\n&gt;&gt;&gt; kernel(x,x)\narray([1., 1., 1., 1.])\n&gt;&gt;&gt; kernel(x[:,None,:],x[None,:,:])\narray([[1.        , 0.79309639, 0.93871358, 0.82137958],\n       [0.79309639, 1.        , 0.74137353, 0.66516872],\n       [0.93871358, 0.74137353, 1.        , 0.9471166 ],\n       [0.82137958, 0.66516872, 0.9471166 , 1.        ]])\n</code></pre> <p>Multiple randomizations </p> <pre><code>&gt;&gt;&gt; x = rng.uniform(low=0,high=1,size=(6,5,2))\n&gt;&gt;&gt; kernel(x,x).shape \n(6, 5)\n&gt;&gt;&gt; kernel(x[:,:,None,:],x[:,None,:,:]).shape\n(6, 5, 5)\n&gt;&gt;&gt; kernel(x[:,None,:,None,:],x[None,:,None,:,:]).shape\n(6, 6, 5, 5)\n</code></pre> <p>Batch hyperparameters </p> <pre><code>&gt;&gt;&gt; kernel = KernelMatern32(\n...     d = 2, \n...     shape_scale = [4,3,1],\n...     shape_lengthscales = [3,1])\n&gt;&gt;&gt; kernel(x,x).shape \n(4, 3, 6, 5)\n&gt;&gt;&gt; kernel(x[:,:,None,:],x[:,None,:,:]).shape\n(4, 3, 6, 5, 5)\n&gt;&gt;&gt; kernel(x[:,None,:,None,:],x[None,:,None,:,:]).shape\n(4, 3, 6, 6, 5, 5)\n</code></pre> <p>PyTorch</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; rng = np.random.Generator(np.random.PCG64(7))\n&gt;&gt;&gt; kernel = KernelMatern32(d=2,torchify=True)\n&gt;&gt;&gt; x = torch.from_numpy(rng.uniform(low=0,high=1,size=(4,2)))\n&gt;&gt;&gt; kernel(x[0],x[0]).item()\n1.0\n&gt;&gt;&gt; kernel(x,x)\ntensor([1., 1., 1., 1.], dtype=torch.float64, grad_fn=&lt;MulBackward0&gt;)\n&gt;&gt;&gt; kernel(x[:,None,:],x[None,:,:])\ntensor([[1.0000, 0.7931, 0.9387, 0.8214],\n        [0.7931, 1.0000, 0.7414, 0.6652],\n        [0.9387, 0.7414, 1.0000, 0.9471],\n        [0.8214, 0.6652, 0.9471, 1.0000]], dtype=torch.float64,\n       grad_fn=&lt;MulBackward0&gt;)\n</code></pre> <p>Multiple randomizations </p> <pre><code>&gt;&gt;&gt; x = torch.from_numpy(rng.uniform(low=0,high=1,size=(6,5,2)))\n&gt;&gt;&gt; kernel(x,x).shape \ntorch.Size([6, 5])\n&gt;&gt;&gt; kernel(x[:,:,None,:],x[:,None,:,:]).shape\ntorch.Size([6, 5, 5])\n&gt;&gt;&gt; kernel(x[:,None,:,None,:],x[None,:,None,:,:]).shape\ntorch.Size([6, 6, 5, 5])\n</code></pre> <p>Batch hyperparameters </p> <pre><code>&gt;&gt;&gt; kernel = KernelMatern32(\n...     d = 2, \n...     shape_scale = [4,3,1],\n...     shape_lengthscales = [3,1],\n...     torchify = True)\n&gt;&gt;&gt; kernel(x,x).shape \ntorch.Size([4, 3, 6, 5])\n&gt;&gt;&gt; kernel(x[:,:,None,:],x[:,None,:,:]).shape\ntorch.Size([4, 3, 6, 5, 5])\n&gt;&gt;&gt; kernel(x[:,None,:,None,:],x[None,:,None,:,:]).shape\ntorch.Size([4, 3, 6, 6, 5, 5])\n</code></pre> Source code in <code>qmcpy/kernel/abstract_kernel.py</code> <pre><code>def __init__(self,\n        d, \n        scale = 1.,\n        lengthscales = 1.,\n        shape_scale = [1],\n        shape_lengthscales = None,\n        tfs_scale = (tf_exp_eps_inv,tf_exp_eps),\n        tfs_lengthscales = (tf_exp_eps_inv,tf_exp_eps),\n        torchify = False,\n        requires_grad_scale = True, \n        requires_grad_lengthscales = True, \n        device = \"cpu\",\n        compile_call = False,\n        comiple_call_kwargs = {},\n        ):\n    r\"\"\"\n    Args:\n        d (int): Dimension. \n        scale (Union[np.ndarray,torch.Tensor]): Scaling factor $S$.\n        lengthscales (Union[np.ndarray,torch.Tensor]): Lengthscales $\\boldsymbol{\\gamma}$.\n        shape_scale (list): Shape of `scale` when `np.isscalar(scale)`. \n        shape_lengthscales (list): Shape of `lengthscales` when `np.isscalar(lengthscales)`\n        tfs_scale (Tuple[callable,callable]): The first argument transforms to the raw value to be optimized; the second applies the inverse transform.\n        tfs_lengthscales (Tuple[callable,callable]): The first argument transforms to the raw value to be optimized; the second applies the inverse transform.\n        torchify (bool): If `True`, use the `torch` backend. Set to `True` if computing gradients with respect to inputs and/or hyperparameters.\n        requires_grad_scale (bool): If `True` and `torchify`, set `requires_grad=True` for `scale`.\n        requires_grad_lengthscales (bool): If `True` and `torchify`, set `requires_grad=True` for `lengthscales`.\n        device (torch.device): If `torchify`, put things onto this device.\n        compile_call (bool): If `True`, `torch.compile` the `parsed___call__` method. \n        comiple_call_kwargs (dict): When `compile_call` is `True`, pass these keyword arguments to `torch.compile`.\n    \"\"\"\n    super().__init__(d=d,torchify=torchify,device=device,compile_call=compile_call,comiple_call_kwargs=comiple_call_kwargs)\n    self.raw_scale,self.tf_scale = self.parse_assign_param(\n        pname = \"scale\",\n        param = scale, \n        shape_param = shape_scale,\n        requires_grad_param = requires_grad_scale,\n        tfs_param = tfs_scale,\n        endsize_ops = [1],\n        constraints = [\"POSITIVE\"])\n    self.batch_param_names.append(\"scale\")\n    self.raw_lengthscales,self.tf_lengthscales = self.parse_assign_param(\n        pname = \"lengthscales\",\n        param = lengthscales, \n        shape_param = [self.d] if shape_lengthscales is None else shape_lengthscales,\n        requires_grad_param = requires_grad_lengthscales,\n        tfs_param = tfs_lengthscales,\n        endsize_ops = [1,self.d],\n        constraints = [\"POSITIVE\"])\n    self.batch_param_names.append(\"lengthscales\")\n</code></pre>"},{"location":"api/kernels/#kernelmatern52","title":"<code>KernelMatern52</code>","text":"<p>               Bases: <code>AbstractKernelScaleLengthscales</code></p> <p>Matern kernel with \\(\\alpha=5/2\\). </p> \\[K(\\boldsymbol{x},\\boldsymbol{z}) = S \\left(1+\\sqrt{5} d_{\\boldsymbol{\\gamma}}(\\boldsymbol{x},\\boldsymbol{z}) + \\frac{5}{3} d_{\\boldsymbol{\\gamma}}^2(\\boldsymbol{x},\\boldsymbol{z})\\right)\\exp\\left(-\\sqrt{5}d_{\\boldsymbol{\\gamma}}(\\boldsymbol{x},\\boldsymbol{z})\\right), \\qquad d_{\\boldsymbol{\\gamma}}(\\boldsymbol{x},\\boldsymbol{z}) = \\left\\lVert\\frac{\\boldsymbol{x}-\\boldsymbol{z}}{\\sqrt{2}\\boldsymbol{\\gamma}}\\right\\rVert_2.\\] <p>Examples:</p> <pre><code>&gt;&gt;&gt; rng = np.random.Generator(np.random.PCG64(7))\n&gt;&gt;&gt; kernel = KernelMatern52(d=2)\n&gt;&gt;&gt; x = rng.uniform(low=0,high=1,size=(4,2))\n&gt;&gt;&gt; kernel(x[0],x[0]).item()\n1.0\n&gt;&gt;&gt; kernel(x,x)\narray([1., 1., 1., 1.])\n&gt;&gt;&gt; kernel(x[:,None,:],x[None,:,:])\narray([[1.        , 0.83612941, 0.95801903, 0.861472  ],\n       [0.83612941, 1.        , 0.78812397, 0.71396963],\n       [0.95801903, 0.78812397, 1.        , 0.96425994],\n       [0.861472  , 0.71396963, 0.96425994, 1.        ]])\n</code></pre> <p>Multiple randomizations </p> <pre><code>&gt;&gt;&gt; x = rng.uniform(low=0,high=1,size=(6,5,2))\n&gt;&gt;&gt; kernel(x,x).shape \n(6, 5)\n&gt;&gt;&gt; kernel(x[:,:,None,:],x[:,None,:,:]).shape\n(6, 5, 5)\n&gt;&gt;&gt; kernel(x[:,None,:,None,:],x[None,:,None,:,:]).shape\n(6, 6, 5, 5)\n</code></pre> <p>Batch hyperparameters </p> <pre><code>&gt;&gt;&gt; kernel = KernelMatern52(\n...     d = 2, \n...     shape_scale = [4,3,1],\n...     shape_lengthscales = [3,1])\n&gt;&gt;&gt; kernel(x,x).shape \n(4, 3, 6, 5)\n&gt;&gt;&gt; kernel(x[:,:,None,:],x[:,None,:,:]).shape\n(4, 3, 6, 5, 5)\n&gt;&gt;&gt; kernel(x[:,None,:,None,:],x[None,:,None,:,:]).shape\n(4, 3, 6, 6, 5, 5)\n</code></pre> <p>PyTorch</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; rng = np.random.Generator(np.random.PCG64(7))\n&gt;&gt;&gt; kernel = KernelMatern52(d=2,torchify=True)\n&gt;&gt;&gt; x = torch.from_numpy(rng.uniform(low=0,high=1,size=(4,2)))\n&gt;&gt;&gt; kernel(x[0],x[0]).item()\n1.0\n&gt;&gt;&gt; kernel(x,x)\ntensor([1., 1., 1., 1.], dtype=torch.float64, grad_fn=&lt;MulBackward0&gt;)\n&gt;&gt;&gt; kernel(x[:,None,:],x[None,:,:])\ntensor([[1.0000, 0.8361, 0.9580, 0.8615],\n        [0.8361, 1.0000, 0.7881, 0.7140],\n        [0.9580, 0.7881, 1.0000, 0.9643],\n        [0.8615, 0.7140, 0.9643, 1.0000]], dtype=torch.float64,\n       grad_fn=&lt;MulBackward0&gt;)\n</code></pre> <p>Multiple randomizations </p> <pre><code>&gt;&gt;&gt; x = torch.from_numpy(rng.uniform(low=0,high=1,size=(6,5,2)))\n&gt;&gt;&gt; kernel(x,x).shape \ntorch.Size([6, 5])\n&gt;&gt;&gt; kernel(x[:,:,None,:],x[:,None,:,:]).shape\ntorch.Size([6, 5, 5])\n&gt;&gt;&gt; kernel(x[:,None,:,None,:],x[None,:,None,:,:]).shape\ntorch.Size([6, 6, 5, 5])\n</code></pre> <p>Batch hyperparameters </p> <pre><code>&gt;&gt;&gt; kernel = KernelMatern52(\n...     d = 2, \n...     shape_scale = [4,3,1],\n...     shape_lengthscales = [3,1],\n...     torchify = True)\n&gt;&gt;&gt; kernel(x,x).shape \ntorch.Size([4, 3, 6, 5])\n&gt;&gt;&gt; kernel(x[:,:,None,:],x[:,None,:,:]).shape\ntorch.Size([4, 3, 6, 5, 5])\n&gt;&gt;&gt; kernel(x[:,None,:,None,:],x[None,:,None,:,:]).shape\ntorch.Size([4, 3, 6, 6, 5, 5])\n</code></pre> Source code in <code>qmcpy/kernel/abstract_kernel.py</code> <pre><code>def __init__(self,\n        d, \n        scale = 1.,\n        lengthscales = 1.,\n        shape_scale = [1],\n        shape_lengthscales = None,\n        tfs_scale = (tf_exp_eps_inv,tf_exp_eps),\n        tfs_lengthscales = (tf_exp_eps_inv,tf_exp_eps),\n        torchify = False,\n        requires_grad_scale = True, \n        requires_grad_lengthscales = True, \n        device = \"cpu\",\n        compile_call = False,\n        comiple_call_kwargs = {},\n        ):\n    r\"\"\"\n    Args:\n        d (int): Dimension. \n        scale (Union[np.ndarray,torch.Tensor]): Scaling factor $S$.\n        lengthscales (Union[np.ndarray,torch.Tensor]): Lengthscales $\\boldsymbol{\\gamma}$.\n        shape_scale (list): Shape of `scale` when `np.isscalar(scale)`. \n        shape_lengthscales (list): Shape of `lengthscales` when `np.isscalar(lengthscales)`\n        tfs_scale (Tuple[callable,callable]): The first argument transforms to the raw value to be optimized; the second applies the inverse transform.\n        tfs_lengthscales (Tuple[callable,callable]): The first argument transforms to the raw value to be optimized; the second applies the inverse transform.\n        torchify (bool): If `True`, use the `torch` backend. Set to `True` if computing gradients with respect to inputs and/or hyperparameters.\n        requires_grad_scale (bool): If `True` and `torchify`, set `requires_grad=True` for `scale`.\n        requires_grad_lengthscales (bool): If `True` and `torchify`, set `requires_grad=True` for `lengthscales`.\n        device (torch.device): If `torchify`, put things onto this device.\n        compile_call (bool): If `True`, `torch.compile` the `parsed___call__` method. \n        comiple_call_kwargs (dict): When `compile_call` is `True`, pass these keyword arguments to `torch.compile`.\n    \"\"\"\n    super().__init__(d=d,torchify=torchify,device=device,compile_call=compile_call,comiple_call_kwargs=comiple_call_kwargs)\n    self.raw_scale,self.tf_scale = self.parse_assign_param(\n        pname = \"scale\",\n        param = scale, \n        shape_param = shape_scale,\n        requires_grad_param = requires_grad_scale,\n        tfs_param = tfs_scale,\n        endsize_ops = [1],\n        constraints = [\"POSITIVE\"])\n    self.batch_param_names.append(\"scale\")\n    self.raw_lengthscales,self.tf_lengthscales = self.parse_assign_param(\n        pname = \"lengthscales\",\n        param = lengthscales, \n        shape_param = [self.d] if shape_lengthscales is None else shape_lengthscales,\n        requires_grad_param = requires_grad_lengthscales,\n        tfs_param = tfs_lengthscales,\n        endsize_ops = [1,self.d],\n        constraints = [\"POSITIVE\"])\n    self.batch_param_names.append(\"lengthscales\")\n</code></pre>"},{"location":"api/kernels/#kernelmultitask","title":"<code>KernelMultiTask</code>","text":"<p>               Bases: <code>AbstractKernel</code></p> <p>Multi-task kernel</p> \\[K((i,\\boldsymbol{x}),(j,\\boldsymbol{z})) = K_{\\mathrm{task}}(i,j) K_{\\mathrm{base}}(\\boldsymbol{x},\\boldsymbol{z})\\] <p>parameterized for \\(T\\) tasks by a factor \\(\\mathsf{F} \\in \\mathbb{R}^{T \\times r}\\) and a diagonal \\(\\boldsymbol{v} \\in \\mathbb{R}^T\\) so that </p> \\[\\left[K_{\\mathrm{task}}(i,j)\\right]_{i,j=1}^T = \\mathsf{F} \\mathsf{F}^T + \\mathrm{diag}(\\boldsymbol{v}).\\] <p>Examples:</p> <pre><code>&gt;&gt;&gt; kmt = KernelMultiTask(KernelGaussian(d=2),num_tasks=3,diag=[1,2,3])\n&gt;&gt;&gt; x = np.random.rand(4,2) \n&gt;&gt;&gt; task = np.arange(3) \n&gt;&gt;&gt; kmt(task[1],task[1],x[0],x[0]).item()\n3.0\n&gt;&gt;&gt; kmt(task,task,x[0],x[0])\narray([2., 3., 4.])\n&gt;&gt;&gt; kmt(task[:,None],task[None,:],x[0],x[0]).shape\n(3, 3)\n&gt;&gt;&gt; kmt(task[1],task[1],x,x,)\narray([3., 3., 3., 3.])\n&gt;&gt;&gt; kmt(task,task,x[:3],x[:3])\narray([2., 3., 4.])\n&gt;&gt;&gt; kmt(task[:,None],task[:,None],x,x).shape\n(3, 4)\n&gt;&gt;&gt; v = kmt(task,task,x[:3,None,:],x[None,:3,:])\n&gt;&gt;&gt; v.shape\n(3, 3)\n&gt;&gt;&gt; np.allclose(v,kmt.base_kernel(x[:3,None,:],x[None,:3,:])*kmt.taskmat[task,task])\nTrue\n&gt;&gt;&gt; kmt(task[:,None,None],task[:,None,None],x[:,None,:],x[None,:,:]).shape\n(3, 4, 4)\n&gt;&gt;&gt; kmt(task[:,None,None,None],task[None,:,None,None],x[:,None,:],x[None,:,:]).shape\n(3, 3, 4, 4)\n</code></pre> <p>Batched inference</p> <pre><code>&gt;&gt;&gt; kernel_base = KernelGaussian(d=10,shape_lengthscales=(5,1),shape_scale=(3,5,1))\n&gt;&gt;&gt; rng = np.random.Generator(np.random.PCG64(7))\n&gt;&gt;&gt; kmt = KernelMultiTask(kernel_base,\n...     num_tasks=4,\n...     shape_factor=(6,3,5,4,2),\n...     diag = rng.uniform(low=0,high=1,size=(3,5,4)))\n&gt;&gt;&gt; x = np.random.rand(8,10)\n&gt;&gt;&gt; task = np.arange(4) \n&gt;&gt;&gt; kmt(task[0],task[0],x[0],x[0]).shape\n(6, 3, 5)\n&gt;&gt;&gt; kmt(task,task,x[0],x[0]).shape\n(6, 3, 5, 4)\n&gt;&gt;&gt; kmt(task[0],task[0],x,x).shape\n(6, 3, 5, 8)\n&gt;&gt;&gt; v = kmt(task,task,x[:4,None,:],x[None,:4,:])\n&gt;&gt;&gt; v.shape\n(6, 3, 5, 4, 4)\n&gt;&gt;&gt; kmat_x = kmt.base_kernel(x[:4,None,:],x[None,:4,:])\n&gt;&gt;&gt; kmat_x.shape\n(3, 5, 4, 4)\n&gt;&gt;&gt; kmat_tasks = kmt.taskmat[...,task,task]\n&gt;&gt;&gt; kmat_tasks.shape\n(6, 3, 5, 4)\n&gt;&gt;&gt; np.allclose(v,kmat_tasks[...,None,:]*kmat_x)\nTrue\n&gt;&gt;&gt; np.allclose(v,kmat_tasks[...,:,None]*kmat_x)\nFalse\n&gt;&gt;&gt; kmt(task[:,None,None,None],task[None,:,None,None],x[:,None,:],x[None,:,:]).shape\n(6, 3, 5, 4, 4, 8, 8)\n&gt;&gt;&gt; kmt(task[:,None],task[None,:],x[:,None,None,None,:],x[None,:,None,None,:]).shape\n(6, 3, 5, 8, 8, 4, 4)\n</code></pre> <p>Integrals </p> <pre><code>&gt;&gt;&gt; kernel = KernelGaussian(\n...     d = 2,\n...     scale = rng.uniform(low=0,high=1,size=(3,1)),\n...     lengthscales = rng.uniform(low=0,high=1,size=(1,2)))\n&gt;&gt;&gt; kmt = KernelMultiTask(\n...     base_kernel=kernel,\n...     num_tasks=5,\n...     diag = rng.uniform(low=0,high=1,size=(6,3,5)),\n...     factor = rng.uniform(low=0,high=1,size=(3,5,2)))\n&gt;&gt;&gt; task = np.arange(5) \n&gt;&gt;&gt; kmt.double_integral_01d(task0=task[0],task1=task[1]).shape\n(6, 3)\n&gt;&gt;&gt; kmt.double_integral_01d(task0=task,task1=task).shape\n(6, 3, 5)\n&gt;&gt;&gt; kmt.double_integral_01d(task0=task[:,None],task1=task[None,:]).shape\n(6, 3, 5, 5)\n&gt;&gt;&gt; x = rng.uniform(low=0,high=1,size=(7,4,2))\n&gt;&gt;&gt; kmt.single_integral_01d(task0=task[0],task1=task[1],x=x).shape\n(6, 3, 7, 4)\n&gt;&gt;&gt; kmt.single_integral_01d(task0=task[:,None,None],task1=task[:,None,None],x=x).shape\n(6, 3, 5, 7, 4)\n&gt;&gt;&gt; kmt.single_integral_01d(task0=task[:,None,None,None],task1=task[None,:,None,None],x=x).shape\n(6, 3, 5, 5, 7, 4)\n</code></pre> <p>PyTorch</p> <pre><code>&gt;&gt;&gt; import torch \n&gt;&gt;&gt; kmt = KernelMultiTask(KernelGaussian(d=2,torchify=True),num_tasks=3,diag=[1,2,3])\n&gt;&gt;&gt; x = torch.from_numpy(np.random.rand(4,2))\n&gt;&gt;&gt; task = np.arange(3) \n&gt;&gt;&gt; kmt(task[1],task[1],x[0],x[0]).item()\n3.0\n&gt;&gt;&gt; kmt(task,task,x[0],x[0])\ntensor([2., 3., 4.], dtype=torch.float64, grad_fn=&lt;SelectBackward0&gt;)\n&gt;&gt;&gt; kmt(task[:,None],task[None,:],x[0],x[0]).shape\ntorch.Size([3, 3])\n&gt;&gt;&gt; kmt(task[1],task[1],x,x)\ntensor([3., 3., 3., 3.], dtype=torch.float64, grad_fn=&lt;SelectBackward0&gt;)\n&gt;&gt;&gt; kmt(task,task,x[:3],x[:3])\ntensor([2., 3., 4.], dtype=torch.float64, grad_fn=&lt;SelectBackward0&gt;)\n&gt;&gt;&gt; kmt(task[:,None],task[:,None],x,x).shape\ntorch.Size([3, 4])\n&gt;&gt;&gt; v = kmt(task,task,x[:3,None,:],x[None,:3,:])\n&gt;&gt;&gt; v.shape\ntorch.Size([3, 3])\n&gt;&gt;&gt; torch.allclose(v,kmt.base_kernel(x[:3,None,:],x[None,:3,:])*kmt.taskmat[task,task])\nTrue\n&gt;&gt;&gt; kmt(task[:,None,None],task[:,None,None],x[:,None,:],x[None,:,:]).shape\ntorch.Size([3, 4, 4])\n&gt;&gt;&gt; kmt(task[:,None,None,None],task[None,:,None,None],x[:,None,:],x[None,:,:]).shape\ntorch.Size([3, 3, 4, 4])\n</code></pre> <p>Batched inference</p> <pre><code>&gt;&gt;&gt; kernel_base = KernelGaussian(d=10,shape_lengthscales=(5,1),shape_scale=(3,5,1),torchify=True)\n&gt;&gt;&gt; rng = np.random.Generator(np.random.PCG64(7))\n&gt;&gt;&gt; kmt = KernelMultiTask(kernel_base,\n...     num_tasks=4,\n...     shape_factor=(6,3,5,4,2),\n...     diag = rng.uniform(low=0,high=1,size=(3,5,4)))\n&gt;&gt;&gt; x = torch.from_numpy(np.random.rand(8,10))\n&gt;&gt;&gt; task = torch.arange(4) \n&gt;&gt;&gt; kmt(task[0],task[0],x[0],x[0]).shape\ntorch.Size([6, 3, 5])\n&gt;&gt;&gt; kmt(task,task,x[0],x[0]).shape\ntorch.Size([6, 3, 5, 4])\n&gt;&gt;&gt; kmt(task[0],task[0],x,x).shape\ntorch.Size([6, 3, 5, 8])\n&gt;&gt;&gt; v = kmt(task,task,x[:4,None,:],x[None,:4,:])\n&gt;&gt;&gt; v.shape\ntorch.Size([6, 3, 5, 4, 4])\n&gt;&gt;&gt; kmat_x = kmt.base_kernel(x[:4,None,:],x[None,:4,:])\n&gt;&gt;&gt; kmat_x.shape\ntorch.Size([3, 5, 4, 4])\n&gt;&gt;&gt; kmat_tasks = kmt.taskmat[...,task,task]\n&gt;&gt;&gt; kmat_tasks.shape\ntorch.Size([6, 3, 5, 4])\n&gt;&gt;&gt; torch.allclose(v,kmat_tasks[...,None,:]*kmat_x)\nTrue\n&gt;&gt;&gt; torch.allclose(v,kmat_tasks[...,:,None]*kmat_x)\nFalse\n&gt;&gt;&gt; kmt(task[:,None,None,None],task[None,:,None,None],x[:,None,:],x[None,:,:]).shape\ntorch.Size([6, 3, 5, 4, 4, 8, 8])\n&gt;&gt;&gt; kmt(task[:,None],task[None,:],x[:,None,None,None,:],x[None,:,None,None,:]).shape\ntorch.Size([6, 3, 5, 8, 8, 4, 4])\n&gt;&gt;&gt; kmt.factor.dtype\ntorch.float32\n&gt;&gt;&gt; kmt.diag.dtype\ntorch.float64\n</code></pre> <p>Integrals </p> <pre><code>&gt;&gt;&gt; kernel = KernelGaussian(\n...     d = 2,\n...     torchify = True,\n...     scale = rng.uniform(low=0,high=1,size=(3,1)),\n...     lengthscales = rng.uniform(low=0,high=1,size=(1,2)))\n&gt;&gt;&gt; kmt = KernelMultiTask(\n...     base_kernel=kernel,\n...     num_tasks=5,\n...     diag = rng.uniform(low=0,high=1,size=(6,3,5)),\n...     factor = rng.uniform(low=0,high=1,size=(3,5,2)))\n&gt;&gt;&gt; task = torch.arange(5) \n&gt;&gt;&gt; kmt.double_integral_01d(task0=task[0],task1=task[1]).shape\ntorch.Size([6, 3])\n&gt;&gt;&gt; kmt.double_integral_01d(task0=task,task1=task).shape\ntorch.Size([6, 3, 5])\n&gt;&gt;&gt; kmt.double_integral_01d(task0=task[:,None],task1=task[None,:]).shape\ntorch.Size([6, 3, 5, 5])\n&gt;&gt;&gt; x = torch.from_numpy(rng.uniform(low=0,high=1,size=(7,4,2)))\n&gt;&gt;&gt; kmt.single_integral_01d(task0=task[0],task1=task[1],x=x).shape\ntorch.Size([6, 3, 7, 4])\n&gt;&gt;&gt; kmt.single_integral_01d(task0=task[:,None,None],task1=task[:,None,None],x=x).shape\ntorch.Size([6, 3, 5, 7, 4])\n&gt;&gt;&gt; kmt.single_integral_01d(task0=task[:,None,None,None],task1=task[None,:,None,None],x=x).shape\ntorch.Size([6, 3, 5, 5, 7, 4])\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>base_kernel</code> <code>AbstractKernel</code> <p>\\(K_{\\mathrm{base}}\\). </p> required <code>num_tasks</code> <code>int</code> <p>Number of tasks \\(T&gt;1\\). </p> required <code>factor</code> <code>Union[ndarray, Tensor]</code> <p>Factor \\(\\mathsf{F}\\). </p> <code>1.0</code> <code>diag</code> <code>Union[ndarray, Tensor]</code> <p>Diagonal parameter \\(\\boldsymbol{v}\\). </p> <code>1.0</code> <code>shape_factor</code> <code>list</code> <p>Shape of <code>factor</code> when <code>np.isscalar(factor)</code>. </p> <code>None</code> <code>shape_diag</code> <code>list</code> <p>Shape of <code>diag</code> when <code>np.isscalar(diag)</code>. </p> <code>None</code> <code>tfs_factor</code> <code>Tuple[callable, callable]</code> <p>The first argument transforms to the raw value to be optimized; the second applies the inverse transform.</p> <code>(tf_identity, tf_identity)</code> <code>tfs_diag</code> <code>Tuple[callable, callable]</code> <p>The first argument transforms to the raw value to be optimized; the second applies the inverse transform.</p> <code>(tf_exp_eps_inv, tf_exp_eps)</code> <code>requires_grad_factor</code> <code>bool</code> <p>If <code>True</code> and <code>torchify</code>, set <code>requires_grad=True</code> for <code>factor</code>.</p> <code>True</code> <code>requires_grad_diag</code> <code>bool</code> <p>If <code>True</code> and <code>torchify</code>, set <code>requires_grad=True</code> for <code>diag</code>.</p> <code>True</code> Source code in <code>qmcpy/kernel/multitask_kernel.py</code> <pre><code>def __init__(self,\n        base_kernel,\n        num_tasks, \n        factor = 1.,\n        diag =  1.,\n        shape_factor = None, \n        shape_diag = None,\n        tfs_factor = (tf_identity,tf_identity),\n        tfs_diag = (tf_exp_eps_inv,tf_exp_eps),\n        requires_grad_factor = True, \n        requires_grad_diag = True,\n        rank_factor = 1\n        ):\n    r\"\"\"\n    Args:\n        base_kernel (AbstractKernel): $K_{\\mathrm{base}}$. \n        num_tasks (int): Number of tasks $T&gt;1$. \n        factor (Union[np.ndarray,torch.Tensor]): Factor $\\mathsf{F}$. \n        diag (Union[np.ndarray,torch.Tensor]): Diagonal parameter $\\boldsymbol{v}$. \n        shape_factor (list): Shape of `factor` when `np.isscalar(factor)`. \n        shape_diag (list): Shape of `diag` when `np.isscalar(diag)`. \n        tfs_factor (Tuple[callable,callable]): The first argument transforms to the raw value to be optimized; the second applies the inverse transform.\n        tfs_diag (Tuple[callable,callable]): The first argument transforms to the raw value to be optimized; the second applies the inverse transform.\n        requires_grad_factor (bool): If `True` and `torchify`, set `requires_grad=True` for `factor`.\n        requires_grad_diag (bool): If `True` and `torchify`, set `requires_grad=True` for `diag`.\n    \"\"\"\n    assert isinstance(base_kernel,AbstractKernel)\n    super().__init__(\n        d = base_kernel.d,\n        torchify = base_kernel.torchify,\n        device = base_kernel.device,\n        compile_call = False,\n        comiple_call_kwargs = {})\n    self.base_kernel = base_kernel\n    self.AUTOGRADKERNEL = base_kernel.AUTOGRADKERNEL \n    assert np.isscalar(num_tasks) and num_tasks%1==0\n    self.num_tasks = num_tasks\n    assert np.isscalar(rank_factor) and rank_factor%1==0 and 0&lt;=rank_factor&lt;=self.num_tasks\n    self.raw_factor,self.tf_factor = self.parse_assign_param(\n        pname = \"factor\",\n        param = factor,\n        shape_param = [self.num_tasks,rank_factor] if shape_factor is None else shape_factor,\n        requires_grad_param = requires_grad_factor,\n        tfs_param = tfs_factor,\n        endsize_ops = list(range(self.num_tasks+1)),\n        constraints = [])\n    assert self.raw_factor.shape[-2]==self.num_tasks\n    self.raw_diag,self.tf_diag = self.parse_assign_param(\n        pname = \"diag\",\n        param = diag, \n        shape_param = [self.num_tasks] if shape_diag is None else shape_diag,\n        requires_grad_param = requires_grad_diag,\n        tfs_param = tfs_diag,\n        endsize_ops = [1,self.num_tasks],\n        constraints = [\"NON-NEGATIVE\"])\n    self.eye_num_tasks = self.npt.eye(self.num_tasks,**self.nptkwargs)\n    self.batch_param_names.append(\"taskmat\")\n    self._nbdim_base = None\n</code></pre>"},{"location":"api/kernels/#qmcpy.kernel.multitask_kernel.KernelMultiTask.__call__","title":"__call__","text":"<pre><code>__call__(task0, task1, x0, x1, beta0=None, beta1=None, c=None)\n</code></pre> <p>Evaluate the kernel with (optional) partial derivatives </p> \\[\\sum_{\\ell=1}^p c_\\ell \\partial_{\\boldsymbol{x}_0}^{\\boldsymbol{\\beta}_{\\ell,0}} \\partial_{\\boldsymbol{x}_1}^{\\boldsymbol{\\beta}_{\\ell,1}} K((i_0,\\boldsymbol{x}_0),(i_1,\\boldsymbol{x}_1)).\\] <p>Parameters:</p> Name Type Description Default <code>task0</code> <code>Union[int, ndarray, Tensor]</code> <p>First task indices \\(i_0\\). </p> required <code>task1</code> <code>Union[int, ndarray, Tensor]</code> <p>Second task indices \\(i_1\\).</p> required <code>x0</code> <code>Union[ndarray, Tensor]</code> <p>Shape <code>x0.shape=(...,d)</code> first input to kernel.</p> required <code>x1</code> <code>Union[ndarray, Tensor]</code> <p>Shape <code>x1.shape=(...,d)</code> second input to kernel. </p> required <code>beta0</code> <code>Union[ndarray, Tensor]</code> <p>Shape <code>beta0.shape=(p,d)</code> derivative orders with respect to first inputs, \\(\\boldsymbol{\\beta}_0\\).</p> <code>None</code> <code>beta1</code> <code>Union[ndarray, Tensor]</code> <p>Shape <code>beta1.shape=(p,d)</code> derivative orders with respect to first inputs, \\(\\boldsymbol{\\beta}_1\\).</p> <code>None</code> <code>c</code> <code>Union[ndarray, Tensor]</code> <p>Shape <code>c.shape=(p,)</code> coefficients of derivatives.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>k</code> <code>Union[ndarray, Tensor]</code> <p>Kernel evaluations with batched shape, see the doctests for examples.</p> Source code in <code>qmcpy/kernel/multitask_kernel.py</code> <pre><code>def __call__(self, task0, task1, x0, x1, beta0=None, beta1=None, c=None):\n    r\"\"\"\n    Evaluate the kernel with (optional) partial derivatives \n\n    $$\\sum_{\\ell=1}^p c_\\ell \\partial_{\\boldsymbol{x}_0}^{\\boldsymbol{\\beta}_{\\ell,0}} \\partial_{\\boldsymbol{x}_1}^{\\boldsymbol{\\beta}_{\\ell,1}} K((i_0,\\boldsymbol{x}_0),(i_1,\\boldsymbol{x}_1)).$$\n\n    Args:\n        task0 (Union[int,np.ndarray,torch.Tensor]): First task indices $i_0$. \n        task1 (Union[int,np.ndarray,torch.Tensor]): Second task indices $i_1$.\n        x0 (Union[np.ndarray,torch.Tensor]): Shape `x0.shape=(...,d)` first input to kernel.\n        x1 (Union[np.ndarray,torch.Tensor]): Shape `x1.shape=(...,d)` second input to kernel. \n        beta0 (Union[np.ndarray,torch.Tensor]): Shape `beta0.shape=(p,d)` derivative orders with respect to first inputs, $\\boldsymbol{\\beta}_0$.\n        beta1 (Union[np.ndarray,torch.Tensor]): Shape `beta1.shape=(p,d)` derivative orders with respect to first inputs, $\\boldsymbol{\\beta}_1$.\n        c (Union[np.ndarray,torch.Tensor]): Shape `c.shape=(p,)` coefficients of derivatives.\n\n    Returns:\n        k (Union[np.ndarray,torch.Tensor]): Kernel evaluations with batched shape, see the doctests for examples. \n    \"\"\"\n    kmat_x = self.base_kernel.__call__(x0,x1,beta0,beta1,c)\n    return self._parsed__call__(task0,task1,kmat_x)\n</code></pre>"},{"location":"api/kernels/#qmcpy.kernel.multitask_kernel.KernelMultiTask.single_integral_01d","title":"single_integral_01d","text":"<pre><code>single_integral_01d(task0, task1, x)\n</code></pre> <p>Evaluate the integral of the kernel over the unit cube</p> \\[\\tilde{K}((i_0,\\boldsymbol{x}),i_1) = \\int_{[0,1]^d} K((i_0,\\boldsymbol{x}),(i_1,\\boldsymbol{z}) \\; \\mathrm{d} \\boldsymbol{z}.\\] <p>Parameters:</p> Name Type Description Default <code>task0</code> <code>Union[int, ndarray, Tensor]</code> <p>First task indices \\(i_0\\). </p> required <code>task1</code> <code>Union[int, ndarray, Tensor]</code> <p>Second task indices \\(i_1\\).</p> required <code>x</code> <code>Union[ndarray, Tensor]</code> <p>Shape <code>x0.shape=(...,d)</code> first input to kernel with </p> required <p>Returns:</p> Name Type Description <code>tildek</code> <code>Union[ndarray, Tensor]</code> <p>Shape <code>y.shape=x.shape[:-1]</code> integral kernel evaluations.</p> Source code in <code>qmcpy/kernel/multitask_kernel.py</code> <pre><code>def single_integral_01d(self, task0, task1, x):\n    r\"\"\"\n    Evaluate the integral of the kernel over the unit cube\n\n    $$\\tilde{K}((i_0,\\boldsymbol{x}),i_1) = \\int_{[0,1]^d} K((i_0,\\boldsymbol{x}),(i_1,\\boldsymbol{z}) \\; \\mathrm{d} \\boldsymbol{z}.$$\n\n    Args:\n        task0 (Union[int,np.ndarray,torch.Tensor]): First task indices $i_0$. \n        task1 (Union[int,np.ndarray,torch.Tensor]): Second task indices $i_1$.\n        x (Union[np.ndarray,torch.Tensor]): Shape `x0.shape=(...,d)` first input to kernel with \n\n    Returns:\n        tildek (Union[np.ndarray,torch.Tensor]): Shape `y.shape=x.shape[:-1]` integral kernel evaluations. \n    \"\"\"\n    kint_x = self.base_kernel.single_integral_01d(x)\n    return self._parsed__call__(task0,task1,kint_x)\n</code></pre>"},{"location":"api/kernels/#qmcpy.kernel.multitask_kernel.KernelMultiTask.double_integral_01d","title":"double_integral_01d","text":"<pre><code>double_integral_01d(task0, task1)\n</code></pre> <p>Evaluate the integral of the kernel over the unit cube</p> \\[\\tilde{K}(i_0,i_1) = \\int_{[0,1]^d} \\int_{[0,1]^d} K((i_0,\\boldsymbol{x}),(i_1,\\boldsymbol{z})) \\; \\mathrm{d} \\boldsymbol{x} \\; \\mathrm{d} \\boldsymbol{z}.\\] <p>Parameters:</p> Name Type Description Default <code>task0</code> <code>Union[int, ndarray, Tensor]</code> <p>First task indices \\(i_0\\). </p> required <code>task1</code> <code>Union[int, ndarray, Tensor]</code> <p>Second task indices \\(i_1\\).</p> required <p>Returns:</p> Name Type Description <code>tildek</code> <code>Union[ndarray, Tensor]</code> <p>Double integral kernel evaluations.</p> Source code in <code>qmcpy/kernel/multitask_kernel.py</code> <pre><code>def double_integral_01d(self, task0, task1):\n    r\"\"\"\n    Evaluate the integral of the kernel over the unit cube\n\n    $$\\tilde{K}(i_0,i_1) = \\int_{[0,1]^d} \\int_{[0,1]^d} K((i_0,\\boldsymbol{x}),(i_1,\\boldsymbol{z})) \\; \\mathrm{d} \\boldsymbol{x} \\; \\mathrm{d} \\boldsymbol{z}.$$\n\n    Args:\n        task0 (Union[int,np.ndarray,torch.Tensor]): First task indices $i_0$. \n        task1 (Union[int,np.ndarray,torch.Tensor]): Second task indices $i_1$.\n\n    Returns:\n        tildek (Union[np.ndarray,torch.Tensor]): Double integral kernel evaluations.\n    \"\"\"\n    kint_x = self.base_kernel.double_integral_01d()\n    return self._parsed__call__(task0,task1,kint_x)\n</code></pre>"},{"location":"api/kernels/#uml-specific","title":"UML Specific","text":""},{"location":"api/stopping_criteria/","title":"Stopping Criteria","text":""},{"location":"api/stopping_criteria/#uml-overview","title":"UML Overview","text":""},{"location":"api/stopping_criteria/#abstractstoppingcriterion","title":"<code>AbstractStoppingCriterion</code>","text":"<p>               Bases: <code>object</code></p> <p>Parameters:</p> Name Type Description Default <code>allowed_distribs</code> <code>list</code> <p>Allowed discrete distribution classes.</p> required <code>allow_vectorized_integrals</code> <code>bool</code> <p>Whether or not to allow integrands with vectorized outputs,  i.e., those with <code>integrand.d_indv!=()</code>.</p> required Source code in <code>qmcpy/stopping_criterion/abstract_stopping_criterion.py</code> <pre><code>def __init__(self, allowed_distribs, allow_vectorized_integrals):\n    \"\"\"\n    Args:\n        allowed_distribs (list): Allowed discrete distribution classes.\n        allow_vectorized_integrals (bool): Whether or not to allow integrands with vectorized outputs, \n            i.e., those with `integrand.d_indv!=()`. \n    \"\"\"\n    sname = type(self).__name__\n    prefix = 'A concrete implementation of StoppingCriterion must have '\n    # integrand check\n    if (not hasattr(self, 'integrand')) or (not isinstance(self.integrand,AbstractIntegrand)):\n        raise ParameterError(prefix + 'self.integrand, an Integrand instance')\n    # true measure check\n    if (not hasattr(self, 'true_measure')) or (self.true_measure!=self.integrand.true_measure):\n        raise ParameterError(prefix + 'self.true_measure=self.integrand.true_measure')\n    # discrete distribution check\n    if (not hasattr(self, 'discrete_distrib')) or (self.discrete_distrib!=self.integrand.discrete_distrib):\n        raise ParameterError(prefix + 'self.discrete_distrib=self.integrand.discrete_distrib')\n    if not isinstance(self.discrete_distrib,tuple(allowed_distribs)):\n        raise DistributionCompatibilityError('%s must have a DiscreteDistribution in %s'%(sname,str(allowed_distribs)))\n    if (not allow_vectorized_integrals) and len(self.integrand.d_indv)&gt;0:\n        raise ParameterError('Vectorized integrals (with d_indv!=() outputs per sample) are not supported by this stopping criterion')\n    # parameter checks\n    if not hasattr(self,'parameters'):\n        self.parameters = []\n</code></pre>"},{"location":"api/stopping_criteria/#qmcpy.stopping_criterion.abstract_stopping_criterion.AbstractStoppingCriterion.integrate","title":"integrate","text":"<pre><code>integrate()\n</code></pre> <p>Abstract method to determine the number of samples needed to satisfy the tolerance(s).</p> <p>Returns:</p> Name Type Description <code>solution</code> <code>Union[float, ndarray]</code> <p>Approximation to the integral with shape <code>integrand.d_comb</code>.</p> <code>data</code> <code>Data</code> <p>A data object.</p> Source code in <code>qmcpy/stopping_criterion/abstract_stopping_criterion.py</code> <pre><code>def integrate(self):\n    \"\"\"\n    *Abstract method* to determine the number of samples needed to satisfy the tolerance(s).\n\n    Returns:\n        solution (Union[float,np.ndarray]): Approximation to the integral with shape `integrand.d_comb`.\n        data (Data): A data object.\n    \"\"\"\n    raise MethodImplementationError(self, 'integrate')\n</code></pre>"},{"location":"api/stopping_criteria/#qmcpy.stopping_criterion.abstract_stopping_criterion.AbstractStoppingCriterion.set_tolerance","title":"set_tolerance","text":"<pre><code>set_tolerance(abs_tol=None, rel_tol=None, rmse_tol=None)\n</code></pre> <p>Reset the tolerances.</p> <p>Parameters:</p> Name Type Description Default <code>abs_tol</code> <code>float</code> <p>Absolute tolerance (if applicable). Reset if supplied, ignored otherwise. </p> <code>None</code> <code>rel_tol</code> <code>float</code> <p>Relative tolerance (if applicable). Reset if supplied, ignored otherwise. </p> <code>None</code> <code>rmse_tol</code> <code>float</code> <p>RMSE tolerance (if applicable). Reset if supplied, ignored if not.  If <code>rmse_tol</code> is not supplied but <code>abs_tol</code> is, then <code>rmse_tol = abs_tol / norm.ppf(1-alpha/2)</code>.</p> <code>None</code> Source code in <code>qmcpy/stopping_criterion/abstract_stopping_criterion.py</code> <pre><code>def set_tolerance(self, abs_tol=None, rel_tol=None, rmse_tol=None):\n    \"\"\"\n    Reset the tolerances.\n\n    Args:\n        abs_tol (float): Absolute tolerance (if applicable). Reset if supplied, ignored otherwise. \n        rel_tol (float): Relative tolerance (if applicable). Reset if supplied, ignored otherwise. \n        rmse_tol (float): RMSE tolerance (if applicable). Reset if supplied, ignored if not. \n            If `rmse_tol` is not supplied but `abs_tol` is, then `rmse_tol = abs_tol / norm.ppf(1-alpha/2)`. \n    \"\"\"\n    raise MethodImplementationError(self, 'integrate')\n</code></pre>"},{"location":"api/stopping_criteria/#qmc","title":"QMC","text":""},{"location":"api/stopping_criteria/#cubqmcnetg","title":"<code>CubQMCNetG</code>","text":"<p>               Bases: <code>AbstractCubQMCLDG</code></p> <p>Quasi-Monte Carlo stopping criterion using digital net cubature  with guarantees for cones of functions with a predictable decay in the Walsh coefficients.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; k = Keister(DigitalNetB2(seed=7))\n&gt;&gt;&gt; sc = CubQMCNetG(k,abs_tol=1e-3,rel_tol=0,check_cone=True)\n&gt;&gt;&gt; solution,data = sc.integrate()\n&gt;&gt;&gt; solution\narray(1.38046669)\n&gt;&gt;&gt; data\nData (Data)\n    solution        1.380\n    comb_bound_low  1.380\n    comb_bound_high 1.381\n    comb_bound_diff 6.72e-04\n    comb_flags      1\n    n_total         2^(11)\n    n               2^(11)\n    time_integrate  ...\nCubQMCNetG (AbstractStoppingCriterion)\n    abs_tol         0.001\n    rel_tol         0\n    n_init          2^(10)\n    n_limit         2^(35)\nKeister (AbstractIntegrand)\nGaussian (AbstractTrueMeasure)\n    mean            0\n    covariance      2^(-1)\n    decomp_type     PCA\nDigitalNetB2 (AbstractLDDiscreteDistribution)\n    d               1\n    replications    1\n    randomize       LMS DS\n    gen_mats_source joe_kuo.6.21201.txt\n    order           RADICAL INVERSE\n    t               63\n    alpha           1\n    n_limit         2^(32)\n    entropy         7\n</code></pre> <p>Vector outputs</p> <pre><code>&gt;&gt;&gt; f = BoxIntegral(DigitalNetB2(3,seed=7),s=[-1,1])\n&gt;&gt;&gt; abs_tol = 1e-3\n&gt;&gt;&gt; sc = CubQMCNetG(f,abs_tol=abs_tol,rel_tol=0,check_cone=True)\n&gt;&gt;&gt; solution,data = sc.integrate()\n&gt;&gt;&gt; solution\narray([1.19003352, 0.96068403])\n&gt;&gt;&gt; data\nData (Data)\n    solution        [1.19  0.961]\n    comb_bound_low  [1.189 0.96 ]\n    comb_bound_high [1.191 0.962]\n    comb_bound_diff [0.001 0.002]\n    comb_flags      [ True  True]\n    n_total         2^(14)\n    n               [16384  1024]\n    time_integrate  ...\nCubQMCNetG (AbstractStoppingCriterion)\n    abs_tol         0.001\n    rel_tol         0\n    n_init          2^(10)\n    n_limit         2^(35)\nBoxIntegral (AbstractIntegrand)\n    s               [-1  1]\nUniform (AbstractTrueMeasure)\n    lower_bound     0\n    upper_bound     1\nDigitalNetB2 (AbstractLDDiscreteDistribution)\n    d               3\n    replications    1\n    randomize       LMS DS\n    gen_mats_source joe_kuo.6.21201.txt\n    order           RADICAL INVERSE\n    t               63\n    alpha           1\n    n_limit         2^(32)\n    entropy         7\n&gt;&gt;&gt; sol3neg1 = -np.pi/4-1/2*np.log(2)+np.log(5+3*np.sqrt(3))\n&gt;&gt;&gt; sol31 = np.sqrt(3)/4+1/2*np.log(2+np.sqrt(3))-np.pi/24\n&gt;&gt;&gt; true_value = np.array([sol3neg1,sol31])\n&gt;&gt;&gt; assert (abs(true_value-solution)&lt;abs_tol).all()\n</code></pre> <p>Sensitivity indices </p> <pre><code>&gt;&gt;&gt; function = Genz(DigitalNetB2(3,seed=7))\n&gt;&gt;&gt; integrand = SensitivityIndices(function)\n&gt;&gt;&gt; sc = CubQMCNetG(integrand,abs_tol=5e-4,rel_tol=0,check_cone=True)\n&gt;&gt;&gt; solution,data = sc.integrate()\n&gt;&gt;&gt; data\nData (Data)\n    solution        [[0.02  0.196 0.667]\n                     [0.036 0.303 0.782]]\n    comb_bound_low  [[0.019 0.195 0.667]\n                     [0.035 0.303 0.781]]\n    comb_bound_high [[0.02  0.196 0.667]\n                     [0.036 0.303 0.782]]\n    comb_bound_diff [[0.001 0.    0.001]\n                     [0.001 0.001 0.001]]\n    comb_flags      [[ True  True  True]\n                     [ True  True  True]]\n    n_total         2^(16)\n    n               [[[16384 65536 65536]\n                      [16384 65536 65536]\n                      [16384 65536 65536]]\n\n                     [[ 2048 16384 32768]\n                      [ 2048 16384 32768]\n                      [ 2048 16384 32768]]]\n    time_integrate  ...\nCubQMCNetG (AbstractStoppingCriterion)\n    abs_tol         5.00e-04\n    rel_tol         0\n    n_init          2^(10)\n    n_limit         2^(35)\nSensitivityIndices (AbstractIntegrand)\n    indices         [[ True False False]\n                     [False  True False]\n                     [False False  True]]\nUniform (AbstractTrueMeasure)\n    lower_bound     0\n    upper_bound     1\nDigitalNetB2 (AbstractLDDiscreteDistribution)\n    d               3\n    replications    1\n    randomize       LMS DS\n    gen_mats_source joe_kuo.6.21201.txt\n    order           RADICAL INVERSE\n    t               63\n    alpha           1\n    n_limit         2^(32)\n    entropy         7\n</code></pre> <p>Control Variates</p> <pre><code>&gt;&gt;&gt; dnb2 = DigitalNetB2(dimension=4,seed=7)\n&gt;&gt;&gt; integrand = CustomFun(\n...     true_measure = Uniform(dnb2),\n...     g = lambda t: np.stack([\n...         1*t[...,0]+2*t[...,0]**2+3*t[...,0]**3,\n...         2*t[...,1]+3*t[...,1]**2+4*t[...,1]**3,\n...         3*t[...,2]+4*t[...,2]**2+5*t[...,2]**3]),\n...     dimension_indv = (3,))\n&gt;&gt;&gt; control_variates = [\n...     CustomFun(\n...         true_measure = Uniform(dnb2),\n...         g = lambda t: np.stack([t[...,0],t[...,1],t[...,2]],axis=0),\n...         dimension_indv = (3,)),\n...     CustomFun(\n...         true_measure = Uniform(dnb2),\n...         g = lambda t: np.stack([t[...,0]**2,t[...,1]**2,t[...,2]**2],axis=0),\n...         dimension_indv = (3,))]\n&gt;&gt;&gt; control_variate_means = np.array([[1/2,1/2,1/2],[1/3,1/3,1/3]])\n&gt;&gt;&gt; true_value = np.array([23/12,3,49/12])\n&gt;&gt;&gt; abs_tol = 1e-6\n&gt;&gt;&gt; sc = CubQMCNetG(integrand,abs_tol=abs_tol,rel_tol=0,control_variates=control_variates,control_variate_means=control_variate_means,update_cv_coeffs=False)\n&gt;&gt;&gt; solution,data = sc.integrate()\n&gt;&gt;&gt; solution\narray([1.91666667, 3.        , 4.08333333])\n&gt;&gt;&gt; data.n\narray([ 8192,  8192, 16384])\n&gt;&gt;&gt; assert (np.abs(true_value-solution)&lt;abs_tol).all()\n&gt;&gt;&gt; sc = CubQMCNetG(integrand,abs_tol=abs_tol,rel_tol=0,control_variates=control_variates,control_variate_means=control_variate_means,update_cv_coeffs=True)\n&gt;&gt;&gt; solution,data = sc.integrate()\n&gt;&gt;&gt; solution\narray([1.91666667, 3.        , 4.08333333])\n&gt;&gt;&gt; data.n\narray([16384, 16384, 16384])\n&gt;&gt;&gt; assert (np.abs(true_value-solution)&lt;abs_tol).all()\n</code></pre> <p>References:</p> <ol> <li> <p>Hickernell, Fred J., and Llu\u00eds Antoni Jim\u00e9nez Rugama.     \"Reliable adaptive cubature using digital sequences.\"     Monte Carlo and Quasi-Monte Carlo Methods: MCQMC, Leuven, Belgium, April 2014.     Springer International Publishing, 2016.</p> </li> <li> <p>Sou-Cheng T. Choi, Yuhan Ding, Fred J. Hickernell, Lan Jiang, Lluis Antoni Jimenez Rugama,     Da Li, Jagadeeswaran Rathinavel, Xin Tong, Kan Zhang, Yizhi Zhang, and Xuan Zhou,     GAIL: Guaranteed Automatic Integration Library (Version 2.3) [MATLAB Software], 2019. http://gailgithub.github.io/GAIL_Dev/. https://github.com/GailGithub/GAIL_Dev/blob/master/Algorithms/IntegrationExpectation/cubSobol_g.m.</p> </li> </ol> <p>Parameters:</p> Name Type Description Default <code>integrand</code> <code>AbstractIntegrand</code> <p>The integrand.</p> required <code>abs_tol</code> <code>ndarray</code> <p>Absolute error tolerance.</p> <code>0.01</code> <code>rel_tol</code> <code>ndarray</code> <p>Relative error tolerance.</p> <code>0.0</code> <code>n_init</code> <code>int</code> <p>Initial number of samples. </p> <code>2 ** 10</code> <code>n_limit</code> <code>int</code> <p>Maximum number of samples.</p> <code>2 ** 35</code> <code>error_fun</code> <code>Union[str, callable]</code> <p>Function mapping the approximate solution, absolute error tolerance, and relative error tolerance to the current error bound.</p> <ul> <li><code>'EITHER'</code>, the default, requires the approximation error must be below either the absolue or relative tolerance.     Equivalent to setting     <pre><code>error_fun = lambda sv,abs_tol,rel_tol: np.maximum(abs_tol,abs(sv)*rel_tol)\n</code></pre></li> <li><code>'BOTH'</code> requires the approximation error to be below both the absolue and relative tolerance.      Equivalent to setting     <pre><code>error_fun = lambda sv,abs_tol,rel_tol: np.minimum(abs_tol,abs(sv)*rel_tol)\n</code></pre></li> </ul> <code>'EITHER'</code> <code>fudge</code> <code>function</code> <p>Positive function multiplying the finite sum of the Fourier coefficients specified in the cone of functions. </p> <code>lambda m: 5.0 * 2.0 ** -m</code> <code>check_cone</code> <code>bool</code> <p>Whether or not to check if the function falls in the cone.</p> <code>False</code> <code>control_variates</code> <code>list</code> <p>Integrands to use as control variates, each with the same underlying discrete distribution instance.</p> <code>[]</code> <code>control_variate_means</code> <code>ndarray</code> <p>Means of each control variate. </p> <code>[]</code> <code>update_cv_coeffs</code> <code>bool</code> <p>If set to true, the control variate coefficients are recomputed at each iteration.  Otherwise they are estimated once after the initial sampling and then fixed.</p> <code>False</code> Source code in <code>qmcpy/stopping_criterion/cub_qmc_net_g.py</code> <pre><code>def __init__(self, \n             integrand, \n             abs_tol = 1e-2,\n             rel_tol = 0., \n             n_init = 2**10, \n             n_limit = 2**35,\n             error_fun = \"EITHER\",\n             fudge = lambda m: 5.*2.**(-m), \n             check_cone = False, \n             control_variates = [], \n             control_variate_means = [], \n             update_cv_coeffs = False,\n             ):\n    r\"\"\"\n    Args:\n        integrand (AbstractIntegrand): The integrand.\n        abs_tol (np.ndarray): Absolute error tolerance.\n        rel_tol (np.ndarray): Relative error tolerance.\n        n_init (int): Initial number of samples. \n        n_limit (int): Maximum number of samples.\n        error_fun (Union[str,callable]): Function mapping the approximate solution, absolute error tolerance, and relative error tolerance to the current error bound.\n\n            - `'EITHER'`, the default, requires the approximation error must be below either the absolue *or* relative tolerance.\n                Equivalent to setting\n                ```python\n                error_fun = lambda sv,abs_tol,rel_tol: np.maximum(abs_tol,abs(sv)*rel_tol)\n                ```\n            - `'BOTH'` requires the approximation error to be below both the absolue *and* relative tolerance. \n                Equivalent to setting\n                ```python\n                error_fun = lambda sv,abs_tol,rel_tol: np.minimum(abs_tol,abs(sv)*rel_tol)\n                ```\n        fudge (function): Positive function multiplying the finite sum of the Fourier coefficients specified in the cone of functions. \n        check_cone (bool): Whether or not to check if the function falls in the cone.\n        control_variates (list): Integrands to use as control variates, each with the same underlying discrete distribution instance.\n        control_variate_means (np.ndarray): Means of each control variate. \n        update_cv_coeffs (bool): If set to true, the control variate coefficients are recomputed at each iteration. \n            Otherwise they are estimated once after the initial sampling and then fixed.\n    \"\"\"\n    super(CubQMCNetG,self).__init__(integrand,abs_tol,rel_tol,n_init,n_limit,fudge,\n        check_cone,control_variates,control_variate_means,update_cv_coeffs,\n        ptransform = 'none',\n        ft = fwht,\n        omega = omega_fwht,\n        allowed_distribs = [DigitalNetB2],\n        cast_complex = False,\n        error_fun = error_fun)\n    if self.discrete_distrib.order!='RADICAL INVERSE':\n        raise ParameterError(\"CubQMCNet_g requires DigitalNetB2 with 'RADICAL INVERSE' order\")\n</code></pre>"},{"location":"api/stopping_criteria/#cubqmclatticeg","title":"<code>CubQMCLatticeG</code>","text":"<p>               Bases: <code>AbstractCubQMCLDG</code></p> <p>Quasi-Monte Carlo stopping criterion using rank-1 lattice cubature  with guarantees for cones of functions with a predictable decay in the Fourier coefficients.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; k = Keister(Lattice(seed=7))\n&gt;&gt;&gt; sc = CubQMCLatticeG(k,abs_tol=1e-3,rel_tol=0,check_cone=True)\n&gt;&gt;&gt; solution,data = sc.integrate()\n&gt;&gt;&gt; solution\narray(1.38037385)\n&gt;&gt;&gt; data\nData (Data)\n    solution        1.380\n    comb_bound_low  1.380\n    comb_bound_high 1.381\n    comb_bound_diff 0.001\n    comb_flags      1\n    n_total         2^(11)\n    n               2^(11)\n    time_integrate  ...\nCubQMCLatticeG (AbstractStoppingCriterion)\n    abs_tol         0.001\n    rel_tol         0\n    n_init          2^(10)\n    n_limit         2^(30)\nKeister (AbstractIntegrand)\nGaussian (AbstractTrueMeasure)\n    mean            0\n    covariance      2^(-1)\n    decomp_type     PCA\nLattice (AbstractLDDiscreteDistribution)\n    d               1\n    replications    1\n    randomize       SHIFT\n    gen_vec_source  kuo.lattice-33002-1024-1048576.9125.txt\n    order           RADICAL INVERSE\n    n_limit         2^(20)\n    entropy         7\n</code></pre> <p>Vector outputs</p> <pre><code>&gt;&gt;&gt; f = BoxIntegral(Lattice(3,seed=11),s=[-1,1])\n&gt;&gt;&gt; abs_tol = 1e-3\n&gt;&gt;&gt; sc = CubQMCLatticeG(f,abs_tol=abs_tol,rel_tol=0,check_cone=True)\n&gt;&gt;&gt; solution,data = sc.integrate()\n&gt;&gt;&gt; solution\narray([1.18947477, 0.96060862])\n&gt;&gt;&gt; data\nData (Data)\n    solution        [1.189 0.961]\n    comb_bound_low  [1.189 0.96 ]\n    comb_bound_high [1.19  0.961]\n    comb_bound_diff [0.001 0.001]\n    comb_flags      [ True  True]\n    n_total         2^(13)\n    n               [8192 1024]\n    time_integrate  ...\nCubQMCLatticeG (AbstractStoppingCriterion)\n    abs_tol         0.001\n    rel_tol         0\n    n_init          2^(10)\n    n_limit         2^(30)\nBoxIntegral (AbstractIntegrand)\n    s               [-1  1]\nUniform (AbstractTrueMeasure)\n    lower_bound     0\n    upper_bound     1\nLattice (AbstractLDDiscreteDistribution)\n    d               3\n    replications    1\n    randomize       SHIFT\n    gen_vec_source  kuo.lattice-33002-1024-1048576.9125.txt\n    order           RADICAL INVERSE\n    n_limit         2^(20)\n    entropy         11\n&gt;&gt;&gt; sol3neg1 = -np.pi/4-1/2*np.log(2)+np.log(5+3*np.sqrt(3))\n&gt;&gt;&gt; sol31 = np.sqrt(3)/4+1/2*np.log(2+np.sqrt(3))-np.pi/24\n&gt;&gt;&gt; true_value = np.array([sol3neg1,sol31])\n&gt;&gt;&gt; assert (abs(true_value-solution)&lt;abs_tol).all()\n</code></pre> <p>Sensitivity indices </p> <pre><code>&gt;&gt;&gt; function = Genz(Lattice(3,seed=7))\n&gt;&gt;&gt; integrand = SensitivityIndices(function)\n&gt;&gt;&gt; sc = CubQMCLatticeG(integrand,abs_tol=5e-4,rel_tol=0,check_cone=True)\n&gt;&gt;&gt; solution,data = sc.integrate()\n&gt;&gt;&gt; data\nData (Data)\n    solution        [[0.021 0.196 0.667]\n                     [0.036 0.303 0.782]]\n    comb_bound_low  [[0.02  0.196 0.667]\n                     [0.035 0.303 0.781]]\n    comb_bound_high [[0.021 0.196 0.667]\n                     [0.036 0.303 0.782]]\n    comb_bound_diff [[0.001 0.001 0.   ]\n                     [0.001 0.001 0.001]]\n    comb_flags      [[ True  True  True]\n                     [ True  True  True]]\n    n_total         2^(16)\n    n               [[[16384 32768 65536]\n                      [16384 32768 65536]\n                      [16384 32768 65536]]\n\n                     [[ 2048 16384 32768]\n                      [ 2048 16384 32768]\n                      [ 2048 16384 32768]]]\n    time_integrate  ...\nCubQMCLatticeG (AbstractStoppingCriterion)\n    abs_tol         5.00e-04\n    rel_tol         0\n    n_init          2^(10)\n    n_limit         2^(30)\nSensitivityIndices (AbstractIntegrand)\n    indices         [[ True False False]\n                     [False  True False]\n                     [False False  True]]\nUniform (AbstractTrueMeasure)\n    lower_bound     0\n    upper_bound     1\nLattice (AbstractLDDiscreteDistribution)\n    d               3\n    replications    1\n    randomize       SHIFT\n    gen_vec_source  kuo.lattice-33002-1024-1048576.9125.txt\n    order           RADICAL INVERSE\n    n_limit         2^(20)\n    entropy         7\n</code></pre> <p>References:</p> <ol> <li> <p>Lluis Antoni Jimenez Rugama and Fred J. Hickernell.     \"Adaptive multidimensional integration based on rank-1 lattices,\"      Monte Carlo and Quasi-Monte Carlo Methods: MCQMC, Leuven, Belgium,     April 2014 (R. Cools and D. Nuyens, eds.), Springer Proceedings in Mathematics.     and Statistics, vol. 163, Springer-Verlag, Berlin, 2016, arXiv:1411.1966, pp. 407-422.</p> </li> <li> <p>Sou-Cheng T. Choi, Yuhan Ding, Fred J. Hickernell, Lan Jiang, Lluis Antoni Jimenez Rugama,     Da Li, Jagadeeswaran Rathinavel, Xin Tong, Kan Zhang, Yizhi Zhang, and Xuan Zhou,     GAIL: Guaranteed Automatic Integration Library (Version 2.3) [MATLAB Software], 2019. http://gailgithub.github.io/GAIL_Dev/. https://github.com/GailGithub/GAIL_Dev/blob/master/Algorithms/IntegrationExpectation/cubLattice_g.m.</p> </li> </ol> <p>Parameters:</p> Name Type Description Default <code>integrand</code> <code>AbstractIntegrand</code> <p>The integrand.</p> required <code>abs_tol</code> <code>ndarray</code> <p>Absolute error tolerance.</p> <code>0.01</code> <code>rel_tol</code> <code>ndarray</code> <p>Relative error tolerance.</p> <code>0.0</code> <code>n_init</code> <code>int</code> <p>Initial number of samples. </p> <code>2 ** 10</code> <code>n_limit</code> <code>int</code> <p>Maximum number of samples.</p> <code>2 ** 30</code> <code>error_fun</code> <code>Union[str, callable]</code> <p>Function mapping the approximate solution, absolute error tolerance, and relative error tolerance to the current error bound.</p> <ul> <li><code>'EITHER'</code>, the default, requires the approximation error must be below either the absolue or relative tolerance.     Equivalent to setting     <pre><code>error_fun = lambda sv,abs_tol,rel_tol: np.maximum(abs_tol,abs(sv)*rel_tol)\n</code></pre></li> <li><code>'BOTH'</code> requires the approximation error to be below both the absolue and relative tolerance.      Equivalent to setting     <pre><code>error_fun = lambda sv,abs_tol,rel_tol: np.minimum(abs_tol,abs(sv)*rel_tol)\n</code></pre></li> </ul> <code>'EITHER'</code> <code>fudge</code> <code>function</code> <p>Positive function multiplying the finite sum of the Fourier coefficients specified in the cone of functions. </p> <code>lambda m: 5.0 * 2.0 ** -m</code> <code>check_cone</code> <code>bool</code> <p>Whether or not to check if the function falls in the cone.</p> <code>False</code> <code>ptransform</code> <code>str</code> <p>Periodization transform, see the options in <code>AbstractIntegrand.f</code>.</p> <code>'BAKER'</code> Source code in <code>qmcpy/stopping_criterion/cub_qmc_lattice_g.py</code> <pre><code>def __init__(self, \n             integrand, \n             abs_tol = 1e-2, \n             rel_tol = 0., \n             n_init = 2**10,\n             n_limit = 2**30,\n             error_fun = \"EITHER\",\n             fudge = lambda m: 5.*2.**(-m), \n             check_cone = False,\n             ptransform = 'BAKER',\n             ):\n    r\"\"\"\n    Args:\n        integrand (AbstractIntegrand): The integrand.\n        abs_tol (np.ndarray): Absolute error tolerance.\n        rel_tol (np.ndarray): Relative error tolerance.\n        n_init (int): Initial number of samples. \n        n_limit (int): Maximum number of samples.\n        error_fun (Union[str,callable]): Function mapping the approximate solution, absolute error tolerance, and relative error tolerance to the current error bound.\n\n            - `'EITHER'`, the default, requires the approximation error must be below either the absolue *or* relative tolerance.\n                Equivalent to setting\n                ```python\n                error_fun = lambda sv,abs_tol,rel_tol: np.maximum(abs_tol,abs(sv)*rel_tol)\n                ```\n            - `'BOTH'` requires the approximation error to be below both the absolue *and* relative tolerance. \n                Equivalent to setting\n                ```python\n                error_fun = lambda sv,abs_tol,rel_tol: np.minimum(abs_tol,abs(sv)*rel_tol)\n                ```\n        fudge (function): Positive function multiplying the finite sum of the Fourier coefficients specified in the cone of functions. \n        check_cone (bool): Whether or not to check if the function falls in the cone.\n        ptransform (str): Periodization transform, see the options in [`AbstractIntegrand.f`][qmcpy.AbstractIntegrand.f].\n    \"\"\"\n    super(CubQMCLatticeG,self).__init__(integrand,abs_tol,rel_tol,n_init,n_limit,fudge,check_cone,\n        control_variates = [],\n        control_variate_means = [],\n        update_beta = False,\n        ptransform = ptransform,\n        ft = fftbr,\n        omega = omega_fftbr,\n        allowed_distribs = [Lattice],\n        cast_complex = True,\n        error_fun = error_fun)\n    if self.discrete_distrib.order!='RADICAL INVERSE':\n        raise ParameterError(\"CubLattice_g requires Lattice with 'RADICAL INVERSE' order\")\n</code></pre>"},{"location":"api/stopping_criteria/#cubqmcbayesnetg","title":"<code>CubQMCBayesNetG</code>","text":"<p>               Bases: <code>AbstractCubBayesLDG</code></p> <p>Quasi-Monte Carlo stopping criterion using fast Bayesian cubature and digital nets with guarantees for Gaussian processes having certain digitally shift invariant kernels.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; k = Keister(DigitalNetB2(2, seed=123456789))\n&gt;&gt;&gt; sc = CubQMCBayesNetG(k,abs_tol=5e-3)\n&gt;&gt;&gt; solution,data = sc.integrate()\n&gt;&gt;&gt; data\nData (Data)\n    solution        1.809\n    comb_bound_low  1.807\n    comb_bound_high 1.810\n    comb_bound_diff 0.003\n    comb_flags      1\n    n_total         2^(10)\n    n               2^(10)\n    time_integrate  ...\nCubQMCBayesNetG (AbstractStoppingCriterion)\n    abs_tol         0.005\n    rel_tol         0\n    n_init          2^(8)\n    n_limit         2^(22)\n    order           1\nKeister (AbstractIntegrand)\nGaussian (AbstractTrueMeasure)\n    mean            0\n    covariance      2^(-1)\n    decomp_type     PCA\nDigitalNetB2 (AbstractLDDiscreteDistribution)\n    d               2^(1)\n    replications    1\n    randomize       LMS DS\n    gen_mats_source joe_kuo.6.21201.txt\n    order           RADICAL INVERSE\n    t               63\n    alpha           1\n    n_limit         2^(32)\n    entropy         123456789\n</code></pre> <p>Vector outputs</p> <pre><code>&gt;&gt;&gt; f = BoxIntegral(DigitalNetB2(3,seed=7),s=[-1,1])\n&gt;&gt;&gt; abs_tol = 1e-2\n&gt;&gt;&gt; sc = CubQMCBayesNetG(f,abs_tol=abs_tol,rel_tol=0)\n&gt;&gt;&gt; solution,data = sc.integrate()\n&gt;&gt;&gt; solution\narray([1.18750491, 0.96076395])\n&gt;&gt;&gt; data\nData (Data)\n    solution        [1.188 0.961]\n    comb_bound_low  [1.18 0.96]\n    comb_bound_high [1.195 0.962]\n    comb_bound_diff [0.014 0.002]\n    comb_flags      [ True  True]\n    n_total         2^(11)\n    n               [2048  256]\n    time_integrate  ...\nCubQMCBayesNetG (AbstractStoppingCriterion)\n    abs_tol         0.010\n    rel_tol         0\n    n_init          2^(8)\n    n_limit         2^(22)\n    order           1\nBoxIntegral (AbstractIntegrand)\n    s               [-1  1]\nUniform (AbstractTrueMeasure)\n    lower_bound     0\n    upper_bound     1\nDigitalNetB2 (AbstractLDDiscreteDistribution)\n    d               3\n    replications    1\n    randomize       LMS DS\n    gen_mats_source joe_kuo.6.21201.txt\n    order           RADICAL INVERSE\n    t               63\n    alpha           1\n    n_limit         2^(32)\n    entropy         7\n&gt;&gt;&gt; sol3neg1 = -np.pi/4-1/2*np.log(2)+np.log(5+3*np.sqrt(3))\n&gt;&gt;&gt; sol31 = np.sqrt(3)/4+1/2*np.log(2+np.sqrt(3))-np.pi/24\n&gt;&gt;&gt; true_value = np.array([sol3neg1,sol31])\n&gt;&gt;&gt; assert (abs(true_value-solution)&lt;abs_tol).all()\n</code></pre> <p>Sensitivity indices </p> <pre><code>&gt;&gt;&gt; function = Genz(DigitalNetB2(3,seed=7))\n&gt;&gt;&gt; integrand = SensitivityIndices(function)\n&gt;&gt;&gt; sc = CubQMCBayesNetG(integrand,abs_tol=5e-2,rel_tol=0)\n&gt;&gt;&gt; solution,data = sc.integrate()\n&gt;&gt;&gt; data\nData (Data)\n    solution        [[0.009 0.194 0.657]\n                     [0.036 0.312 0.783]]\n    comb_bound_low  [[0.007 0.178 0.636]\n                     [0.033 0.297 0.762]]\n    comb_bound_high [[0.012 0.209 0.679]\n                     [0.038 0.327 0.804]]\n    comb_bound_diff [[0.005 0.031 0.043]\n                     [0.004 0.03  0.042]]\n    comb_flags      [[ True  True  True]\n                     [ True  True  True]]\n    n_total         2^(8)\n    n               [[[256 256 256]\n                      [256 256 256]\n                      [256 256 256]]\n\n                     [[256 256 256]\n                      [256 256 256]\n                      [256 256 256]]]\n    time_integrate  ...\nCubQMCBayesNetG (AbstractStoppingCriterion)\n    abs_tol         0.050\n    rel_tol         0\n    n_init          2^(8)\n    n_limit         2^(22)\n    order           1\nSensitivityIndices (AbstractIntegrand)\n    indices         [[ True False False]\n                     [False  True False]\n                     [False False  True]]\nUniform (AbstractTrueMeasure)\n    lower_bound     0\n    upper_bound     1\nDigitalNetB2 (AbstractLDDiscreteDistribution)\n    d               3\n    replications    1\n    randomize       LMS DS\n    gen_mats_source joe_kuo.6.21201.txt\n    order           RADICAL INVERSE\n    t               63\n    alpha           1\n    n_limit         2^(32)\n    entropy         7\n</code></pre> <p>References:</p> <ol> <li> <p>Jagadeeswaran, Rathinavel, and Fred J. Hickernell.     \"Fast automatic Bayesian cubature using Sobol\u2019sampling.\"     Advances in Modeling and Simulation: Festschrift for Pierre L'Ecuyer.     Springer International Publishing, 2022. 301-318.</p> </li> <li> <p>Jagadeeswaran Rathinavel,     Fast automatic Bayesian cubature using matching kernels and designs,     PhD thesis, Illinois Institute of Technology, 2019.</p> </li> <li> <p>Sou-Cheng T. Choi, Yuhan Ding, Fred J. Hickernell, Lan Jiang, Lluis Antoni Jimenez Rugama,     Da Li, Jagadeeswaran Rathinavel, Xin Tong, Kan Zhang, Yizhi Zhang, and Xuan Zhou,     GAIL: Guaranteed Automatic Integration Library (Version 2.3) [MATLAB Software], 2019. http://gailgithub.github.io/GAIL_Dev/. https://github.com/GailGithub/GAIL_Dev/blob/master/Algorithms/IntegrationExpectation/cubBayesNet_g.m.</p> </li> </ol> <p>Parameters:</p> Name Type Description Default <code>integrand</code> <code>AbstractIntegrand</code> <p>The integrand.</p> required <code>abs_tol</code> <code>ndarray</code> <p>Absolute error tolerance.</p> <code>0.01</code> <code>rel_tol</code> <code>ndarray</code> <p>Relative error tolerance.</p> <code>0</code> <code>n_init</code> <code>int</code> <p>Initial number of samples. </p> <code>2 ** 8</code> <code>n_limit</code> <code>int</code> <p>Maximum number of samples.</p> <code>2 ** 22</code> <code>error_fun</code> <code>Union[str, callable]</code> <p>Function mapping the approximate solution, absolute error tolerance, and relative error tolerance to the current error bound.</p> <ul> <li><code>'EITHER'</code>, the default, requires the approximation error must be below either the absolue or relative tolerance.     Equivalent to setting     <pre><code>error_fun = lambda sv,abs_tol,rel_tol: np.maximum(abs_tol,abs(sv)*rel_tol)\n</code></pre></li> <li><code>'BOTH'</code> requires the approximation error to be below both the absolue and relative tolerance.      Equivalent to setting     <pre><code>error_fun = lambda sv,abs_tol,rel_tol: np.minimum(abs_tol,abs(sv)*rel_tol)\n</code></pre></li> </ul> <code>'EITHER'</code> <code>alpha</code> <code>ndarray</code> <p>Uncertainty level in \\((0,1)\\). </p> <code>0.01</code> <code>errbd_type</code> <code>str</code> <p>Options are </p> <ul> <li><code>'MLE'</code>: Marginal Log Likelihood. </li> <li><code>'GCV'</code>: Generalized Cross Validation. </li> <li><code>'FULL'</code>: Full Bayes.</li> </ul> <code>'MLE'</code> Source code in <code>qmcpy/stopping_criterion/cub_qmc_bayes_net_g.py</code> <pre><code>def __init__(self, \n             integrand, \n             abs_tol = 1e-2, \n             rel_tol = 0,\n             n_init = 2**8, \n             n_limit = 2**22, \n             error_fun = \"EITHER\", \n             alpha = 0.01,\n             errbd_type=\"MLE\",\n             ):\n    r\"\"\"\n    Args:\n        integrand (AbstractIntegrand): The integrand.\n        abs_tol (np.ndarray): Absolute error tolerance.\n        rel_tol (np.ndarray): Relative error tolerance.\n        n_init (int): Initial number of samples. \n        n_limit (int): Maximum number of samples.\n        error_fun (Union[str,callable]): Function mapping the approximate solution, absolute error tolerance, and relative error tolerance to the current error bound.\n\n            - `'EITHER'`, the default, requires the approximation error must be below either the absolue *or* relative tolerance.\n                Equivalent to setting\n                ```python\n                error_fun = lambda sv,abs_tol,rel_tol: np.maximum(abs_tol,abs(sv)*rel_tol)\n                ```\n            - `'BOTH'` requires the approximation error to be below both the absolue *and* relative tolerance. \n                Equivalent to setting\n                ```python\n                error_fun = lambda sv,abs_tol,rel_tol: np.minimum(abs_tol,abs(sv)*rel_tol)\n                ```\n        alpha (np.ndarray): Uncertainty level in $(0,1)$. \n        errbd_type (str): Options are \n\n            - `'MLE'`: Marginal Log Likelihood. \n            - `'GCV'`: Generalized Cross Validation. \n            - `'FULL'`: Full Bayes.\n    \"\"\"\n    super(CubQMCBayesNetG, self).__init__(integrand, ft=fwht, omega=omega_fwht,\n                                       ptransform=None,\n                                       allowed_distribs=[DigitalNetB2],\n                                       kernel=self._shift_inv_kernel_digital,\n                                       abs_tol=abs_tol, rel_tol=rel_tol,\n                                       n_init=n_init, n_limit=n_limit, alpha=alpha, error_fun=error_fun, errbd_type=errbd_type)\n    self.order = 1  # Currently supports only order=1\n    # private properties\n    # Full Bayes - assumes m and s^2 as hyperparameters\n    # GCV - Generalized cross validation\n    self.kernType = 1  # Type-1:\n    self._xfullundtype = np.uint64\n    if self.discrete_distrib.order!='RADICAL INVERSE':\n        raise ParameterError(\"CubQMCNet_g requires DigitalNetB2 with 'RADICAL INVERSE' order\")\n</code></pre>"},{"location":"api/stopping_criteria/#cubqmcbayeslatticeg","title":"<code>CubQMCBayesLatticeG</code>","text":"<p>               Bases: <code>AbstractCubBayesLDG</code></p> <p>Quasi-Monte Carlo stopping criterion using fast Bayesian cubature and rank-1 lattices  with guarantees for Gaussian processes having certain shift invariant kernels.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; k = Keister(Lattice(2, seed=123456789))\n&gt;&gt;&gt; sc = CubQMCBayesLatticeG(k,abs_tol=1e-4)\n&gt;&gt;&gt; solution,data = sc.integrate()\n&gt;&gt;&gt; data\nData (Data)\n    solution        1.808\n    comb_bound_low  1.808\n    comb_bound_high 1.808\n    comb_bound_diff 3.60e-05\n    comb_flags      1\n    n_total         2^(10)\n    n               2^(10)\n    time_integrate  ...\nCubQMCBayesLatticeG (AbstractStoppingCriterion)\n    abs_tol         1.00e-04\n    rel_tol         0\n    n_init          2^(8)\n    n_limit         2^(22)\n    order           2^(1)\nKeister (AbstractIntegrand)\nGaussian (AbstractTrueMeasure)\n    mean            0\n    covariance      2^(-1)\n    decomp_type     PCA\nLattice (AbstractLDDiscreteDistribution)\n    d               2^(1)\n    replications    1\n    randomize       SHIFT\n    gen_vec_source  kuo.lattice-33002-1024-1048576.9125.txt\n    order           RADICAL INVERSE\n    n_limit         2^(20)\n    entropy         123456789\n</code></pre> <p>Vector outputs</p> <pre><code>&gt;&gt;&gt; f = BoxIntegral(Lattice(3,seed=7),s=[-1,1])\n&gt;&gt;&gt; abs_tol = 1e-2\n&gt;&gt;&gt; sc = CubQMCBayesLatticeG(f,abs_tol=abs_tol,rel_tol=0)\n&gt;&gt;&gt; solution,data = sc.integrate()\n&gt;&gt;&gt; solution\narray([1.18837601, 0.95984299])\n&gt;&gt;&gt; data\nData (Data)\n    solution        [1.188 0.96 ]\n    comb_bound_low  [1.183 0.95 ]\n    comb_bound_high [1.194 0.969]\n    comb_bound_diff [0.011 0.019]\n    comb_flags      [ True  True]\n    n_total         2^(9)\n    n               [512 256]\n    time_integrate  ...\nCubQMCBayesLatticeG (AbstractStoppingCriterion)\n    abs_tol         0.010\n    rel_tol         0\n    n_init          2^(8)\n    n_limit         2^(22)\n    order           2^(1)\nBoxIntegral (AbstractIntegrand)\n    s               [-1  1]\nUniform (AbstractTrueMeasure)\n    lower_bound     0\n    upper_bound     1\nLattice (AbstractLDDiscreteDistribution)\n    d               3\n    replications    1\n    randomize       SHIFT\n    gen_vec_source  kuo.lattice-33002-1024-1048576.9125.txt\n    order           RADICAL INVERSE\n    n_limit         2^(20)\n    entropy         7\n&gt;&gt;&gt; sol3neg1 = -np.pi/4-1/2*np.log(2)+np.log(5+3*np.sqrt(3))\n&gt;&gt;&gt; sol31 = np.sqrt(3)/4+1/2*np.log(2+np.sqrt(3))-np.pi/24\n&gt;&gt;&gt; true_value = np.array([sol3neg1,sol31])\n&gt;&gt;&gt; assert (abs(true_value-solution)&lt;abs_tol).all()\n</code></pre> <p>Sensitivity indices </p> <pre><code>&gt;&gt;&gt; function = Genz(Lattice(3,seed=7))\n&gt;&gt;&gt; integrand = SensitivityIndices(function)\n&gt;&gt;&gt; sc = CubQMCBayesLatticeG(integrand,abs_tol=5e-2,rel_tol=0)\n&gt;&gt;&gt; solution,data = sc.integrate()\n&gt;&gt;&gt; data\nData (Data)\n    solution        [[0.057 0.131 0.269]\n                     [0.386 0.523 0.741]]\n    comb_bound_low  [[0.048 0.12  0.256]\n                     [0.377 0.511 0.721]]\n    comb_bound_high [[0.066 0.141 0.283]\n                     [0.395 0.535 0.762]]\n    comb_bound_diff [[0.018 0.021 0.027]\n                     [0.018 0.024 0.04 ]]\n    comb_flags      [[ True  True  True]\n                     [ True  True  True]]\n    n_total         2^(10)\n    n               [[[1024 1024 1024]\n                      [1024 1024 1024]\n                      [1024 1024 1024]]\n\n                     [[1024 1024 1024]\n                      [1024 1024 1024]\n                      [1024 1024 1024]]]\n    time_integrate  ...\nCubQMCBayesLatticeG (AbstractStoppingCriterion)\n    abs_tol         0.050\n    rel_tol         0\n    n_init          2^(8)\n    n_limit         2^(22)\n    order           2^(1)\nSensitivityIndices (AbstractIntegrand)\n    indices         [[ True False False]\n                     [False  True False]\n                     [False False  True]]\nUniform (AbstractTrueMeasure)\n    lower_bound     0\n    upper_bound     1\nLattice (AbstractLDDiscreteDistribution)\n    d               3\n    replications    1\n    randomize       SHIFT\n    gen_vec_source  kuo.lattice-33002-1024-1048576.9125.txt\n    order           RADICAL INVERSE\n    n_limit         2^(20)\n    entropy         7\n</code></pre> <p>References:</p> <ol> <li> <p>Jagadeeswaran, Rathinavel, and Fred J. Hickernell.     \"Fast automatic Bayesian cubature using lattice sampling.\"     Statistics and Computing 29.6 (2019): 1215-1229.</p> </li> <li> <p>Jagadeeswaran Rathinavel and Fred J. Hickernell,     Fast automatic Bayesian cubature using lattice sampling.     Stat Comput 29, 1215-1229 (2019).     Available from Springer https://doi.org/10.1007/s11222-019-09895-9.</p> </li> <li> <p>Sou-Cheng T. Choi, Yuhan Ding, Fred J. Hickernell, Lan Jiang, Lluis Antoni Jimenez Rugama,     Da Li, Jagadeeswaran Rathinavel, Xin Tong, Kan Zhang, Yizhi Zhang, and Xuan Zhou,     GAIL: Guaranteed Automatic Integration Library (Version 2.3) [MATLAB Software], 2019. http://gailgithub.github.io/GAIL_Dev/. https://github.com/GailGithub/GAIL_Dev/blob/master/Algorithms/IntegrationExpectation/cubBayesLattice_g.m.</p> </li> </ol> <p>Parameters:</p> Name Type Description Default <code>integrand</code> <code>AbstractIntegrand</code> <p>The integrand.</p> required <code>abs_tol</code> <code>ndarray</code> <p>Absolute error tolerance.</p> <code>0.01</code> <code>rel_tol</code> <code>ndarray</code> <p>Relative error tolerance.</p> <code>0</code> <code>n_init</code> <code>int</code> <p>Initial number of samples. </p> <code>2 ** 8</code> <code>n_limit</code> <code>int</code> <p>Maximum number of samples.</p> <code>2 ** 22</code> <code>error_fun</code> <code>Union[str, callable]</code> <p>Function mapping the approximate solution, absolute error tolerance, and relative error tolerance to the current error bound.</p> <ul> <li><code>'EITHER'</code>, the default, requires the approximation error must be below either the absolue or relative tolerance.     Equivalent to setting     <pre><code>error_fun = lambda sv,abs_tol,rel_tol: np.maximum(abs_tol,abs(sv)*rel_tol)\n</code></pre></li> <li><code>'BOTH'</code> requires the approximation error to be below both the absolue and relative tolerance.      Equivalent to setting     <pre><code>error_fun = lambda sv,abs_tol,rel_tol: np.minimum(abs_tol,abs(sv)*rel_tol)\n</code></pre></li> </ul> <code>'EITHER'</code> <code>alpha</code> <code>ndarray</code> <p>Uncertainty level in \\((0,1)\\). </p> <code>0.01</code> <code>ptransform</code> <code>str</code> <p>Periodization transform, see the options in <code>AbstractIntegrand.f</code>.</p> <code>'C1SIN'</code> <code>errbd_type</code> <code>str</code> <p>Options are </p> <ul> <li><code>'MLE'</code>: Marginal Log Likelihood. </li> <li><code>'GCV'</code>: Generalized Cross Validation. </li> <li><code>'FULL'</code>: Full Bayes.</li> </ul> <code>'MLE'</code> <code>order</code> <code>int</code> <p>Bernoulli kernel's order. If zero, choose order automatically</p> <code>2</code> Source code in <code>qmcpy/stopping_criterion/cub_qmc_bayes_lattice_g.py</code> <pre><code>def __init__(self, \n             integrand, \n             abs_tol = 1e-2, \n             rel_tol = 0,\n             n_init = 2**8, \n             n_limit = 2**22, \n             error_fun=\"EITHER\", \n             alpha = 0.01, \n             ptransform = 'C1SIN',\n             errbd_type = \"MLE\", \n             order = 2,\n             ):\n    r\"\"\"\n    Args:\n        integrand (AbstractIntegrand): The integrand.\n        abs_tol (np.ndarray): Absolute error tolerance.\n        rel_tol (np.ndarray): Relative error tolerance.\n        n_init (int): Initial number of samples. \n        n_limit (int): Maximum number of samples.\n        error_fun (Union[str,callable]): Function mapping the approximate solution, absolute error tolerance, and relative error tolerance to the current error bound.\n\n            - `'EITHER'`, the default, requires the approximation error must be below either the absolue *or* relative tolerance.\n                Equivalent to setting\n                ```python\n                error_fun = lambda sv,abs_tol,rel_tol: np.maximum(abs_tol,abs(sv)*rel_tol)\n                ```\n            - `'BOTH'` requires the approximation error to be below both the absolue *and* relative tolerance. \n                Equivalent to setting\n                ```python\n                error_fun = lambda sv,abs_tol,rel_tol: np.minimum(abs_tol,abs(sv)*rel_tol)\n                ```\n        alpha (np.ndarray): Uncertainty level in $(0,1)$. \n        ptransform (str): Periodization transform, see the options in [`AbstractIntegrand.f`][qmcpy.AbstractIntegrand.f].\n        errbd_type (str): Options are \n\n            - `'MLE'`: Marginal Log Likelihood. \n            - `'GCV'`: Generalized Cross Validation. \n            - `'FULL'`: Full Bayes.\n        order (int): Bernoulli kernel's order. If zero, choose order automatically\n    \"\"\"\n    super(CubQMCBayesLatticeG, self).__init__(integrand, ft=fftbr, omega=omega_fftbr,\n                                           ptransform=ptransform,\n                                           allowed_distribs=[Lattice],\n                                           kernel=self._shift_inv_kernel,\n                                           abs_tol=abs_tol, rel_tol=rel_tol,\n                                           n_init=n_init, n_limit=n_limit, alpha=alpha, error_fun=error_fun, errbd_type=errbd_type)\n    self.order = order  # Bernoulli kernel's order. If zero, choose order automatically\n    # private properties\n    # full_Bayes - Full Bayes - assumes m and s^2 as hyperparameters\n    # GCV - Generalized cross validation\n    self.kernType = 1  # Type-1: Bernoulli polynomial based algebraic convergence, Type-2: Truncated series\n    self._xfullundtype = float\n    if self.discrete_distrib.order!='RADICAL INVERSE':\n        raise ParameterError(\"CubLattice_g requires Lattice with 'RADICAL INVERSE' order\")\n</code></pre>"},{"location":"api/stopping_criteria/#cubqmcrepstudentt","title":"<code>CubQMCRepStudentT</code>","text":"<p>               Bases: <code>AbstractStoppingCriterion</code></p> <p>Quasi-Monte Carlo stopping criterion based on Student's \\(t\\)-distribution for multiple replications.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; k = Keister(DigitalNetB2(seed=7,replications=25))\n&gt;&gt;&gt; sc = CubQMCRepStudentT(k,abs_tol=1e-3,rel_tol=0)\n&gt;&gt;&gt; solution,data = sc.integrate()\n&gt;&gt;&gt; solution\narray(1.3803927)\n&gt;&gt;&gt; data\nData (Data)\n    solution        1.380\n    comb_bound_low  1.380\n    comb_bound_high 1.381\n    comb_bound_diff 0.002\n    comb_flags      1\n    n_total         6400\n    n               6400\n    n_rep           2^(8)\n    time_integrate  ...\nCubQMCRepStudentT (AbstractStoppingCriterion)\n    inflate         1\n    alpha           0.010\n    abs_tol         0.001\n    rel_tol         0\n    n_init          2^(8)\n    n_limit         2^(30)\nKeister (AbstractIntegrand)\nGaussian (AbstractTrueMeasure)\n    mean            0\n    covariance      2^(-1)\n    decomp_type     PCA\nDigitalNetB2 (AbstractLDDiscreteDistribution)\n    d               1\n    replications    25\n    randomize       LMS DS\n    gen_mats_source joe_kuo.6.21201.txt\n    order           RADICAL INVERSE\n    t               63\n    alpha           1\n    n_limit         2^(32)\n    entropy         7\n</code></pre> <p>Vector outputs</p> <pre><code>&gt;&gt;&gt; f = BoxIntegral(DigitalNetB2(3,seed=7,replications=25),s=[-1,1])\n&gt;&gt;&gt; abs_tol = 1e-3\n&gt;&gt;&gt; sc = CubQMCRepStudentT(f,abs_tol=abs_tol,rel_tol=0)\n&gt;&gt;&gt; solution,data = sc.integrate()\n&gt;&gt;&gt; solution\narray([1.19025707, 0.96062762])\n&gt;&gt;&gt; data\nData (Data)\n    solution        [1.19  0.961]\n    comb_bound_low  [1.19  0.961]\n    comb_bound_high [1.191 0.961]\n    comb_bound_diff [0.001 0.   ]\n    comb_flags      [ True  True]\n    n_total         409600\n    n               [409600   6400]\n    n_rep           [16384   256]\n    time_integrate  ...\nCubQMCRepStudentT (AbstractStoppingCriterion)\n    inflate         1\n    alpha           0.010\n    abs_tol         0.001\n    rel_tol         0\n    n_init          2^(8)\n    n_limit         2^(30)\nBoxIntegral (AbstractIntegrand)\n    s               [-1  1]\nUniform (AbstractTrueMeasure)\n    lower_bound     0\n    upper_bound     1\nDigitalNetB2 (AbstractLDDiscreteDistribution)\n    d               3\n    replications    25\n    randomize       LMS DS\n    gen_mats_source joe_kuo.6.21201.txt\n    order           RADICAL INVERSE\n    t               63\n    alpha           1\n    n_limit         2^(32)\n    entropy         7\n&gt;&gt;&gt; sol3neg1 = -np.pi/4-1/2*np.log(2)+np.log(5+3*np.sqrt(3))\n&gt;&gt;&gt; sol31 = np.sqrt(3)/4+1/2*np.log(2+np.sqrt(3))-np.pi/24\n&gt;&gt;&gt; true_value = np.array([sol3neg1,sol31])\n&gt;&gt;&gt; assert (abs(true_value-solution)&lt;abs_tol).all()\n</code></pre> <p>Sensitivity indices </p> <pre><code>&gt;&gt;&gt; function = Genz(DigitalNetB2(3,seed=7,replications=25))\n&gt;&gt;&gt; integrand = SensitivityIndices(function)\n&gt;&gt;&gt; sc = CubQMCRepStudentT(integrand,abs_tol=5e-4,rel_tol=0)\n&gt;&gt;&gt; solution,data = sc.integrate()\n&gt;&gt;&gt; data\nData (Data)\n    solution        [[0.02  0.196 0.667]\n                     [0.036 0.303 0.782]]\n    comb_bound_low  [[0.019 0.195 0.667]\n                     [0.035 0.303 0.781]]\n    comb_bound_high [[0.02  0.196 0.667]\n                     [0.036 0.303 0.782]]\n    comb_bound_diff [[0.001 0.001 0.001]\n                     [0.001 0.001 0.001]]\n    comb_flags      [[ True  True  True]\n                     [ True  True  True]]\n    n_total         204800\n    n               [[[102400 204800 204800]\n                      [102400 204800 204800]\n                      [102400 204800 204800]]\n\n                     [[ 12800 102400 102400]\n                      [ 12800 102400 102400]\n                      [ 12800 102400 102400]]]\n    n_rep           [[[4096 8192 8192]\n                      [4096 8192 8192]\n                      [4096 8192 8192]]\n\n                     [[ 512 4096 4096]\n                      [ 512 4096 4096]\n                      [ 512 4096 4096]]]\n    time_integrate  ...\nCubQMCRepStudentT (AbstractStoppingCriterion)\n    inflate         1\n    alpha           0.010\n    abs_tol         5.00e-04\n    rel_tol         0\n    n_init          2^(8)\n    n_limit         2^(30)\nSensitivityIndices (AbstractIntegrand)\n    indices         [[ True False False]\n                     [False  True False]\n                     [False False  True]]\nUniform (AbstractTrueMeasure)\n    lower_bound     0\n    upper_bound     1\nDigitalNetB2 (AbstractLDDiscreteDistribution)\n    d               3\n    replications    25\n    randomize       LMS DS\n    gen_mats_source joe_kuo.6.21201.txt\n    order           RADICAL INVERSE\n    t               63\n    alpha           1\n    n_limit         2^(32)\n    entropy         7\n</code></pre> <p>References:</p> <ol> <li> <p>Art B. Owen. \"Practical Quasi-Monte Carlo Integration.\" 2023. https://artowen.su.domains/mc/. </p> </li> <li> <p>Pierre l\u2019Ecuyer et al.     \"Confidence intervals for randomized quasi-Monte Carlo estimators.\"     2023 Winter Simulation Conference (WSC). IEEE, 2023. https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10408613.</p> </li> </ol> <p>Parameters:</p> Name Type Description Default <code>integrand</code> <code>AbstractIntegrand</code> <p>The integrand.</p> required <code>abs_tol</code> <code>ndarray</code> <p>Absolute error tolerance.</p> <code>0.01</code> <code>rel_tol</code> <code>ndarray</code> <p>Relative error tolerance.</p> <code>0.0</code> <code>n_init</code> <code>int</code> <p>Initial number of samples. </p> <code>256.0</code> <code>n_limit</code> <code>int</code> <p>Maximum number of samples.</p> <code>2 ** 30</code> <code>error_fun</code> <code>Union[str, callable]</code> <p>Function mapping the approximate solution, absolute error tolerance, and relative error tolerance to the current error bound.</p> <ul> <li><code>'EITHER'</code>, the default, requires the approximation error must be below either the absolue or relative tolerance.     Equivalent to setting     <pre><code>error_fun = lambda sv,abs_tol,rel_tol: np.maximum(abs_tol,abs(sv)*rel_tol)\n</code></pre></li> <li><code>'BOTH'</code> requires the approximation error to be below both the absolue and relative tolerance.      Equivalent to setting     <pre><code>error_fun = lambda sv,abs_tol,rel_tol: np.minimum(abs_tol,abs(sv)*rel_tol)\n</code></pre></li> </ul> <code>'EITHER'</code> <code>inflate</code> <code>float</code> <p>Inflation factor \\(\\geq 1\\) to multiply by the variance estimate to make it more conservative.</p> <code>1</code> <code>alpha</code> <code>ndarray</code> <p>Uncertainty level in \\((0,1)\\).</p> <code>0.01</code> Source code in <code>qmcpy/stopping_criterion/cub_qmc_rep_student_t.py</code> <pre><code>def __init__(self,\n             integrand, \n             abs_tol = 1e-2,\n             rel_tol = 0.,\n             n_init = 256.,\n             n_limit = 2**30,\n             error_fun = \"EITHER\",\n             inflate = 1,\n             alpha = 0.01, \n             ):\n    r\"\"\"\n    Args:\n        integrand (AbstractIntegrand): The integrand.\n        abs_tol (np.ndarray): Absolute error tolerance.\n        rel_tol (np.ndarray): Relative error tolerance.\n        n_init (int): Initial number of samples. \n        n_limit (int): Maximum number of samples.\n        error_fun (Union[str,callable]): Function mapping the approximate solution, absolute error tolerance, and relative error tolerance to the current error bound.\n\n            - `'EITHER'`, the default, requires the approximation error must be below either the absolue *or* relative tolerance.\n                Equivalent to setting\n                ```python\n                error_fun = lambda sv,abs_tol,rel_tol: np.maximum(abs_tol,abs(sv)*rel_tol)\n                ```\n            - `'BOTH'` requires the approximation error to be below both the absolue *and* relative tolerance. \n                Equivalent to setting\n                ```python\n                error_fun = lambda sv,abs_tol,rel_tol: np.minimum(abs_tol,abs(sv)*rel_tol)\n                ```\n        inflate (float): Inflation factor $\\geq 1$ to multiply by the variance estimate to make it more conservative.\n        alpha (np.ndarray): Uncertainty level in $(0,1)$. \n    \"\"\"\n    self.parameters = ['inflate','alpha','abs_tol','rel_tol','n_init','n_limit']\n    # Input Checks\n    if np.log2(n_init)%1!=0:\n        warnings.warn('n_init must be a power of two. Using n_init = 2**5',ParameterWarning)\n        n_init = 2**5\n    if np.log2(n_limit)%1!=0:\n        warnings.warn('n_init must be a power of two. Using n_limit = 2**30',ParameterWarning)\n        n_limit = 2**30\n    # Set Attributes\n    self.n_init = int(n_init)\n    self.n_limit = int(n_limit)\n    assert isinstance(error_fun,str) or callable(error_fun)\n    if isinstance(error_fun,str):\n        if error_fun.upper()==\"EITHER\":\n            error_fun = lambda sv,abs_tol,rel_tol: np.maximum(abs_tol,abs(sv)*rel_tol)\n        elif error_fun.upper()==\"BOTH\":\n            error_fun = lambda sv,abs_tol,rel_tol: np.minimum(abs_tol,abs(sv)*rel_tol)\n        else:\n            raise ParameterError(\"str error_fun must be 'EITHER' or 'BOTH'\")\n    self.error_fun = error_fun\n    self.alpha = alpha\n    self.inflate = float(inflate)\n    assert self.inflate&gt;=1\n    assert 0&lt;self.alpha&lt;1\n    # QMCPy Objs\n    self.integrand = integrand\n    self.true_measure = self.integrand.true_measure\n    self.discrete_distrib = self.integrand.discrete_distrib\n    super(CubQMCRepStudentT,self).__init__(allowed_distribs=[AbstractLDDiscreteDistribution],allow_vectorized_integrals=True)\n    assert self.integrand.discrete_distrib.replications&gt;1, \"Require the discrete distribution has replications&gt;1\"\n    assert self.integrand.discrete_distrib.randomize!=\"FALSE\", \"Require discrete distribution is randomized\"\n    self.alphas_indv,identity_dependency = self._compute_indv_alphas(np.full(self.integrand.d_comb,self.alpha))\n    self.set_tolerance(abs_tol,rel_tol)\n    self.t_star = -t.ppf(self.alphas_indv/2,df=self.integrand.discrete_distrib.replications-1)\n</code></pre>"},{"location":"api/stopping_criteria/#iid-mc","title":"IID MC","text":""},{"location":"api/stopping_criteria/#cubmccltvec","title":"<code>CubMCCLTVec</code>","text":"<p>               Bases: <code>AbstractStoppingCriterion</code></p> <p>IID Monte Carlo stopping criterion stopping criterion based on the Central Limit Theorem with doubling sample sizes.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; k = Keister(IIDStdUniform(seed=7))\n&gt;&gt;&gt; sc = CubMCCLTVec(k,abs_tol=5e-2,rel_tol=0)\n&gt;&gt;&gt; solution,data = sc.integrate()\n&gt;&gt;&gt; solution\narray(1.38366791)\n&gt;&gt;&gt; data\nData (Data)\n    solution        1.384\n    comb_bound_low  1.343\n    comb_bound_high 1.424\n    comb_bound_diff 0.080\n    comb_flags      1\n    n_total         1024\n    n               2^(10)\n    time_integrate  ...\nCubMCCLTVec (AbstractStoppingCriterion)\n    inflate         1\n    alpha           0.010\n    abs_tol         0.050\n    rel_tol         0\n    n_init          2^(8)\n    n_limit         2^(30)\nKeister (AbstractIntegrand)\nGaussian (AbstractTrueMeasure)\n    mean            0\n    covariance      2^(-1)\n    decomp_type     PCA\nIIDStdUniform (AbstractIIDDiscreteDistribution)\n    d               1\n    replications    1\n    entropy         7\n</code></pre> <p>Vector outputs</p> <pre><code>&gt;&gt;&gt; f = BoxIntegral(IIDStdUniform(3,seed=7),s=[-1,1])\n&gt;&gt;&gt; abs_tol = 2.5e-2\n&gt;&gt;&gt; sc = CubMCCLTVec(f,abs_tol=abs_tol,rel_tol=0)\n&gt;&gt;&gt; solution,data = sc.integrate()\n&gt;&gt;&gt; solution\narray([1.18448043, 0.95435347])\n&gt;&gt;&gt; data\nData (Data)\n    solution        [1.184 0.954]\n    comb_bound_low  [1.165 0.932]\n    comb_bound_high [1.203 0.977]\n    comb_bound_diff [0.038 0.045]\n    comb_flags      [ True  True]\n    n_total         8192\n    n               [8192 1024]\n    time_integrate  ...\nCubMCCLTVec (AbstractStoppingCriterion)\n    inflate         1\n    alpha           0.010\n    abs_tol         0.025\n    rel_tol         0\n    n_init          2^(8)\n    n_limit         2^(30)\nBoxIntegral (AbstractIntegrand)\n    s               [-1  1]\nUniform (AbstractTrueMeasure)\n    lower_bound     0\n    upper_bound     1\nIIDStdUniform (AbstractIIDDiscreteDistribution)\n    d               3\n    replications    1\n    entropy         7\n&gt;&gt;&gt; sol3neg1 = -np.pi/4-1/2*np.log(2)+np.log(5+3*np.sqrt(3))\n&gt;&gt;&gt; sol31 = np.sqrt(3)/4+1/2*np.log(2+np.sqrt(3))-np.pi/24\n&gt;&gt;&gt; true_value = np.array([sol3neg1,sol31])\n&gt;&gt;&gt; assert (abs(true_value-solution)&lt;abs_tol).all()\n</code></pre> <p>Sensitivity indices </p> <pre><code>&gt;&gt;&gt; function = Genz(IIDStdUniform(3,seed=7))\n&gt;&gt;&gt; integrand = SensitivityIndices(function)\n&gt;&gt;&gt; sc = CubMCCLTVec(integrand,abs_tol=2.5e-2,rel_tol=0)\n&gt;&gt;&gt; solution,data = sc.integrate()\n&gt;&gt;&gt; data\nData (Data)\n    solution        [[0.024 0.203 0.662]\n                     [0.044 0.308 0.78 ]]\n    comb_bound_low  [[0.006 0.186 0.644]\n                     [0.02  0.286 0.761]]\n    comb_bound_high [[0.042 0.221 0.681]\n                     [0.067 0.329 0.798]]\n    comb_bound_diff [[0.036 0.035 0.037]\n                     [0.047 0.043 0.037]]\n    comb_flags      [[ True  True  True]\n                     [ True  True  True]]\n    n_total         262144\n    n               [[[  4096  65536 262144]\n                      [  4096  65536 262144]\n                      [  4096  65536 262144]]\n\n                     [[   512  32768 262144]\n                      [   512  32768 262144]\n                      [   512  32768 262144]]]\n    time_integrate  ...\nCubMCCLTVec (AbstractStoppingCriterion)\n    inflate         1\n    alpha           0.010\n    abs_tol         0.025\n    rel_tol         0\n    n_init          2^(8)\n    n_limit         2^(30)\nSensitivityIndices (AbstractIntegrand)\n    indices         [[ True False False]\n                     [False  True False]\n                     [False False  True]]\nUniform (AbstractTrueMeasure)\n    lower_bound     0\n    upper_bound     1\nIIDStdUniform (AbstractIIDDiscreteDistribution)\n    d               3\n    replications    1\n    entropy         7\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>integrand</code> <code>AbstractIntegrand</code> <p>The integrand.</p> required <code>abs_tol</code> <code>ndarray</code> <p>Absolute error tolerance.</p> <code>0.01</code> <code>rel_tol</code> <code>ndarray</code> <p>Relative error tolerance.</p> <code>0.0</code> <code>n_init</code> <code>int</code> <p>Initial number of samples. </p> <code>256.0</code> <code>n_limit</code> <code>int</code> <p>Maximum number of samples.</p> <code>2 ** 30</code> <code>error_fun</code> <code>Union[str, callable]</code> <p>Function mapping the approximate solution, absolute error tolerance, and relative error tolerance to the current error bound.</p> <ul> <li><code>'EITHER'</code>, the default, requires the approximation error must be below either the absolue or relative tolerance.     Equivalent to setting     <pre><code>error_fun = lambda sv,abs_tol,rel_tol: np.maximum(abs_tol,abs(sv)*rel_tol)\n</code></pre></li> <li><code>'BOTH'</code> requires the approximation error to be below both the absolue and relative tolerance.      Equivalent to setting     <pre><code>error_fun = lambda sv,abs_tol,rel_tol: np.minimum(abs_tol,abs(sv)*rel_tol)\n</code></pre></li> </ul> <code>'EITHER'</code> <code>inflate</code> <code>float</code> <p>Inflation factor \\(\\geq 1\\) to multiply by the variance estimate to make it more conservative.</p> <code>1</code> <code>alpha</code> <code>ndarray</code> <p>Uncertainty level in \\((0,1)\\).</p> <code>0.01</code> Source code in <code>qmcpy/stopping_criterion/cub_mc_clt_vec.py</code> <pre><code>def __init__(self,\n             integrand, \n             abs_tol = 1e-2,\n             rel_tol = 0.,\n             n_init = 256.,\n             n_limit = 2**30,\n             error_fun = \"EITHER\",\n             inflate = 1,\n             alpha = 0.01,\n             ):\n    r\"\"\"\n    Args:\n        integrand (AbstractIntegrand): The integrand.\n        abs_tol (np.ndarray): Absolute error tolerance.\n        rel_tol (np.ndarray): Relative error tolerance.\n        n_init (int): Initial number of samples. \n        n_limit (int): Maximum number of samples.\n        error_fun (Union[str,callable]): Function mapping the approximate solution, absolute error tolerance, and relative error tolerance to the current error bound.\n\n            - `'EITHER'`, the default, requires the approximation error must be below either the absolue *or* relative tolerance.\n                Equivalent to setting\n                ```python\n                error_fun = lambda sv,abs_tol,rel_tol: np.maximum(abs_tol,abs(sv)*rel_tol)\n                ```\n            - `'BOTH'` requires the approximation error to be below both the absolue *and* relative tolerance. \n                Equivalent to setting\n                ```python\n                error_fun = lambda sv,abs_tol,rel_tol: np.minimum(abs_tol,abs(sv)*rel_tol)\n                ```\n        inflate (float): Inflation factor $\\geq 1$ to multiply by the variance estimate to make it more conservative.\n        alpha (np.ndarray): Uncertainty level in $(0,1)$. \n    \"\"\"\n    self.parameters = ['inflate','alpha','abs_tol','rel_tol','n_init','n_limit']\n    # Input Checks\n    if np.log2(n_init)%1!=0:\n        warnings.warn('n_init must be a power of two. Using n_init = 2**5',ParameterWarning)\n        n_init = 2**5\n    if np.log2(n_limit)%1!=0:\n        warnings.warn('n_init must be a power of two. Using n_limit = 2**30',ParameterWarning)\n        n_limit = 2**30\n    # Set Attributes\n    self.n_init = int(n_init)\n    self.n_limit = int(n_limit)\n    assert isinstance(error_fun,str) or callable(error_fun)\n    if isinstance(error_fun,str):\n        if error_fun.upper()==\"EITHER\":\n            error_fun = lambda sv,abs_tol,rel_tol: np.maximum(abs_tol,abs(sv)*rel_tol)\n        elif error_fun.upper()==\"BOTH\":\n            error_fun = lambda sv,abs_tol,rel_tol: np.minimum(abs_tol,abs(sv)*rel_tol)\n        else:\n            raise ParameterError(\"str error_fun must be 'EITHER' or 'BOTH'\")\n    self.error_fun = error_fun\n    self.alpha = alpha\n    self.inflate = float(inflate)\n    assert self.inflate&gt;=1\n    # QMCPy Objs\n    self.integrand = integrand\n    self.true_measure = self.integrand.true_measure\n    self.discrete_distrib = self.integrand.discrete_distrib\n    super(CubMCCLTVec,self).__init__(allowed_distribs=[AbstractIIDDiscreteDistribution],allow_vectorized_integrals=True)\n    assert self.integrand.discrete_distrib.no_replications==True, \"Require the discrete distribution has replications=None\"\n    self.alphas_indv,identity_dependency = self._compute_indv_alphas(np.full(self.integrand.d_comb,self.alpha))\n    self.set_tolerance(abs_tol,rel_tol)\n    self.z_star = -norm.ppf(self.alphas_indv/2)\n</code></pre>"},{"location":"api/stopping_criteria/#cubmcg","title":"<code>CubMCG</code>","text":"<p>               Bases: <code>AbstractStoppingCriterion</code></p> <p>IID Monte Carlo stopping criterion using Berry-Esseen inequalities in a two step method with guarantees for functions with bounded kurtosis.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; ao = FinancialOption(IIDStdUniform(52,seed=7))\n&gt;&gt;&gt; sc = CubMCG(ao,abs_tol=.05)\n&gt;&gt;&gt; solution,data = sc.integrate()\n&gt;&gt;&gt; data\nData (Data)\n    solution        1.779\n    bound_low       1.729\n    bound_high      1.829\n    bound_diff      0.100\n    n_total         112314\n    time_integrate  ...\nCubMCG (AbstractStoppingCriterion)\n    abs_tol         0.050\n    rel_tol         0\n    n_init          2^(10)\n    n_limit         2^(30)\n    inflate         1.200\n    alpha           0.010\n    kurtmax         1.478\nFinancialOption (AbstractIntegrand)\n    option          ASIAN\n    call_put        CALL\n    volatility      2^(-1)\n    start_price     30\n    strike_price    35\n    interest_rate   0\n    t_final         1\n    asian_mean      ARITHMETIC\nBrownianMotion (AbstractTrueMeasure)\n    time_vec        [0.019 0.038 0.058 ... 0.962 0.981 1.   ]\n    drift           0\n    mean            [0. 0. 0. ... 0. 0. 0.]\n    covariance      [[0.019 0.019 0.019 ... 0.019 0.019 0.019]\n                     [0.019 0.038 0.038 ... 0.038 0.038 0.038]\n                     [0.019 0.038 0.058 ... 0.058 0.058 0.058]\n                     ...\n                     [0.019 0.038 0.058 ... 0.962 0.962 0.962]\n                     [0.019 0.038 0.058 ... 0.962 0.981 0.981]\n                     [0.019 0.038 0.058 ... 0.962 0.981 1.   ]]\n    decomp_type     PCA\nIIDStdUniform (AbstractIIDDiscreteDistribution)\n    d               52\n    replications    1\n    entropy         7\n</code></pre> <p>Control variates </p> <pre><code>&gt;&gt;&gt; iid = IIDStdUniform(52,seed=7)\n&gt;&gt;&gt; ao = FinancialOption(iid,option=\"ASIAN\")\n&gt;&gt;&gt; eo = FinancialOption(iid,option=\"EUROPEAN\")\n&gt;&gt;&gt; lin0 = Linear0(iid)\n&gt;&gt;&gt; sc = CubMCG(ao,abs_tol=.05,\n...     control_variates = [eo,lin0],\n...     control_variate_means = [eo.get_exact_value(),0])\n&gt;&gt;&gt; solution,data = sc.integrate()\n&gt;&gt;&gt; data\nData (Data)\n    solution        1.787\n    bound_low       1.737\n    bound_high      1.837\n    bound_diff      0.100\n    n_total         52147\n    time_integrate  ...\nCubMCG (AbstractStoppingCriterion)\n    abs_tol         0.050\n    rel_tol         0\n    n_init          2^(10)\n    n_limit         2^(30)\n    inflate         1.200\n    alpha           0.010\n    kurtmax         1.478\n    cv              [FinancialOption (AbstractIntegrand)\n                         option          EUROPEAN\n                         call_put        CALL\n                         volatility      2^(-1)\n                         start_price     30\n                         strike_price    35\n                         interest_rate   0\n                         t_final         1               Linear0 (AbstractIntegrand)]\n    cv_mu           [4.211 0.   ]\nFinancialOption (AbstractIntegrand)\n    option          ASIAN\n    call_put        CALL\n    volatility      2^(-1)\n    start_price     30\n    strike_price    35\n    interest_rate   0\n    t_final         1\n    asian_mean      ARITHMETIC\nBrownianMotion (AbstractTrueMeasure)\n    time_vec        [0.019 0.038 0.058 ... 0.962 0.981 1.   ]\n    drift           0\n    mean            [0. 0. 0. ... 0. 0. 0.]\n    covariance      [[0.019 0.019 0.019 ... 0.019 0.019 0.019]\n                     [0.019 0.038 0.038 ... 0.038 0.038 0.038]\n                     [0.019 0.038 0.058 ... 0.058 0.058 0.058]\n                     ...\n                     [0.019 0.038 0.058 ... 0.962 0.962 0.962]\n                     [0.019 0.038 0.058 ... 0.962 0.981 0.981]\n                     [0.019 0.038 0.058 ... 0.962 0.981 1.   ]]\n    decomp_type     PCA\nIIDStdUniform (AbstractIIDDiscreteDistribution)\n    d               52\n    replications    1\n    entropy         7\n</code></pre> <p>Relative tolerance </p> <pre><code>&gt;&gt;&gt; ao = FinancialOption(IIDStdUniform(52,seed=7))\n&gt;&gt;&gt; sc = CubMCG(ao,abs_tol=1e-3,rel_tol=5e-2)\n&gt;&gt;&gt; solution,data = sc.integrate()\n&gt;&gt;&gt; data\nData (Data)\n    solution        1.743\n    bound_low       1.661\n    bound_high      1.825\n    bound_diff      0.164\n    n_total         27503\n    time_integrate  ...\nCubMCG (AbstractStoppingCriterion)\n    abs_tol         0.001\n    rel_tol         0.050\n    n_init          2^(10)\n    n_limit         2^(30)\n    inflate         1.200\n    alpha           0.010\n    kurtmax         1.478\nFinancialOption (AbstractIntegrand)\n    option          ASIAN\n    call_put        CALL\n    volatility      2^(-1)\n    start_price     30\n    strike_price    35\n    interest_rate   0\n    t_final         1\n    asian_mean      ARITHMETIC\nBrownianMotion (AbstractTrueMeasure)\n    time_vec        [0.019 0.038 0.058 ... 0.962 0.981 1.   ]\n    drift           0\n    mean            [0. 0. 0. ... 0. 0. 0.]\n    covariance      [[0.019 0.019 0.019 ... 0.019 0.019 0.019]\n                     [0.019 0.038 0.038 ... 0.038 0.038 0.038]\n                     [0.019 0.038 0.058 ... 0.058 0.058 0.058]\n                     ...\n                     [0.019 0.038 0.058 ... 0.962 0.962 0.962]\n                     [0.019 0.038 0.058 ... 0.962 0.981 0.981]\n                     [0.019 0.038 0.058 ... 0.962 0.981 1.   ]]\n    decomp_type     PCA\nIIDStdUniform (AbstractIIDDiscreteDistribution)\n    d               52\n    replications    1\n    entropy         7\n</code></pre> <p>Relative tolerance and control variates </p> <pre><code>&gt;&gt;&gt; iid = IIDStdUniform(52,seed=7)\n&gt;&gt;&gt; ao = FinancialOption(iid,option=\"ASIAN\")\n&gt;&gt;&gt; eo = FinancialOption(iid,option=\"EUROPEAN\")\n&gt;&gt;&gt; lin0 = Linear0(iid)\n&gt;&gt;&gt; sc = CubMCG(ao,abs_tol=1e-3,rel_tol=5e-2,\n...     control_variates = [eo,lin0],\n...     control_variate_means = [eo.get_exact_value(),0])\n&gt;&gt;&gt; solution,data = sc.integrate()\n&gt;&gt;&gt; data\nData (Data)\n    solution        1.776\n    bound_low       1.692\n    bound_high      1.859\n    bound_diff      0.167\n    n_total         12074\n    time_integrate  ...\nCubMCG (AbstractStoppingCriterion)\n    abs_tol         0.001\n    rel_tol         0.050\n    n_init          2^(10)\n    n_limit         2^(30)\n    inflate         1.200\n    alpha           0.010\n    kurtmax         1.478\n    cv              [FinancialOption (AbstractIntegrand)\n                         option          EUROPEAN\n                         call_put        CALL\n                         volatility      2^(-1)\n                         start_price     30\n                         strike_price    35\n                         interest_rate   0\n                         t_final         1               Linear0 (AbstractIntegrand)]\n    cv_mu           [4.211 0.   ]\nFinancialOption (AbstractIntegrand)\n    option          ASIAN\n    call_put        CALL\n    volatility      2^(-1)\n    start_price     30\n    strike_price    35\n    interest_rate   0\n    t_final         1\n    asian_mean      ARITHMETIC\nBrownianMotion (AbstractTrueMeasure)\n    time_vec        [0.019 0.038 0.058 ... 0.962 0.981 1.   ]\n    drift           0\n    mean            [0. 0. 0. ... 0. 0. 0.]\n    covariance      [[0.019 0.019 0.019 ... 0.019 0.019 0.019]\n                     [0.019 0.038 0.038 ... 0.038 0.038 0.038]\n                     [0.019 0.038 0.058 ... 0.058 0.058 0.058]\n                     ...\n                     [0.019 0.038 0.058 ... 0.962 0.962 0.962]\n                     [0.019 0.038 0.058 ... 0.962 0.981 0.981]\n                     [0.019 0.038 0.058 ... 0.962 0.981 1.   ]]\n    decomp_type     PCA\nIIDStdUniform (AbstractIIDDiscreteDistribution)\n    d               52\n    replications    1\n    entropy         7\n</code></pre> <p>References:</p> <ol> <li> <p>Fred J. Hickernell, Lan Jiang, Yuewei Liu, and Art B. Owen,     \"Guaranteed conservative fixed width confidence intervals via Monte Carlo sampling,\"     Monte Carlo and Quasi-Monte Carlo Methods 2012 (J. Dick, F. Y. Kuo, G. W. Peters, and I. H. Sloan, eds.), pp. 105-128,     Springer-Verlag, Berlin, 2014. DOI: 10.1007/978-3-642-41095-6_5</p> </li> <li> <p>Sou-Cheng T. Choi, Yuhan Ding, Fred J. Hickernell, Lan Jiang, Lluis Antoni Jimenez Rugama,     Da Li, Jagadeeswaran Rathinavel, Xin Tong, Kan Zhang, Yizhi Zhang, and Xuan Zhou,     GAIL: Guaranteed Automatic Integration Library (Version 2.3) [MATLAB Software], 2019. http://gailgithub.github.io/GAIL_Dev/. https://github.com/GailGithub/GAIL_Dev/blob/master/Algorithms/IntegrationExpectation/meanMC_g.m.</p> </li> </ol> <p>Parameters:</p> Name Type Description Default <code>integrand</code> <code>AbstractIntegrand</code> <p>The integrand.</p> required <code>abs_tol</code> <code>ndarray</code> <p>Absolute error tolerance.</p> <code>0.01</code> <code>rel_tol</code> <code>ndarray</code> <p>Relative error tolerance.</p> <code>0.0</code> <code>n_init</code> <code>int</code> <p>Initial number of samples. </p> <code>1024</code> <code>n_limit</code> <code>int</code> <p>Maximum number of samples.</p> <code>2 ** 30</code> <code>inflate</code> <code>float</code> <p>Inflation factor \\(\\geq 1\\) to multiply by the variance estimate to make it more conservative.</p> <code>1.2</code> <code>alpha</code> <code>ndarray</code> <p>Uncertainty level in \\((0,1)\\). </p> <code>0.01</code> <code>control_variates</code> <code>list</code> <p>Integrands to use as control variates, each with the same underlying discrete distribution instance.</p> <code>[]</code> <code>control_variate_means</code> <code>ndarray</code> <p>Means of each control variate.</p> <code>[]</code> Source code in <code>qmcpy/stopping_criterion/cub_mc_g.py</code> <pre><code>def __init__(self, \n             integrand, \n             abs_tol = 1e-2, \n             rel_tol = 0., \n             n_init = 1024, \n             n_limit = 2**30,\n             inflate = 1.2, \n             alpha = 0.01, \n             control_variates = [], \n             control_variate_means = [],\n             ):\n    r\"\"\"\n    Args:\n        integrand (AbstractIntegrand): The integrand.\n        abs_tol (np.ndarray): Absolute error tolerance.\n        rel_tol (np.ndarray): Relative error tolerance.\n        n_init (int): Initial number of samples. \n        n_limit (int): Maximum number of samples.\n        inflate (float): Inflation factor $\\geq 1$ to multiply by the variance estimate to make it more conservative.\n        alpha (np.ndarray): Uncertainty level in $(0,1)$. \n        control_variates (list): Integrands to use as control variates, each with the same underlying discrete distribution instance.\n        control_variate_means (np.ndarray): Means of each control variate. \n    \"\"\"\n    self.parameters = ['abs_tol','rel_tol','n_init','n_limit','inflate','alpha','kurtmax']\n    # Set Attributes\n    self.abs_tol = float(abs_tol)\n    self.rel_tol = float(rel_tol)\n    self.n_init = float(n_init)\n    self.n_limit = float(n_limit)\n    self.alpha = float(alpha)\n    self.inflate = float(inflate)\n    self.alpha_sigma = float(self.alpha) / 2.  # the uncertainty for variance estimation\n    self.kurtmax = (n_init - 3) / (n_init - 1) + \\\n        (self.alpha_sigma * n_init) / (1 - self.alpha_sigma) * \\\n        (1 - 1. / self.inflate**2)**2\n    # QMCPy Objs\n    self.integrand = integrand\n    self.true_measure = self.integrand.true_measure\n    self.discrete_distrib = self.integrand.discrete_distrib\n    super(CubMCG,self).__init__(allowed_distribs=[AbstractIIDDiscreteDistribution],allow_vectorized_integrals=False)\n    assert self.integrand.d_indv==()\n    # control variates\n    self.cv_mu = np.atleast_1d(control_variate_means)\n    self.cv = control_variates\n    if isinstance(self.cv,AbstractIntegrand):\n        self.cv = [self.cv]\n        self.cv_mu = self.cv_mu[None,...]\n    assert isinstance(self.cv,list), \"cv must be a list of AbstractIntegrand objects\"\n    for cv in self.cv:\n        if (not isinstance(cv,AbstractIntegrand)) or (cv.discrete_distrib!=self.discrete_distrib) or (cv.d_indv!=self.integrand.d_indv):\n            raise ParameterError('''\n                    Each control variates discrete distribution must be an AbstractIntegrand instance \n                    with the same discrete distribution as the main integrand. d_indv must also match \n                    that of the main integrand instance for each control variate.''')\n    self.ncv = len(self.cv)\n    if self.ncv&gt;0:\n        assert self.cv_mu.shape==((self.ncv,)+self.integrand.d_indv), \"Control variate means should have shape (len(control variates),d_indv).\"\n        self.parameters += ['cv','cv_mu']\n</code></pre>"},{"location":"api/stopping_criteria/#cubmcclt","title":"<code>CubMCCLT</code>","text":"<p>               Bases: <code>AbstractStoppingCriterion</code></p> <p>IID Monte Carlo stopping criterion based on the Central Limit Theorem in a two step method.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; ao = FinancialOption(IIDStdUniform(52,seed=7))\n&gt;&gt;&gt; sc = CubMCCLT(ao,abs_tol=.05)\n&gt;&gt;&gt; solution,data = sc.integrate()\n&gt;&gt;&gt; data\nData (Data)\n    solution        1.777\n    bound_low       1.723\n    bound_high      1.831\n    bound_diff      0.107\n    n_total         74235\n    time_integrate  ...\nCubMCCLT (AbstractStoppingCriterion)\n    abs_tol         0.050\n    rel_tol         0\n    n_init          2^(10)\n    n_limit         2^(30)\n    inflate         1.200\n    alpha           0.010\nFinancialOption (AbstractIntegrand)\n    option          ASIAN\n    call_put        CALL\n    volatility      2^(-1)\n    start_price     30\n    strike_price    35\n    interest_rate   0\n    t_final         1\n    asian_mean      ARITHMETIC\nBrownianMotion (AbstractTrueMeasure)\n    time_vec        [0.019 0.038 0.058 ... 0.962 0.981 1.   ]\n    drift           0\n    mean            [0. 0. 0. ... 0. 0. 0.]\n    covariance      [[0.019 0.019 0.019 ... 0.019 0.019 0.019]\n                     [0.019 0.038 0.038 ... 0.038 0.038 0.038]\n                     [0.019 0.038 0.058 ... 0.058 0.058 0.058]\n                     ...\n                     [0.019 0.038 0.058 ... 0.962 0.962 0.962]\n                     [0.019 0.038 0.058 ... 0.962 0.981 0.981]\n                     [0.019 0.038 0.058 ... 0.962 0.981 1.   ]]\n    decomp_type     PCA\nIIDStdUniform (AbstractIIDDiscreteDistribution)\n    d               52\n    replications    1\n    entropy         7\n</code></pre> <p>Control variates </p> <pre><code>&gt;&gt;&gt; iid = IIDStdUniform(52,seed=7)\n&gt;&gt;&gt; ao = FinancialOption(iid,option=\"ASIAN\")\n&gt;&gt;&gt; eo = FinancialOption(iid,option=\"EUROPEAN\")\n&gt;&gt;&gt; lin0 = Linear0(iid)\n&gt;&gt;&gt; sc = CubMCCLT(ao,abs_tol=.05,\n...     control_variates = [eo,lin0],\n...     control_variate_means = [eo.get_exact_value(),0])\n&gt;&gt;&gt; solution,data = sc.integrate()\n&gt;&gt;&gt; data\nData (Data)\n    solution        1.790\n    bound_low       1.738\n    bound_high      1.843\n    bound_diff      0.104\n    n_total         27777\n    time_integrate  ...\nCubMCCLT (AbstractStoppingCriterion)\n    abs_tol         0.050\n    rel_tol         0\n    n_init          2^(10)\n    n_limit         2^(30)\n    inflate         1.200\n    alpha           0.010\n    cv              [FinancialOption (AbstractIntegrand)\n                         option          EUROPEAN\n                         call_put        CALL\n                         volatility      2^(-1)\n                         start_price     30\n                         strike_price    35\n                         interest_rate   0\n                         t_final         1               Linear0 (AbstractIntegrand)]\n    cv_mu           [4.211 0.   ]\nFinancialOption (AbstractIntegrand)\n    option          ASIAN\n    call_put        CALL\n    volatility      2^(-1)\n    start_price     30\n    strike_price    35\n    interest_rate   0\n    t_final         1\n    asian_mean      ARITHMETIC\nBrownianMotion (AbstractTrueMeasure)\n    time_vec        [0.019 0.038 0.058 ... 0.962 0.981 1.   ]\n    drift           0\n    mean            [0. 0. 0. ... 0. 0. 0.]\n    covariance      [[0.019 0.019 0.019 ... 0.019 0.019 0.019]\n                     [0.019 0.038 0.038 ... 0.038 0.038 0.038]\n                     [0.019 0.038 0.058 ... 0.058 0.058 0.058]\n                     ...\n                     [0.019 0.038 0.058 ... 0.962 0.962 0.962]\n                     [0.019 0.038 0.058 ... 0.962 0.981 0.981]\n                     [0.019 0.038 0.058 ... 0.962 0.981 1.   ]]\n    decomp_type     PCA\nIIDStdUniform (AbstractIIDDiscreteDistribution)\n    d               52\n    replications    1\n    entropy         7\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>integrand</code> <code>AbstractIntegrand</code> <p>The integrand.</p> required <code>abs_tol</code> <code>ndarray</code> <p>Absolute error tolerance.</p> <code>0.01</code> <code>rel_tol</code> <code>ndarray</code> <p>Relative error tolerance.</p> <code>0.0</code> <code>n_init</code> <code>int</code> <p>Initial number of samples. </p> <code>1024</code> <code>n_limit</code> <code>int</code> <p>Maximum number of samples.</p> <code>2 ** 30</code> <code>inflate</code> <code>float</code> <p>Inflation factor \\(\\geq 1\\) to multiply by the variance estimate to make it more conservative.</p> <code>1.2</code> <code>alpha</code> <code>ndarray</code> <p>Uncertainty level in \\((0,1)\\). </p> <code>0.01</code> <code>control_variates</code> <code>list</code> <p>Integrands to use as control variates, each with the same underlying discrete distribution instance.</p> <code>[]</code> <code>control_variate_means</code> <code>ndarray</code> <p>Means of each control variate.</p> <code>[]</code> Source code in <code>qmcpy/stopping_criterion/cub_mc_clt.py</code> <pre><code>def __init__(self, \n             integrand, \n             abs_tol = 1e-2, \n             rel_tol = 0., \n             n_init = 1024, \n             n_limit = 2**30,\n             inflate = 1.2, \n             alpha = 0.01, \n             control_variates = [], \n             control_variate_means = [],\n             ):\n    r\"\"\"\n    Args:\n        integrand (AbstractIntegrand): The integrand.\n        abs_tol (np.ndarray): Absolute error tolerance.\n        rel_tol (np.ndarray): Relative error tolerance.\n        n_init (int): Initial number of samples. \n        n_limit (int): Maximum number of samples.\n        inflate (float): Inflation factor $\\geq 1$ to multiply by the variance estimate to make it more conservative.\n        alpha (np.ndarray): Uncertainty level in $(0,1)$. \n        control_variates (list): Integrands to use as control variates, each with the same underlying discrete distribution instance.\n        control_variate_means (np.ndarray): Means of each control variate. \n    \"\"\"\n    self.parameters = ['abs_tol','rel_tol','n_init','n_limit','inflate','alpha']\n    # Set Attributes\n    self.abs_tol = abs_tol\n    self.rel_tol = rel_tol\n    self.n_init = n_init\n    self.n_limit = n_limit\n    assert self.n_limit&gt;(2*self.n_init), \"require n_limit is at least twic as much as n_init\"\n    self.alpha = alpha\n    self.inflate = inflate\n    assert self.inflate&gt;=1\n    assert 0&lt;self.alpha&lt;1\n    # QMCPy Objs\n    self.integrand = integrand\n    self.true_measure = self.integrand.true_measure\n    self.discrete_distrib = self.integrand.discrete_distrib\n    super(CubMCCLT,self).__init__(allowed_distribs=[AbstractIIDDiscreteDistribution],allow_vectorized_integrals=True)\n    assert self.integrand.d_indv==()\n    # control variates\n    self.cv_mu = np.atleast_1d(control_variate_means)\n    self.cv = control_variates\n    if isinstance(self.cv,AbstractIntegrand):\n        self.cv = [self.cv]\n        self.cv_mu = self.cv_mu[None,...]\n    assert isinstance(self.cv,list), \"cv must be a list of AbstractIntegrand objects\"\n    for cv in self.cv:\n        if (not isinstance(cv,AbstractIntegrand)) or (cv.discrete_distrib!=self.discrete_distrib) or (cv.d_indv!=self.integrand.d_indv):\n            raise ParameterError('''\n                    Each control variates discrete distribution must be an AbstractIntegrand instance \n                    with the same discrete distribution as the main integrand. d_indv must also match \n                    that of the main integrand instance for each control variate.''')\n    self.ncv = len(self.cv)\n    if self.ncv&gt;0:\n        assert self.cv_mu.shape==((self.ncv,)+self.integrand.d_indv), \"Control variate means should have shape (len(control variates),d_indv).\"\n        self.parameters += ['cv','cv_mu']\n    self.z_star = -norm.ppf(self.alpha/2.)\n</code></pre>"},{"location":"api/stopping_criteria/#multilevel-qmc","title":"Multilevel QMC","text":""},{"location":"api/stopping_criteria/#cubmlqmccont","title":"<code>CubMLQMCCont</code>","text":"<p>               Bases: <code>AbstractCubMLQMC</code></p> <p>Multilevel Quasi-Monte Carlo stopping criterion with continuation.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; fo = FinancialOption(DigitalNetB2(seed=7,replications=32))\n&gt;&gt;&gt; sc = CubMLQMCCont(fo,abs_tol=1e-3)\n&gt;&gt;&gt; solution,data = sc.integrate()\n&gt;&gt;&gt; data\nData (Data)\n    solution        1.784\n    n_total         4718592\n    levels          2^(2)\n    n_level         [65536 32768 32768 16384]\n    mean_level      [1.718 0.051 0.012 0.003]\n    var_level       [1.169e-08 2.569e-08 1.850e-08 5.209e-08]\n    bias_estimate   2.78e-04\n    time_integrate  ...\nCubMLQMCCont (AbstractStoppingCriterion)\n    rmse_tol        3.88e-04\n    n_init          2^(8)\n    n_limit         10000000000\n    replications    2^(5)\n    levels_min      2^(1)\n    levels_max      10\n    n_tols          10\n    inflate         1.668\n    theta_init      2^(-1)\n    theta           2^(-3)\nFinancialOption (AbstractIntegrand)\n    option          ASIAN\n    call_put        CALL\n    volatility      2^(-1)\n    start_price     30\n    strike_price    35\n    interest_rate   0\n    t_final         1\n    asian_mean      ARITHMETIC\nBrownianMotion (AbstractTrueMeasure)\n    time_vec        1\n    drift           0\n    mean            0\n    covariance      1\n    decomp_type     PCA\nDigitalNetB2 (AbstractLDDiscreteDistribution)\n    d               1\n    replications    2^(5)\n    randomize       LMS DS\n    gen_mats_source joe_kuo.6.21201.txt\n    order           RADICAL INVERSE\n    t               63\n    alpha           1\n    n_limit         2^(32)\n    entropy         7\n</code></pre> <p>References:</p> <ol> <li>https://github.com/PieterjanRobbe/MultilevelEstimators.jl.</li> </ol> <p>Parameters:</p> Name Type Description Default <code>integrand</code> <code>AbstractIntegrand</code> <p>The integrand.</p> required <code>abs_tol</code> <code>ndarray</code> <p>Absolute error tolerance.</p> <code>0.05</code> <code>rmse_tol</code> <code>ndarray</code> <p>Root mean squared error tolerance.  If supplied, then absolute tolerance and alpha are ignored in favor of the rmse tolerance. </p> <code>None</code> <code>n_init</code> <code>int</code> <p>Initial number of samples. </p> <code>256</code> <code>n_limit</code> <code>int</code> <p>Maximum number of samples.</p> <code>10000000000.0</code> <code>inflate</code> <code>float</code> <p>Coarser tolerance multiplication factor \\(\\geq 1\\).</p> <code>100 ** (1 / 9)</code> <code>alpha</code> <code>ndarray</code> <p>Uncertainty level in \\((0,1)\\). </p> <code>0.01</code> <code>levels_min</code> <code>int</code> <p>Minimum level of refinement \\(\\geq 2\\).</p> <code>2</code> <code>levels_max</code> <code>int</code> <p>Maximum level of refinement \\(\\geq\\) <code>levels_min</code>.</p> <code>10</code> <code>n_tols</code> <code>int</code> <p>Number of coarser tolerances to run.</p> <code>10</code> <code>theta_init</code> <code>float</code> <p>Initial error splitting constant.</p> <code>0.5</code> Source code in <code>qmcpy/stopping_criterion/cub_mlqmc_cont.py</code> <pre><code>def __init__(self, \n             integrand, \n             abs_tol = .05, \n             rmse_tol = None,\n             n_init = 256,\n             n_limit = 1e10,\n             inflate = 100**(1/9),\n             alpha = .01,   \n             levels_min = 2, \n             levels_max = 10, \n             n_tols = 10, \n             theta_init = 0.5,\n             ):\n    r\"\"\"\n    Args:\n        integrand (AbstractIntegrand): The integrand.\n        abs_tol (np.ndarray): Absolute error tolerance.\n        rmse_tol (np.ndarray): Root mean squared error tolerance. \n            If supplied, then absolute tolerance and alpha are ignored in favor of the rmse tolerance. \n        n_init (int): Initial number of samples. \n        n_limit (int): Maximum number of samples.\n        inflate (float): Coarser tolerance multiplication factor $\\geq 1$.\n        alpha (np.ndarray): Uncertainty level in $(0,1)$. \n        levels_min (int): Minimum level of refinement $\\geq 2$.\n        levels_max (int): Maximum level of refinement $\\geq$ `levels_min`.\n        n_tols (int): Number of coarser tolerances to run.\n        theta_init (float): Initial error splitting constant.\n    \"\"\"\n    self.parameters = ['rmse_tol','n_init','n_limit','replications','levels_min',\n        'levels_max','n_tols','inflate','theta_init','theta']\n    # initialization\n    if rmse_tol:\n        self.target_tol = float(rmse_tol)\n    else: # use absolute tolerance\n        self.target_tol =  float(abs_tol) / norm.ppf(1-alpha/2)\n    self.n_init = n_init\n    self.n_limit = n_limit\n    self.levels_min = levels_min\n    self.levels_max = levels_max\n    self.theta_init = theta_init\n    self.theta = theta_init\n    self.n_tols = n_tols\n    self.alpha = alpha\n    self.inflate = inflate\n    assert self.inflate&gt;=1\n    assert 0&lt;self.alpha&lt;1\n    # QMCPy Objs\n    self.integrand = integrand\n    self.true_measure = self.integrand.true_measure\n    self.discrete_distrib = self.integrand.discrete_distrib\n    super(CubMLQMCCont,self).__init__(allowed_distribs=[AbstractLDDiscreteDistribution],allow_vectorized_integrals=False)\n    self.replications = self.discrete_distrib.replications \n    assert self.replications&gt;=4, \"require at least 4 replications\"\n</code></pre>"},{"location":"api/stopping_criteria/#cubmlqmc","title":"<code>CubMLQMC</code>","text":"<p>               Bases: <code>AbstractCubMLQMC</code></p> <p>Multilevel Quasi-Monte Carlo stopping criterion.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; fo = FinancialOption(DigitalNetB2(seed=7,replications=32))\n&gt;&gt;&gt; sc = CubMLQMC(fo,abs_tol=3e-3)\n&gt;&gt;&gt; solution,data = sc.integrate()\n&gt;&gt;&gt; data\nData (Data)\n    solution        1.784\n    n_total         2359296\n    levels          2^(2)\n    n_level         [32768 16384 16384  8192]\n    mean_level      [1.718 0.051 0.012 0.003]\n    var_level       [7.119e-08 1.409e-07 9.668e-08 1.852e-07]\n    bias_estimate   2.99e-04\n    time_integrate  ...\nCubMLQMC (AbstractStoppingCriterion)\n    rmse_tol        0.001\n    n_init          2^(8)\n    n_limit         10000000000\n    replications    2^(5)\nFinancialOption (AbstractIntegrand)\n    option          ASIAN\n    call_put        CALL\n    volatility      2^(-1)\n    start_price     30\n    strike_price    35\n    interest_rate   0\n    t_final         1\n    asian_mean      ARITHMETIC\nBrownianMotion (AbstractTrueMeasure)\n    time_vec        1\n    drift           0\n    mean            0\n    covariance      1\n    decomp_type     PCA\nDigitalNetB2 (AbstractLDDiscreteDistribution)\n    d               1\n    replications    2^(5)\n    randomize       LMS DS\n    gen_mats_source joe_kuo.6.21201.txt\n    order           RADICAL INVERSE\n    t               63\n    alpha           1\n    n_limit         2^(32)\n    entropy         7\n</code></pre> <p>References:</p> <ol> <li>M.B. Giles and B.J. Waterhouse.     'Multilevel quasi-Monte Carlo path simulation'.     pp.165-181 in Advanced Financial Modelling, in Radon Series on Computational and Applied Mathematics, de Gruyter, 2009. http://people.maths.ox.ac.uk/~gilesm/files/radon.pdf.</li> </ol> <p>Parameters:</p> Name Type Description Default <code>integrand</code> <code>AbstractIntegrand</code> <p>The integrand.</p> required <code>abs_tol</code> <code>ndarray</code> <p>Absolute error tolerance.</p> <code>0.05</code> <code>rmse_tol</code> <code>ndarray</code> <p>Root mean squared error tolerance.  If supplied, then absolute tolerance and alpha are ignored in favor of the rmse tolerance. </p> <code>None</code> <code>n_init</code> <code>int</code> <p>Initial number of samples. </p> <code>256</code> <code>n_limit</code> <code>int</code> <p>Maximum number of samples.</p> <code>10000000000.0</code> <code>alpha</code> <code>ndarray</code> <p>Uncertainty level in \\((0,1)\\). </p> <code>0.01</code> <code>levels_min</code> <code>int</code> <p>Minimum level of refinement \\(\\geq 2\\).</p> <code>2</code> <code>levels_max</code> <code>int</code> <p>Maximum level of refinement \\(\\geq\\) <code>levels_min</code>.</p> <code>10</code> Source code in <code>qmcpy/stopping_criterion/cub_mlqmc.py</code> <pre><code>def __init__(self, \n             integrand, \n             abs_tol = .05, \n             rmse_tol = None, \n             n_init = 256, \n             n_limit = 1e10,\n             alpha = .01, \n             levels_min = 2,\n             levels_max = 10,\n             ):\n    r\"\"\"\n    Args:\n        integrand (AbstractIntegrand): The integrand.\n        abs_tol (np.ndarray): Absolute error tolerance.\n        rmse_tol (np.ndarray): Root mean squared error tolerance. \n            If supplied, then absolute tolerance and alpha are ignored in favor of the rmse tolerance. \n        n_init (int): Initial number of samples. \n        n_limit (int): Maximum number of samples.\n        alpha (np.ndarray): Uncertainty level in $(0,1)$. \n        levels_min (int): Minimum level of refinement $\\geq 2$.\n        levels_max (int): Maximum level of refinement $\\geq$ `levels_min`.\n    \"\"\"\n    self.parameters = ['rmse_tol','n_init','n_limit','replications']\n    # initialization\n    if rmse_tol:\n        self.rmse_tol = float(rmse_tol)\n    else: # use absolute tolerance\n        self.rmse_tol =  float(abs_tol) / norm.ppf(1-alpha/2)\n    self.alpha = alpha \n    assert 0&lt;self.alpha&lt;1\n    self.n_init = n_init\n    self.n_limit = n_limit\n    self.levels_min = levels_min\n    self.levels_max = levels_max\n    # QMCPy Objs\n    self.integrand = integrand\n    self.true_measure = self.integrand.true_measure\n    self.discrete_distrib = self.integrand.discrete_distrib\n    super(CubMLQMC,self).__init__(allowed_distribs=[AbstractLDDiscreteDistribution],allow_vectorized_integrals=False)\n    self.replications = self.discrete_distrib.replications \n    assert self.replications&gt;=4, \"require at least 4 replications\"\n</code></pre>"},{"location":"api/stopping_criteria/#multilevel-iid-mc","title":"Multilevel IID MC","text":""},{"location":"api/stopping_criteria/#cubmlmccont","title":"<code>CubMLMCCont</code>","text":"<p>               Bases: <code>AbstractCubMLMC</code></p> <p>Multilevel IID Monte Carlo stopping criterion with continuation.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; fo = FinancialOption(IIDStdUniform(seed=7))\n&gt;&gt;&gt; sc = CubMLMCCont(fo,abs_tol=1.5e-2)\n&gt;&gt;&gt; solution,data = sc.integrate()\n&gt;&gt;&gt; data\nData (Data)\n    solution        1.771\n    n_total         2291120\n    levels          3\n    n_level         [1094715  222428   79666     912     256]\n    mean_level      [1.71  0.048 0.012]\n    var_level       [21.826  1.768  0.453]\n    cost_per_sample [2. 4. 8.]\n    alpha           1.970\n    beta            1.965\n    gamma           1.000\n    time_integrate  ...\nCubMLMCCont (AbstractStoppingCriterion)\n    rmse_tol        0.006\n    n_init          2^(8)\n    levels_min      2^(1)\n    levels_max      10\n    n_tols          10\n    inflate         1.668\n    theta_init      2^(-1)\n    theta           0.010\nFinancialOption (AbstractIntegrand)\n    option          ASIAN\n    call_put        CALL\n    volatility      2^(-1)\n    start_price     30\n    strike_price    35\n    interest_rate   0\n    t_final         1\n    asian_mean      ARITHMETIC\nBrownianMotion (AbstractTrueMeasure)\n    time_vec        1\n    drift           0\n    mean            0\n    covariance      1\n    decomp_type     PCA\nIIDStdUniform (AbstractIIDDiscreteDistribution)\n    d               1\n    replications    1\n    entropy         7\n</code></pre> <p>References:</p> <ol> <li>https://github.com/PieterjanRobbe/MultilevelEstimators.jl.</li> </ol> <p>Parameters:</p> Name Type Description Default <code>integrand</code> <code>AbstractIntegrand</code> <p>The integrand.</p> required <code>abs_tol</code> <code>ndarray</code> <p>Absolute error tolerance.</p> <code>0.05</code> <code>rmse_tol</code> <code>ndarray</code> <p>Root mean squared error tolerance.  If supplied, then absolute tolerance and alpha are ignored in favor of the rmse tolerance. </p> <code>None</code> <code>n_init</code> <code>int</code> <p>Initial number of samples. </p> <code>256</code> <code>n_limit</code> <code>int</code> <p>Maximum number of samples.</p> <code>10000000000.0</code> <code>inflate</code> <code>float</code> <p>Coarser tolerance multiplication factor \\(\\geq 1\\).</p> <code>100 ** (1 / 9)</code> <code>alpha</code> <code>ndarray</code> <p>Uncertainty level in \\((0,1)\\). </p> <code>0.01</code> <code>levels_min</code> <code>int</code> <p>Minimum level of refinement \\(\\geq 2\\).</p> <code>2</code> <code>levels_max</code> <code>int</code> <p>Maximum level of refinement \\(\\geq\\) <code>levels_min</code>.</p> <code>10</code> <code>n_tols</code> <code>int</code> <p>Number of coarser tolerances to run.</p> <code>10</code> <code>theta_init</code> <code>float</code> <p>Initial error splitting constant.</p> <code>0.5</code> Source code in <code>qmcpy/stopping_criterion/cub_mlmc_cont.py</code> <pre><code>def __init__(self, \n             integrand, \n             abs_tol = .05, \n             rmse_tol = None, \n             n_init = 256, \n             n_limit = 1e10,\n             inflate = 100**(1/9),\n             alpha = .01, \n             levels_min = 2,\n             levels_max = 10, \n             n_tols = 10, \n             theta_init = 0.5,\n             ):\n    r\"\"\"\n    Args:\n        integrand (AbstractIntegrand): The integrand.\n        abs_tol (np.ndarray): Absolute error tolerance.\n        rmse_tol (np.ndarray): Root mean squared error tolerance. \n            If supplied, then absolute tolerance and alpha are ignored in favor of the rmse tolerance. \n        n_init (int): Initial number of samples. \n        n_limit (int): Maximum number of samples.\n        inflate (float): Coarser tolerance multiplication factor $\\geq 1$.\n        alpha (np.ndarray): Uncertainty level in $(0,1)$. \n        levels_min (int): Minimum level of refinement $\\geq 2$.\n        levels_max (int): Maximum level of refinement $\\geq$ `levels_min`.\n        n_tols (int): Number of coarser tolerances to run.\n        theta_init (float): Initial error splitting constant.\n    \"\"\"\n    self.parameters = ['rmse_tol','n_init','levels_min','levels_max','n_tols',\n        'inflate','theta_init','theta']\n    if levels_min &lt; 2:\n        raise ParameterError('needs levels_min &gt;= 2')\n    if levels_max &lt; levels_min:\n        raise ParameterError('needs levels_max &gt;= levels_min')\n    if n_init &lt;= 0:\n        raise ParameterError('needs n_init &gt; 0')\n    # initialization\n    if rmse_tol:\n        self.target_tol = float(rmse_tol)\n    else: # use absolute tolerance\n        self.target_tol =  float(abs_tol) / norm.ppf(1-alpha/2)\n    self.n_init = n_init\n    self.n_limit = n_limit\n    self.levels_min = levels_min\n    self.levels_max = levels_max\n    self.theta_init = theta_init\n    self.theta = theta_init\n    self.n_tols = n_tols\n    self.inflate = inflate\n    self.integrand = integrand\n    self.true_measure = self.integrand.true_measure\n    self.discrete_distrib = self.integrand.discrete_distrib\n    self.alpha0 = -1 \n    self.beta0 = -1 \n    self.gamma0 = -1\n    self.alpha = alpha\n    self.inflate = inflate\n    assert self.inflate&gt;=1\n    assert 0&lt;self.alpha&lt;1\n    super(CubMLMCCont,self).__init__(allowed_distribs=[AbstractIIDDiscreteDistribution],allow_vectorized_integrals=False)\n</code></pre>"},{"location":"api/stopping_criteria/#cubmlmc","title":"<code>CubMLMC</code>","text":"<p>               Bases: <code>AbstractCubMLMC</code></p> <p>Multilevel IID Monte Carlo stopping criterion.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; fo = FinancialOption(IIDStdUniform(seed=7))\n&gt;&gt;&gt; sc = CubMLMC(fo,abs_tol=1.5e-2)\n&gt;&gt;&gt; solution,data = sc.integrate()\n&gt;&gt;&gt; data\nData (Data)\n    solution        1.785\n    n_total         3577556\n    levels          2^(2)\n    n_level         [2438191  490331  207606   62905]\n    mean_level      [1.715 0.053 0.013 0.003]\n    var_level       [21.829  1.761  0.452  0.11 ]\n    cost_per_sample [ 2.  4.  8. 16.]\n    alpha           2.008\n    beta            1.997\n    gamma           1.000\n    time_integrate  ...\nCubMLMC (AbstractStoppingCriterion)\n    rmse_tol        0.006\n    n_init          2^(8)\n    levels_min      2^(1)\n    levels_max      10\n    theta           2^(-1)\nFinancialOption (AbstractIntegrand)\n    option          ASIAN\n    call_put        CALL\n    volatility      2^(-1)\n    start_price     30\n    strike_price    35\n    interest_rate   0\n    t_final         1\n    asian_mean      ARITHMETIC\nBrownianMotion (AbstractTrueMeasure)\n    time_vec        1\n    drift           0\n    mean            0\n    covariance      1\n    decomp_type     PCA\nIIDStdUniform (AbstractIIDDiscreteDistribution)\n    d               1\n    replications    1\n    entropy         7\n</code></pre> <p>References:</p> <ol> <li> <p>M.B. Giles. 'Multi-level Monte Carlo path simulation'.      Operations Research, 56(3):607-617, 2008. http://people.maths.ox.ac.uk/~gilesm/files/OPRE_2008.pdf.</p> </li> <li> <p>http://people.maths.ox.ac.uk/~gilesm/mlmc/#MATLAB.</p> </li> </ol> <p>Parameters:</p> Name Type Description Default <code>integrand</code> <code>AbstractIntegrand</code> <p>The integrand.</p> required <code>abs_tol</code> <code>ndarray</code> <p>Absolute error tolerance.</p> <code>0.05</code> <code>rmse_tol</code> <code>ndarray</code> <p>Root mean squared error tolerance.  If supplied, then absolute tolerance and alpha are ignored in favor of the rmse tolerance. </p> <code>None</code> <code>n_init</code> <code>int</code> <p>Initial number of samples. </p> <code>256</code> <code>n_limit</code> <code>int</code> <p>Maximum number of samples.</p> <code>10000000000.0</code> <code>alpha</code> <code>ndarray</code> <p>Uncertainty level in \\((0,1)\\). </p> <code>0.01</code> <code>levels_min</code> <code>int</code> <p>Minimum level of refinement \\(\\geq 2\\).</p> <code>2</code> <code>levels_max</code> <code>int</code> <p>Maximum level of refinement \\(\\geq\\) <code>levels_min</code>.</p> <code>10</code> <code>alpha0</code> <code>float</code> <p>Weak error is \\(\\mathcal{O}(2^{-\\alpha_0\\ell})\\) in the level \\(\\ell\\). If <code>alpha0</code>\\(\\leq 0\\) then it will be estimated. </p> <code>-1.0</code> <code>beta0</code> <code>float</code> <p>Variance is \\(\\mathcal{O}(2^{-\\beta_0\\ell})\\) in the level \\(\\ell\\). If <code>beta0</code>\\(\\leq 0\\) then it will be estimated. </p> <code>-1.0</code> <code>gamma0</code> <code>float</code> <p>Sample cost is \\(\\mathcal{O}(2^{\\gamma_0\\ell})\\) in the level \\(\\ell\\). If <code>gamma0</code>\\(\\leq 0\\) then it will be estimated.</p> <code>-1.0</code> Source code in <code>qmcpy/stopping_criterion/cub_mlmc.py</code> <pre><code>def __init__(self, \n             integrand, \n             abs_tol = .05, \n             rmse_tol = None, \n             n_init = 256, \n             n_limit = 1e10, \n             alpha = .01, \n             levels_min = 2, \n             levels_max = 10, \n             alpha0 = -1., \n             beta0 = -1., \n             gamma0 = -1.,\n             ):\n    r\"\"\"\n    Args:\n        integrand (AbstractIntegrand): The integrand.\n        abs_tol (np.ndarray): Absolute error tolerance.\n        rmse_tol (np.ndarray): Root mean squared error tolerance. \n            If supplied, then absolute tolerance and alpha are ignored in favor of the rmse tolerance. \n        n_init (int): Initial number of samples. \n        n_limit (int): Maximum number of samples.\n        alpha (np.ndarray): Uncertainty level in $(0,1)$. \n        levels_min (int): Minimum level of refinement $\\geq 2$.\n        levels_max (int): Maximum level of refinement $\\geq$ `levels_min`.\n        alpha0 (float): Weak error is $\\mathcal{O}(2^{-\\alpha_0\\ell})$ in the level $\\ell$. If `alpha0`$\\leq 0$ then it will be estimated. \n        beta0 (float): Variance is $\\mathcal{O}(2^{-\\beta_0\\ell})$ in the level $\\ell$. If `beta0`$\\leq 0$ then it will be estimated. \n        gamma0 (float): Sample cost is $\\mathcal{O}(2^{\\gamma_0\\ell})$ in the level $\\ell$. If `gamma0`$\\leq 0$ then it will be estimated. \n    \"\"\"\n    self.parameters = ['rmse_tol','n_init','levels_min','levels_max','theta']\n    if levels_min &lt; 2:\n        raise ParameterError('needs levels_min &gt;= 2')\n    if levels_max &lt; levels_min:\n        raise ParameterError('needs levels_max &gt;= levels_min')\n    if n_init &lt;= 0:\n        raise ParameterError('needs n_init &gt; 0')\n    # initialization\n    if rmse_tol:\n        self.rmse_tol = float(rmse_tol)\n    else: # use absolute tolerance\n        self.rmse_tol =  float(abs_tol) / norm.ppf(1-alpha/2)\n    self.alpha = alpha \n    assert 0&lt;self.alpha&lt;1\n    self.n_init = n_init\n    self.n_limit = n_limit\n    self.levels_min = levels_min\n    self.levels_max = levels_max\n    self.theta = 0.5\n    self.alpha0 = alpha0\n    self.beta0 = beta0\n    self.gamma0 = gamma0\n    # QMCPy Objs\n    self.integrand = integrand\n    self.true_measure = self.integrand.true_measure\n    self.discrete_distrib = self.integrand.discrete_distrib\n    super(CubMLMC,self).__init__(allowed_distribs=[AbstractIIDDiscreteDistribution], allow_vectorized_integrals=False)\n</code></pre>"},{"location":"api/stopping_criteria/#failure-probability","title":"Failure Probability","text":""},{"location":"api/stopping_criteria/#pfgpci","title":"<code>PFGPCI</code>","text":"<p>               Bases: <code>AbstractStoppingCriterion</code></p> <p>Probability of failure estimation using adaptive Gaussian process construction and resulting credible intervals. </p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; pfgpci = PFGPCI(\n...     integrand = Ishigami(DigitalNetB2(3,seed=17)),\n...     failure_threshold = 0,\n...     failure_above_threshold = False, \n...     abs_tol = 2.5e-2,\n...     alpha = 1e-1,\n...     n_init = 64,\n...     init_samples = None,\n...     batch_sampler = PFSampleErrorDensityAR(verbose=False),\n...     n_batch = 16,\n...     n_limit = 128,\n...     n_approx = 2**18,\n...     gpytorch_prior_mean = gpytorch.means.ZeroMean(),\n...     gpytorch_prior_cov = gpytorch.kernels.ScaleKernel(\n...         gpytorch.kernels.MaternKernel(nu=2.5,lengthscale_constraint = gpytorch.constraints.Interval(.5,1)),\n...         outputscale_constraint = gpytorch.constraints.Interval(1e-8,.5)),\n...     gpytorch_likelihood = gpytorch.likelihoods.GaussianLikelihood(noise_constraint = gpytorch.constraints.Interval(1e-12,1e-8)),\n...     gpytorch_marginal_log_likelihood_func = lambda likelihood,gpyt_model: gpytorch.mlls.ExactMarginalLogLikelihood(likelihood,gpyt_model),\n...     torch_optimizer_func = lambda gpyt_model: torch.optim.Adam(gpyt_model.parameters(),lr=0.1),\n...     gpytorch_train_iter = 100,\n...     gpytorch_use_gpu = False,\n...     verbose = False,\n...     n_ref_approx = 2**22,\n...     seed_ref_approx = 11)\n&gt;&gt;&gt; solution,data = pfgpci.integrate(seed=7,refit=True)\n&gt;&gt;&gt; data\nPFGPCIData (Data)\n    solution        0.158\n    error_bound     0.022\n    bound_low       0.136\n    bound_high      0.180\n    n_total         112\n    time_integrate  ...\nPFGPCI (AbstractStoppingCriterion)\n    abs_tol         0.025\n    n_init          2^(6)\n    n_limit         2^(7)\n    n_batch         2^(4)\nIshigami (AbstractIntegrand)\nUniform (AbstractTrueMeasure)\n    lower_bound     -3.142\n    upper_bound     3.142\nDigitalNetB2 (AbstractLDDiscreteDistribution)\n    d               3\n    replications    1\n    randomize       LMS DS\n    gen_mats_source joe_kuo.6.21201.txt\n    order           RADICAL INVERSE\n    t               63\n    alpha           1\n    n_limit         2^(32)\n    entropy         17\n&gt;&gt;&gt; df = data.get_results_dict()\n&gt;&gt;&gt; with np.printoptions(formatter={\"float\": lambda x: \"%-10.2e\"%x, \"int\": lambda x: \"%-10d\"%x, \"bool\": lambda x: \"%-10s\"%x}):\n...     for k,v in df.items():\n...         print(\"%15s: %s\"%(k,str(v)))\n           iter: [0          1          2          3         ]\n          n_sum: [64         80         96         112       ]\n        n_batch: [64         16         16         16        ]\n   error_bounds: [5.58e-02   3.92e-02   3.05e-02   2.16e-02  ]\n         ci_low: [8.66e-02   1.16e-01   1.19e-01   1.36e-01  ]\n        ci_high: [1.98e-01   1.95e-01   1.80e-01   1.80e-01  ]\n      solutions: [1.42e-01   1.55e-01   1.50e-01   1.58e-01  ]\n  solutions_ref: [1.62e-01   1.62e-01   1.62e-01   1.62e-01  ]\n      error_ref: [2.01e-02   7.02e-03   1.28e-02   4.52e-03  ]\n          in_ci: [True       True       True       True      ]\n</code></pre> <p>References:</p> <ol> <li>Sorokin, Aleksei G., and Vishwas Rao.     \"Credible Intervals for Probability of Failure with Gaussian Processes.\"     arXiv preprint arXiv:2311.07733 (2023).</li> </ol> <p>Parameters:</p> Name Type Description Default <code>integrand</code> <code>AbstractIntegrand</code> <p>The integrand.</p> required <code>failure_threshold</code> <code>float</code> <p>Thresholds for failure. </p> required <code>failure_above_threshold</code> <code>bool</code> <p>Set to <code>True</code> if failure occurs when the simulation exceeds <code>failure_threshold</code> and False otherwise.</p> required <code>abs_tol</code> <code>float</code> <p>The desired maximum distance from the estimate to either end of the credible interval.</p> <code>0.005</code> <code>n_init</code> <code>float</code> <p>Initial number of samples from integrand.discrete_distrib from which to build the first surrogate GP</p> <code>64</code> <code>n_limit</code> <code>int</code> <p>Budget of simulations.</p> <code>1000</code> <code>n_batch</code> <code>int</code> <p>The number of samples per batch to draw from batch_sampler. </p> <code>4</code> <code>alpha</code> <code>float</code> <p>The credible interval is constructed to hold with probability at least 1 - alpha</p> <code>0.01</code> <code>init_samples</code> <code>float</code> <p>If the simulation has already been run, pass in (x,y) where x are past samples from the discrete distribution and y are corresponding simulation evaluations. </p> <code>None</code> <code>batch_sampler</code> <code>Suggester or AbstractDiscreteDistribution</code> <p>A suggestion scheme for future samples. </p> <code>PFSampleErrorDensityAR()</code> <code>n_approx</code> <code>int</code> <p>Number of points from integrand.discrete_distrib used to approximate estimate and credible interval bounds</p> <code>2 ** 20</code> <code>gpytorch_prior_mean</code> <code>means</code> <p>prior mean function of the GP</p> <code>ZeroMean()</code> <code>gpytorch_prior_cov</code> <code>kernels</code> <p>Prior covariance kernel of the GP</p> <code>ScaleKernel(MaternKernel(nu=2.5))</code> <code>gpytorch_likelihood</code> <code>likelihoods</code> <p>GP likelihood, require one of gpytorch.likelihoods.{GaussianLikelihood, GaussianLikelihoodWithMissingObs, FixedNoiseGaussianLikelihood}</p> <code>GaussianLikelihood(noise_constraint=Interval(1e-12, 1e-08))</code> <code>gpytorch_marginal_log_likelihood_func</code> <code>callable</code> <p>Function taking in the likelihood and gpytorch model and returning a marginal log likelihood from gpytorch.mlls</p> <code>lambda likelihood, gpyt_model: ExactMarginalLogLikelihood(likelihood, gpyt_model)</code> <code>torch_optimizer_func</code> <code>callable</code> <p>Function taking in the gpytorch model and returning an optimizer from torch.optim</p> <code>lambda gpyt_model: Adam(parameters(), lr=0.1)</code> <code>gpytorch_train_iter</code> <code>int</code> <p>Training iterations for the GP in gpytorch</p> <code>100</code> <code>gpytorch_use_gpu</code> <code>bool</code> <p>If True, have gpytorch use a GPU for fitting and trining the GP</p> <code>False</code> <code>verbose</code> <code>int</code> <p>If verbose &gt; 0, print information through the call to integrate()</p> <code>False</code> <code>n_ref_approx</code> <code>int</code> <p>If n_ref_approx &gt; 0, use n_ref_approx points to get a reference QMC approximation of the true solution.  Caution: If n_ref_approx &gt; 0, it should be a large int e.g. 2**22, in which case it is only helpful for cheap to evaluate simulations </p> <code>2 ** 22</code> <code>seed_ref_approx</code> <code>int</code> <p>Seed for the reference approximation. Only applies when n_ref_approx&gt;0</p> <code>None</code> Source code in <code>qmcpy/stopping_criterion/pf_gp_ci.py</code> <pre><code>def __init__(self, \n    integrand,\n    failure_threshold,\n    failure_above_threshold,\n    abs_tol = 5e-3,\n    n_init = 64,\n    n_limit = 1000,\n    alpha = 1e-2,\n    init_samples = None,\n    batch_sampler = PFSampleErrorDensityAR(),\n    n_batch = 4,\n    n_approx = 2**20,\n    gpytorch_prior_mean = gpytorch.means.ZeroMean(),\n    gpytorch_prior_cov = gpytorch.kernels.ScaleKernel(gpytorch.kernels.MaternKernel(nu = 2.5)),\n    gpytorch_likelihood = gpytorch.likelihoods.GaussianLikelihood(noise_constraint = gpytorch.constraints.Interval(1e-12,1e-8)),\n    gpytorch_marginal_log_likelihood_func = lambda likelihood,gpyt_model: gpytorch.mlls.ExactMarginalLogLikelihood(likelihood,gpyt_model),\n    torch_optimizer_func = lambda gpyt_model: torch.optim.Adam(gpyt_model.parameters(),lr=0.1),\n    gpytorch_train_iter = 100,\n    gpytorch_use_gpu = False,\n    verbose = False,\n    n_ref_approx = 2**22,\n    seed_ref_approx = None):\n    '''\n    Args:\n        integrand (AbstractIntegrand): The integrand.\n        failure_threshold (float): Thresholds for failure. \n        failure_above_threshold (bool): Set to `True` if failure occurs when the simulation exceeds `failure_threshold` and False otherwise.\n        abs_tol (float): The desired maximum distance from the estimate to either end of the credible interval.\n        n_init (float): Initial number of samples from integrand.discrete_distrib from which to build the first surrogate GP\n        n_limit (int): Budget of simulations.\n        n_batch (int): The number of samples per batch to draw from batch_sampler. \n        alpha (float): The credible interval is constructed to hold with probability at least 1 - alpha\n        init_samples (float): If the simulation has already been run, pass in (x,y) where x are past samples from the discrete distribution and y are corresponding simulation evaluations. \n        batch_sampler (Suggester or AbstractDiscreteDistribution): A suggestion scheme for future samples. \n        n_approx (int): Number of points from integrand.discrete_distrib used to approximate estimate and credible interval bounds\n        gpytorch_prior_mean (gpytorch.means): prior mean function of the GP\n        gpytorch_prior_cov (gpytorch.kernels): Prior covariance kernel of the GP\n        gpytorch_likelihood (gpytorch.likelihoods): GP likelihood, require one of gpytorch.likelihoods.{GaussianLikelihood, GaussianLikelihoodWithMissingObs, FixedNoiseGaussianLikelihood}\n        gpytorch_marginal_log_likelihood_func (callable): Function taking in the likelihood and gpytorch model and returning a marginal log likelihood from gpytorch.mlls\n        torch_optimizer_func (callable): Function taking in the gpytorch model and returning an optimizer from torch.optim\n        gpytorch_train_iter (int): Training iterations for the GP in gpytorch\n        gpytorch_use_gpu (bool): If True, have gpytorch use a GPU for fitting and trining the GP\n        verbose (int): If verbose &gt; 0, print information through the call to integrate()\n        n_ref_approx (int): If n_ref_approx &gt; 0, use n_ref_approx points to get a reference QMC approximation of the true solution. \n            Caution: If n_ref_approx &gt; 0, it should be a large int e.g. 2**22, in which case it is only helpful for cheap to evaluate simulations \n        seed_ref_approx (int): Seed for the reference approximation. Only applies when n_ref_approx&gt;0\n    '''\n    self.parameters = ['abs_tol','n_init','n_limit','n_batch']\n    self.integrand = integrand\n    self.true_measure = self.integrand.true_measure\n    self.discrete_distrib = self.integrand.discrete_distrib\n    self.d = self.integrand.d\n    self.sampler = self.d\n    self.failure_threshold = failure_threshold\n    self.failure_above_threshold = failure_above_threshold\n    self.abs_tol = abs_tol\n    self.alpha = alpha\n    assert 0&lt;self.alpha&lt;1\n    self.n_init = n_init\n    self.init_samples = init_samples is not None\n    if self.init_samples: \n        self.x_init,self.y_init = init_samples\n        assert self.x_init.ndim==2 and self.y_init.ndim==1\n        assert self.x_init.shape[1]==self.d and len(self.y_init)==len(self.x_init)\n        assert self.n_init==len(self.x_init)\n        self.ytf_init = self._affine_tf(self.y_init)\n    self.batch_sampler = batch_sampler\n    self.n_batch = n_batch\n    self.n_limit = n_limit\n    assert self.n_limit &gt;= self.n_init\n    self.n_approx = n_approx\n    assert (self.n_approx+self.n_init) &lt;= 2**32\n    self.gpytorch_prior_mean = gpytorch_prior_mean\n    self.gpytorch_prior_cov = gpytorch_prior_cov\n    self.gpytorch_likelihood = gpytorch_likelihood\n    self.gpytorch_marginal_log_likelihood_func = gpytorch_marginal_log_likelihood_func\n    self.torch_optimizer_func = torch_optimizer_func\n    self.gpytorch_train_iter = gpytorch_train_iter\n    self.gpytorch_use_gpu = gpytorch_use_gpu\n    self.verbose = verbose\n    self.approx_true_solution = n_ref_approx&gt;0\n    if self.approx_true_solution: \n        x = DigitalNetB2(self.d,order=\"GRAY\",seed=seed_ref_approx)(n_ref_approx)\n        y = self.integrand.f(x).squeeze()\n        ytf = self._affine_tf(y)\n        self.ref_approx = (ytf&gt;=0).mean(0)\n        if self.verbose: print('reference approximation with d=%d: %s'%(self.d,self.ref_approx))\n    super(PFGPCI,self).__init__(allowed_distribs=[AbstractDiscreteDistribution],allow_vectorized_integrals=False)\n</code></pre>"},{"location":"api/stopping_criteria/#uml-specific","title":"UML Specific","text":""},{"location":"api/true_measures/","title":"True Measures","text":""},{"location":"api/true_measures/#uml-overview","title":"UML Overview","text":""},{"location":"api/true_measures/#abstracttruemeasure","title":"<code>AbstractTrueMeasure</code>","text":"<p>               Bases: <code>object</code></p> Source code in <code>qmcpy/true_measure/abstract_true_measure.py</code> <pre><code>def __init__(self):\n    prefix = 'A concrete implementation of TrueMeasure must have '\n    if not hasattr(self,'domain'):\n        raise ParameterError(prefix + 'self.domain, 2xd ndarray of domain lower bounds (first col) and upper bounds (second col)')\n    if not hasattr(self,'range'):\n        raise ParameterError(prefix + 'self.range, 2xd ndarray of range lower bounds (first col) and upper bounds (second col)')\n    if not hasattr(self,'parameters'):\n        self.parameters = []\n</code></pre>"},{"location":"api/true_measures/#qmcpy.true_measure.abstract_true_measure.AbstractTrueMeasure.__call__","title":"__call__","text":"<pre><code>__call__(n=None, n_min=None, n_max=None, return_weights=False, warn=True)\n</code></pre> <ul> <li>If just <code>n</code> is supplied, generate samples from the sequence at indices 0,...,<code>n</code>-1.</li> <li>If <code>n_min</code> and <code>n_max</code> are supplied, generate samples from the sequence at indices <code>n_min</code>,...,<code>n_max</code>-1.</li> <li>If <code>n</code> and <code>n_min</code> are supplied, then generate samples from the sequence at indices <code>n</code>,...,<code>n_min</code>-1.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>Union[None, int]</code> <p>Number of points to generate.</p> <code>None</code> <code>n_min</code> <code>Union[None, int]</code> <p>Starting index of sequence.</p> <code>None</code> <code>n_max</code> <code>Union[None, int]</code> <p>Final index of sequence.</p> <code>None</code> <code>return_weights</code> <code>bool</code> <p>If <code>True</code>, return <code>weights</code> as well</p> <code>False</code> <code>warn</code> <code>bool</code> <p>If <code>False</code>, disable warnings when generating samples.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>t</code> <code>ndarray</code> <p>Samples from the sequence. </p> <ul> <li>If <code>replications</code> is <code>None</code> then this will be of size (<code>n_max</code>-<code>n_min</code>) \\(\\times\\) <code>dimension</code> </li> <li>If <code>replications</code> is a positive int, then <code>t</code> will be of size <code>replications</code> \\(\\times\\) (<code>n_max</code>-<code>n_min</code>) \\(\\times\\) <code>dimension</code> </li> </ul> <code>weights</code> <code>ndarray</code> <p>Only returned when <code>return_weights=True</code>. The Jacobian weights for the transformation</p> Source code in <code>qmcpy/true_measure/abstract_true_measure.py</code> <pre><code>def __call__(self, n=None, n_min=None, n_max=None, return_weights=False, warn=True):\n    r\"\"\"\n    - If just `n` is supplied, generate samples from the sequence at indices 0,...,`n`-1.\n    - If `n_min` and `n_max` are supplied, generate samples from the sequence at indices `n_min`,...,`n_max`-1.\n    - If `n` and `n_min` are supplied, then generate samples from the sequence at indices `n`,...,`n_min`-1.\n\n    Args:\n        n (Union[None,int]): Number of points to generate.\n        n_min (Union[None,int]): Starting index of sequence.\n        n_max (Union[None,int]): Final index of sequence.\n        return_weights (bool): If `True`, return `weights` as well\n        warn (bool): If `False`, disable warnings when generating samples.\n\n    Returns:\n        t (np.ndarray): Samples from the sequence. \n\n            - If `replications` is `None` then this will be of size (`n_max`-`n_min`) $\\times$ `dimension` \n            - If `replications` is a positive int, then `t` will be of size `replications` $\\times$ (`n_max`-`n_min`) $\\times$ `dimension` \n        weights (np.ndarray): Only returned when `return_weights=True`. The Jacobian weights for the transformation\n    \"\"\"\n    return self.gen_samples(n=n,n_min=n_min,n_max=n_max,return_weights=return_weights,warn=warn)\n</code></pre>"},{"location":"api/true_measures/#qmcpy.true_measure.abstract_true_measure.AbstractTrueMeasure.spawn","title":"spawn","text":"<pre><code>spawn(s=1, dimensions=None)\n</code></pre> <p>Spawn new instances of the current true measure but with new seeds and dimensions. Used by multi-level QMC algorithms which require different seeds and dimensions on each level.</p> Note <p>Use <code>replications</code> instead of using <code>spawn</code> when possible, e.g., when spawning copies which all have the same dimension.</p> <p>Parameters:</p> Name Type Description Default <code>s</code> <code>int</code> <p>Number of copies to spawn</p> <code>1</code> <code>dimensions</code> <code>ndarray</code> <p>Length <code>s</code> array of dimensions for each copy. Defaults to the current dimension. </p> <code>None</code> <p>Returns:</p> Name Type Description <code>spawned_true_measures</code> <code>list</code> <p>True measure with new seeds and dimensions.</p> Source code in <code>qmcpy/true_measure/abstract_true_measure.py</code> <pre><code>def spawn(self, s=1, dimensions=None):\n    r\"\"\"\n    Spawn new instances of the current true measure but with new seeds and dimensions.\n    Used by multi-level QMC algorithms which require different seeds and dimensions on each level.\n\n    Note:\n        Use `replications` instead of using `spawn` when possible, e.g., when spawning copies which all have the same dimension.\n\n    Args:\n        s (int): Number of copies to spawn\n        dimensions (np.ndarray): Length `s` array of dimensions for each copy. Defaults to the current dimension. \n\n    Returns:\n        spawned_true_measures (list): True measure with new seeds and dimensions.\n    \"\"\"\n    sampler = self.discrete_distrib if self.transform==self else self.transform\n    sampler_spawns = sampler.spawn(s=s,dimensions=dimensions)\n    spawned_true_measures = [None]*len(sampler_spawns)\n    for i in range(s):\n        spawned_true_measures[i] = self._spawn(sampler_spawns[i],sampler_spawns[i].d)\n    return spawned_true_measures\n</code></pre>"},{"location":"api/true_measures/#scipywrapper","title":"<code>SciPyWrapper</code>","text":"<p>               Bases: <code>AbstractTrueMeasure</code></p> <p>Multivariate distribution with independent marginals from <code>scipy.stats</code>.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; true_measure = SciPyWrapper(\n...     sampler = DigitalNetB2(3,seed=7),\n...     scipy_distribs = [\n...         scipy.stats.uniform(loc=1,scale=2),\n...         scipy.stats.norm(loc=3,scale=4),\n...         scipy.stats.gamma(a=5,loc=6,scale=7)])\n&gt;&gt;&gt; true_measure.range\narray([[  1.,   3.],\n       [-inf,  inf],\n       [  6.,  inf]])\n&gt;&gt;&gt; true_measure(4)\narray([[ 2.26535046,  6.36077755, 46.10334984],\n       [ 1.37949875,  0.8419074 , 38.66873073],\n       [ 2.79562889, -0.65019733, 17.63758514],\n       [ 1.91172032,  4.67357136, 55.20163754]])\n</code></pre> <pre><code>&gt;&gt;&gt; true_measure = SciPyWrapper(sampler=DigitalNetB2(2,seed=7),scipy_distribs=scipy.stats.beta(a=5,b=1))\n&gt;&gt;&gt; true_measure(4)\narray([[0.93683292, 0.98238098],\n       [0.69611329, 0.84454476],\n       [0.99733838, 0.50958933],\n       [0.84451252, 0.89011392]])\n</code></pre> <p>With independent replications </p> <pre><code>&gt;&gt;&gt; x = SciPyWrapper(\n...     sampler = DigitalNetB2(3,seed=7,replications=2),\n...     scipy_distribs = [\n...         scipy.stats.uniform(loc=1,scale=2),\n...         scipy.stats.norm(loc=3,scale=4),\n...         scipy.stats.gamma(a=5,loc=6,scale=7)])(4)\n&gt;&gt;&gt; x.shape \n(2, 4, 3)\n&gt;&gt;&gt; x\narray([[[ 1.49306553, -0.62826013, 49.7677589 ],\n        [ 2.36305807,  4.6683678 , 36.07674167],\n        [ 1.96279709,  6.34058526, 21.99536017],\n        [ 2.8308265 ,  0.84704612, 51.41443437]],\n\n       [[ 1.89753782,  7.30327854, 38.9039967 ],\n        [ 2.07271848, -3.84426539, 32.72574799],\n        [ 1.46428286,  0.81928225, 21.13131114],\n        [ 2.50591431,  4.03840677, 51.08541774]]])\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>sampler</code> <code>AbstractDiscreteDistribution</code> <p>A discrete distribution from which to transform samples.</p> required <code>scipy_distribs</code> <code>list</code> <p>instantiated continuous univariate distributions from <code>scipy.stats</code>.</p> required Source code in <code>qmcpy/true_measure/scipy_wrapper.py</code> <pre><code>def __init__(self, sampler, scipy_distribs):\n    r\"\"\"\n    Args:\n        sampler (AbstractDiscreteDistribution): A discrete distribution from which to transform samples.\n        scipy_distribs (list): instantiated *continuous univariate* distributions from [`scipy.stats`](https://docs.scipy.org/doc/scipy/reference/stats.html#continuous-distributions).\n    \"\"\"\n    self.domain = np.array([[0,1]])\n    if not isinstance(sampler,AbstractDiscreteDistribution):\n        raise ParameterError(\"SciPyWrapper requires sampler be an AbstractDiscreteDistribution.\")\n    self._parse_sampler(sampler)\n    self.scipy_distrib = list(scipy_distribs) if not isinstance(scipy_distribs,scipy.stats._distn_infrastructure.rv_continuous_frozen) else [scipy_distribs]\n    for sd in self.scipy_distrib:\n        if isinstance(sd,scipy.stats._distn_infrastructure.rv_continuous_frozen): continue\n        raise ParameterError('''\n            SciPyWrapper requires each value of scipy_distribs to be a \n            1 dimensional scipy.stats continuous distribution, \n            see https://docs.scipy.org/doc/scipy/reference/stats.html#continuous-distributions.''')\n    self.sds = self.scipy_distrib if len(self.scipy_distrib)&gt;1 else self.scipy_distrib*sampler.d\n    if len(self.sds)!=sampler.d:\n        raise DimensionError(\"length of scipy_distribs must match the dimension of the sampler\")\n    self.range = np.array([sd.interval(1) for sd in self.sds])\n    super(SciPyWrapper,self).__init__()\n    assert len(self.sds)==self.d and all(isinstance(sdsi,scipy.stats._distn_infrastructure.rv_continuous_frozen) for sdsi in self.sds)\n</code></pre>"},{"location":"api/true_measures/#uniform","title":"<code>Uniform</code>","text":"<p>               Bases: <code>AbstractTrueMeasure</code></p> <p>Uniform distribution, see https://en.wikipedia.org/wiki/Continuous_uniform_distribution. </p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; true_measure = Uniform(DigitalNetB2(2,seed=7),lower_bound=[0,.5],upper_bound=[2,3])\n&gt;&gt;&gt; true_measure(4)\narray([[1.44324713, 2.7873875 ],\n       [0.32691107, 1.5741214 ],\n       [1.97352511, 0.58590959],\n       [0.8591331 , 1.89690854]])\n&gt;&gt;&gt; true_measure\nUniform (AbstractTrueMeasure)\n    lower_bound     [0.  0.5]\n    upper_bound     [2 3]\n</code></pre> <p>With independent replications </p> <pre><code>&gt;&gt;&gt; x = Uniform(DigitalNetB2(3,seed=7,replications=2),lower_bound=[.25,.5,.75],upper_bound=[1.75,1.5,1.25])(4)\n&gt;&gt;&gt; x.shape \n(2, 4, 3)\n&gt;&gt;&gt; x\narray([[[0.61979915, 0.6821862 , 1.12366296],\n        [1.27229355, 1.16169442, 0.9644598 ],\n        [0.97209782, 1.29818233, 0.79100643],\n        [1.62311988, 0.79520621, 1.13747905]],\n\n       [[0.92315337, 1.35899604, 1.0027484 ],\n        [1.05453886, 0.54353443, 0.91782473],\n        [0.59821215, 0.79281506, 0.78420518],\n        [1.37943573, 1.10241448, 1.13481488]]])\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>sampler</code> <code>Union[AbstractDiscreteDistribution, AbstractTrueMeasure]</code> <p>Either  </p> <ul> <li>a discrete distribution from which to transform samples, or</li> <li>a true measure by which to compose a transform.</li> </ul> required <code>lower_bound</code> <code>Union[float, ndarray]</code> <p>Lower bound.</p> <code>0</code> <code>upper_bound</code> <code>Union[float, ndarray]</code> <p>Upper bound.</p> <code>1</code> Source code in <code>qmcpy/true_measure/uniform.py</code> <pre><code>def __init__(self, sampler, lower_bound=0, upper_bound=1):\n    r\"\"\"\n    Args:\n        sampler (Union[AbstractDiscreteDistribution,AbstractTrueMeasure]): Either  \n\n            - a discrete distribution from which to transform samples, or\n            - a true measure by which to compose a transform.\n        lower_bound (Union[float,np.ndarray]): Lower bound.\n        upper_bound (Union[float,np.ndarray]): Upper bound.\n    \"\"\"\n    self.parameters = ['lower_bound', 'upper_bound']\n    self.domain = np.array([[0,1]])\n    self._parse_sampler(sampler)\n    self.lower_bound = lower_bound\n    self.upper_bound = upper_bound\n    if np.isscalar(self.lower_bound):\n        lower_bound = np.tile(self.lower_bound,self.d)\n    if np.isscalar(self.upper_bound):\n        upper_bound = np.tile(self.upper_bound,self.d)\n    self.a = np.array(lower_bound)\n    self.b = np.array(upper_bound)\n    if len(self.a)!=self.d or len(self.b)!=self.d:\n        raise DimensionError('upper bound and lower bound must be of length dimension')\n    self.delta = self.b - self.a\n    self.inv_delta_prod = 1/self.delta.prod()\n    self.range = np.hstack((self.a.reshape((self.d,1)),self.b.reshape((self.d,1))))\n    super(Uniform,self).__init__()\n    assert self.a.shape==(self.d,) and self.b.shape==(self.d,)\n</code></pre>"},{"location":"api/true_measures/#gaussian","title":"<code>Gaussian</code>","text":"<p>               Bases: <code>AbstractTrueMeasure</code></p> <p>Gaussian (Normal) distribution as described in https://en.wikipedia.org/wiki/Multivariate_normal_distribution.</p> Note <ul> <li><code>Normal</code> is an alias for <code>Gaussian</code></li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; true_measure = Gaussian(DigitalNetB2(2,seed=7),mean=[1,2],covariance=[[9,4],[4,5]])\n&gt;&gt;&gt; true_measure(4)\narray([[ 3.83994612,  1.19097885],\n       [-1.9727727 ,  0.49405353],\n       [ 5.87242307,  8.41341485],\n       [ 0.61222205,  1.48402653]])\n&gt;&gt;&gt; true_measure\nGaussian (AbstractTrueMeasure)\n    mean            [1 2]\n    covariance      [[9 4]\n                     [4 5]]\n    decomp_type     PCA\n</code></pre> <p>With independent replications </p> <pre><code>&gt;&gt;&gt; x = Gaussian(DigitalNetB2(3,seed=7,replications=2),mean=0,covariance=3)(4)\n&gt;&gt;&gt; x.shape \n(2, 4, 3)\n&gt;&gt;&gt; x\narray([[[-1.18721904, -1.57108272,  1.15371635],\n        [ 0.81749123,  0.72242445, -0.31025434],\n        [-0.0807895 ,  1.44651585, -2.41042379],\n        [ 2.38133494, -0.93225637,  1.30817519]],\n\n       [[-0.22304017,  1.86337427,  0.02386568],\n        [ 0.15807672, -2.96365385, -0.73502346],\n        [-1.26753687, -0.94427848, -2.57683314],\n        [ 1.1844196 ,  0.44964332,  1.27760936]]])\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>sampler</code> <code>Union[AbstractDiscreteDistribution, AbstractTrueMeasure]</code> <p>Either  </p> <ul> <li>a discrete distribution from which to transform samples, or</li> <li>a true measure by which to compose a transform.</li> </ul> required <code>mean</code> <code>Union[float, ndarray]</code> <p>Mean vector. </p> <code>0.0</code> <code>covariance</code> <code>Union[float, ndarray]</code> <p>Covariance matrix. A float or vector will be expanded into a diagonal matrix.  </p> <code>1.0</code> <code>decomp_type</code> <code>str</code> <p>Method for decomposition for covariance matrix. Options include</p> <ul> <li><code>'PCA'</code> for principal component analysis, or </li> <li><code>'Cholesky'</code> for cholesky decomposition.</li> </ul> <code>'PCA'</code> Source code in <code>qmcpy/true_measure/gaussian.py</code> <pre><code>def __init__(self, sampler, mean=0., covariance=1., decomp_type='PCA'):\n    \"\"\"\n    Args:\n        sampler (Union[AbstractDiscreteDistribution,AbstractTrueMeasure]): Either  \n\n            - a discrete distribution from which to transform samples, or\n            - a true measure by which to compose a transform.\n        mean (Union[float,np.ndarray]): Mean vector. \n        covariance (Union[float,np.ndarray]): Covariance matrix. A float or vector will be expanded into a diagonal matrix.  \n        decomp_type (str): Method for decomposition for covariance matrix. Options include\n\n            - `'PCA'` for principal component analysis, or \n            - `'Cholesky'` for cholesky decomposition.\n    \"\"\"\n    self.parameters = ['mean', 'covariance', 'decomp_type']\n    # default to transform from standard uniform\n    self.domain = np.array([[0,1]])\n    self._parse_sampler(sampler)\n    self._parse_gaussian_params(mean,covariance,decomp_type)\n    self.range = np.array([[-np.inf,np.inf]])\n    super(Gaussian,self).__init__()\n    assert self.mu.shape==(self.d,) and self.a.shape==(self.d,self.d)\n</code></pre>"},{"location":"api/true_measures/#brownianmotion","title":"<code>BrownianMotion</code>","text":"<p>               Bases: <code>Gaussian</code></p> <p>Brownian Motion as described in https://en.wikipedia.org/wiki/Brownian_motion. For a standard Brownian Motion \\(W\\) we define the Brownian Motion \\(B\\) with initial value \\(B_0\\), drift \\(\\gamma\\), and diffusion \\(\\sigma^2\\) to be</p> \\[B(t) = B_0 + \\gamma t + \\sigma W(t).\\] <p>Examples:</p> <pre><code>&gt;&gt;&gt; true_measure = BrownianMotion(DigitalNetB2(4,seed=7),t_final=2,drift=2)\n&gt;&gt;&gt; true_measure(2)\narray([[0.82189263, 2.7851793 , 3.60126805, 3.98054724],\n       [0.2610643 , 0.06170064, 1.06448269, 2.30990767]])\n&gt;&gt;&gt; true_measure\nBrownianMotion (AbstractTrueMeasure)\n    time_vec        [0.5 1.  1.5 2. ]\n    drift           2^(1)\n    mean            [1. 2. 3. 4.]\n    covariance      [[0.5 0.5 0.5 0.5]\n                     [0.5 1.  1.  1. ]\n                     [0.5 1.  1.5 1.5]\n                     [0.5 1.  1.5 2. ]]\n    decomp_type     PCA\n</code></pre> <p>With independent replications </p> <pre><code>&gt;&gt;&gt; x = BrownianMotion(DigitalNetB2(3,seed=7,replications=2),t_final=2,drift=2)(4)\n&gt;&gt;&gt; x.shape \n(2, 4, 3)\n&gt;&gt;&gt; x\narray([[[0.66154685, 1.50620966, 3.52322901],\n        [1.77064217, 3.32782204, 4.45013223],\n        [1.33558688, 3.26017547, 3.40692337],\n        [2.10317345, 3.78961839, 6.17948096]],\n\n       [[1.77868019, 2.75347902, 3.41161419],\n        [0.44891984, 2.53987304, 4.7224811 ],\n        [0.23147948, 2.25289769, 3.00039101],\n        [2.06762574, 3.21756319, 4.93375923]]])\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>sampler</code> <code>Union[AbstractDiscreteDistribution, AbstractTrueMeasure]</code> <p>Either  </p> <ul> <li>a discrete distribution from which to transform samples, or</li> <li>a true measure by which to compose a transform.</li> </ul> required <code>t_final</code> <code>float</code> <p>End time. </p> <code>1</code> <code>initial_value</code> <code>float</code> <p>Initial value \\(B_0\\). </p> <code>0</code> <code>drift</code> <code>int</code> <p>Drift \\(\\gamma\\). </p> <code>0</code> <code>diffusion</code> <code>int</code> <p>Diffusion \\(\\sigma^2\\). </p> <code>1</code> <code>decomp_type</code> <code>str</code> <p>Method for decomposition for covariance matrix. Options include</p> <ul> <li><code>'PCA'</code> for principal component analysis, or </li> <li><code>'Cholesky'</code> for cholesky decomposition.</li> </ul> <code>'PCA'</code> Source code in <code>qmcpy/true_measure/brownian_motion.py</code> <pre><code>def __init__(self, sampler, t_final=1, initial_value=0, drift=0, diffusion=1, decomp_type='PCA'):\n    r\"\"\"\n    Args:\n        sampler (Union[AbstractDiscreteDistribution,AbstractTrueMeasure]): Either  \n\n            - a discrete distribution from which to transform samples, or\n            - a true measure by which to compose a transform.\n        t_final (float): End time. \n        initial_value (float): Initial value $B_0$. \n        drift (int): Drift $\\gamma$. \n        diffusion (int): Diffusion $\\sigma^2$. \n        decomp_type (str): Method for decomposition for covariance matrix. Options include\n\n            - `'PCA'` for principal component analysis, or \n            - `'Cholesky'` for cholesky decomposition.\n    \"\"\"\n    self.parameters = ['time_vec', 'drift', 'mean', 'covariance', 'decomp_type']\n    # default to transform from standard uniform\n    self.domain = np.array([[0,1]])\n    self._parse_sampler(sampler)\n    self.t = t_final # exercise time\n    self.initial_value = initial_value\n    self.drift = drift\n    self.diffusion = diffusion\n    self.time_vec = np.linspace(self.t/self.d,self.t,self.d) # evenly spaced\n    self.diffused_sigma_bm = self.diffusion*np.array([[min(self.time_vec[i],self.time_vec[j]) for i in range(self.d)] for j in range(self.d)])\n    self.drift_time_vec_plus_init = self.drift*self.time_vec+self.initial_value # mean\n    self._parse_gaussian_params(self.drift_time_vec_plus_init,self.diffused_sigma_bm,decomp_type)\n    self.range = np.array([[-np.inf,np.inf]])\n    super(Gaussian,self).__init__()\n</code></pre>"},{"location":"api/true_measures/#geometricbrownianmotion","title":"<code>GeometricBrownianMotion</code>","text":"<p>               Bases: <code>BrownianMotion</code></p> <p>A Geometric Brownian Motion (GBM) with initial value \\(S_0\\), drift \\(\\gamma\\), and diffusion \\(\\sigma^2\\) is </p> \\[\\mathrm{GBM}(t) = S_0 \\exp[(\\gamma - \\sigma/2) t + \\sigma \\mathrm{BM}(t)]\\] <p>where BM is a Brownian Motion drift \\(\\gamma\\) and diffusion \\(\\sigma^2\\). </p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; gbm = GeometricBrownianMotion(DigitalNetB2(4,seed=7), t_final=2, drift=0.1, diffusion=0.2)\n&gt;&gt;&gt; gbm.gen_samples(2)\narray([[0.92343761, 1.42069027, 1.30851806, 0.99133819],\n       [0.7185916 , 0.42028013, 0.42080335, 0.4696196 ]])\n&gt;&gt;&gt; gbm\nGeometricBrownianMotion (AbstractTrueMeasure)\n    time_vec        [0.5 1.  1.5 2. ]\n    drift           0.100\n    diffusion       0.200\n    mean_gbm        [1.051 1.105 1.162 1.221]\n    covariance_gbm  [[0.116 0.122 0.128 0.135]\n                     [0.122 0.27  0.284 0.299]\n                     [0.128 0.284 0.472 0.496]\n                     [0.135 0.299 0.496 0.734]]\n    decomp_type     PCA\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>sampler</code> <code>DiscreteDistribution / TrueMeasure</code> <p>A discrete distribution or true measure.</p> required <code>t_final</code> <code>float</code> <p>End time for the geometric Brownian motion, non-negative.</p> <code>1</code> <code>initial_value</code> <code>float</code> <p>Positive initial value of the process, \\(S_0\\).</p> <code>1</code> <code>drift</code> <code>float</code> <p>Drift coefficient \\(\\gamma\\).</p> <code>0</code> <code>diffusion</code> <code>float</code> <p>Positive diffusion coefficient \\(\\sigma^2\\).</p> <code>1</code> <code>decomp_type</code> <code>str</code> <p>Method of decomposition, either \"PCA\" or \"Cholesky\".</p> <code>'PCA'</code> Source code in <code>qmcpy/true_measure/geometric_brownian_motion.py</code> <pre><code>def __init__(self, sampler, t_final=1, initial_value=1, drift=0, diffusion=1, decomp_type='PCA'):\n    r\"\"\"\n    Args:\n        sampler (DiscreteDistribution/TrueMeasure): A discrete distribution or true measure.\n        t_final (float): End time for the geometric Brownian motion, non-negative.\n        initial_value (float): Positive initial value of the process, $S_0$.\n        drift (float): Drift coefficient $\\gamma$.\n        diffusion (float): Positive diffusion coefficient $\\sigma^2$.\n        decomp_type (str): Method of decomposition, either \"PCA\" or \"Cholesky\".\n    \"\"\"\n    super().__init__(sampler, t_final=t_final, drift=0, diffusion=diffusion, decomp_type=decomp_type)\n    self.parameters = ['time_vec', 'drift', 'diffusion', 'mean_gbm', 'covariance_gbm', 'decomp_type']\n    self.initial_value = initial_value\n    self.drift = drift\n    self.diffusion = diffusion\n    self.mean_gbm = self._compute_gbm_mean()\n    self.covariance_gbm = self._compute_gbm_covariance()\n    self._setup_lognormal_distribution()\n    self._validate_input()\n</code></pre>"},{"location":"api/true_measures/#materngp","title":"<code>MaternGP</code>","text":"<p>               Bases: <code>Gaussian</code></p> <p>A Gaussian process with Mat\u00e9rn covariance kernel.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; true_measure = MaternGP(DigitalNetB2(dimension=3,seed=7),points=np.linspace(0,1,3)[:,None],nu=3/2,length_scale=[3,4,5],variance=0.01,mean=np.array([.3,.4,.5]))\n&gt;&gt;&gt; true_measure(4)\narray([[0.3515401 , 0.43083384, 0.51801119],\n       [0.20272448, 0.31312011, 0.4241431 ],\n       [0.40189226, 0.53502934, 0.63826677],\n       [0.29943567, 0.38491661, 0.48296594]])\n&gt;&gt;&gt; true_measure\nMaternGP (AbstractTrueMeasure)\n    mean            [0.3 0.4 0.5]\n    covariance      [[0.01  0.01  0.01 ]\n                     [0.01  0.01  0.01 ]\n                     [0.009 0.01  0.01 ]]\n    decomp_type     PCA\n</code></pre> <p>With independent replications </p> <pre><code>&gt;&gt;&gt; x = MaternGP(DigitalNetB2(dimension=3,seed=7,replications=2),points=np.linspace(0,1,3)[:,None],nu=3/2,length_scale=[3,4,5],variance=0.01,mean=np.array([.3,.4,.5]))(4)\n&gt;&gt;&gt; x.shape \n(2, 4, 3)\n&gt;&gt;&gt; x\narray([[[0.21490091, 0.33078241, 0.45151042],\n        [0.35465127, 0.44705898, 0.53793358],\n        [0.31091595, 0.39868187, 0.47660193],\n        [0.42419919, 0.53572415, 0.64674883]],\n\n       [[0.31010701, 0.38522001, 0.46670381],\n        [0.27221177, 0.413546  , 0.54101758],\n        [0.2147053 , 0.33293508, 0.43572791],\n        [0.37343973, 0.46534628, 0.56356714]]])\n</code></pre> <p>References:</p> <ol> <li> <p><code>sklearn.gaussian_process.kernels.Matern</code>.</p> </li> <li> <p>https://en.wikipedia.org/wiki/Mat%C3%A9rn_covariance_function.</p> </li> </ol> <p>Parameters:</p> Name Type Description Default <code>sampler</code> <code>Union[AbstractDiscreteDistribution, AbstractTrueMeasure]</code> <p>Either  </p> <ul> <li>a discrete distribution from which to transform samples, or</li> <li>a true measure by which to compose a transform.</li> </ul> required <code>points</code> <code>ndarray</code> <p>The positions of points on a metric space. The array should have shape \\((d,k)\\) where \\(d\\) is the dimension of the sampler and \\(k\\) is the latent dimension. </p> required <code>nu</code> <code>float</code> <p>The \"smoothness\" of the MaternGP function, e.g.,</p> <ul> <li>\\(\\nu = 1/2\\) is equivalent to the absolute exponential kernel, </li> <li>\\(\\nu = 3/2\\) implies a once-differentiable function,</li> <li>\\(\\nu = 5/2\\) implies twice differentiability. </li> <li>as \\(\\nu \\to \\infty\\) the kernel becomes equivalent to the RBF kernel, see <code>sklearn.gaussian_process.kernels.RBF</code>. </li> </ul> <p>Note that when \\(\\nu \\notin \\{1/2, 3/2, 5/2, \\infty \\}\\) the kernel is around \\(10\\) times slower to evaluate. </p> <code>1.5</code> <code>length_scale</code> <code>Union[float, ndarray]</code> <p>Determines \"peakiness\", or how correlated two points are based on their distance.</p> <code>1.0</code> <code>variance</code> <code>float</code> <p>Global scaling factor.</p> <code>1.0</code> <code>mean</code> <code>Union[float, ndarray]</code> <p>Mean vectorfor multivariante <code>Gaussian</code>. </p> <code>0.0</code> <code>nugget</code> <code>float</code> <p>Positive nugget to add to diagonal.</p> <code>1e-06</code> <code>decomp_type</code> <code>str</code> <p>Method for decomposition for covariance matrix. Options include</p> <ul> <li><code>'PCA'</code> for principal component analysis, or </li> <li><code>'Cholesky'</code> for cholesky decomposition.</li> </ul> <code>'PCA'</code> Source code in <code>qmcpy/true_measure/matern_gp.py</code> <pre><code>def __init__(self, sampler, points, length_scale=1.0, nu=1.5, variance=1.0, mean=0., nugget=1e-6, decomp_type='PCA'):\n    r\"\"\"\n    Args:\n        sampler (Union[AbstractDiscreteDistribution,AbstractTrueMeasure]): Either  \n\n            - a discrete distribution from which to transform samples, or\n            - a true measure by which to compose a transform.\n        points (np.ndarray): The positions of points on a metric space. The array should have shape $(d,k)$ where $d$ is the dimension of the sampler and $k$ is the latent dimension. \n        nu (float): The \"smoothness\" of the MaternGP function, e.g.,\n\n            - $\\nu = 1/2$ is equivalent to the absolute exponential kernel, \n            - $\\nu = 3/2$ implies a once-differentiable function,\n            - $\\nu = 5/2$ implies twice differentiability. \n            - as $\\nu \\to \\infty$ the kernel becomes equivalent to the RBF kernel, see [`sklearn.gaussian_process.kernels.RBF`](https://scikit-learn.org/stable/modules/generated/sklearn.gaussian_process.kernels.RBF.html#sklearn.gaussian_process.kernels.RBF). \n\n            Note that when $\\nu \\notin \\{1/2, 3/2, 5/2, \\infty \\}$ the kernel is around $10$ times slower to evaluate. \n        length_scale (Union[float,np.ndarray]): Determines \"peakiness\", or how correlated two points are based on their distance.\n        variance (float): Global scaling factor.\n        mean (Union[float,np.ndarray]): Mean vectorfor multivariante `Gaussian`. \n        nugget (float): Positive nugget to add to diagonal.\n        decomp_type (str): Method for decomposition for covariance matrix. Options include\n\n            - `'PCA'` for principal component analysis, or \n            - `'Cholesky'` for cholesky decomposition.\n    \"\"\"\n    if not (isinstance(sampler, AbstractDiscreteDistribution) or isinstance(sampler, AbstractTrueMeasure)):\n        raise ParameterError(\"sampler input should either be an AbstractDiscreteDistribution or AbstractTrueMeasure.\")\n    if not (isinstance(points, np.ndarray) and (points.ndim==1 or points.ndim==2)):\n        raise ParameterError(\"points must be a one or two dimensional np.ndarray.\")\n    if points.ndim==1: points = points[:,None] \n    assert points.ndim==2 and points.shape[0]==sampler.d, \"points should be a two dimenssion array with the number of points equal to the dimension of the sampler\"\n    mean = np.array(mean)\n    if mean.size==1: mean = mean.item()*np.ones(sampler.d)\n    assert mean.shape==(sampler.d,), \"mean should be a length d vector\"\n    assert np.isscalar(nu) and nu&gt;0, \"nu should be a positive scalar\"\n    length_scale = np.array(length_scale) \n    if length_scale.size==1: length_scale = length_scale.item()*np.ones(sampler.d)\n    assert length_scale.shape==(sampler.d,) and (length_scale&gt;0).all(), \"length_scale should be a vector with length equal to the dimension of the sampler\"\n    assert np.isscalar(variance) and variance&gt;0, \"length_scale should be a positive scalar\"\n    assert np.isscalar(nugget) and nugget&gt;0, \"nugget should be a positive scalar\"\n    self.points = points\n    self.length_scale = length_scale\n    self.nu = nu\n    self.variance = variance\n    dists = np.linalg.norm(points[...,:,None,:]-points[...,None,:,:],axis=-1)\n    if nu==1/2:\n        covariance = np.exp(-dists/self.length_scale)\n    elif nu==3/2:\n        covariance = (1+np.sqrt(3)*dists/self.length_scale)*np.exp(-np.sqrt(3)*dists/self.length_scale)\n    elif nu==5/2:\n        covariance = ((1+np.sqrt(5)*dists/self.length_scale+5*dists**2/(3*self.length_scale**2))*np.exp(-np.sqrt(5)*dists/self.length_scale))\n    elif nu==np.inf:\n        covariance = np.exp(-dists**2/(2*self.length_scale**2))\n    else:\n        k = np.sqrt(2*nu)*dists/self.length_scale\n        covariance = 2**(1-nu)/gamma(nu)*k**nu*kv(nu,k)\n    covariance = variance*covariance+nugget*np.eye(sampler.d)\n    super().__init__(sampler, mean=mean, covariance=covariance, decomp_type=decomp_type)\n</code></pre>"},{"location":"api/true_measures/#lebesgue","title":"<code>Lebesgue</code>","text":"<p>               Bases: <code>AbstractTrueMeasure</code></p> <p>Lebesgue measure as described in https://en.wikipedia.org/wiki/Lebesgue_measure.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; Lebesgue(Gaussian(DigitalNetB2(2,seed=7)))\nLebesgue (AbstractTrueMeasure)\n    transform       Gaussian (AbstractTrueMeasure)\n                        mean            0\n                        covariance      1\n                        decomp_type     PCA\n&gt;&gt;&gt; Lebesgue(Uniform(DigitalNetB2(2,seed=7)))\nLebesgue (AbstractTrueMeasure)\n    transform       Uniform (AbstractTrueMeasure)\n                        lower_bound     0\n                        upper_bound     1\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>sampler</code> <code>AbstractTrueMeasure</code> <p>A true measure by which to compose a transform.</p> required Source code in <code>qmcpy/true_measure/lebesgue.py</code> <pre><code>def __init__(self, sampler):\n    r\"\"\"\n    Args:\n        sampler (AbstractTrueMeasure): A true measure by which to compose a transform.\n    \"\"\"\n    self.parameters = []\n    if not isinstance(sampler,AbstractTrueMeasure):\n        raise ParameterError(\"Lebesgue sampler must be an AbstractTrueMeasure by which to transform samples.\")\n    self.domain = sampler.range # hack to make sure Lebesgue is compatible with any transform\n    self.range = sampler.range\n    self._parse_sampler(sampler)\n    super(Lebesgue,self).__init__()\n</code></pre>"},{"location":"api/true_measures/#bernoullicont","title":"<code>BernoulliCont</code>","text":"<p>               Bases: <code>AbstractTrueMeasure</code></p> <p>Continuous Bernoulli distribution with independent marginals as described in https://en.wikipedia.org/wiki/Continuous_Bernoulli_distribution.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; true_measure = BernoulliCont(DigitalNetB2(2,seed=7),lam=.2)\n&gt;&gt;&gt; true_measure(4)\narray([[0.56205914, 0.83607872],\n       [0.09433983, 0.28057299],\n       [0.97190779, 0.01883497],\n       [0.28050753, 0.39178506]])\n&gt;&gt;&gt; true_measure\nBernoulliCont (AbstractTrueMeasure)\n    lam             0.200\n</code></pre> <p>With independent replications </p> <pre><code>&gt;&gt;&gt; x = BernoulliCont(DigitalNetB2(3,seed=7,replications=2),lam=[.25,.5,.75])(4)\n&gt;&gt;&gt; x.shape \n(2, 4, 3)\n&gt;&gt;&gt; x\narray([[[0.16343492, 0.1821862 , 0.83209443],\n        [0.55140696, 0.66169442, 0.56381501],\n        [0.35229402, 0.79818233, 0.13825119],\n        [0.85773226, 0.29520621, 0.85203898]],\n\n       [[0.32359293, 0.85899604, 0.63591945],\n        [0.40278251, 0.04353443, 0.46749989],\n        [0.1530438 , 0.29281506, 0.116725  ],\n        [0.6345258 , 0.60241448, 0.84822692]]])\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>sampler</code> <code>Union[AbstractDiscreteDistribution, AbstractTrueMeasure]</code> <p>Either  </p> <ul> <li>a discrete distribution from which to transform samples, or</li> <li>a true measure by which to compose a transform.</li> </ul> required <code>lam</code> <code>Union[float, ndarray]</code> <p>Vector of shape parameters, each in \\((0,1)\\).</p> <code>1 / 2</code> Source code in <code>qmcpy/true_measure/bernoulli_cont.py</code> <pre><code>def __init__(self, sampler, lam=1/2):\n    r\"\"\"\n    Args:\n        sampler (Union[AbstractDiscreteDistribution,AbstractTrueMeasure]): Either  \n\n            - a discrete distribution from which to transform samples, or\n            - a true measure by which to compose a transform.\n        lam (Union[float,np.ndarray]): Vector of shape parameters, each in $(0,1)$.\n    \"\"\"\n    self.parameters = ['lam']\n    self.domain = np.array([[0,1]])\n    self.range = np.array([[0,1]])\n    self._parse_sampler(sampler)\n    self.lam = lam\n    self.l = np.array(lam)\n    if self.l.size==1:\n        self.l = self.l.item()*np.ones(self.d)\n    if not (self.l.shape==(self.d,) and (0&lt;=self.l).all() and (self.l&lt;=1).all()):\n        raise DimensionError('lam must be scalar or have length equal to dimension and must be in (0,1).')\n    super(BernoulliCont,self).__init__()\n</code></pre>"},{"location":"api/true_measures/#johnsonssu","title":"<code>JohnsonsSU</code>","text":"<p>               Bases: <code>AbstractTrueMeasure</code></p> <p>Johnson's \\(S_U\\)-distribution with independent marginals as described in https://en.wikipedia.org/wiki/Johnson%27s_SU-distribution.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; true_measure = JohnsonsSU(DigitalNetB2(2,seed=7),gamma=1,xi=2,delta=3,lam=4)\n&gt;&gt;&gt; true_measure(4)\narray([[ 1.44849599,  2.49715741],\n       [-0.83646172,  0.38970902],\n       [ 3.67068094, -2.33911196],\n       [ 0.38940887,  0.84843818]])\n&gt;&gt;&gt; true_measure\nJohnsonsSU (AbstractTrueMeasure)\n    gamma           1\n    xi              2^(1)\n    delta           3\n    lam             2^(2)\n</code></pre> <p>With independent replications </p> <pre><code>&gt;&gt;&gt; x = JohnsonsSU(DigitalNetB2(3,seed=7,replications=2),gamma=1,xi=2,delta=3,lam=4)(4)\n&gt;&gt;&gt; x.shape \n(2, 4, 3)\n&gt;&gt;&gt; x\narray([[[-0.36735335, -0.71750135,  1.55387818],\n        [ 1.29233112,  1.21788962,  0.3870404 ],\n        [ 0.57599197,  1.78008445, -1.53756327],\n        [ 2.50112084, -0.14204369,  1.67333839]],\n\n       [[ 0.45920696,  2.10110361,  0.66122546],\n        [ 0.76973983, -2.12724026,  0.02868419],\n        [-0.43948474, -0.1525475 , -1.71041918],\n        [ 1.57765245,  1.00275   ,  1.64972468]]])\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>sampler</code> <code>Union[AbstractDiscreteDistribution, AbstractTrueMeasure]</code> <p>Either  </p> <ul> <li>a discrete distribution from which to transform samples, or</li> <li>a true measure by which to compose a transform.</li> </ul> required <code>gamma</code> <code>Union[float, ndarray]</code> <p>First parameter \\(\\gamma\\).</p> <code>1</code> <code>xi</code> <code>Union[float, ndarray]</code> <p>Second parameter \\(\\xi\\).</p> <code>1</code> <code>delta</code> <code>Union[float, ndarray]</code> <p>Third parameter \\(\\delta &gt; 0\\). </p> <code>2</code> <code>lam</code> <code>Union[float, ndarray]</code> <p>Fourth parameter \\(\\lambda &gt; 0\\).</p> <code>2</code> Source code in <code>qmcpy/true_measure/johnsons_su.py</code> <pre><code>def __init__(self, sampler, gamma=1, xi=1, delta=2, lam=2):\n    r\"\"\"\n    Args:\n        sampler (Union[AbstractDiscreteDistribution,AbstractTrueMeasure]): Either  \n\n            - a discrete distribution from which to transform samples, or\n            - a true measure by which to compose a transform.\n        gamma (Union[float,np.ndarray]): First parameter $\\gamma$.\n        xi (Union[float,np.ndarray]): Second parameter $\\xi$.\n        delta (Union[float,np.ndarray]): Third parameter $\\delta &gt; 0$. \n        lam (Union[float,np.ndarray]): Fourth parameter $\\lambda &gt; 0$.\n    \"\"\"\n    self.parameters = ['gamma', 'xi', 'delta', 'lam']\n    self.domain = np.array([[0,1]])\n    self.range = np.array([[-np.inf,np.inf]])\n    self._parse_sampler(sampler)\n    self.gamma = gamma\n    self.xi = xi\n    self.delta = delta\n    self.lam = lam\n    self._gamma = np.array(gamma)\n    if self._gamma.size==1:\n        self._gamma = self._gamma.item()*np.ones(self.d)\n    self._xi = np.array(xi)\n    if self._xi.size==1:\n        self._xi = self._xi.item()*np.ones(self.d)\n    self._delta = np.array(delta)\n    if self._delta.size==1:\n        self._delta = self._delta.item()*np.ones(self.d)\n    self._lam = np.array(lam)\n    if self._lam.size==1:\n        self._lam = self._lam.item()*np.ones(self.d)\n    if not (self._gamma.shape==(self.d,) and self._xi.shape==(self.d,) and self._delta.shape==(self.d,) and self._lam.shape==(self.d,)):\n        raise DimensionError(\"all Johnson's S_U parameters be scalar or have length equal to dimension.\")\n    if not ((self._delta&gt;0).all() and (self._lam&gt;0).all()):\n        raise ParameterError(\"delta and lam must be all be positive\")\n    super(JohnsonsSU,self).__init__()\n    assert self._gamma.shape==(self.d,) and self._xi.shape==(self.d,) and self._delta.shape==(self.d,) and self._lam.shape==(self.d,)\n</code></pre>"},{"location":"api/true_measures/#kumaraswamy","title":"<code>Kumaraswamy</code>","text":"<p>               Bases: <code>AbstractTrueMeasure</code></p> <p>Kumaraswamy distribution as described in https://en.wikipedia.org/wiki/Kumaraswamy_distribution.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; true_measure = Kumaraswamy(DigitalNetB2(2,seed=7),a=[1,2],b=[3,4])\n&gt;&gt;&gt; true_measure(4)\narray([[0.34705366, 0.6782161 ],\n       [0.0577568 , 0.36189538],\n       [0.76344358, 0.0932949 ],\n       [0.17065545, 0.43009386]])\n&gt;&gt;&gt; true_measure\nKumaraswamy (AbstractTrueMeasure)\n    a               [1 2]\n    b               [3 4]\n</code></pre> <p>With independent replications </p> <pre><code>&gt;&gt;&gt; x = Kumaraswamy(DigitalNetB2(3,seed=7,replications=2),a=[1,2,3],b=[3,4,5])(4)\n&gt;&gt;&gt; x.shape \n(2, 4, 3)\n&gt;&gt;&gt; x\narray([[[0.09004177, 0.22144305, 0.62190133],\n        [0.31710078, 0.48718217, 0.47325643],\n        [0.19657641, 0.57423463, 0.25697057],\n        [0.56103074, 0.28939035, 0.63654112]],\n\n       [[0.18006788, 0.62226635, 0.5083556 ],\n        [0.22602452, 0.10519477, 0.42823814],\n        [0.08428482, 0.28804621, 0.2414302 ],\n        [0.37253319, 0.45379743, 0.63366422]]])\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>sampler</code> <code>Union[AbstractDiscreteDistribution, AbstractTrueMeasure]</code> <p>Either  </p> <ul> <li>a discrete distribution from which to transform samples, or</li> <li>a true measure by which to compose a transform.</li> </ul> required <code>a</code> <code>Union[float, ndarray]</code> <p>First parameter \\(\\alpha &gt; 0\\).</p> <code>2</code> <code>b</code> <code>Union[float, ndarray]</code> <p>Second parameter \\(\\beta &gt; 0\\).</p> <code>2</code> Source code in <code>qmcpy/true_measure/kumaraswamy.py</code> <pre><code>def __init__(self, sampler, a=2, b=2):\n    r\"\"\"\n    Args:\n        sampler (Union[AbstractDiscreteDistribution,AbstractTrueMeasure]): Either  \n\n            - a discrete distribution from which to transform samples, or\n            - a true measure by which to compose a transform.\n        a (Union[float,np.ndarray]): First parameter $\\alpha &gt; 0$.\n        b (Union[float,np.ndarray]): Second parameter $\\beta &gt; 0$.\n    \"\"\"\n    self.parameters = ['a', 'b']\n    self.domain = np.array([[0,1]])\n    self.range = np.array([[0,1]])\n    self._parse_sampler(sampler)\n    self.a = a\n    self.b = b\n    self.alpha = np.array(a)\n    if self.alpha.size==1:\n        self.alpha = self.alpha.item()*np.ones(self.d)\n        a = np.tile(self.a,self.d)\n    self.beta = np.array(b)\n    if self.beta.size==1:\n        self.beta = self.beta.item()*np.ones(self.d)\n    if not (self.alpha.shape==(self.d,) and self.beta.shape==(self.d,)):\n        raise DimensionError('a and b must be scalar or have length equal to dimension.')\n    if not ((self.alpha&gt;0).all() and (self.beta&gt;0).all()):\n        raise ParameterError(\"Kumaraswamy requires a,b&gt;0.\")\n    super(Kumaraswamy,self).__init__()\n    assert self.alpha.shape==(self.d,) and self.beta.shape==(self.d,)\n</code></pre>"},{"location":"api/true_measures/#uml-specific","title":"UML Specific","text":""},{"location":"api/util/","title":"Utilities","text":""},{"location":"api/util/#plot_proj","title":"<code>plot_proj</code>","text":"<p>Parameters:</p> Name Type Description Default <code>sampler</code> <code>(DiscreteDistribution, TrueMeasure)</code> <p>The generator of samples to be plotted.</p> required <code>n</code> <code>Union[int, list]</code> <p>The number of samples or a list of samples(used for extensibility) to be plotted.</p> <code>64</code> <code>d_horizontal</code> <code>Union[int, list]</code> <p>The dimension or list of dimensions to be plotted on the horizontal axes. </p> <code>1</code> <code>d_vertical</code> <code>Union[int, list]</code> <p>The dimension or list of dimensions to be plotted on the vertical axes. </p> <code>2</code> <code>math_ind</code> <code>bool</code> <p>Setting to <code>True</code> will enable user to pass in math indices.</p> <code>True</code> <code>marker_size</code> <code>float</code> <p>The marker size (typographic points are 1/72 in.).</p> <code>5</code> <code>figfac</code> <code>float</code> <p>The figure size factor.</p> <code>5</code> <code>fig_title</code> <code>str</code> <p>The title of the figure.</p> <code>'Projection of Samples'</code> <code>axis_pad</code> <code>float</code> <p>The padding of the axis so that points on the boundaries can be seen.</p> <code>0</code> <code>want_grid</code> <code>bool</code> <p>Setting to <code>True</code> will enable grid on the plot.</p> <code>True</code> <code>font_family</code> <code>str</code> <p>The font family of the plot.</p> <code>'sans-serif'</code> <code>where_title</code> <code>float</code> <p>the position of the title on the plot. Default value is 1.</p> <code>1</code> <code>**kwargs</code> <code>dict</code> <p>Additional keyword arguments passed to <code>matplotlib.pyplot.scatter</code>.</p> <code>{}</code> Source code in <code>qmcpy/util/plot_functions.py</code> <pre><code>def plot_proj(\n        sampler, \n        n = 64,\n        d_horizontal = 1, \n        d_vertical = 2,\n        math_ind = True, \n        marker_size = 5, \n        figfac = 5, \n        fig_title = 'Projection of Samples', \n        axis_pad = 0, \n        want_grid = True, \n        font_family = \"sans-serif\", \n        where_title = 1, \n        **kwargs):\n    \"\"\"\n    Args:\n        sampler (DiscreteDistribution,TrueMeasure): The generator of samples to be plotted.\n        n (Union[int,list]): The number of samples or a list of samples(used for extensibility) to be plotted.\n        d_horizontal (Union[int,list]): The dimension or list of dimensions to be plotted on the horizontal axes. \n        d_vertical (Union[int,list]): The dimension or list of dimensions to be plotted on the vertical axes. \n        math_ind (bool): Setting to `True` will enable user to pass in math indices.\n        marker_size (float): The marker size (typographic points are 1/72 in.).\n        figfac (float): The figure size factor.\n        fig_title (str): The title of the figure.\n        axis_pad (float): The padding of the axis so that points on the boundaries can be seen.\n        want_grid (bool): Setting to `True` will enable grid on the plot.\n        font_family (str): The font family of the plot.\n        where_title (float): the position of the title on the plot. Default value is 1.\n        **kwargs (dict): Additional keyword arguments passed to `matplotlib.pyplot.scatter`.\n    \"\"\"\n    try:\n        import matplotlib.pyplot as plt\n        from matplotlib import colors\n        dir_path = os.path.dirname(os.path.realpath(__file__))\n        plt.style.use(os.path.join(dir_path, \"qmcpy.mplstyle\"))\n    except:\n        raise ImportError(\"Missing matplotlib.pyplot as plt, Matplotlib must be installed to run plot_proj function\")\n    plt.rcParams['font.family'] = font_family \n    n = np.atleast_1d(n)\n    d_horizontal = np.atleast_1d(d_horizontal)\n    d_vertical = np.atleast_1d(d_vertical)\n    samples = sampler(n[n.size - 1])    \n    d = samples.shape[1]\n    fig, ax = plt.subplots(nrows=d_horizontal.size, ncols=d_vertical.size, figsize=(figfac*d_horizontal.size, figfac*d_vertical.size),squeeze=False)                    \n\n    colors = plt.rcParams['axes.prop_cycle'].by_key()['color']\n    for i in range(d_horizontal.size):         \n        for j in range(d_vertical.size):\n                n_min = 0\n                for m in range(n.size):\n                    n_max = n[m]  \n                    if(d_horizontal[i] == d_vertical[j]):\n                        ax[i,j].remove()  \n                        break\n                    if(math_ind is True):\n                        x = d_horizontal[i] - 1\n                        y = d_vertical[j] - 1\n                        x_label_num = d_horizontal[i]\n                        y_label_num = d_vertical[j]\n                    else:\n                        x = d_horizontal[i]\n                        y = d_vertical[j]\n                        x_label_num = d_horizontal[i] + 1\n                        y_label_num = d_vertical[j] + 1\n\n                    if type(sampler).__name__==\"AbstractDiscreteDistribution\":\n                        ax[i,j].set_xlim([0-axis_pad,1+axis_pad])\n                        ax[i,j].set_ylim([0-axis_pad,1+axis_pad])\n                        ax[i,j].set_xticks([0,1/4,1/2,3/4,1])\n                        ax[i,j].set_yticks([0,1/4,1/2,3/4,1])\n                        ax[i,j].set_aspect(1)\n                        if not want_grid:\n                            ax[i,j].grid(False) \n                            ax[i,j].tick_params(axis='both', which='both', direction='in', length=5)\n                        ax[i,j].grid(want_grid)\n                        x_label = r'$x_{i%d}$'%(x_label_num)\n                        y_label = r'$x_{i%d}$'%(y_label_num)\n                    else:\n                        x_label = r'$t_{i%d}$'%(x_label_num)\n                        y_label = r'$t_{i%d}$'%(y_label_num)    \n\n                    ax[i,j].set_xlabel(x_label,fontsize = 15)\n                    if(d &gt; 1):\n                        ax[i,j].set_ylabel(y_label,fontsize = 15)\n                        y_axis = samples[n_min:n_max,y]\n                    else:\n                        y_axis = []\n                        for h in range(n_max-n_min):\n                            y_axis.append(0.5)\n                    ax[i,j].scatter(samples[n_min:n_max,x],y_axis,s=marker_size,color=colors[m],label='n_min = %d, n_max = %d'%(n_min,n_max),**kwargs)\n                    n_min = n[m]\n    plt.suptitle(fig_title,fontsize = 20, y = where_title)\n    #fig.text(0.55,0.55,fig_title, ha = 'center', va = 'center', fontsize = 20) %replaced by plt.suptitle\n    fig.tight_layout()#pad=2)\n    return fig, ax\n</code></pre>"},{"location":"api/util/#mlmc_test","title":"<code>mlmc_test</code>","text":"<p>Multilevel Monte Carlo test routine.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; fo = qp.FinancialOption(\n...     sampler=qp.IIDStdUniform(seed=7),\n...     option = \"ASIAN\",\n...     asian_mean = \"GEOMETRIC\",\n...     volatility = 0.2, \n...     start_price = 100, \n...     strike_price = 100, \n...     interest_rate = 0.05, \n...     t_final = 1)\n&gt;&gt;&gt; print('Exact Value: %s'%fo.get_exact_value_inf_dim())\nExact Value: 5.546818633789201\n&gt;&gt;&gt; mlmc_test(fo)\nConvergence tests, kurtosis, telescoping sum check using N =  20000 samples\n    l              ave(Pf-Pc)     ave(Pf)        var(Pf-Pc)     var(Pf)        kurtosis       check          cost\n    0              5.4486e+00     5.4486e+00     5.673e+01      5.673e+01      0.00e+00       0.00e+00       2.00e+00\n    1              1.4925e-01     5.5937e+00     3.839e+00      5.838e+01      5.48e+00       1.14e-02       4.00e+00\n    2              3.5921e-02     5.6024e+00     9.585e-01      5.990e+01      5.59e+00       7.86e-02       8.00e+00\n    3              8.7217e-03     5.5128e+00     2.332e-01      5.828e+01      5.36e+00       2.92e-01       1.60e+01\n    4              1.9773e-03     5.6850e+00     6.021e-02      6.081e+01      5.46e+00       5.12e-01       3.20e+01\n    5              9.5925e-04     5.5628e+00     1.512e-02      5.939e+01      5.37e+00       3.71e-01       6.40e+01\n    6              8.5998e-04     5.5706e+00     3.773e-03      5.995e+01      5.48e+00       2.10e-02       1.28e+02\n    7              1.3592e-04     5.4359e+00     9.285e-04      5.808e+01      5.51e+00       4.13e-01       2.56e+02\n    8              3.4520e-05     5.5322e+00     2.313e-04      5.881e+01      5.57e+00       2.96e-01       5.12e+02\nLinear regression estimates of MLMC parameters\n    alpha = 1.617207  (exponent for MLMC weak convergence)\n    beta  = 2.000355  (exponent for MLMC variance)\n    gamma = 1.000000  (exponent for MLMC cost)\nMLMC complexity tests\n    rmse_tol       value          mlmc_cost      std_cost       savings        N_l\n    5.000e-03      5.545e+00      3.339e+07      1.038e+08      3.11           8605392      1566846      559701       198886       70359        \n    1.000e-02      5.539e+00      7.272e+06      1.243e+07      1.71           2009192      365451       130781       46623        \n    2.000e-02      5.549e+00      1.827e+06      3.108e+06      1.70           503397       91821        33196        11736        \n    5.000e-02      5.474e+00      2.324e+05      2.556e+05      1.10           71432        13143        4617         \n    1.000e-01      5.466e+00      6.220e+04      6.389e+04      1.03           19477        3361         1225         \n</code></pre> <p>Parameters:</p> Name Type Description Default <code>integrand</code> <code>AbstractIntegrand</code> <p>multilevel integrand</p> required <code>n</code> <code>int</code> <p>number of samples for convergence tests</p> <code>20000</code> <code>l</code> <code>int</code> <p>number of levels for convergence tests</p> <code>8</code> <code>n_init</code> <code>int</code> <p>initial number of samples for MLMC calcs</p> <code>200</code> <code>rmse_tols</code> <code>ndarray</code> <p>desired accuracy array for MLMC calcs</p> <code>array([0.005, 0.01, 0.02, 0.05, 0.1])</code> <code>levels_min</code> <code>int</code> <p>minimum number of levels for MLMC calcs</p> <code>2</code> <code>levels_max</code> <code>int</code> <p>maximum number of levels for MLMC calcs</p> <code>10</code> Source code in <code>qmcpy/util/mlmc_test.py</code> <pre><code>def mlmc_test(\n    integrand,\n    n = 20000,\n    l = 8,\n    n_init = 200,\n    rmse_tols = np.array([.005, 0.01, 0.02, 0.05, 0.1]),\n    levels_min = 2,\n    levels_max = 10,\n    ):\n    r\"\"\"\n    Multilevel Monte Carlo test routine.\n\n    Examples:\n        &gt;&gt;&gt; fo = qp.FinancialOption(\n        ...     sampler=qp.IIDStdUniform(seed=7),\n        ...     option = \"ASIAN\",\n        ...     asian_mean = \"GEOMETRIC\",\n        ...     volatility = 0.2, \n        ...     start_price = 100, \n        ...     strike_price = 100, \n        ...     interest_rate = 0.05, \n        ...     t_final = 1)\n        &gt;&gt;&gt; print('Exact Value: %s'%fo.get_exact_value_inf_dim())\n        Exact Value: 5.546818633789201\n        &gt;&gt;&gt; mlmc_test(fo)\n        Convergence tests, kurtosis, telescoping sum check using N =  20000 samples\n            l              ave(Pf-Pc)     ave(Pf)        var(Pf-Pc)     var(Pf)        kurtosis       check          cost\n            0              5.4486e+00     5.4486e+00     5.673e+01      5.673e+01      0.00e+00       0.00e+00       2.00e+00\n            1              1.4925e-01     5.5937e+00     3.839e+00      5.838e+01      5.48e+00       1.14e-02       4.00e+00\n            2              3.5921e-02     5.6024e+00     9.585e-01      5.990e+01      5.59e+00       7.86e-02       8.00e+00\n            3              8.7217e-03     5.5128e+00     2.332e-01      5.828e+01      5.36e+00       2.92e-01       1.60e+01\n            4              1.9773e-03     5.6850e+00     6.021e-02      6.081e+01      5.46e+00       5.12e-01       3.20e+01\n            5              9.5925e-04     5.5628e+00     1.512e-02      5.939e+01      5.37e+00       3.71e-01       6.40e+01\n            6              8.5998e-04     5.5706e+00     3.773e-03      5.995e+01      5.48e+00       2.10e-02       1.28e+02\n            7              1.3592e-04     5.4359e+00     9.285e-04      5.808e+01      5.51e+00       4.13e-01       2.56e+02\n            8              3.4520e-05     5.5322e+00     2.313e-04      5.881e+01      5.57e+00       2.96e-01       5.12e+02\n        Linear regression estimates of MLMC parameters\n            alpha = 1.617207  (exponent for MLMC weak convergence)\n            beta  = 2.000355  (exponent for MLMC variance)\n            gamma = 1.000000  (exponent for MLMC cost)\n        MLMC complexity tests\n            rmse_tol       value          mlmc_cost      std_cost       savings        N_l\n            5.000e-03      5.545e+00      3.339e+07      1.038e+08      3.11           8605392      1566846      559701       198886       70359        \n            1.000e-02      5.539e+00      7.272e+06      1.243e+07      1.71           2009192      365451       130781       46623        \n            2.000e-02      5.549e+00      1.827e+06      3.108e+06      1.70           503397       91821        33196        11736        \n            5.000e-02      5.474e+00      2.324e+05      2.556e+05      1.10           71432        13143        4617         \n            1.000e-01      5.466e+00      6.220e+04      6.389e+04      1.03           19477        3361         1225         \n\n    Args:\n        integrand (AbstractIntegrand): multilevel integrand\n        n (int): number of samples for convergence tests\n        l (int): number of levels for convergence tests\n        n_init (int): initial number of samples for MLMC calcs\n        rmse_tols (np.ndarray): desired accuracy array for MLMC calcs\n        levels_min (int): minimum number of levels for MLMC calcs\n        levels_max (int): maximum number of levels for MLMC calcs\n    \"\"\"\n    # first, convergence tests\n    n = 100*np.ceil(n/100) # make N a multiple of 100\n    print('Convergence tests, kurtosis, telescoping sum check using N =%7d samples'%n)\n    print('    %-15s%-15s%-15s%-15s%-15s%-15s%-15s%s'\\\n        %('l','ave(Pf-Pc)','ave(Pf)','var(Pf-Pc)','var(Pf)','kurtosis','check','cost'))\n    del1 = np.array([])\n    del2 = np.array([])\n    var1 = np.array([])\n    var2 = np.array([])\n    kur1 = np.array([])\n    chk1 = np.array([])\n    cost = np.array([])\n    integrand_spawns = integrand.spawn(levels=np.arange(l+1))\n    for ll in range(l+1):\n        sums = np.zeros(6)\n        cst = 0\n        integrand_spawn = integrand_spawns[ll]\n        for j in range(1,101):\n            # evaluate integral at sampleing points samples\n            samples = integrand_spawn.discrete_distrib.gen_samples(n=n/100)\n            Pc,Pf = integrand_spawn.f(samples)\n            dP = Pf-Pc\n            sums_j = np.array([\n                np.sum(dP),\n                np.sum(dP**2),\n                np.sum(dP**3),\n                np.sum(dP**4),\n                np.sum(Pf),\n                np.sum(Pf**2),\n            ])\n            cst_j = integrand_spawn.cost*(n/100)\n            sums = sums + sums_j/n\n            cst = cst + cst_j/n\n        if ll == 0:\n            kurt = 0.\n        else:\n            kurt = ( sums[3] - 4*sums[2]*sums[0] + 6*sums[1]*sums[0]**2 - \n                     3*sums[0]*sums[0]**3 ) /  (sums[1]-sums[0]**2)**2.\n        cost = np.hstack((cost, cst))\n        del1 = np.hstack((del1, sums[0]))\n        del2 = np.hstack((del2, sums[4]))\n        var1 = np.hstack((var1, sums[1]-sums[0]**2))\n        var2 = np.hstack((var2, sums[5]-sums[4]**2))\n        var2 = np.maximum(var2, 1e-10) # fix for cases with var=0\n        kur1 = np.hstack((kur1, kurt))\n        if ll == 0:\n            check = 0\n        else:\n            check = abs( del1[ll] + del2[ll-1] - del2[ll]) / \\\n                    ( 3.*( np.sqrt(var1[ll]) + np.sqrt(var2[ll-1]) + np.sqrt(var2[ll]) ) / np.sqrt(n))\n        chk1 = np.hstack((chk1, check))\n\n        print('    %-15d%-15.4e%-15.4e%-15.3e%-15.3e%-15.2e%-15.2e%.2e'\\\n              %(ll,del1[ll],del2[ll],var1[ll],var2[ll],kur1[ll],chk1[ll],cst))\n    # print out a warning if kurtosis or consistency check looks bad\n    if kur1[-1] &gt; 100.:\n        print('WARNING: kurtosis on finest level = %f'%kur1[-1])\n        print(' indicates MLMC correction dominated by a few rare paths;')\n        print(' for information on the connection to variance of sample variances,')\n        print(' see http://mathworld.wolfram.com/SampleVarianceDistribution.html\\n')\n    if np.max(chk1) &gt; 1.:\n        print('WARNING: maximum consistency error = %f'%max(chk1))\n        print(' indicates identity E[Pf-Pc] = E[Pf] - E[Pc] not satisfied;')\n        print(' to be more certain, re-run mlmc_test with larger N\\n')\n    # use linear regression to estimate alpha, beta and gamma\n    l1 = 2\n    l2 = l+1\n    x = np.ones((l2+1-l1,2))\n    x[:,1] = np.arange(l1,l2+1)\n    pa = np.linalg.lstsq(x,np.log2(np.absolute(del1[(l1-1):l2])),rcond=None)[0]\n    alpha = -pa[1]\n    pb = np.linalg.lstsq(x,np.log2(np.absolute(var1[(l1-1):l2])),rcond=None)[0]\n    beta = -pb[1]\n    pg = np.linalg.lstsq(x,np.log2(np.absolute(cost[(l1-1):l2])),rcond=None)[0]\n    gamma = pg[1]\n    print('Linear regression estimates of MLMC parameters')\n    print('    alpha = %f  (exponent for MLMC weak convergence)'%alpha)\n    print('    beta  = %f  (exponent for MLMC variance)'%beta)\n    print('    gamma = %f  (exponent for MLMC cost)'%gamma)\n    #second, mlmc complexity tests\n    print('MLMC complexity tests')\n    print('    %-15s%-15s%-15s%-15s%-15s%s'\\\n        %('rmse_tol','value','mlmc_cost','std_cost','savings','N_l'))\n    alpha = np.maximum(alpha,0.5)\n    beta  = np.maximum(beta,0.5)\n    theta = 0.25\n    for i in range(len(rmse_tols)):\n        mlmc_qmcpy = qp.CubMLMC(integrand,\n            rmse_tol = rmse_tols[i],\n            n_init = n_init,\n            levels_min = levels_min,\n            levels_max = levels_max,\n            alpha0 = alpha,\n            beta0 = beta,\n            gamma0 = gamma)\n        sol,data = mlmc_qmcpy.integrate()\n        p = data.solution\n        nl = data.n_level\n        cl = data.cost_per_sample\n        mlmc_cost = sum(nl*cl)\n        idx = np.minimum(len(var2),len(nl))-1\n        std_cost = var2[idx]*cl[-1] / ((1.-theta)*rmse_tols[i]**2)\n        print('    %-15.3e%-15.3e%-15.3e%-15.3e%-15.2f%s'\\\n            %(rmse_tols[i], p, mlmc_cost, std_cost, std_cost/mlmc_cost,''.join('%-13d'%nli for nli in nl)))\n</code></pre>"},{"location":"api/util/#shift-invariant-ops","title":"Shift Invariant Ops","text":""},{"location":"api/util/#utilbernoulli_poly","title":"<code>util.bernoulli_poly</code>","text":"<p>\\(n^\\text{th}\\) Bernoulli polynomial</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; x = np.arange(6).reshape((2,3))/6\n&gt;&gt;&gt; available_n = list(BERNOULLIPOLYSDICT.keys())\n&gt;&gt;&gt; available_n\n[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n&gt;&gt;&gt; for n in available_n:\n...     y = bernoulli_poly(n,x)\n...     with np.printoptions(precision=2):\n...         print(\"n = %d\\n%s\"%(n,y))\nn = 1\n[[-0.5  -0.33 -0.17]\n [ 0.    0.17  0.33]]\nn = 2\n[[ 0.17  0.03 -0.06]\n [-0.08 -0.06  0.03]]\nn = 3\n[[ 0.    0.05  0.04]\n [ 0.   -0.04 -0.05]]\nn = 4\n[[-0.03 -0.01  0.02]\n [ 0.03  0.02 -0.01]]\nn = 5\n[[ 0.00e+00 -2.19e-02 -2.06e-02]\n [ 1.39e-17  2.06e-02  2.19e-02]]\nn = 6\n[[ 0.02  0.01 -0.01]\n [-0.02 -0.01  0.01]]\nn = 7\n[[ 0.00e+00  2.28e-02  2.24e-02]\n [-1.39e-17 -2.24e-02 -2.28e-02]]\nn = 8\n[[-0.03 -0.02  0.02]\n [ 0.03  0.02 -0.02]]\nn = 9\n[[ 0.   -0.04 -0.04]\n [ 0.    0.04  0.04]]\nn = 10\n[[ 0.08  0.04 -0.04]\n [-0.08 -0.04  0.04]]\n&gt;&gt;&gt; import scipy.special\n&gt;&gt;&gt; for n in available_n:\n...     bpoly_coeffs = BERNOULLIPOLYSDICT[n].coeffs\n...     bpoly_coeffs_true = scipy.special.bernoulli(n)*scipy.special.comb(n,np.arange(n,-1,-1))\n...     assert np.allclose(bpoly_coeffs_true,bpoly_coeffs,atol=1e-12)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>int</code> <p>Polynomial order.</p> required <code>x</code> <code>Union[ndarray, Tensor]</code> <p>Points at which to evaluate the Bernoulli polynomial.</p> required <p>Returns:</p> Name Type Description <code>y</code> <code>Union[ndarray, Tensor]</code> <p>Bernoulli polynomial values.</p> Source code in <code>qmcpy/util/shift_invar_ops.py</code> <pre><code>def bernoulli_poly(n, x):\n    r\"\"\"\n    $n^\\text{th}$ Bernoulli polynomial\n\n    Examples:\n        &gt;&gt;&gt; x = np.arange(6).reshape((2,3))/6\n        &gt;&gt;&gt; available_n = list(BERNOULLIPOLYSDICT.keys())\n        &gt;&gt;&gt; available_n\n        [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n        &gt;&gt;&gt; for n in available_n:\n        ...     y = bernoulli_poly(n,x)\n        ...     with np.printoptions(precision=2):\n        ...         print(\"n = %d\\n%s\"%(n,y))\n        n = 1\n        [[-0.5  -0.33 -0.17]\n         [ 0.    0.17  0.33]]\n        n = 2\n        [[ 0.17  0.03 -0.06]\n         [-0.08 -0.06  0.03]]\n        n = 3\n        [[ 0.    0.05  0.04]\n         [ 0.   -0.04 -0.05]]\n        n = 4\n        [[-0.03 -0.01  0.02]\n         [ 0.03  0.02 -0.01]]\n        n = 5\n        [[ 0.00e+00 -2.19e-02 -2.06e-02]\n         [ 1.39e-17  2.06e-02  2.19e-02]]\n        n = 6\n        [[ 0.02  0.01 -0.01]\n         [-0.02 -0.01  0.01]]\n        n = 7\n        [[ 0.00e+00  2.28e-02  2.24e-02]\n         [-1.39e-17 -2.24e-02 -2.28e-02]]\n        n = 8\n        [[-0.03 -0.02  0.02]\n         [ 0.03  0.02 -0.02]]\n        n = 9\n        [[ 0.   -0.04 -0.04]\n         [ 0.    0.04  0.04]]\n        n = 10\n        [[ 0.08  0.04 -0.04]\n         [-0.08 -0.04  0.04]]\n        &gt;&gt;&gt; import scipy.special\n        &gt;&gt;&gt; for n in available_n:\n        ...     bpoly_coeffs = BERNOULLIPOLYSDICT[n].coeffs\n        ...     bpoly_coeffs_true = scipy.special.bernoulli(n)*scipy.special.comb(n,np.arange(n,-1,-1))\n        ...     assert np.allclose(bpoly_coeffs_true,bpoly_coeffs,atol=1e-12)\n\n    Args:\n        n (int): Polynomial order.\n        x (Union[np.ndarray,torch.Tensor]): Points at which to evaluate the Bernoulli polynomial.\n\n    Returns:\n        y (Union[np.ndarray,torch.Tensor]): Bernoulli polynomial values.\n    \"\"\"\n    assert isinstance(n,int)\n    assert n in BERNOULLIPOLYSDICT, \"n = %d not in BERNOULLIPOLYSDICT\"%n\n    bpoly = BERNOULLIPOLYSDICT[n]\n    y = bpoly(x) \n    return y\n</code></pre>"},{"location":"api/util/#digitally-shift-invariant-ops","title":"Digitally Shift Invariant Ops","text":""},{"location":"api/util/#utilweighted_walsh_funcs","title":"<code>util.weighted_walsh_funcs</code>","text":"<p>Weighted walsh functions </p> \\[\\sum_{k=0}^\\infty \\mathrm{wal}_k(x) 2^{-\\mu_\\alpha(k)}\\] <p>where \\(\\mathrm{wal}_k\\) is the \\(k^\\text{th}\\) Walsh function  and \\(\\mu_\\alpha\\) is the Dick weight function which sums the first \\(\\alpha\\) largest indices of \\(1\\) bits in the binary expansion of \\(k\\)  e.g. \\(k=13=1101_2\\) has 1-bit indexes \\((4,3,1)\\) so </p> \\[\\mu_1(k) = 4, \\mu_2(k) = 4+3, \\mu_3(k) = 4+3+1 = \\mu_4(k) = \\mu_5(k) = \\dots\\] <p>Examples:</p> <pre><code>&gt;&gt;&gt; t = 3 \n&gt;&gt;&gt; rng = np.random.Generator(np.random.SFC64(11))\n&gt;&gt;&gt; xb = rng.integers(low=0,high=2**t,size=(2,3))\n&gt;&gt;&gt; available_alpha = list(WEIGHTEDWALSHFUNCSPOS.keys())\n&gt;&gt;&gt; available_alpha\n[2, 3, 4]\n&gt;&gt;&gt; for alpha in available_alpha:\n...     y = weighted_walsh_funcs(alpha,xb,t)\n...     with np.printoptions(precision=2):\n...         print(\"alpha = %d\\n%s\"%(alpha,y))\nalpha = 2\n[[1.81 1.38 1.81]\n [0.62 1.81 2.5 ]]\nalpha = 3\n[[1.85 1.43 1.85]\n [0.62 1.85 2.39]]\nalpha = 4\n[[1.85 1.43 1.85]\n [0.62 1.85 2.38]]\n&gt;&gt;&gt; import torch\n&gt;&gt;&gt; for alpha in available_alpha:\n...     y = weighted_walsh_funcs(alpha,torch.from_numpy(xb),t)\n...     with torch._tensor_str.printoptions(precision=2):\n...         print(\"alpha = %d\\n%s\"%(alpha,y))\nalpha = 2\ntensor([[1.81, 1.38, 1.81],\n        [0.62, 1.81, 2.50]])\nalpha = 3\ntensor([[1.85, 1.43, 1.85],\n        [0.62, 1.85, 2.39]])\nalpha = 4\ntensor([[1.85, 1.43, 1.85],\n        [0.62, 1.85, 2.38]])\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>alpha</code> <code>int</code> <p>Weighted walsh functions order.</p> required <code>xb</code> <code>Union[ndarray, Tensor]</code> <p>Jnteger points at which to evaluate the weighted Walsh function.</p> required <code>t</code> <code>int</code> <p>Number of bits in each integer in xb.</p> required <p>Returns:</p> Name Type Description <code>y</code> <code>Union[ndarray, Tensor]</code> <p>Weighted Walsh function values.</p> <p>References:</p> <ol> <li> <p>Dick, Josef.     \"Walsh spaces containing smooth functions and quasi\u2013Monte Carlo rules of arbitrary high order.\"     SIAM Journal on Numerical Analysis 46.3 (2008): 1519-1553.</p> </li> <li> <p>Dick, Josef.     \"The decay of the Walsh coefficients of smooth functions.\"     Bulletin of the Australian Mathematical Society 80.3 (2009): 430-453.</p> </li> </ol> Source code in <code>qmcpy/util/dig_shift_invar_ops.py</code> <pre><code>def weighted_walsh_funcs(alpha, xb, t):\n    r\"\"\"\n    Weighted walsh functions \n\n    $$\\sum_{k=0}^\\infty \\mathrm{wal}_k(x) 2^{-\\mu_\\alpha(k)}$$ \n\n    where $\\mathrm{wal}_k$ is the $k^\\text{th}$ Walsh function \n    and $\\mu_\\alpha$ is the Dick weight function which sums the first $\\alpha$ largest indices of $1$ bits in the binary expansion of $k$ \n    e.g. $k=13=1101_2$ has 1-bit indexes $(4,3,1)$ so \n\n    $$\\mu_1(k) = 4, \\mu_2(k) = 4+3, \\mu_3(k) = 4+3+1 = \\mu_4(k) = \\mu_5(k) = \\dots$$\n\n    Examples:\n        &gt;&gt;&gt; t = 3 \n        &gt;&gt;&gt; rng = np.random.Generator(np.random.SFC64(11))\n        &gt;&gt;&gt; xb = rng.integers(low=0,high=2**t,size=(2,3))\n        &gt;&gt;&gt; available_alpha = list(WEIGHTEDWALSHFUNCSPOS.keys())\n        &gt;&gt;&gt; available_alpha\n        [2, 3, 4]\n        &gt;&gt;&gt; for alpha in available_alpha:\n        ...     y = weighted_walsh_funcs(alpha,xb,t)\n        ...     with np.printoptions(precision=2):\n        ...         print(\"alpha = %d\\n%s\"%(alpha,y))\n        alpha = 2\n        [[1.81 1.38 1.81]\n         [0.62 1.81 2.5 ]]\n        alpha = 3\n        [[1.85 1.43 1.85]\n         [0.62 1.85 2.39]]\n        alpha = 4\n        [[1.85 1.43 1.85]\n         [0.62 1.85 2.38]]\n        &gt;&gt;&gt; import torch\n        &gt;&gt;&gt; for alpha in available_alpha:\n        ...     y = weighted_walsh_funcs(alpha,torch.from_numpy(xb),t)\n        ...     with torch._tensor_str.printoptions(precision=2):\n        ...         print(\"alpha = %d\\n%s\"%(alpha,y))\n        alpha = 2\n        tensor([[1.81, 1.38, 1.81],\n                [0.62, 1.81, 2.50]])\n        alpha = 3\n        tensor([[1.85, 1.43, 1.85],\n                [0.62, 1.85, 2.39]])\n        alpha = 4\n        tensor([[1.85, 1.43, 1.85],\n                [0.62, 1.85, 2.38]])\n\n    Args:\n        alpha (int): Weighted walsh functions order.\n        xb (Union[np.ndarray,torch.Tensor]): Jnteger points at which to evaluate the weighted Walsh function.\n        t (int): Number of bits in each integer in xb.\n\n    returns:\n        y (Union[np.ndarray,torch.Tensor]): Weighted Walsh function values.\n\n    **References:**\n\n    1.  Dick, Josef.  \n        \"Walsh spaces containing smooth functions and quasi\u2013Monte Carlo rules of arbitrary high order.\"  \n        SIAM Journal on Numerical Analysis 46.3 (2008): 1519-1553.\n\n    2.  Dick, Josef.  \n        \"The decay of the Walsh coefficients of smooth functions.\"  \n        Bulletin of the Australian Mathematical Society 80.3 (2009): 430-453.    \n    \"\"\"\n    assert isinstance(alpha,int)\n    assert alpha in WEIGHTEDWALSHFUNCSPOS, \"alpha = %d not in WEIGHTEDWALSHFUNCSPOS\"%alpha\n    assert alpha in WEIGHTEDWALSHFUNCSZEROS, \"alpha = %d not in WEIGHTEDWALSHFUNCSZEROS\"%alpha\n    if isinstance(xb,np.ndarray):\n        np_or_torch = np \n        y = np.ones(xb.shape) \n    else:\n        import torch \n        np_or_torch = torch \n        y = torch.ones(xb.shape,device=xb.device)\n    pidxs = xb&gt;0\n    y[~pidxs] = WEIGHTEDWALSHFUNCSZEROS[alpha]\n    xfpidxs = (2**(-t))*xb[pidxs]\n    betapidxs = -np_or_torch.floor(np_or_torch.log2(xfpidxs))\n    y[pidxs] = WEIGHTEDWALSHFUNCSPOS[alpha](betapidxs,xfpidxs,xb[pidxs],t)\n    return y\n</code></pre>"},{"location":"api/util/#utilk4sumterm","title":"<code>util.k4sumterm</code>","text":"\\[K_4(x) = \\sum_{a=0}^{t-1} \\frac{x_a}{2^{3a}}\\] <p>where \\(x_a\\) is the bit at index \\(a\\) in the binary expansion of \\(x\\) e.g. \\(x = 6\\) with \\(t=3\\) has \\((x_0,x_1,x_2) = (1,1,0)\\)</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; t = 3\n&gt;&gt;&gt; rng = np.random.Generator(np.random.SFC64(11))\n&gt;&gt;&gt; x = rng.integers(low=0,high=2**t,size=(5,4))\n&gt;&gt;&gt; with np.printoptions(precision=2):\n...     k4sumterm(x,t)\narray([[ 1.11,  0.89,  1.11, -0.89],\n       [ 1.11,  1.14,  1.11, -0.86],\n       [-0.89,  0.86, -1.11,  0.89],\n       [-1.11,  0.89, -0.89,  0.89],\n       [-1.14, -0.89, -0.89, -0.86]])\n&gt;&gt;&gt; import torch \n&gt;&gt;&gt; with torch._tensor_str.printoptions(precision=2):\n...     k4sumterm(torch.from_numpy(x),t)\ntensor([[ 1.11,  0.89,  1.11, -0.89],\n        [ 1.11,  1.14,  1.11, -0.86],\n        [-0.89,  0.86, -1.11,  0.89],\n        [-1.11,  0.89, -0.89,  0.89],\n        [-1.14, -0.89, -0.89, -0.86]])\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Union[np.ndarray torch.Tensor]</code> <p>Integer arrays.</p> required <code>t</code> <code>int</code> <p>Number of bits in each integer.</p> required <p>Returns:</p> Name Type Description <code>y</code> <code>Union[np.ndarray torch.Tensor]</code> <p>The \\(K_4\\) sum term.</p> Source code in <code>qmcpy/util/dig_shift_invar_ops.py</code> <pre><code>def k4sumterm(x, t, cutoff=1e-8):\n    r\"\"\" \n    $$K_4(x) = \\sum_{a=0}^{t-1} \\frac{x_a}{2^{3a}}$$\n\n    where $x_a$ is the bit at index $a$ in the binary expansion of $x$\n    e.g. $x = 6$ with $t=3$ has $(x_0,x_1,x_2) = (1,1,0)$\n\n    Examples:\n        &gt;&gt;&gt; t = 3\n        &gt;&gt;&gt; rng = np.random.Generator(np.random.SFC64(11))\n        &gt;&gt;&gt; x = rng.integers(low=0,high=2**t,size=(5,4))\n        &gt;&gt;&gt; with np.printoptions(precision=2):\n        ...     k4sumterm(x,t)\n        array([[ 1.11,  0.89,  1.11, -0.89],\n               [ 1.11,  1.14,  1.11, -0.86],\n               [-0.89,  0.86, -1.11,  0.89],\n               [-1.11,  0.89, -0.89,  0.89],\n               [-1.14, -0.89, -0.89, -0.86]])\n        &gt;&gt;&gt; import torch \n        &gt;&gt;&gt; with torch._tensor_str.printoptions(precision=2):\n        ...     k4sumterm(torch.from_numpy(x),t)\n        tensor([[ 1.11,  0.89,  1.11, -0.89],\n                [ 1.11,  1.14,  1.11, -0.86],\n                [-0.89,  0.86, -1.11,  0.89],\n                [-1.11,  0.89, -0.89,  0.89],\n                [-1.14, -0.89, -0.89, -0.86]])\n\n    Args:\n        x (Union[np.ndarray torch.Tensor]): Integer arrays.\n        t (int): Number of bits in each integer.\n\n    Returns:\n        y (Union[np.ndarray torch.Tensor]): The $K_4$ sum term.\n    \"\"\"\n    total = 0.\n    for a in range(0,t):\n        factor = 1/float(2.**(3*a))\n        if factor&lt;cutoff: break\n        total += (-1.)**((x&gt;&gt;(t-a-1))&amp;1)*factor\n    return total\n</code></pre>"},{"location":"api/util/#utilto_float","title":"<code>util.to_float</code>","text":"<p>Convert binary representations of digital net samples in base \\(b=2\\) to floating point representations.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; xb = np.arange(8,dtype=np.uint64)\n&gt;&gt;&gt; xb \narray([0, 1, 2, 3, 4, 5, 6, 7], dtype=uint64)\n&gt;&gt;&gt; to_float(xb,3)\narray([0.   , 0.125, 0.25 , 0.375, 0.5  , 0.625, 0.75 , 0.875])\n&gt;&gt;&gt; xbtorch = bin_from_numpy_to_torch(xb)\n&gt;&gt;&gt; xbtorch\ntensor([0, 1, 2, 3, 4, 5, 6, 7])\n&gt;&gt;&gt; to_float(xbtorch,3)\ntensor([0.0000, 0.1250, 0.2500, 0.3750, 0.5000, 0.6250, 0.7500, 0.8750])\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Union[ndarray, Tensor]</code> <p>binary representation of samples with <code>dtype</code> either <code>np.uint64</code> or <code>torch.int64</code>.</p> required <code>t</code> <code>int</code> <p>number of bits in binary represtnations. Typically <code>dnb2.t</code> where <code>isinstance(dnb2,DigitalNetB2)</code>.</p> required <p>Returns:</p> Name Type Description <code>xf</code> <code>Unioin[ndarray, Tensor]</code> <p>floating point representation of samples.</p> Source code in <code>qmcpy/util/dig_shift_invar_ops.py</code> <pre><code>def to_float(x, t):\n    r\"\"\"\n    Convert binary representations of digital net samples in base $b=2$ to floating point representations.\n\n    Examples:\n        &gt;&gt;&gt; xb = np.arange(8,dtype=np.uint64)\n        &gt;&gt;&gt; xb \n        array([0, 1, 2, 3, 4, 5, 6, 7], dtype=uint64)\n        &gt;&gt;&gt; to_float(xb,3)\n        array([0.   , 0.125, 0.25 , 0.375, 0.5  , 0.625, 0.75 , 0.875])\n        &gt;&gt;&gt; xbtorch = bin_from_numpy_to_torch(xb)\n        &gt;&gt;&gt; xbtorch\n        tensor([0, 1, 2, 3, 4, 5, 6, 7])\n        &gt;&gt;&gt; to_float(xbtorch,3)\n        tensor([0.0000, 0.1250, 0.2500, 0.3750, 0.5000, 0.6250, 0.7500, 0.8750])\n\n    Args:\n        x (Union[np.ndarray,torch.Tensor]): binary representation of samples with `dtype` either `np.uint64` or `torch.int64`.\n        t (int): number of bits in binary represtnations. Typically `dnb2.t` where `isinstance(dnb2,DigitalNetB2)`.\n\n    Returns: \n        xf (Unioin[np.ndarray,torch.Tensor]): floating point representation of samples.  \n    \"\"\"\n    npt = get_npt(x)\n    if npt==np: # npt==torch\n        if x.dtype==np.uint64: \n            return x.astype(np.float64)*2.**(-t)\n        elif npt.is_floating_point(x):\n            return x \n        else: \n            raise ParameterError(\"x.dtype must be np.uint64, got %s\"%str(x.dtype))\n    else:\n        if x.dtype==npt.int64: \n            return x.to(npt.get_default_dtype())*2.**(-t) \n        elif npt.is_floating_point(x):\n            return x \n        else:\n            raise ParameterError(\"x.dtype must be torch.int64, got %s\"%str(x.dtype))\n</code></pre>"},{"location":"api/util/#utilto_bin","title":"<code>util.to_bin</code>","text":"<p>Convert floating point representations of digital net samples in base \\(b=2\\) to binary representations.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; xf = np.random.Generator(np.random.PCG64(7)).uniform(low=0,high=1,size=(5))\n&gt;&gt;&gt; xf \narray([0.62509547, 0.8972138 , 0.77568569, 0.22520719, 0.30016628])\n&gt;&gt;&gt; xb = to_bin(xf,2)\n&gt;&gt;&gt; xb\narray([2, 3, 3, 0, 1], dtype=uint64)\n&gt;&gt;&gt; to_bin(xb,2) \narray([2, 3, 3, 0, 1], dtype=uint64)\n&gt;&gt;&gt; import torch \n&gt;&gt;&gt; xftorch = torch.from_numpy(xf) \n&gt;&gt;&gt; xftorch\ntensor([0.6251, 0.8972, 0.7757, 0.2252, 0.3002], dtype=torch.float64)\n&gt;&gt;&gt; xbtorch = to_bin(xftorch,2)\n&gt;&gt;&gt; xbtorch\ntensor([2, 3, 3, 0, 1])\n&gt;&gt;&gt; to_bin(xbtorch,2)\ntensor([2, 3, 3, 0, 1])\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Union[ndarray, Tensor]</code> <p>floating point representation of samples. </p> required <code>t</code> <code>int</code> <p>number of bits in binary represtnations. Typically <code>dnb2.t</code> where <code>isinstance(dnb2,DigitalNetB2)</code>.</p> required <p>Returns:</p> Name Type Description <code>xb</code> <code>Unioin[ndarray, Tensor]</code> <p>binary representation of samples with <code>dtype</code> either <code>np.uint64</code> or <code>torch.int64</code>.</p> Source code in <code>qmcpy/util/dig_shift_invar_ops.py</code> <pre><code>def to_bin(x, t):\n    r\"\"\"\n    Convert floating point representations of digital net samples in base $b=2$ to binary representations.\n\n    Examples:\n        &gt;&gt;&gt; xf = np.random.Generator(np.random.PCG64(7)).uniform(low=0,high=1,size=(5))\n        &gt;&gt;&gt; xf \n        array([0.62509547, 0.8972138 , 0.77568569, 0.22520719, 0.30016628])\n        &gt;&gt;&gt; xb = to_bin(xf,2)\n        &gt;&gt;&gt; xb\n        array([2, 3, 3, 0, 1], dtype=uint64)\n        &gt;&gt;&gt; to_bin(xb,2) \n        array([2, 3, 3, 0, 1], dtype=uint64)\n        &gt;&gt;&gt; import torch \n        &gt;&gt;&gt; xftorch = torch.from_numpy(xf) \n        &gt;&gt;&gt; xftorch\n        tensor([0.6251, 0.8972, 0.7757, 0.2252, 0.3002], dtype=torch.float64)\n        &gt;&gt;&gt; xbtorch = to_bin(xftorch,2)\n        &gt;&gt;&gt; xbtorch\n        tensor([2, 3, 3, 0, 1])\n        &gt;&gt;&gt; to_bin(xbtorch,2)\n        tensor([2, 3, 3, 0, 1])\n\n\n    Args:\n        x (Union[np.ndarray,torch.Tensor]): floating point representation of samples. \n        t (int): number of bits in binary represtnations. Typically `dnb2.t` where `isinstance(dnb2,DigitalNetB2)`.\n\n    Returns: \n        xb (Unioin[np.ndarray,torch.Tensor]): binary representation of samples with `dtype` either `np.uint64` or `torch.int64`. \n    \"\"\"\n    npt = get_npt(x)\n    if npt==np:\n        if npt.issubdtype(x.dtype,npt.floating):\n            return npt.floor((x%1)*2.**t).astype(npt.uint64)\n        elif npt.issubdtype(x.dtype,npt.integer):\n            return x\n        else:\n            raise ParameterError(\"x.dtype must be float or int, got %s\"%str(x.dtype))\n    else: # npt==torch\n        if npt.is_floating_point(x):\n            return npt.floor((x%1)*2.**t).to(npt.int64)\n        elif (not npt.is_floating_point(x)) and (not npt.is_complex(x)): # int type \n            return x \n        else :\n            raise ParameterError(\"x.dtype must be float or int, got %s\"%str(x.dtype))\n    return xb\n</code></pre>"},{"location":"api/util/#utilbin_from_numpy_to_torch","title":"<code>util.bin_from_numpy_to_torch</code>","text":"<p>Convert <code>numpy.uint64</code> to <code>torch.int64</code>, useful for converting binary samples from <code>DigitalNetB2</code> to torch representations.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; xb = np.arange(8,dtype=np.uint64)\n&gt;&gt;&gt; xb \narray([0, 1, 2, 3, 4, 5, 6, 7], dtype=uint64)\n&gt;&gt;&gt; bin_from_numpy_to_torch(xb)\ntensor([0, 1, 2, 3, 4, 5, 6, 7])\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>xb</code> <code>Union[ndarray]</code> <p>binary representation of samples with <code>dtype=np.uint64</code></p> required <p>Returns:</p> Name Type Description <code>xbtorch</code> <code>Unioin[Tensor]</code> <p>binary representation of samples with <code>dtype=torch.int64</code>.</p> Source code in <code>qmcpy/util/dig_shift_invar_ops.py</code> <pre><code>def bin_from_numpy_to_torch(xb):\n    r\"\"\"\n    Convert `numpy.uint64` to `torch.int64`, useful for converting binary samples from `DigitalNetB2` to torch representations.\n\n    Examples:\n        &gt;&gt;&gt; xb = np.arange(8,dtype=np.uint64)\n        &gt;&gt;&gt; xb \n        array([0, 1, 2, 3, 4, 5, 6, 7], dtype=uint64)\n        &gt;&gt;&gt; bin_from_numpy_to_torch(xb)\n        tensor([0, 1, 2, 3, 4, 5, 6, 7])\n\n    Args:\n        xb (Union[np.ndarray]): binary representation of samples with `dtype=np.uint64`\n\n    Returns: \n        xbtorch (Unioin[torch.Tensor]): binary representation of samples with `dtype=torch.int64`.  \n    \"\"\"\n    assert xb.dtype==np.uint64\n    assert xb.max()&lt;=(2**63-1), \"require all xb &lt; 2^63\"\n    import torch\n    return torch.from_numpy(xb.astype(np.int64))\n</code></pre>"},{"location":"demos/asian-option-mlqmc/","title":"Multilevel (Q)MC for Option Pricing","text":"In\u00a0[1]: Copied! <pre>import matplotlib.pyplot as plt\nimport numpy as np\nimport qmcpy as qp\n</pre> import matplotlib.pyplot as plt import numpy as np import qmcpy as qp In\u00a0[2]: Copied! <pre>seed = 7\n</pre> seed = 7 <p>Compute the exact value of the Asian option with single level QMC, for an increasing number of time steps:</p> In\u00a0[4]: Copied! <pre>for level in range(5):\n    aco = qp.FinancialOption(qp.Sobol(2*2**level, seed=seed), option=\"ASIAN\", volatility=.2, start_price=100, strike_price=100, interest_rate=.05)\n    approx_solution, data = qp.CubQMCSobolG(aco, abs_tol=1e-4).integrate()\n    print(\"Asian Option true value (%d time steps): %.5f (to within 1e-4)\"%(2*2**level, approx_solution))\n</pre> for level in range(5):     aco = qp.FinancialOption(qp.Sobol(2*2**level, seed=seed), option=\"ASIAN\", volatility=.2, start_price=100, strike_price=100, interest_rate=.05)     approx_solution, data = qp.CubQMCSobolG(aco, abs_tol=1e-4).integrate()     print(\"Asian Option true value (%d time steps): %.5f (to within 1e-4)\"%(2*2**level, approx_solution)) <pre>Asian Option true value (2 time steps): 5.63593 (to within 1e-4)\nAsian Option true value (4 time steps): 5.73171 (to within 1e-4)\nAsian Option true value (8 time steps): 5.75526 (to within 1e-4)\nAsian Option true value (16 time steps): 5.76112 (to within 1e-4)\nAsian Option true value (32 time steps): 5.76260 (to within 1e-4)\n</pre> <p>This function compares 4 different algorithms: Multilevel Monte Carlo (<code>CubMLMC</code>), Multilevel Quasi-Monte Carlo (<code>CubMLQMC</code>), continuation Multilevel Monte Carlo (<code>CubMLMCCont</code>) and Multilevel Quasi-Monte Carlo (<code>CubMLQMCCont</code>):</p> In\u00a0[5]: Copied! <pre>def eval_option(option_mc, option_qmc, abs_tol):\n    stopping_criteria = {\n        \"MLMC\" : qp.CubMLMC(option_mc, abs_tol=abs_tol, levels_max=15),\n        \"continuation MLMC\" : qp.CubMLMCCont(option_mc, abs_tol=abs_tol, levels_max=15),\n        \"MLQMC\" : qp.CubMLQMC(option_qmc, abs_tol=abs_tol, levels_max=15),\n        \"continuation MLQMC\" : qp.CubMLQMCCont(option_qmc, abs_tol=abs_tol, levels_max=15)\n    }\n    \n    levels = []\n    times = []\n    for name, stopper in stopping_criteria.items():\n        sol, data = stopper.integrate()\n        levels.append(data.levels)\n        times.append(data.time_integrate)\n        print(\"\\t%-20s solution %-10.4f number of levels %-6d time %.3f\"%(name, sol, levels[-1], times[-1]))\n        \n    return levels, times\n</pre> def eval_option(option_mc, option_qmc, abs_tol):     stopping_criteria = {         \"MLMC\" : qp.CubMLMC(option_mc, abs_tol=abs_tol, levels_max=15),         \"continuation MLMC\" : qp.CubMLMCCont(option_mc, abs_tol=abs_tol, levels_max=15),         \"MLQMC\" : qp.CubMLQMC(option_qmc, abs_tol=abs_tol, levels_max=15),         \"continuation MLQMC\" : qp.CubMLQMCCont(option_qmc, abs_tol=abs_tol, levels_max=15)     }          levels = []     times = []     for name, stopper in stopping_criteria.items():         sol, data = stopper.integrate()         levels.append(data.levels)         times.append(data.time_integrate)         print(\"\\t%-20s solution %-10.4f number of levels %-6d time %.3f\"%(name, sol, levels[-1], times[-1]))              return levels, times <p>Define the Multilevel Asian options:</p> In\u00a0[8]: Copied! <pre>option_mc = qp.FinancialOption(qp.IIDStdUniform(seed=seed), option=\"asian\")\noption_qmc = qp.FinancialOption(qp.Lattice(seed=seed, replications=8), option=\"asian\")\n</pre> option_mc = qp.FinancialOption(qp.IIDStdUniform(seed=seed), option=\"asian\") option_qmc = qp.FinancialOption(qp.Lattice(seed=seed, replications=8), option=\"asian\") <p>Run and compare each of the 4 algorithms for the Asian option problem:</p> In\u00a0[9]: Copied! <pre>eval_option(option_mc, option_qmc, abs_tol=5e-3);\n</pre> eval_option(option_mc, option_qmc, abs_tol=5e-3); <pre>\tMLMC                 solution 1.7859     number of levels 5      time 14.886\n\tcontinuation MLMC    solution 1.7825     number of levels 3      time 9.339\n\tMLQMC                solution 1.7818     number of levels 3      time 0.492\n\tcontinuation MLQMC   solution 1.7812     number of levels 3      time 0.180\n</pre> <p>Repeat this comparison for a sequence of decreasing tolerances, with 5 different random seeds each. This will allow us to visualize the asymptotic cost complexity of each method.</p> In\u00a0[10]: Copied! <pre>repetitions = 5\ntolerances = 5*np.logspace(-1, -3, num=5)\n\nlevels = {}\ntimes = {}\nfor t in range(len(tolerances)):\n    for r in range(repetitions):\n        print(\"tolerance = %10.4e, repetition = %d/%d\"%(tolerances[t], r + 1, repetitions))\n        levels[t, r], times[t, r] = eval_option(option_mc, option_qmc, tolerances[t])\n</pre> repetitions = 5 tolerances = 5*np.logspace(-1, -3, num=5)  levels = {} times = {} for t in range(len(tolerances)):     for r in range(repetitions):         print(\"tolerance = %10.4e, repetition = %d/%d\"%(tolerances[t], r + 1, repetitions))         levels[t, r], times[t, r] = eval_option(option_mc, option_qmc, tolerances[t]) <pre>tolerance = 5.0000e-01, repetition = 1/5\n\tMLMC                 solution 1.7574     number of levels 3      time 0.012\n\tcontinuation MLMC    solution 1.7765     number of levels 3      time 0.014\n\tMLQMC                solution 1.7840     number of levels 3      time 0.012\n\tcontinuation MLQMC   solution 1.7897     number of levels 3      time 0.012\ntolerance = 5.0000e-01, repetition = 2/5\n\tMLMC                 solution 1.8907     number of levels 3      time 0.005\n\tcontinuation MLMC    solution 1.5579     number of levels 3      time 0.010\n\tMLQMC                solution 1.7822     number of levels 3      time 0.009\n\tcontinuation MLQMC   solution 1.7790     number of levels 3      time 0.011\ntolerance = 5.0000e-01, repetition = 3/5\n\tMLMC                 solution 1.5368     number of levels 3      time 0.005\n\tcontinuation MLMC    solution 1.5291     number of levels 3      time 0.009\n\tMLQMC                solution 1.8200     number of levels 3      time 0.009\n\tcontinuation MLQMC   solution 1.8018     number of levels 3      time 0.010\ntolerance = 5.0000e-01, repetition = 4/5\n\tMLMC                 solution 1.6869     number of levels 3      time 0.005\n\tcontinuation MLMC    solution 1.8590     number of levels 3      time 0.009\n\tMLQMC                solution 1.8063     number of levels 3      time 0.008\n\tcontinuation MLQMC   solution 1.8205     number of levels 3      time 0.010\ntolerance = 5.0000e-01, repetition = 5/5\n\tMLMC                 solution 1.5989     number of levels 3      time 0.006\n\tcontinuation MLMC    solution 1.7292     number of levels 3      time 0.009\n\tMLQMC                solution 1.7672     number of levels 3      time 0.007\n\tcontinuation MLQMC   solution 1.7521     number of levels 3      time 0.008\ntolerance = 1.5811e-01, repetition = 1/5\n\tMLMC                 solution 1.6774     number of levels 3      time 0.010\n\tcontinuation MLMC    solution 1.8315     number of levels 3      time 0.016\n\tMLQMC                solution 1.7883     number of levels 3      time 0.010\n\tcontinuation MLQMC   solution 1.7917     number of levels 3      time 0.011\ntolerance = 1.5811e-01, repetition = 2/5\n\tMLMC                 solution 1.7921     number of levels 3      time 0.012\n\tcontinuation MLMC    solution 1.6978     number of levels 3      time 0.017\n\tMLQMC                solution 1.7939     number of levels 3      time 0.015\n\tcontinuation MLQMC   solution 1.7888     number of levels 3      time 0.012\ntolerance = 1.5811e-01, repetition = 3/5\n\tMLMC                 solution 1.7723     number of levels 3      time 0.013\n\tcontinuation MLMC    solution 1.8004     number of levels 4      time 0.021\n\tMLQMC                solution 1.7896     number of levels 3      time 0.011\n\tcontinuation MLQMC   solution 1.7780     number of levels 3      time 0.009\ntolerance = 1.5811e-01, repetition = 4/5\n\tMLMC                 solution 1.7671     number of levels 3      time 0.011\n\tcontinuation MLMC    solution 1.7734     number of levels 4      time 0.015\n\tMLQMC                solution 1.7888     number of levels 3      time 0.012\n\tcontinuation MLQMC   solution 1.7693     number of levels 3      time 0.008\ntolerance = 1.5811e-01, repetition = 5/5\n\tMLMC                 solution 1.7919     number of levels 3      time 0.012\n\tcontinuation MLMC    solution 1.8158     number of levels 3      time 0.010\n\tMLQMC                solution 1.7866     number of levels 3      time 0.011\n\tcontinuation MLQMC   solution 1.7808     number of levels 3      time 0.012\ntolerance = 5.0000e-02, repetition = 1/5\n\tMLMC                 solution 1.7964     number of levels 3      time 0.127\n\tcontinuation MLMC    solution 1.8056     number of levels 4      time 0.065\n\tMLQMC                solution 1.7835     number of levels 3      time 0.042\n\tcontinuation MLQMC   solution 1.7734     number of levels 3      time 0.029\ntolerance = 5.0000e-02, repetition = 2/5\n\tMLMC                 solution 1.7826     number of levels 3      time 0.096\n\tcontinuation MLMC    solution 1.7682     number of levels 3      time 0.100\n\tMLQMC                solution 1.7884     number of levels 3      time 0.043\n\tcontinuation MLQMC   solution 1.7705     number of levels 3      time 0.029\ntolerance = 5.0000e-02, repetition = 3/5\n\tMLMC                 solution 1.7604     number of levels 3      time 0.072\n\tcontinuation MLMC    solution 1.8240     number of levels 3      time 0.064\n\tMLQMC                solution 1.7786     number of levels 3      time 0.050\n\tcontinuation MLQMC   solution 1.7769     number of levels 3      time 0.025\ntolerance = 5.0000e-02, repetition = 4/5\n\tMLMC                 solution 1.7693     number of levels 3      time 0.094\n\tcontinuation MLMC    solution 1.8409     number of levels 4      time 0.178\n\tMLQMC                solution 1.7839     number of levels 3      time 0.041\n\tcontinuation MLQMC   solution 1.7835     number of levels 3      time 0.039\ntolerance = 5.0000e-02, repetition = 5/5\n\tMLMC                 solution 1.7808     number of levels 3      time 0.090\n\tcontinuation MLMC    solution 1.7848     number of levels 3      time 0.062\n\tMLQMC                solution 1.7768     number of levels 3      time 0.027\n\tcontinuation MLQMC   solution 1.7758     number of levels 3      time 0.024\ntolerance = 1.5811e-02, repetition = 1/5\n\tMLMC                 solution 1.7800     number of levels 3      time 1.225\n\tcontinuation MLMC    solution 1.7838     number of levels 3      time 0.410\n\tMLQMC                solution 1.7835     number of levels 3      time 0.099\n\tcontinuation MLQMC   solution 1.7781     number of levels 3      time 0.059\ntolerance = 1.5811e-02, repetition = 2/5\n\tMLMC                 solution 1.7941     number of levels 4      time 1.002\n\tcontinuation MLMC    solution 1.7848     number of levels 3      time 0.380\n\tMLQMC                solution 1.7783     number of levels 3      time 0.096\n\tcontinuation MLQMC   solution 1.7760     number of levels 3      time 0.056\ntolerance = 1.5811e-02, repetition = 3/5\n\tMLMC                 solution 1.7787     number of levels 3      time 0.809\n\tcontinuation MLMC    solution 1.7846     number of levels 3      time 0.791\n\tMLQMC                solution 1.7820     number of levels 3      time 0.132\n\tcontinuation MLQMC   solution 1.7776     number of levels 3      time 0.059\ntolerance = 1.5811e-02, repetition = 4/5\n\tMLMC                 solution 1.7806     number of levels 4      time 1.074\n\tcontinuation MLMC    solution 1.7810     number of levels 3      time 0.482\n\tMLQMC                solution 1.7806     number of levels 3      time 0.151\n\tcontinuation MLQMC   solution 1.7823     number of levels 3      time 0.051\ntolerance = 1.5811e-02, repetition = 5/5\n\tMLMC                 solution 1.7914     number of levels 4      time 0.952\n\tcontinuation MLMC    solution 1.7858     number of levels 3      time 0.769\n\tMLQMC                solution 1.7830     number of levels 3      time 0.111\n\tcontinuation MLQMC   solution 1.7804     number of levels 3      time 0.072\ntolerance = 5.0000e-03, repetition = 1/5\n\tMLMC                 solution 1.7827     number of levels 5      time 18.777\n\tcontinuation MLMC    solution 1.7823     number of levels 3      time 4.222\n\tMLQMC                solution 1.7817     number of levels 3      time 0.291\n\tcontinuation MLQMC   solution 1.7814     number of levels 3      time 0.425\ntolerance = 5.0000e-03, repetition = 2/5\n\tMLMC                 solution 1.7844     number of levels 4      time 10.280\n\tcontinuation MLMC    solution 1.7807     number of levels 3      time 8.755\n\tMLQMC                solution 1.7811     number of levels 3      time 0.355\n\tcontinuation MLQMC   solution 1.7812     number of levels 3      time 0.460\ntolerance = 5.0000e-03, repetition = 3/5\n\tMLMC                 solution 1.7819     number of levels 4      time 16.041\n\tcontinuation MLMC    solution 1.7825     number of levels 3      time 6.892\n\tMLQMC                solution 1.7813     number of levels 3      time 0.272\n\tcontinuation MLQMC   solution 1.7814     number of levels 3      time 0.369\ntolerance = 5.0000e-03, repetition = 4/5\n\tMLMC                 solution 1.7843     number of levels 4      time 12.328\n\tcontinuation MLMC    solution 1.7804     number of levels 3      time 4.089\n\tMLQMC                solution 1.7805     number of levels 3      time 0.498\n\tcontinuation MLQMC   solution 1.7823     number of levels 3      time 0.162\ntolerance = 5.0000e-03, repetition = 5/5\n\tMLMC                 solution 1.7822     number of levels 4      time 10.548\n\tcontinuation MLMC    solution 1.7805     number of levels 3      time 9.212\n\tMLQMC                solution 1.7818     number of levels 3      time 0.460\n\tcontinuation MLQMC   solution 1.7805     number of levels 3      time 0.508\n</pre> <p>Compute and plot the asymptotic cost complexity.</p> In\u00a0[11]: Copied! <pre>avg_time = {}\nfor method in range(4):\n    avg_time[method] = [np.mean([times[t, r][method] for r in range(repetitions)]) for t in range(len(tolerances))]\n</pre> avg_time = {} for method in range(4):     avg_time[method] = [np.mean([times[t, r][method] for r in range(repetitions)]) for t in range(len(tolerances))] In\u00a0[12]: Copied! <pre>plt.figure(figsize=(10,7))\nplt.plot(tolerances, avg_time[0], label=\"MLMC\")\nplt.plot(tolerances, avg_time[1], label=\"continuation MLMC\")\nplt.plot(tolerances, avg_time[2], label=\"MLQMC\")\nplt.plot(tolerances, avg_time[3], label=\"continuation MLQMC\")\nplt.xscale(\"log\")\nplt.yscale(\"log\")\nplt.xlabel(\"requested absolute tolerance\")\nplt.ylabel(\"average run time in seconds\")\nplt.legend();\n</pre> plt.figure(figsize=(10,7)) plt.plot(tolerances, avg_time[0], label=\"MLMC\") plt.plot(tolerances, avg_time[1], label=\"continuation MLMC\") plt.plot(tolerances, avg_time[2], label=\"MLQMC\") plt.plot(tolerances, avg_time[3], label=\"continuation MLQMC\") plt.xscale(\"log\") plt.yscale(\"log\") plt.xlabel(\"requested absolute tolerance\") plt.ylabel(\"average run time in seconds\") plt.legend(); In\u00a0[13]: Copied! <pre>max_levels = {}\nfor method in range(4):\n    levels_rep = np.array([levels[len(tolerances)-1, r][method] for r in range(repetitions)])\n    max_levels[method] = [np.count_nonzero(levels_rep == level)/repetitions for level in range(15)]\n</pre> max_levels = {} for method in range(4):     levels_rep = np.array([levels[len(tolerances)-1, r][method] for r in range(repetitions)])     max_levels[method] = [np.count_nonzero(levels_rep == level)/repetitions for level in range(15)] In\u00a0[14]: Copied! <pre>plt.figure(figsize=(14,10))\nplt.subplot(2,2,1); plt.bar(range(15), max_levels[0], label=\"MLMC old\", color=\"C0\"); plt.xlabel(\"max level\"); plt.ylabel(\"fraction of runs\"); plt.ylim(0, 1); plt.legend()\nplt.subplot(2,2,2); plt.bar(range(15), max_levels[1], label=\"MLMC new\", color=\"C1\"); plt.xlabel(\"max level\"); plt.ylabel(\"fraction of runs\"); plt.ylim(0, 1); plt.legend()\nplt.subplot(2,2,3); plt.bar(range(15), max_levels[2], label=\"MLQMC old\", color=\"C2\"); plt.xlabel(\"max level\"); plt.ylabel(\"fraction of runs\"); plt.ylim(0, 1); plt.legend()\nplt.subplot(2,2,4); plt.bar(range(15), max_levels[3], label=\"MLQMC new\", color=\"C3\"); plt.xlabel(\"max level\"); plt.ylabel(\"fraction of runs\"); plt.ylim(0, 1); plt.legend();\n</pre> plt.figure(figsize=(14,10)) plt.subplot(2,2,1); plt.bar(range(15), max_levels[0], label=\"MLMC old\", color=\"C0\"); plt.xlabel(\"max level\"); plt.ylabel(\"fraction of runs\"); plt.ylim(0, 1); plt.legend() plt.subplot(2,2,2); plt.bar(range(15), max_levels[1], label=\"MLMC new\", color=\"C1\"); plt.xlabel(\"max level\"); plt.ylabel(\"fraction of runs\"); plt.ylim(0, 1); plt.legend() plt.subplot(2,2,3); plt.bar(range(15), max_levels[2], label=\"MLQMC old\", color=\"C2\"); plt.xlabel(\"max level\"); plt.ylabel(\"fraction of runs\"); plt.ylim(0, 1); plt.legend() plt.subplot(2,2,4); plt.bar(range(15), max_levels[3], label=\"MLQMC new\", color=\"C3\"); plt.xlabel(\"max level\"); plt.ylabel(\"fraction of runs\"); plt.ylim(0, 1); plt.legend(); In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"demos/asian-option-mlqmc/#comparison-of-multilevel-quasi-monte-carlo-for-an-asian-option-problem","title":"Comparison of multilevel (Quasi-)Monte Carlo for an Asian option problem\u00b6","text":""},{"location":"demos/control_variates/","title":"Control Variates","text":"In\u00a0[1]: Copied! <pre>from qmcpy import *\nfrom numpy import *\nfrom qmcpy import *\nfrom numpy import *\n</pre> from qmcpy import * from numpy import * from qmcpy import * from numpy import * In\u00a0[2]: Copied! <pre>from matplotlib import pyplot\n%matplotlib inline\nsize = 20\npyplot.rc('font', size=size)          # controls default text sizes\npyplot.rc('axes', titlesize=size)     # fontsize of the axes title\npyplot.rc('axes', labelsize=size)     # fontsize of the x and y labels\npyplot.rc('xtick', labelsize=size)    # fontsize of the tick labels\npyplot.rc('ytick', labelsize=size)    # fontsize of the tick labels\npyplot.rc('legend', fontsize=size)    # legend fontsize\npyplot.rc('figure', titlesize=size)   # fontsize of the figure title\n</pre> from matplotlib import pyplot %matplotlib inline size = 20 pyplot.rc('font', size=size)          # controls default text sizes pyplot.rc('axes', titlesize=size)     # fontsize of the axes title pyplot.rc('axes', labelsize=size)     # fontsize of the x and y labels pyplot.rc('xtick', labelsize=size)    # fontsize of the tick labels pyplot.rc('ytick', labelsize=size)    # fontsize of the tick labels pyplot.rc('legend', fontsize=size)    # legend fontsize pyplot.rc('figure', titlesize=size)   # fontsize of the figure title In\u00a0[3]: Copied! <pre>def compare(problem,discrete_distrib,stopping_crit,abs_tol):\n  g1,cvs,cvmus = problem(discrete_distrib)\n  sc1 = stopping_crit(g1,abs_tol=abs_tol)\n  name = type(sc1).__name__\n  print('Stopping Criterion: %-15s absolute tolerance: %-5.1e'%(name,abs_tol))\n  sol,data = sc1.integrate()\n  print('\\tW CV:  Solution %-10.2f time %-10.2f samples %.1e'%(sol,data.time_integrate,data.n_total))\n  sc1 = stopping_crit(g1,abs_tol=abs_tol,control_variates=cvs,control_variate_means=cvmus)\n  solcv,datacv = sc1.integrate()\n  print('\\tWO CV: Solution %-10.2f time %-10.2f samples %.1e'%(solcv,datacv.time_integrate,datacv.n_total))\n  print('\\tControl variates took %.1f%% the time and %.1f%% the samples\\n'%\\\n        (100*datacv.time_integrate/data.time_integrate,100*datacv.n_total/data.n_total))\n</pre> def compare(problem,discrete_distrib,stopping_crit,abs_tol):   g1,cvs,cvmus = problem(discrete_distrib)   sc1 = stopping_crit(g1,abs_tol=abs_tol)   name = type(sc1).__name__   print('Stopping Criterion: %-15s absolute tolerance: %-5.1e'%(name,abs_tol))   sol,data = sc1.integrate()   print('\\tW CV:  Solution %-10.2f time %-10.2f samples %.1e'%(sol,data.time_integrate,data.n_total))   sc1 = stopping_crit(g1,abs_tol=abs_tol,control_variates=cvs,control_variate_means=cvmus)   solcv,datacv = sc1.integrate()   print('\\tWO CV: Solution %-10.2f time %-10.2f samples %.1e'%(solcv,datacv.time_integrate,datacv.n_total))   print('\\tControl variates took %.1f%% the time and %.1f%% the samples\\n'%\\         (100*datacv.time_integrate/data.time_integrate,100*datacv.n_total/data.n_total)) In\u00a0[4]: Copied! <pre># parameters\ndef poly_problem(discrete_distrib):\n  g1 = CustomFun(Uniform(discrete_distrib,0,2),lambda t: 10*t[...,0]-5*t[...,1]**2+t[...,2]**3)\n  cv1 = CustomFun(Uniform(discrete_distrib,0,2),lambda t: t[...,0])\n  cv2 = CustomFun(Uniform(discrete_distrib,0,2),lambda t: t[...,1]**2)\n  return g1,[cv1,cv2],[1,4/3]\ncompare(poly_problem,IIDStdUniform(3,seed=7),CubMCCLT,abs_tol=1e-2)\ncompare(poly_problem,IIDStdUniform(3,seed=7),CubMCCLT,abs_tol=1e-2)\ncompare(poly_problem,Sobol(3,seed=7),CubQMCSobolG,abs_tol=1e-8)\n</pre> # parameters def poly_problem(discrete_distrib):   g1 = CustomFun(Uniform(discrete_distrib,0,2),lambda t: 10*t[...,0]-5*t[...,1]**2+t[...,2]**3)   cv1 = CustomFun(Uniform(discrete_distrib,0,2),lambda t: t[...,0])   cv2 = CustomFun(Uniform(discrete_distrib,0,2),lambda t: t[...,1]**2)   return g1,[cv1,cv2],[1,4/3] compare(poly_problem,IIDStdUniform(3,seed=7),CubMCCLT,abs_tol=1e-2) compare(poly_problem,IIDStdUniform(3,seed=7),CubMCCLT,abs_tol=1e-2) compare(poly_problem,Sobol(3,seed=7),CubQMCSobolG,abs_tol=1e-8) <pre>Stopping Criterion: CubMCCLT        absolute tolerance: 1.0e-02\n\tW CV:  Solution 5.33       time 1.11       samples 6.7e+06\n\tWO CV: Solution 5.34       time 0.12       samples 4.8e+05\n\tControl variates took 10.5% the time and 7.1% the samples\n\nStopping Criterion: CubMCCLT        absolute tolerance: 1.0e-02\n\tW CV:  Solution 5.33       time 0.98       samples 6.7e+06\n\tWO CV: Solution 5.34       time 0.09       samples 4.8e+05\n\tControl variates took 9.1% the time and 7.1% the samples\n\nStopping Criterion: CubQMCNetG      absolute tolerance: 1.0e-08\n\tW CV:  Solution 5.33       time 0.12       samples 2.6e+05\n\tWO CV: Solution 5.33       time 0.04       samples 6.6e+04\n\tControl variates took 33.4% the time and 25.0% the samples\n\n</pre> In\u00a0[5]: Copied! <pre>def keister_problem(discrete_distrib):\n  k = Keister(discrete_distrib)\n  cv1 = CustomFun(Uniform(discrete_distrib),lambda x: sin(pi*x).sum(-1))\n  cv2 = CustomFun(Uniform(discrete_distrib),lambda x: (-3*(x-.5)**2+1).sum(-1))\n  return k,[cv1,cv2],[2/pi,3/4]\ncompare(keister_problem,IIDStdUniform(1,seed=7),CubMCCLT,abs_tol=5e-4)\ncompare(keister_problem,IIDStdUniform(1,seed=7),CubMCCLT,abs_tol=4e-4)\ncompare(keister_problem,Sobol(1,seed=7),CubQMCSobolG,abs_tol=1e-7)\n</pre> def keister_problem(discrete_distrib):   k = Keister(discrete_distrib)   cv1 = CustomFun(Uniform(discrete_distrib),lambda x: sin(pi*x).sum(-1))   cv2 = CustomFun(Uniform(discrete_distrib),lambda x: (-3*(x-.5)**2+1).sum(-1))   return k,[cv1,cv2],[2/pi,3/4] compare(keister_problem,IIDStdUniform(1,seed=7),CubMCCLT,abs_tol=5e-4) compare(keister_problem,IIDStdUniform(1,seed=7),CubMCCLT,abs_tol=4e-4) compare(keister_problem,Sobol(1,seed=7),CubQMCSobolG,abs_tol=1e-7) <pre>Stopping Criterion: CubMCCLT        absolute tolerance: 5.0e-04\n\tW CV:  Solution 1.38       time 1.11       samples 9.5e+06\n\tWO CV: Solution 1.38       time 0.10       samples 3.4e+05\n\tControl variates took 9.1% the time and 3.5% the samples\n\nStopping Criterion: CubMCCLT        absolute tolerance: 4.0e-04\n\tW CV:  Solution 1.38       time 1.71       samples 1.5e+07\n\tWO CV: Solution 1.38       time 0.17       samples 6.8e+05\n\tControl variates took 10.2% the time and 4.6% the samples\n\nStopping Criterion: CubQMCNetG      absolute tolerance: 1.0e-07\n\tW CV:  Solution 1.38       time 0.31       samples 1.0e+06\n\tWO CV: Solution 1.38       time 0.41       samples 1.0e+06\n\tControl variates took 133.0% the time and 100.0% the samples\n\n</pre> In\u00a0[7]: Copied! <pre>call_put = 'call'\nstart_price = 100\nstrike_price = 125\nvolatility = .75\ninterest_rate = .01 # 1% interest\nt_final = 1 # 1 year\ndef option_problem(discrete_distrib):\n  eurocv = FinancialOption(\n    discrete_distrib,\n    option=\"EUROPEAN\",\n    volatility=volatility,\n    start_price=start_price,\n    strike_price=strike_price,\n    interest_rate=interest_rate,\n    t_final=t_final,\n    call_put=call_put)\n  aco = FinancialOption(\n    discrete_distrib,\n    option=\"ASIAN\",\n    volatility=volatility,\n    start_price=start_price,\n    strike_price=strike_price,\n    interest_rate=interest_rate,\n    t_final=t_final,\n    call_put=call_put)\n  mu_eurocv = eurocv.get_exact_value()\n  return aco,[eurocv],[mu_eurocv]\ncompare(option_problem,IIDStdUniform(4,seed=7),CubMCCLT,abs_tol=5e-2)\ncompare(option_problem,IIDStdUniform(4,seed=7),CubMCCLT,abs_tol=5e-2)\ncompare(option_problem,Sobol(4,seed=7),CubQMCSobolG,abs_tol=1e-3)\n</pre> call_put = 'call' start_price = 100 strike_price = 125 volatility = .75 interest_rate = .01 # 1% interest t_final = 1 # 1 year def option_problem(discrete_distrib):   eurocv = FinancialOption(     discrete_distrib,     option=\"EUROPEAN\",     volatility=volatility,     start_price=start_price,     strike_price=strike_price,     interest_rate=interest_rate,     t_final=t_final,     call_put=call_put)   aco = FinancialOption(     discrete_distrib,     option=\"ASIAN\",     volatility=volatility,     start_price=start_price,     strike_price=strike_price,     interest_rate=interest_rate,     t_final=t_final,     call_put=call_put)   mu_eurocv = eurocv.get_exact_value()   return aco,[eurocv],[mu_eurocv] compare(option_problem,IIDStdUniform(4,seed=7),CubMCCLT,abs_tol=5e-2) compare(option_problem,IIDStdUniform(4,seed=7),CubMCCLT,abs_tol=5e-2) compare(option_problem,Sobol(4,seed=7),CubQMCSobolG,abs_tol=1e-3) <pre>Stopping Criterion: CubMCCLT        absolute tolerance: 5.0e-02\n\tW CV:  Solution 9.55       time 1.66       samples 3.1e+06\n\tWO CV: Solution 9.54       time 0.81       samples 7.9e+05\n\tControl variates took 49.0% the time and 25.6% the samples\n\nStopping Criterion: CubMCCLT        absolute tolerance: 5.0e-02\n\tW CV:  Solution 9.55       time 1.44       samples 3.1e+06\n\tWO CV: Solution 9.54       time 0.66       samples 7.9e+05\n\tControl variates took 45.7% the time and 25.6% the samples\n\nStopping Criterion: CubQMCNetG      absolute tolerance: 1.0e-03\n\tW CV:  Solution 9.55       time 0.40       samples 5.2e+05\n\tWO CV: Solution 9.55       time 0.53       samples 5.2e+05\n\tControl variates took 132.0% the time and 100.0% the samples\n\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"demos/control_variates/#control-variates-in-qmcpy","title":"Control Variates in QMCPy\u00b6","text":"<p>This notebook demonstrates QMCPy's current support for control variates.</p>"},{"location":"demos/control_variates/#setup","title":"Setup\u00b6","text":""},{"location":"demos/control_variates/#problem-1-polynomial-function","title":"Problem 1: Polynomial Function\u00b6","text":"<p>We will integrate $$g(t) = 10t_1-5t_2^2+2t_3^3$$ with true measure $\\mathcal{U}[0,2]^3$ and control variates $$\\hat{g}_1(t) = t_1$$ and $$\\hat{g}_2(t) = t_2^2$$ using the same true measure.</p>"},{"location":"demos/control_variates/#problem-2-keister-function","title":"Problem 2: Keister Function\u00b6","text":"<p>This problem will integrate the Keister function while using control variates $$g_1(x) = \\sin(\\pi x)$$ and $$g_2(x) = -3(x-1/2)^2+1.$$ The following code does this problem in one-dimension for visualization purposes, but control variates are compatible with any dimension.</p>"},{"location":"demos/control_variates/#problem-3-option-pricing","title":"Problem 3: Option Pricing\u00b6","text":"<p>We will use a European Call Option as a control variate for pricing the Asian Call Option using various stopping criterion, as done for problem 1</p>"},{"location":"demos/digital_net_b2/","title":"Digital Nets in Base 2 and their Randomizations","text":"In\u00a0[1]: Copied! <pre>from qmcpy import *\nfrom numpy import *\nfrom matplotlib import pyplot\nfrom time import time\nimport os\n</pre> from qmcpy import * from numpy import * from matplotlib import pyplot from time import time import os In\u00a0[2]: Copied! <pre>s = DigitalNetB2(5,seed=7)\ns\n</pre> s = DigitalNetB2(5,seed=7) s Out[2]: <pre>DigitalNetB2 (AbstractLDDiscreteDistribution)\n    d               5\n    replications    1\n    randomize       LMS_DS\n    gen_mats_source joe_kuo.6.21201.txt\n    order           NATURAL\n    t               63\n    alpha           1\n    n_limit         2^(32)\n    entropy         7</pre> In\u00a0[3]: Copied! <pre>s.gen_samples(4) # generate Sobol' samples\n</pre> s.gen_samples(4) # generate Sobol' samples Out[3]: <pre>array([[0.864483  , 0.31330935, 0.09580848, 0.24636182, 0.13239161],\n       [0.03735175, 0.82581618, 0.71117322, 0.9042245 , 0.5285374 ],\n       [0.65627575, 0.57484149, 0.94341128, 0.67738059, 0.38460276],\n       [0.48731325, 0.06429227, 0.37126087, 0.45706178, 0.7690279 ]])</pre> In\u00a0[4]: Copied! <pre>s.gen_samples(n_min=2,n_max=4) # generate from specific range. If range is not powers of 2, use graycode\n</pre> s.gen_samples(n_min=2,n_max=4) # generate from specific range. If range is not powers of 2, use graycode Out[4]: <pre>array([[0.65627575, 0.57484149, 0.94341128, 0.67738059, 0.38460276],\n       [0.48731325, 0.06429227, 0.37126087, 0.45706178, 0.7690279 ]])</pre> In\u00a0[5]: Copied! <pre>t0 = time()\ns.gen_samples(2**25)\nprint('Time: %.2f'%(time()-t0))\n</pre> t0 = time() s.gen_samples(2**25) print('Time: %.2f'%(time()-t0)) <pre>Time: 4.61\n</pre> In\u00a0[6]: Copied! <pre>s = DigitalNetB2(2,randomize='LMS_DS') # linear matrix scramble with digital shift (default)\ns.gen_samples(2)\n</pre> s = DigitalNetB2(2,randomize='LMS_DS') # linear matrix scramble with digital shift (default) s.gen_samples(2) Out[6]: <pre>array([[0.5032812 , 0.3002277 ],\n       [0.37437374, 0.86989486]])</pre> In\u00a0[7]: Copied! <pre>s = DigitalNetB2(2,randomize='LMS') # just linear matrix scrambling\ns.gen_samples(2, warn=False) # suppress warning that the first point is still the origin\n</pre> s = DigitalNetB2(2,randomize='LMS') # just linear matrix scrambling s.gen_samples(2, warn=False) # suppress warning that the first point is still the origin Out[7]: <pre>array([[0.        , 0.        ],\n       [0.95339709, 0.93679526]])</pre> In\u00a0[8]: Copied! <pre>s = DigitalNetB2(2,randomize='DS') # just digital shift\ns.gen_samples(2)\n</pre> s = DigitalNetB2(2,randomize='DS') # just digital shift s.gen_samples(2) Out[8]: <pre>array([[0.72193614, 0.16766092],\n       [0.22193614, 0.66766092]])</pre> In\u00a0[9]: Copied! <pre>s = DigitalNetB2(2,randomize=False,graycode=False)\ns.gen_samples(n_min=4,n_max=8,warn=False) # don't warn about non-randomized samples including the origin\n</pre> s = DigitalNetB2(2,randomize=False,graycode=False) s.gen_samples(n_min=4,n_max=8,warn=False) # don't warn about non-randomized samples including the origin <pre>/Users/alegresor/Desktop/QMCSoftware/qmcpy/discrete_distribution/digital_net_b2/digital_net_b2.py:263: ParameterWarning: graycode argument deprecated, set order='GRAY' or order='NATURAL' instead. Using order='NATURAL'\n  warnings.warn(\"graycode argument deprecated, set order='GRAY' or order='NATURAL' instead. Using order='%s'\"%order,ParameterWarning)\n</pre> Out[9]: <pre>array([[0.125, 0.625],\n       [0.625, 0.125],\n       [0.375, 0.375],\n       [0.875, 0.875]])</pre> In\u00a0[10]: Copied! <pre>s = DigitalNetB2(2,randomize=False,graycode=True)\ns.gen_samples(n_min=4,n_max=8,warn=False)\n</pre> s = DigitalNetB2(2,randomize=False,graycode=True) s.gen_samples(n_min=4,n_max=8,warn=False) <pre>/Users/alegresor/Desktop/QMCSoftware/qmcpy/discrete_distribution/digital_net_b2/digital_net_b2.py:263: ParameterWarning: graycode argument deprecated, set order='GRAY' or order='NATURAL' instead. Using order='GRAY'\n  warnings.warn(\"graycode argument deprecated, set order='GRAY' or order='NATURAL' instead. Using order='%s'\"%order,ParameterWarning)\n</pre> Out[10]: <pre>array([[0.375, 0.375],\n       [0.875, 0.875],\n       [0.625, 0.125],\n       [0.125, 0.625]])</pre> In\u00a0[11]: Copied! <pre>s = DigitalNetB2(3,randomize=False)\ns.gen_samples(n_min=4,n_max=8)\n</pre> s = DigitalNetB2(3,randomize=False) s.gen_samples(n_min=4,n_max=8) Out[11]: <pre>array([[0.125, 0.625, 0.375],\n       [0.625, 0.125, 0.875],\n       [0.375, 0.375, 0.625],\n       [0.875, 0.875, 0.125]])</pre> In\u00a0[12]: Copied! <pre>s = DigitalNetB2([1,2],randomize=False) # use only the second and third dimensions in the sequence\ns.gen_samples(n_min=4,n_max=8)\n</pre> s = DigitalNetB2([1,2],randomize=False) # use only the second and third dimensions in the sequence s.gen_samples(n_min=4,n_max=8) Out[12]: <pre>array([[0.625, 0.375],\n       [0.125, 0.875],\n       [0.375, 0.625],\n       [0.875, 0.125]])</pre> In\u00a0[16]: Copied! <pre>def plt_ei(x,ax,x_cuts,y_cuts):\n    for ix in arange(1,x_cuts,dtype=float): ax.axvline(x=ix/x_cuts,color='r',alpha=.5)\n    for iy in arange(1,y_cuts,dtype=float): ax.axhline(y=iy/y_cuts,color='r',alpha=.5)\n    ax.scatter(x[:,0],x[:,1],color='b',s=25)\n    ax.set_xlim([0,1])\n    ax.set_xticks([0,1])\n    ax.set_ylim([0,1])\n    ax.set_yticks([0,1])\n    ax.set_aspect(1)\n</pre> def plt_ei(x,ax,x_cuts,y_cuts):     for ix in arange(1,x_cuts,dtype=float): ax.axvline(x=ix/x_cuts,color='r',alpha=.5)     for iy in arange(1,y_cuts,dtype=float): ax.axhline(y=iy/y_cuts,color='r',alpha=.5)     ax.scatter(x[:,0],x[:,1],color='b',s=25)     ax.set_xlim([0,1])     ax.set_xticks([0,1])     ax.set_ylim([0,1])     ax.set_yticks([0,1])     ax.set_aspect(1) In\u00a0[17]: Copied! <pre># unrandomized \nfig,ax = pyplot.subplots(ncols=3,nrows=2,figsize=(15,10))\ns = DigitalNetB2(2,randomize=False)\nplt_ei(s.gen_samples(2**2,warn=False),ax[0,0],2,2)\nplt_ei(s.gen_samples(2**3,warn=False),ax[1,0],8,0)\nplt_ei(s.gen_samples(2**4,warn=False),ax[0,1],4,4)\nplt_ei(s.gen_samples(2**4,warn=False),ax[1,1],2,8)\nplt_ei(s.gen_samples(2**6,warn=False),ax[0,2],8,8)\nplt_ei(s.gen_samples(2**6,warn=False),ax[1,2],16,4)\nfig.suptitle(\"Unrandomized Sobol' points\");\n</pre> # unrandomized  fig,ax = pyplot.subplots(ncols=3,nrows=2,figsize=(15,10)) s = DigitalNetB2(2,randomize=False) plt_ei(s.gen_samples(2**2,warn=False),ax[0,0],2,2) plt_ei(s.gen_samples(2**3,warn=False),ax[1,0],8,0) plt_ei(s.gen_samples(2**4,warn=False),ax[0,1],4,4) plt_ei(s.gen_samples(2**4,warn=False),ax[1,1],2,8) plt_ei(s.gen_samples(2**6,warn=False),ax[0,2],8,8) plt_ei(s.gen_samples(2**6,warn=False),ax[1,2],16,4) fig.suptitle(\"Unrandomized Sobol' points\"); In\u00a0[18]: Copied! <pre>fig,ax = pyplot.subplots(ncols=3,nrows=2,figsize=(15,10))\ns = DigitalNetB2(2,randomize='LMS_DS')\nplt_ei(s.gen_samples(2**2,warn=False),ax[0,0],2,2)\nplt_ei(s.gen_samples(2**3,warn=False),ax[1,0],8,0)\nplt_ei(s.gen_samples(2**4,warn=False),ax[0,1],4,4)\nplt_ei(s.gen_samples(2**4,warn=False),ax[1,1],2,8)\nplt_ei(s.gen_samples(2**6,warn=False),ax[0,2],8,8)\nplt_ei(s.gen_samples(2**6,warn=False),ax[1,2],16,4)\nfig.suptitle(\"Sobol' points with linear matrix scramble and digital shift\");\n</pre> fig,ax = pyplot.subplots(ncols=3,nrows=2,figsize=(15,10)) s = DigitalNetB2(2,randomize='LMS_DS') plt_ei(s.gen_samples(2**2,warn=False),ax[0,0],2,2) plt_ei(s.gen_samples(2**3,warn=False),ax[1,0],8,0) plt_ei(s.gen_samples(2**4,warn=False),ax[0,1],4,4) plt_ei(s.gen_samples(2**4,warn=False),ax[1,1],2,8) plt_ei(s.gen_samples(2**6,warn=False),ax[0,2],8,8) plt_ei(s.gen_samples(2**6,warn=False),ax[1,2],16,4) fig.suptitle(\"Sobol' points with linear matrix scramble and digital shift\"); In\u00a0[20]: Copied! <pre>fig,ax = pyplot.subplots(ncols=3,nrows=2,figsize=(15,10))\ns = DigitalNetB2(2,order=\"GRAY\",randomize='LMS')\nplt_ei(s.gen_samples(2**2,warn=False),ax[0,0],2,2)\nplt_ei(s.gen_samples(2**3,warn=False),ax[1,0],8,0)\nplt_ei(s.gen_samples(2**4,warn=False),ax[0,1],4,4)\nplt_ei(s.gen_samples(2**4,warn=False),ax[1,1],2,8)\nplt_ei(s.gen_samples(2**6,warn=False),ax[0,2],8,8)\nplt_ei(s.gen_samples(2**6,warn=False),ax[1,2],16,4)\nfig.suptitle(\"Sobol' points with linear matrix scrambling shift\");\n</pre> fig,ax = pyplot.subplots(ncols=3,nrows=2,figsize=(15,10)) s = DigitalNetB2(2,order=\"GRAY\",randomize='LMS') plt_ei(s.gen_samples(2**2,warn=False),ax[0,0],2,2) plt_ei(s.gen_samples(2**3,warn=False),ax[1,0],8,0) plt_ei(s.gen_samples(2**4,warn=False),ax[0,1],4,4) plt_ei(s.gen_samples(2**4,warn=False),ax[1,1],2,8) plt_ei(s.gen_samples(2**6,warn=False),ax[0,2],8,8) plt_ei(s.gen_samples(2**6,warn=False),ax[1,2],16,4) fig.suptitle(\"Sobol' points with linear matrix scrambling shift\"); In\u00a0[21]: Copied! <pre>fig,ax = pyplot.subplots(ncols=3,nrows=2,figsize=(15,10))\ns = DigitalNetB2(2,order=\"GRAY\",randomize='DS')\nplt_ei(s.gen_samples(2**2,warn=False),ax[0,0],2,2)\nplt_ei(s.gen_samples(2**3,warn=False),ax[1,0],8,0)\nplt_ei(s.gen_samples(2**4,warn=False),ax[0,1],4,4)\nplt_ei(s.gen_samples(2**4,warn=False),ax[1,1],2,8)\nplt_ei(s.gen_samples(2**6,warn=False),ax[0,2],8,8)\nplt_ei(s.gen_samples(2**6,warn=False),ax[1,2],16,4)\nfig.suptitle(\"Sobol' points with digital shift\");\n</pre> fig,ax = pyplot.subplots(ncols=3,nrows=2,figsize=(15,10)) s = DigitalNetB2(2,order=\"GRAY\",randomize='DS') plt_ei(s.gen_samples(2**2,warn=False),ax[0,0],2,2) plt_ei(s.gen_samples(2**3,warn=False),ax[1,0],8,0) plt_ei(s.gen_samples(2**4,warn=False),ax[0,1],4,4) plt_ei(s.gen_samples(2**4,warn=False),ax[1,1],2,8) plt_ei(s.gen_samples(2**6,warn=False),ax[0,2],8,8) plt_ei(s.gen_samples(2**6,warn=False),ax[1,2],16,4) fig.suptitle(\"Sobol' points with digital shift\"); In\u00a0[22]: Copied! <pre>fig,ax = pyplot.subplots(figsize=(15,5))\ns = DigitalNetB2([50,51],randomize='LMS_DS')\nplt_ei(s.gen_samples(2**4),ax,4,4)\nfig.suptitle(\"Sobol' points dimension 50 vs 51\");\n# nice properties do not necessary hold in higher dimensions\n</pre> fig,ax = pyplot.subplots(figsize=(15,5)) s = DigitalNetB2([50,51],randomize='LMS_DS') plt_ei(s.gen_samples(2**4),ax,4,4) fig.suptitle(\"Sobol' points dimension 50 vs 51\"); # nice properties do not necessary hold in higher dimensions In\u00a0[23]: Copied! <pre>fig,ax = pyplot.subplots(figsize=(15,5))\ns = DigitalNetB2(2,randomize='LMS_DS',graycode=True)\nplt_ei(s.gen_samples(n_min=1,n_max=16),ax,4,4)\nx16 = s.gen_samples(n_min=16,n_max=17)\nax.scatter(x16[:,0],x16[:,1],color='g',s=100)\nfig.suptitle(\"Sobol' points 2-17\");\n# better to take points 1-16 instead of 2-16\n</pre> fig,ax = pyplot.subplots(figsize=(15,5)) s = DigitalNetB2(2,randomize='LMS_DS',graycode=True) plt_ei(s.gen_samples(n_min=1,n_max=16),ax,4,4) x16 = s.gen_samples(n_min=16,n_max=17) ax.scatter(x16[:,0],x16[:,1],color='g',s=100) fig.suptitle(\"Sobol' points 2-17\"); # better to take points 1-16 instead of 2-16 <pre>/Users/alegresor/Desktop/QMCSoftware/qmcpy/discrete_distribution/digital_net_b2/digital_net_b2.py:263: ParameterWarning: graycode argument deprecated, set order='GRAY' or order='NATURAL' instead. Using order='GRAY'\n  warnings.warn(\"graycode argument deprecated, set order='GRAY' or order='NATURAL' instead. Using order='%s'\"%order,ParameterWarning)\n</pre> In\u00a0[26]: Copied! <pre>def plt_k2d_sobol(ax,rtype,colors,plts=[1,2,3]):\n    trials = 100\n    ms = arange(10,18)\n    ax.set_xscale('log',base=2)\n    ax.set_yscale('log',base=10)\n    epsilons = {}\n    if 1 in plts:\n        epsilons['$2^m$ points with skipped $1^{st}$ point'] = zeros((trials,len(ms)),dtype=double)\n    if 2 in plts:\n        epsilons['$2^m-1$ points with skipped $1^{st}$ point'] = zeros((trials,len(ms)),dtype=double)\n    if 3 in plts:\n        epsilons['$2^n$ points no skipping'] = zeros((trials,len(ms)),dtype=double)\n    for i,m in enumerate(ms):\n        for t in range(trials):\n            s = DigitalNetB2(2,randomize=rtype,order=\"GRAY\")\n            k = Keister(s)\n            solution = k.get_exact_value(d=2)\n            if 1 in plts:\n                epsilons['$2^m$ points with skipped $1^{st}$ point'][t,i] = \\\n                     abs(k.f(s.gen_samples(n_min=1,n_max=1+2**m)).mean()-solution)\n            if 2 in plts:\n                epsilons['$2^m-1$ points with skipped $1^{st}$ point'][t,i] = \\\n                     abs(k.f(s.gen_samples(n_min=1,n_max=2**m)).mean()-solution)\n            if 3 in plts:\n                epsilons['$2^n$ points no skipping'][t,i] = \\\n                     abs(k.f(s.gen_samples(n=2**m)).mean()-solution)\n    for i,(label,eps) in enumerate(epsilons.items()):\n        bot = percentile(eps, 40, axis=0)\n        med = percentile(eps, 50, axis=0)\n        top = percentile(eps, 60, axis=0)\n        ax.loglog(2**ms,med,label=label+' with %s randomization'%rtype,color=colors[i])\n        ax.fill_between(2**ms, bot, top, color=colors[i], alpha=.3)     \n</pre> def plt_k2d_sobol(ax,rtype,colors,plts=[1,2,3]):     trials = 100     ms = arange(10,18)     ax.set_xscale('log',base=2)     ax.set_yscale('log',base=10)     epsilons = {}     if 1 in plts:         epsilons['$2^m$ points with skipped $1^{st}$ point'] = zeros((trials,len(ms)),dtype=double)     if 2 in plts:         epsilons['$2^m-1$ points with skipped $1^{st}$ point'] = zeros((trials,len(ms)),dtype=double)     if 3 in plts:         epsilons['$2^n$ points no skipping'] = zeros((trials,len(ms)),dtype=double)     for i,m in enumerate(ms):         for t in range(trials):             s = DigitalNetB2(2,randomize=rtype,order=\"GRAY\")             k = Keister(s)             solution = k.get_exact_value(d=2)             if 1 in plts:                 epsilons['$2^m$ points with skipped $1^{st}$ point'][t,i] = \\                      abs(k.f(s.gen_samples(n_min=1,n_max=1+2**m)).mean()-solution)             if 2 in plts:                 epsilons['$2^m-1$ points with skipped $1^{st}$ point'][t,i] = \\                      abs(k.f(s.gen_samples(n_min=1,n_max=2**m)).mean()-solution)             if 3 in plts:                 epsilons['$2^n$ points no skipping'][t,i] = \\                      abs(k.f(s.gen_samples(n=2**m)).mean()-solution)     for i,(label,eps) in enumerate(epsilons.items()):         bot = percentile(eps, 40, axis=0)         med = percentile(eps, 50, axis=0)         top = percentile(eps, 60, axis=0)         ax.loglog(2**ms,med,label=label+' with %s randomization'%rtype,color=colors[i])         ax.fill_between(2**ms, bot, top, color=colors[i], alpha=.3)      In\u00a0[27]: Copied! <pre># compare randomization fig,ax = pyplot.subplots(figsize=(10,10))\nfig,ax = pyplot.subplots(figsize=(10,10))\nplt_k2d_sobol(ax,None,['r','g'],[1,2])\nplt_k2d_sobol(ax,'LMS_DS',['c','m','b'],[1,2,3])\nplt_k2d_sobol(ax,'DS',['y','k','orangered'],[1,2,3])\nax.legend()\nax.set_xlabel('n')\nax.set_ylabel('$\\\\epsilon$');\n</pre> # compare randomization fig,ax = pyplot.subplots(figsize=(10,10)) fig,ax = pyplot.subplots(figsize=(10,10)) plt_k2d_sobol(ax,None,['r','g'],[1,2]) plt_k2d_sobol(ax,'LMS_DS',['c','m','b'],[1,2,3]) plt_k2d_sobol(ax,'DS',['y','k','orangered'],[1,2,3]) ax.legend() ax.set_xlabel('n') ax.set_ylabel('$\\\\epsilon$'); In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"demos/digital_net_b2/#digital-net-base-2-generator","title":"Digital Net Base 2 Generator\u00b6","text":""},{"location":"demos/digital_net_b2/#basic-usage","title":"Basic usage\u00b6","text":""},{"location":"demos/digital_net_b2/#randomize-with-digital-shift-linear-matrix-scramble","title":"Randomize with digital shift / linear matrix scramble\u00b6","text":""},{"location":"demos/digital_net_b2/#support-for-graycode-and-natural-ordering","title":"Support for graycode and natural ordering\u00b6","text":""},{"location":"demos/digital_net_b2/#custom-dimensions","title":"Custom Dimensions\u00b6","text":""},{"location":"demos/digital_net_b2/#elementary-intervals","title":"Elementary intervals\u00b6","text":""},{"location":"demos/digital_net_b2/#skipping-points-vs-randomization","title":"Skipping points vs. randomization\u00b6","text":"<p>The first point in a Sobol' sequence is $\\vec{0}$. Therefore, some software packages skip this point as various transformations will map 0 to NAN. For example, the inverse CDF of a Gaussian density at $\\vec{0}$ is $-\\infty$. However, we argue that it is better to randomize points than to simply skip the first point for the following reasons:</p> <ol> <li>Skipping the first point does not give the uniformity advantages when taking powers of 2. For example, notice the green point in the above plot of \"Sobol' points 2-17\".</li> <li>Randomizing Sobol' points will return 0 with probability 0.</li> </ol> <p>A more thorough explanation can be found in Art Owen's paper On dropping the first Sobol' point</p> <p>So always randomize your Sobol' unless you specifically need unrandomized points. In QMCPy the Sobol' generator defaults to randomization with a linear matrix scramble.</p> <p>The below code runs tests comparing unrandomized vs. randomization with linear matrix scramble with digital shift vs. randomization with just digital shift. Furthermore, we compare taking the first $2^n$ points vs. dropping the first point and taking $2^n$ points vs. dropping the first point and taking $2^n-1$ points.</p> <p>The 2D keister function is used for testing purposes as it can be exactly integrated using Mathematica: <code>N[Integrate[E^(-x1^2 - x2^2) Cos[Sqrt[x1^2 + x2^2]], {x1, -Infinity, Infinity}, {x2, -Infinity, Infinity}]]</code></p> <p>Plots the median (line) and fills the middle 10% of observations</p>"},{"location":"demos/elliptic-pde/","title":"Single and Multilevel (Q)MC for an Elliptic PDE","text":"In\u00a0[1]: Copied! <pre>import copy\nimport numpy as np\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nfrom scipy.special import gamma, kv\nfrom qmcpy.integrand import Integrand\nfrom qmcpy.util.data import Data\nimport qmcpy as qp\n</pre> import copy import numpy as np import matplotlib as mpl import matplotlib.pyplot as plt from scipy.special import gamma, kv from qmcpy.integrand import Integrand from qmcpy.util.data import Data import qmcpy as qp In\u00a0[2]: Copied! <pre># matplotlib options\nrc_fonts = {\n    \"text.usetex\": True,\n    \"font.size\": 14,\n    \"mathtext.default\": \"regular\",\n    \"axes.titlesize\": 14,\n    \"axes.labelsize\": 14,\n    \"legend.fontsize\": 14,\n    \"xtick.labelsize\": 12,\n    \"ytick.labelsize\": 12,\n    \"figure.titlesize\": 16,\n    \"font.family\": \"serif\",\n    \"font.serif\": \"computer modern roman\",\n}\nmpl.rcParams.update(rc_fonts)\n</pre> # matplotlib options rc_fonts = {     \"text.usetex\": True,     \"font.size\": 14,     \"mathtext.default\": \"regular\",     \"axes.titlesize\": 14,     \"axes.labelsize\": 14,     \"legend.fontsize\": 14,     \"xtick.labelsize\": 12,     \"ytick.labelsize\": 12,     \"figure.titlesize\": 16,     \"font.family\": \"serif\",     \"font.serif\": \"computer modern roman\", } mpl.rcParams.update(rc_fonts) In\u00a0[3]: Copied! <pre># set random seed for reproducibility\nnp.random.seed(9999)\n</pre> # set random seed for reproducibility np.random.seed(9999) <p>We will apply various multilevel Monte Carlo and multilevel quasi-Monte Carlo methods to approximate the expected value of a quantity of interest derived from the solution of a one-dimensional partial differential equation (PDE), where the diffusion coefficient of the PDE is a lognormal Gaussian random field. This example problem serves as an important benchmark problem for various methods in the uncertainty quantification and quasi-Monte Carlo literature. It is often referred to as the fruitfly problem of uncertainty quantification.</p> <p>Let $Q$ be a quantity of interest derived from the solution $u(x, \\omega)$ of the one-dimensional partial differential equation (PDE)</p> <p>$$-\\frac{d}{dx}\\bigg(a(x, \\omega) \\frac{d}{dx} u(x, \\omega) \\bigg) = f(x), \\quad 0 \\leq x \\leq 1,$$ $$u(0, \\cdot) = u_0,$$ $$u(1, \\cdot) = u_1.$$</p> <p>The notation $u(x, \\omega)$ is used to indicate that the solution depends on both the spatial variable $x$ and the uncertain parameter $\\omega$. This uncertainty is present because the diffusion coefficient, $a(x, \\omega)$, is given by a lognormal Gaussian random field with given covariance function. A common choice for the covariance function is the so-called Mat\u00e9rn covariance function</p> <p>$$c(x, y) = \\hat{c}\\bigg(\\frac{\\|x - y\\|}{\\lambda}\\bigg)\\quad \\quad \\hat{c}(r) = \\frac{2^{1-\\nu}}{\\Gamma(\\nu)} r^\\nu K_\\nu(r)$$</p> <p>with $\\Gamma$ the gamma function and $K_\\nu$ the Bessel function of the second kind. This covariance function has two parameters: $\\lambda$, the length scale, and $\\nu$, the smoothness parameter.</p> <p>We begin by defining the Mat\u00e9rn covariance function <code>Matern(x, y)</code>:</p> In\u00a0[4]: Copied! <pre>def Matern(x, y, smoothness=1, lengthscale=1):\n    distance = abs(x - y)\n    r = distance/lengthscale\n    prefactor = 2**(1-smoothness)/gamma(smoothness)\n    term1 = r**smoothness\n    term2 = kv(smoothness, r)\n    np.fill_diagonal(term2, 1)\n    cov = prefactor * term1 * term2\n    np.fill_diagonal(cov, 1)\n    return cov\n</pre> def Matern(x, y, smoothness=1, lengthscale=1):     distance = abs(x - y)     r = distance/lengthscale     prefactor = 2**(1-smoothness)/gamma(smoothness)     term1 = r**smoothness     term2 = kv(smoothness, r)     np.fill_diagonal(term2, 1)     cov = prefactor * term1 * term2     np.fill_diagonal(cov, 1)     return cov <p>Let's take a look at the covariance matrix obtained by evaluating the covariance function in <code>n=25</code> equidistant points in <code>[0, 1]</code>.</p> In\u00a0[5]: Copied! <pre>def get_covariance_matrix(pts, smoothness=1, lengthscale=1):\n    X, Y = np.meshgrid(pts, pts)\n    return Matern(X, Y, smoothness, lengthscale)\n</pre> def get_covariance_matrix(pts, smoothness=1, lengthscale=1):     X, Y = np.meshgrid(pts, pts)     return Matern(X, Y, smoothness, lengthscale) In\u00a0[6]: Copied! <pre>n = 25\npts = np.linspace(0, 1, num=n)\nfig, ax = plt.subplots(figsize=(6, 5))\nax.pcolor(get_covariance_matrix(pts).T)\nax.invert_yaxis()\nax.set_ylabel(r\"$x$\")\nax.set_xlabel(r\"$y$\")\nax.set_title(f\"Matern kernel\")\nplt.show()\n</pre> n = 25 pts = np.linspace(0, 1, num=n) fig, ax = plt.subplots(figsize=(6, 5)) ax.pcolor(get_covariance_matrix(pts).T) ax.invert_yaxis() ax.set_ylabel(r\"$x$\") ax.set_xlabel(r\"$y$\") ax.set_title(f\"Matern kernel\") plt.show() <p>A lognormal Gaussian random field $a(x, \\omega)$ can be expressed as $a(x, \\omega) = \\exp(b(x, \\omega))$, where $b(x, \\omega)$ is a Gaussian random field. Samples of the Gaussian random field $b(x, \\omega)$ can be computed from a factorization of the covariance matrix. Specifically, suppose we have a spectral (eigenvalue) expansion of the covariance matrix $C$ as</p> <p>$$C = V W V^T,$$</p> <p>then samples of the Gaussian random field can be computed as</p> <p>$$\\boldsymbol{b} = S \\boldsymbol{x},$$</p> <p>where $S = V W^{1/2}$ and $\\boldsymbol{x}$ is a vector of standard normal independent and identically distributed random variables. This is easy to see, since</p> <p>$$\\mathbb{E}[\\boldsymbol{b}] = \\mathbb{E}[S \\boldsymbol{x}] = S\\mathbb{E}[\\boldsymbol{x}] = \\boldsymbol{0}$$ $$\\mathbb{E}[\\boldsymbol{b} \\boldsymbol{b}^T] =  \\mathbb{E}[S \\boldsymbol{x} \\boldsymbol{x}^T S^T] = S \\mathbb{E}[\\boldsymbol{x} \\boldsymbol{x}^T] S^T = SS^T = VWV^T = C.$$</p> <p>First, let's compute an eigenvalue decomposition of the covariance matrix.</p> In\u00a0[7]: Copied! <pre>def get_eigenpairs(n, smoothness=1, lengthscale=1):\n    h = 1/(n-1)\n    pts = np.linspace(h/2, 1 - h/2, num=n - 1)\n    cov = get_covariance_matrix(pts, smoothness, lengthscale)\n    w, v = np.linalg.eig(cov)\n    # ensure all eigenvectors are correctly oriented\n    for col in range(v.shape[1]):\n        if v[0, col] &lt; 0:\n            v[:, col] *= -1\n    return pts, w, v\n</pre> def get_eigenpairs(n, smoothness=1, lengthscale=1):     h = 1/(n-1)     pts = np.linspace(h/2, 1 - h/2, num=n - 1)     cov = get_covariance_matrix(pts, smoothness, lengthscale)     w, v = np.linalg.eig(cov)     # ensure all eigenvectors are correctly oriented     for col in range(v.shape[1]):         if v[0, col] &lt; 0:             v[:, col] *= -1     return pts, w, v <p>Next, we plot the eigenfunctions for different values of $n$, the number of grid points.</p> In\u00a0[8]: Copied! <pre>n = [8, 16, 32] # list of number of gridpoints to plot\nm = 5 # number of eigenfunctions to plot\nfig, axes = plt.subplots(m, len(n), figsize=(8, 6))\nfor j, k in enumerate(n):\n    x, w, v = get_eigenpairs(k)\n    for i in range(m):\n        axes[i, j].plot(x, v[:, i])\n        axes[i, j].set_xlim(0, 1)\n        axes[i, j].get_yaxis().set_ticks([])\n        if i &lt; m - 1:\n            axes[i, j].get_xaxis().set_ticks([])\n        if i == 0:\n            axes[i, j].set_title(r\"$n = \" + repr(k) + r\"$\")\nplt.tight_layout()\n</pre> n = [8, 16, 32] # list of number of gridpoints to plot m = 5 # number of eigenfunctions to plot fig, axes = plt.subplots(m, len(n), figsize=(8, 6)) for j, k in enumerate(n):     x, w, v = get_eigenpairs(k)     for i in range(m):         axes[i, j].plot(x, v[:, i])         axes[i, j].set_xlim(0, 1)         axes[i, j].get_yaxis().set_ticks([])         if i &lt; m - 1:             axes[i, j].get_xaxis().set_ticks([])         if i == 0:             axes[i, j].set_title(r\"$n = \" + repr(k) + r\"$\") plt.tight_layout() <p>With this eigenvalue decomposition, we can compute samples of the Gaussian random field $b(x, \\omega)$, and hence, also of the lognormal Gaussian random field $a(x, \\omega) = \\exp(b(x, \\omega))$, since $\\boldsymbol{b} = V W^{1/2} \\boldsymbol{x}$.</p> In\u00a0[9]: Copied! <pre>def evaluate(w, v, y=None):\n    if y is None:\n        y = np.random.randn(len(w) - 1)\n    m = len(y)\n    return v[:, :m] @ np.diag(np.sqrt(w[:m])) @ y\n</pre> def evaluate(w, v, y=None):     if y is None:         y = np.random.randn(len(w) - 1)     m = len(y)     return v[:, :m] @ np.diag(np.sqrt(w[:m])) @ y <p>Let's plot a couple of realizations of the Gaussian random field $b(x, \\omega)$.</p> In\u00a0[10]: Copied! <pre>n = 64\nx, w, v = get_eigenpairs(n)\nfig, ax = plt.subplots(figsize=(6, 4))\nfor _ in range(10):\n    ax.plot(x, evaluate(w, v))\nax.set_xlim(0, 1)\nax.set_xlabel(r\"$x$\")\nax.set_ylabel(r\"$\\log(a(x, \\cdot))$\")\nplt.show()\n</pre> n = 64 x, w, v = get_eigenpairs(n) fig, ax = plt.subplots(figsize=(6, 4)) for _ in range(10):     ax.plot(x, evaluate(w, v)) ax.set_xlim(0, 1) ax.set_xlabel(r\"$x$\") ax.set_ylabel(r\"$\\log(a(x, \\cdot))$\") plt.show() <p>Now that we are able to compute realizations of the Gaussian random field, a next step is to compute a numerical solution of the PDE</p> <p>$$-\\frac{d}{dx}\\bigg(a(x, \\omega) \\frac{d}{dx} u(x, \\omega) \\bigg) = f(x), \\quad 0 \\leq x \\leq 1.$$</p> <p>Using a straightforward finite-difference approximation, it is easy to show that the numerical solution $\\boldsymbol{u}$ is the solution of a tridiagonal system. The solutions of such a tridiagonal system can be easily obtained in $O(n)$ (linear) time using the tridiagonal matrix algorithm (also known as the Thomas algorithm). More details can be found here.</p> In\u00a0[11]: Copied! <pre>def thomas(a, b, c, d):\n    n = len(b)\n    x = np.zeros(n)\n    for i in range(1, n):\n        w = a[i-1]/b[i-1]\n        b[i] -= w*c[i-1]\n        d[i] -= w*d[i-1]\n    x[n-1] = d[n-1]/b[n-1]\n    for i in reversed(range(n-1)):\n        x[i] = (d[i] - c[i]*x[i+1])/b[i]\n    return x\n</pre> def thomas(a, b, c, d):     n = len(b)     x = np.zeros(n)     for i in range(1, n):         w = a[i-1]/b[i-1]         b[i] -= w*c[i-1]         d[i] -= w*d[i-1]     x[n-1] = d[n-1]/b[n-1]     for i in reversed(range(n-1)):         x[i] = (d[i] - c[i]*x[i+1])/b[i]     return x <p>For the remainder of this notebook, we will assume that the source term $f(x)=1$ and Dirichlet boundary conditions $u(0) = u(1) = 0$.</p> In\u00a0[12]: Copied! <pre>def pde_solve(a):\n    n = len(a)\n    b = np.full((n-1, 1), 1/n**2)\n    x = thomas(-a[1:n-1], a[:n-1] + a[1:], -a[1:n-1], b)\n    return np.insert(x, [0, n-1], [0, 0])\n</pre> def pde_solve(a):     n = len(a)     b = np.full((n-1, 1), 1/n**2)     x = thomas(-a[1:n-1], a[:n-1] + a[1:], -a[1:n-1], b)     return np.insert(x, [0, n-1], [0, 0]) <p>Let's compute and plot a couple of solutions $u(x, \\omega)$.</p> In\u00a0[13]: Copied! <pre>n = 64\n_, w, v = get_eigenpairs(n)\nx = np.linspace(0, 1, num=n)\nfig, ax = plt.subplots(figsize=(6, 4))\nfor _ in range(10):\n    a = np.exp(evaluate(w, v))\n    u = pde_solve(a)\n    ax.plot(x, u)\nax.set_xlim(0, 1)\nax.set_xlabel(r\"$x$\")\nax.set_ylabel(r\"$u(x, \\cdot)$\")\nplt.show()\n</pre> n = 64 _, w, v = get_eigenpairs(n) x = np.linspace(0, 1, num=n) fig, ax = plt.subplots(figsize=(6, 4)) for _ in range(10):     a = np.exp(evaluate(w, v))     u = pde_solve(a)     ax.plot(x, u) ax.set_xlim(0, 1) ax.set_xlabel(r\"$x$\") ax.set_ylabel(r\"$u(x, \\cdot)$\") plt.show() <p>In the multilevel Monte Carlo method, we will rely on the ability to generate \"correlated\" solutions of the PDE with varying mesh sizes. Such a correlated solutions can be used as efficient control variates to reduce the variance (or statistical error) in the approximation of the expected value $\\mathbb{E}[Q]$. Since we are using a factorization of the covariance matrix to generate realizations of the Gaussian random field, it is quite easy to obtain correlated samples: when sampling from the \"coarse\" solution level, use the same set of random numbers used to sample from the \"fine\" solution level, but truncated to the appropriate size. Since the eigenvalue decomposition will reveal the most important modes in the covariance matrix, that same eigenvalue decomposition on a \"coarse\" approximation level will contain the same eigenfunctions, represented on the coarse grid. Let's illustrate this property on an example using <code>n = 16</code> grid points for the fine solution level and <code>n = 8</code> grid points for the coarse solution level.</p> In\u00a0[14]: Copied! <pre>nf = 16\nnc = nf//2\n_, wf, vf = get_eigenpairs(nf)\n_, wc, vc = get_eigenpairs(nc)\nxf = np.linspace(0, 1, num=nf)\nxc = np.linspace(0, 1, num=nc)\nfig, ax = plt.subplots(figsize=(6, 4))\nfor _ in range(10):\n    yf = np.random.randn(nf - 1)\n    af = np.exp(evaluate(wf, vf, y=yf))\n    uf = pde_solve(af)\n    ax.plot(xf, uf)\n    yc = yf[:nc - 1]\n    ac = np.exp(evaluate(wc, vc, y=yc))\n    uc = pde_solve(ac)\n    ax.plot(xc, uc, color=ax.lines[-1].get_color(), linestyle=\"dashed\", dash_capstyle=\"round\")\nax.set_xlim(0, 1)\nax.set_ylim(bottom=0)\nax.set_xlabel(r\"$x$\")\nax.set_ylabel(r\"$u(x, \\cdot)$\")\nplt.show()\n</pre> nf = 16 nc = nf//2 _, wf, vf = get_eigenpairs(nf) _, wc, vc = get_eigenpairs(nc) xf = np.linspace(0, 1, num=nf) xc = np.linspace(0, 1, num=nc) fig, ax = plt.subplots(figsize=(6, 4)) for _ in range(10):     yf = np.random.randn(nf - 1)     af = np.exp(evaluate(wf, vf, y=yf))     uf = pde_solve(af)     ax.plot(xf, uf)     yc = yf[:nc - 1]     ac = np.exp(evaluate(wc, vc, y=yc))     uc = pde_solve(ac)     ax.plot(xc, uc, color=ax.lines[-1].get_color(), linestyle=\"dashed\", dash_capstyle=\"round\") ax.set_xlim(0, 1) ax.set_ylim(bottom=0) ax.set_xlabel(r\"$x$\") ax.set_ylabel(r\"$u(x, \\cdot)$\") plt.show() <p>The better the coarse solution matches the fine grid solution, the more efficient the multilevel methods in Section 3 will perform.</p> <p>Let's begin by using the single-level Monte Carlo and quasi-Monte Carlo methods to compute the expected value $\\mathbb{E}[Q]$. As quantity of interest $Q$ we take the solution of the PDE at $x=1/2$, i.e., $Q = u(1/2, \\cdot)$.</p> <p>To integrate the elliptic PDE problem into <code>QMCPy</code>, we construct a simple class as follows:</p> In\u00a0[15]: Copied! <pre>class EllipticPDE(Integrand):\n    \n    def __init__(self, sampler, smoothness=1, lengthscale=1):\n        self.parameters = [\"smoothness\", \"lengthscale\", \"n\"]\n        self.smoothness = smoothness\n        self.lengthscale = lengthscale\n        self.n = sampler.gen_samples(n=1).shape[-1] + 1\n        self.compute_eigenpairs()\n        self.sampler = sampler\n        self.true_measure = qp.Gaussian(self.sampler)\n        super(EllipticPDE, self).__init__(dimension_indv=(),dimension_comb=(),parallel=False)\n        \n    def compute_eigenpairs(self):\n        _, w, v = get_eigenpairs(self.n)\n        self.eigenpairs = w, v\n        \n    def g(self, x):\n        y = np.zeros(x.shape[:-1])\n        for i in np.ndindex(y.shape):\n            y[i] = self.__g(x[i])\n        return y \n    \n    def __g(self, x):\n        w, v = self.eigenpairs\n        a = np.exp(evaluate(w, v, y=x))\n        u = pde_solve(a)\n        return u[len(u)//2]\n    \n    def _spawn(self, level, sampler):\n        return EllipticPDE(sampler, smoothness=self.smoothness, lengthscale=self.lengthscale)\n</pre> class EllipticPDE(Integrand):          def __init__(self, sampler, smoothness=1, lengthscale=1):         self.parameters = [\"smoothness\", \"lengthscale\", \"n\"]         self.smoothness = smoothness         self.lengthscale = lengthscale         self.n = sampler.gen_samples(n=1).shape[-1] + 1         self.compute_eigenpairs()         self.sampler = sampler         self.true_measure = qp.Gaussian(self.sampler)         super(EllipticPDE, self).__init__(dimension_indv=(),dimension_comb=(),parallel=False)              def compute_eigenpairs(self):         _, w, v = get_eigenpairs(self.n)         self.eigenpairs = w, v              def g(self, x):         y = np.zeros(x.shape[:-1])         for i in np.ndindex(y.shape):             y[i] = self.__g(x[i])         return y           def __g(self, x):         w, v = self.eigenpairs         a = np.exp(evaluate(w, v, y=x))         u = pde_solve(a)         return u[len(u)//2]          def _spawn(self, level, sampler):         return EllipticPDE(sampler, smoothness=self.smoothness, lengthscale=self.lengthscale) In\u00a0[16]: Copied! <pre># Custom print function\ndef print_data(data):\n    for key, val in vars(data).items():\n        kv = getattr(data, key)\n        if hasattr(kv, \"parameters\"):\n            print(f\"{key}: {type(val).__name__}\")\n            for param in kv.parameters:\n                print(f\"\\t{param}: {getattr(kv, param)}\")\n    for param in data.parameters:\n        print(f\"{param}: {getattr(data, param)}\")\n</pre> # Custom print function def print_data(data):     for key, val in vars(data).items():         kv = getattr(data, key)         if hasattr(kv, \"parameters\"):             print(f\"{key}: {type(val).__name__}\")             for param in kv.parameters:                 print(f\"\\t{param}: {getattr(kv, param)}\")     for param in data.parameters:         print(f\"{param}: {getattr(data, param)}\") In\u00a0[17]: Copied! <pre># Main function to test different methods\ndef test(problem, sampler, stopping_criterion, abs_tol=5e-3, verbose=True, **kwargs):\n    integrand = problem(sampler)\n    solution, data = stopping_criterion(integrand, abs_tol=abs_tol, **kwargs).integrate()\n    if verbose:\n        print(data)\n        print(\"\\nComputed solution %.3f in %.2f s\"%(solution, data.time_integrate))\n</pre> # Main function to test different methods def test(problem, sampler, stopping_criterion, abs_tol=5e-3, verbose=True, **kwargs):     integrand = problem(sampler)     solution, data = stopping_criterion(integrand, abs_tol=abs_tol, **kwargs).integrate()     if verbose:         print(data)         print(\"\\nComputed solution %.3f in %.2f s\"%(solution, data.time_integrate)) <p>Next, let's apply simple Monte Carlo to approximate the expected value $\\mathbb{E}[Q]$. The Monte Carlo estimator for $\\mathbb{E}[Q]$ is simply the sample average over a finite set of samples, i.e.,</p> <p>$$\\mathcal{Q}_N^\\text{MC} := \\frac{1}{N} \\sum_{n=0}^{N-1} Q^{(n)},$$</p> <p>where $Q^{(n)} := u(1/2, \\boldsymbol{x}^{(n)})$ and we explicitly denote the dependency of $Q$ on the standard normal random numbers $\\boldsymbol{x}$ used to sample from the Gaussian random field. We will continue to increase the number of samples $N$ until a certain error criterion is satisfied.</p> In\u00a0[18]: Copied! <pre># MC\ntest(EllipticPDE, qp.IIDStdUniform(32), qp.CubMCCLT)\n</pre> # MC test(EllipticPDE, qp.IIDStdUniform(32), qp.CubMCCLT) <pre>Data (Data)\n    solution        0.189\n    bound_low       0.185\n    bound_high      0.194\n    bound_diff      0.010\n    n_total         22341\n    time_integrate  6.148\nCubMCCLT (AbstractStoppingCriterion)\n    abs_tol         0.005\n    rel_tol         0\n    n_init          2^(10)\n    n_limit         2^(30)\n    inflate         1.200\n    alpha           0.010\nEllipticPDE (AbstractIntegrand)\n    smoothness      1\n    lengthscale     1\n    n               33\nGaussian (AbstractTrueMeasure)\n    mean            0\n    covariance      1\n    decomp_type     PCA\nIIDStdUniform (AbstractIIDDiscreteDistribution)\n    d               2^(5)\n    replications    1\n    entropy         183041573331055986930938415720323172091\n\nComputed solution 0.189 in 6.15 s\n</pre> <p>The solution should be $\\approx 0.189$.</p> <p>Similarly, the quasi-Monte Carlo estimator for $\\mathbb{E}[Q]$ is defined as</p> <p>$$\\mathcal{Q}_N^\\text{QMC} := \\frac{1}{N} \\sum_{n=0}^{N-1} Q^{(n)},$$</p> <p>where $Q^{(n)} := u(1/2, \\boldsymbol{t}^{(n)})$ with $\\boldsymbol{t}^{(n)}$ the $n$th low-discrepancy point transformed to the distribution of interest. For our elliptic PDE, this means that the quasi-Monte Carlo points, generated inside the unit cube $[0, 1)^d$, are mapped to $\\mathbb{R}^d$.</p> <p>Because the quasi-Monte Carlo estimator doesn't come with a reliable error estimator, we run $K$ different quasi-Monte Carlo estimators in parallel. The sample variance over these $K$ different estimators can then be used as an error estimator.</p> In\u00a0[19]: Copied! <pre># QMC\ntest(EllipticPDE, qp.Lattice(32, replications=8), qp.CubQMCCLT, n_init=32)\n</pre> # QMC test(EllipticPDE, qp.Lattice(32, replications=8), qp.CubQMCCLT, n_init=32) <pre>Data (Data)\n    solution        0.185\n    comb_bound_low  0.182\n    comb_bound_high 0.188\n    comb_bound_diff 0.006\n    comb_flags      1\n    n_total         1024\n    n               1024\n    n_rep           2^(7)\n    time_integrate  0.276\nCubQMCRepStudentT (AbstractStoppingCriterion)\n    inflate         1\n    alpha           0.010\n    abs_tol         0.005\n    rel_tol         0\n    n_init          2^(5)\n    n_limit         2^(30)\nEllipticPDE (AbstractIntegrand)\n    smoothness      1\n    lengthscale     1\n    n               33\nGaussian (AbstractTrueMeasure)\n    mean            0\n    covariance      1\n    decomp_type     PCA\nLattice (AbstractLDDiscreteDistribution)\n    d               2^(5)\n    replications    2^(3)\n    randomize       SHIFT\n    gen_vec_source  kuo.lattice-33002-1024-1048576.9125.txt\n    order           NATURAL\n    n_limit         2^(20)\n    entropy         195885308766153261444310305163423607705\n\nComputed solution 0.185 in 0.28 s\n</pre> <p>Implicit to the Monte Carlo and quasi-Monte Carlo methods above is a discretization parameter used in the numerical solution of the PDE. Let's denote this parameter by $\\ell$, $0 \\leq \\ell \\leq L$. Multilevel methods are based on a telescopic sum expansion for the expected value $\\mathbb{E}[Q_L]$, as follows:</p> <p>$$\\mathbb{E}[Q_L] = \\mathbb{E}[Q_0] + \\mathbb{E}[Q_1 - Q_0] + ... + \\mathbb{E}[Q_L - Q_{L-1}].$$</p> <p>Using a Monte Carlo method for each of the terms on the right hand side yields a multilevel Monte Carlo method. Similarly, using a quasi-Monte Carlo method for each term on the right hand side yields a multilevel quasi-Monte Carlo method.</p> <p>Our class <code>EllipticPDE</code> needs some changes to be integrated with the multilevel methods in <code>QMCPy</code>.</p> In\u00a0[20]: Copied! <pre>class MLEllipticPDE(Integrand):\n    \n    def __init__(self, sampler, smoothness=1, lengthscale=1, _level=None):\n        self.l = _level\n        self.parameters = [\"smoothness\", \"lengthscale\", \"n\", \"nb_of_levels\"]\n        self.smoothness = smoothness\n        self.lengthscale = lengthscale\n        dim = sampler.d + 1\n        self.nb_of_levels = int(np.log2(dim + 1))\n        self.n = [2**(l+1) + 1 for l in range(self.nb_of_levels)]\n        self.compute_eigenpairs()\n        self.sampler = sampler\n        self.true_measure = qp.Gaussian(self.sampler)\n        self.cost = nf\n        d_out = () if _level is None else (2,)\n        super(MLEllipticPDE, self).__init__(dimension_indv=d_out,dimension_comb=d_out,parallel=False)\n    \n    def _spawn(self, level, sampler):\n        return MLEllipticPDE(sampler, smoothness=self.smoothness, lengthscale=self.lengthscale, _level=level)\n        \n    def compute_eigenpairs(self):\n        self.eigenpairs = {}\n        for l in range(self.nb_of_levels):\n            _, w, v = get_eigenpairs(self.n[l])\n            self.eigenpairs[l] = w, v\n        \n    def g(self, x): # This function is called by keyword reference for the level parameter \"l\"!\n        Qf = np.zeros(x.shape[:-1])\n        for i in np.ndindex(Qf.shape): \n            Qf[i] = self.__g(x[i], self.l)\n        if self.l==0:\n            Qc = np.zeros_like(Qf) \n        else:\n            Qc = np.zeros(x.shape[:-1])\n            for i in np.ndindex(Qf.shape): \n                Qc[i] = self.__g(x[i], self.l-1)\n        return np.stack([Qc,Qf],axis=0)\n    \n    def __g(self, x, l):\n        w, v = self.eigenpairs[l]\n        n = self.n[l]\n        a = np.exp(evaluate(w, v, y=x[:n-1]))\n        u = pde_solve(a)\n        return u[len(u)//2]\n  \n    def _dimension_at_level(self, l):\n        return self.n[l]\n</pre> class MLEllipticPDE(Integrand):          def __init__(self, sampler, smoothness=1, lengthscale=1, _level=None):         self.l = _level         self.parameters = [\"smoothness\", \"lengthscale\", \"n\", \"nb_of_levels\"]         self.smoothness = smoothness         self.lengthscale = lengthscale         dim = sampler.d + 1         self.nb_of_levels = int(np.log2(dim + 1))         self.n = [2**(l+1) + 1 for l in range(self.nb_of_levels)]         self.compute_eigenpairs()         self.sampler = sampler         self.true_measure = qp.Gaussian(self.sampler)         self.cost = nf         d_out = () if _level is None else (2,)         super(MLEllipticPDE, self).__init__(dimension_indv=d_out,dimension_comb=d_out,parallel=False)          def _spawn(self, level, sampler):         return MLEllipticPDE(sampler, smoothness=self.smoothness, lengthscale=self.lengthscale, _level=level)              def compute_eigenpairs(self):         self.eigenpairs = {}         for l in range(self.nb_of_levels):             _, w, v = get_eigenpairs(self.n[l])             self.eigenpairs[l] = w, v              def g(self, x): # This function is called by keyword reference for the level parameter \"l\"!         Qf = np.zeros(x.shape[:-1])         for i in np.ndindex(Qf.shape):              Qf[i] = self.__g(x[i], self.l)         if self.l==0:             Qc = np.zeros_like(Qf)          else:             Qc = np.zeros(x.shape[:-1])             for i in np.ndindex(Qf.shape):                  Qc[i] = self.__g(x[i], self.l-1)         return np.stack([Qc,Qf],axis=0)          def __g(self, x, l):         w, v = self.eigenpairs[l]         n = self.n[l]         a = np.exp(evaluate(w, v, y=x[:n-1]))         u = pde_solve(a)         return u[len(u)//2]        def _dimension_at_level(self, l):         return self.n[l] <p>Let's apply multilevel Monte Carlo to the elliptic PDE problem.</p> In\u00a0[21]: Copied! <pre>test(MLEllipticPDE, qp.IIDStdUniform(32), qp.CubMLMC)\n</pre> test(MLEllipticPDE, qp.IIDStdUniform(32), qp.CubMLMC) <pre>Data (Data)\n    solution        0.191\n    n_total         86220\n    levels          3\n    n_level         [37594  3456  2444]\n    mean_level      [1.906e-01 1.620e-04 4.757e-05]\n    var_level       [0.058 0.    0.   ]\n    cost_per_sample [16. 16. 16.]\n    alpha           1.768\n    beta            2.000\n    gamma           2^(-1)\n    time_integrate  2.754\nCubMLMC (AbstractStoppingCriterion)\n    rmse_tol        0.002\n    n_init          2^(8)\n    levels_min      2^(1)\n    levels_max      10\n    theta           2^(-1)\nMLEllipticPDE (AbstractIntegrand)\n    smoothness      1\n    lengthscale     1\n    n               [ 3  5  9 17 33]\n    nb_of_levels    5\nGaussian (AbstractTrueMeasure)\n    mean            0\n    covariance      1\n    decomp_type     PCA\nIIDStdUniform (AbstractIIDDiscreteDistribution)\n    d               2^(5)\n    replications    1\n    entropy         307775516609041022608648907501235120384\n\nComputed solution 0.191 in 2.75 s\n</pre> <p>Now it's easy to switch to multilevel quasi-Monte Carlo. Just change the discrete distribution from <code>IIDStdUniform</code> to <code>Lattice</code>.</p> In\u00a0[22]: Copied! <pre>test(MLEllipticPDE, qp.Lattice(32,replications=8), qp.CubMLQMCCont, n_init=32)\n</pre> test(MLEllipticPDE, qp.Lattice(32,replications=8), qp.CubMLQMCCont, n_init=32) <pre>Data (Data)\n    solution        0.189\n    n_total         8704\n    levels          3\n    n_level         [1024   32   32]\n    mean_level      [ 0.19 -0.   -0.  ]\n    var_level       [1.545e-06 1.602e-06 1.390e-07]\n    bias_estimate   5.95e-04\n    time_integrate  0.526\nCubMLQMCCont (AbstractStoppingCriterion)\n    rmse_tol        0.002\n    n_init          2^(5)\n    n_limit         10000000000\n    replications    2^(3)\n    levels_min      2^(1)\n    levels_max      10\n    n_tols          10\n    inflate         1.668\n    theta_init      2^(-1)\n    theta           0.094\nMLEllipticPDE (AbstractIntegrand)\n    smoothness      1\n    lengthscale     1\n    n               [ 3  5  9 17 33]\n    nb_of_levels    5\nGaussian (AbstractTrueMeasure)\n    mean            0\n    covariance      1\n    decomp_type     PCA\nLattice (AbstractLDDiscreteDistribution)\n    d               2^(5)\n    replications    2^(3)\n    randomize       SHIFT\n    gen_vec_source  kuo.lattice-33002-1024-1048576.9125.txt\n    order           NATURAL\n    n_limit         2^(20)\n    entropy         138093628998779790374931977379982164469\n\nComputed solution 0.189 in 0.53 s\n</pre> <p>In the continuation multilevel (quasi-)Monte Carlo method, we run the standard multilevel (quasi-)Monte Carlo method for a sequence of larger tolerances to obtain better estimates of the algorithmic parameters. The continuation multilevel heuristic will generally compute the same solution just a bit faster.</p> In\u00a0[23]: Copied! <pre>test(MLEllipticPDE, qp.IIDStdUniform(32), qp.CubMCMLCont)\n</pre> test(MLEllipticPDE, qp.IIDStdUniform(32), qp.CubMCMLCont) <pre>Data (Data)\n    solution        0.190\n    n_total         29976\n    levels          3\n    n_level         [16665  1220   256   256   256]\n    mean_level      [1.894e-01 3.968e-04 1.736e-04]\n    var_level       [5.481e-02 3.258e-04 8.380e-06]\n    cost_per_sample [16. 16. 16.]\n    alpha           1.192\n    beta            5.281\n    gamma           2^(-1)\n    time_integrate  1.193\nCubMLMCCont (AbstractStoppingCriterion)\n    rmse_tol        0.002\n    n_init          2^(8)\n    levels_min      2^(1)\n    levels_max      10\n    n_tols          10\n    inflate         1.668\n    theta_init      2^(-1)\n    theta           0.010\nMLEllipticPDE (AbstractIntegrand)\n    smoothness      1\n    lengthscale     1\n    n               [ 3  5  9 17 33]\n    nb_of_levels    5\nGaussian (AbstractTrueMeasure)\n    mean            0\n    covariance      1\n    decomp_type     PCA\nIIDStdUniform (AbstractIIDDiscreteDistribution)\n    d               2^(5)\n    replications    1\n    entropy         220267298921554485839372423247101207645\n\nComputed solution 0.190 in 1.19 s\n</pre> In\u00a0[24]: Copied! <pre>test(MLEllipticPDE, qp.Lattice(32,replications=8), qp.CubMLQMCCont, n_init=32)\n</pre> test(MLEllipticPDE, qp.Lattice(32,replications=8), qp.CubMLQMCCont, n_init=32) <pre>Data (Data)\n    solution        0.190\n    n_total         8960\n    levels          3\n    n_level         [1024   64   32]\n    mean_level      [ 0.189  0.001 -0.   ]\n    var_level       [1.488e-06 5.978e-07 2.139e-07]\n    bias_estimate   2.44e-05\n    time_integrate  0.558\nCubMLQMCCont (AbstractStoppingCriterion)\n    rmse_tol        0.002\n    n_init          2^(5)\n    n_limit         10000000000\n    replications    2^(3)\n    levels_min      2^(1)\n    levels_max      10\n    n_tols          10\n    inflate         1.668\n    theta_init      2^(-1)\n    theta           0.010\nMLEllipticPDE (AbstractIntegrand)\n    smoothness      1\n    lengthscale     1\n    n               [ 3  5  9 17 33]\n    nb_of_levels    5\nGaussian (AbstractTrueMeasure)\n    mean            0\n    covariance      1\n    decomp_type     PCA\nLattice (AbstractLDDiscreteDistribution)\n    d               2^(5)\n    replications    2^(3)\n    randomize       SHIFT\n    gen_vec_source  kuo.lattice-33002-1024-1048576.9125.txt\n    order           NATURAL\n    n_limit         2^(20)\n    entropy         308444004426483247734546558914189653748\n\nComputed solution 0.190 in 0.56 s\n</pre> <p>Finally, we will run some convergence tests to see how these methods behave as a function of the error tolerance.</p> In\u00a0[25]: Copied! <pre># Main function to test convergence for given problem\ndef test_convergence(problem, sampler, stopping_criterion, abs_tol=1e-3, verbose=True, smoothness=1, lengthscale=1, **kwargs):\n    integrand = problem(sampler, smoothness=smoothness, lengthscale=lengthscale)\n    stopping_crit = stopping_criterion(integrand, abs_tol=abs_tol, **kwargs)\n    stopping_crit.data = stopping_crit._construct_data()\n    \n    # manually call \"integrate()\"\n    tol = []\n    n_samp = []\n    for t in range(stopping_crit.n_tols):\n        stopping_crit.rmse_tol = stopping_crit.inflate**(stopping_crit.n_tols-t-1)*stopping_crit.target_tol # update tol\n        stopping_crit._integrate(stopping_crit.data) # call _integrate()\n        tol.append(copy.copy(stopping_crit.rmse_tol))\n        n_samp.append(copy.copy(stopping_crit.data.n_level))\n\n        if verbose:\n            print(\"tol = {:5.3e}, number of samples = {}\".format(tol[-1], n_samp[-1]))\n            \n    return tol, n_samp\n</pre> # Main function to test convergence for given problem def test_convergence(problem, sampler, stopping_criterion, abs_tol=1e-3, verbose=True, smoothness=1, lengthscale=1, **kwargs):     integrand = problem(sampler, smoothness=smoothness, lengthscale=lengthscale)     stopping_crit = stopping_criterion(integrand, abs_tol=abs_tol, **kwargs)     stopping_crit.data = stopping_crit._construct_data()          # manually call \"integrate()\"     tol = []     n_samp = []     for t in range(stopping_crit.n_tols):         stopping_crit.rmse_tol = stopping_crit.inflate**(stopping_crit.n_tols-t-1)*stopping_crit.target_tol # update tol         stopping_crit._integrate(stopping_crit.data) # call _integrate()         tol.append(copy.copy(stopping_crit.rmse_tol))         n_samp.append(copy.copy(stopping_crit.data.n_level))          if verbose:             print(\"tol = {:5.3e}, number of samples = {}\".format(tol[-1], n_samp[-1]))                  return tol, n_samp In\u00a0[26]: Copied! <pre># Execute the convergence test\ndef execute_convergence_test(smoothness=1, lengthscale=1):\n    \n    # Convergence test for MLMC\n    tol_mlmc, n_samp_mlmc = test_convergence(MLEllipticPDE, qp.IIDStdUniform(32), qp.CubMLMCCont, verbose=False)\n    \n    # Convergence test for MLQMC\n    tol_mlqmc, n_samp_mlqmc = test_convergence(MLEllipticPDE, qp.Lattice(32,replications=8), qp.CubMLQMCCont, verbose=False, n_init=32)\n    \n    # Compute cost per level\n    max_levels = max(max([len(n_samp) for n_samp in n_samp_mlmc]), max([len(n_samp) for n_samp in n_samp_mlqmc]))\n    cost_per_level = np.array([2**level + int(2**(level-1)) for level in range(max_levels)])\n    cost_per_level = cost_per_level/cost_per_level[-1]\n    \n    # Compute total cost for each tolerance and store the result\n    cost = {}\n    cost[\"mc\"] = (tol_mlmc, [n_samp_mlmc[tol][0] for tol in range(len(tol_mlmc))]) # where we assume V[Q_0] = V[Q_L]\n    cost[\"qmc\"] = (tol_mlqmc, [n_samp_mlqmc[tol][0] for tol in range(len(tol_mlqmc))]) # where we assume V[Q_0] = V[Q_L]\n    cost[\"mlmc\"] = (tol_mlmc, [sum([n_samp*cost_per_level[j] for j, n_samp in enumerate(n_samp_mlmc[tol])]) for tol in range(len(tol_mlmc))])\n    cost[\"mlqmc\"] = (tol_mlqmc, [sum([n_samp*cost_per_level[j] for j, n_samp in enumerate(n_samp_mlqmc[tol])]) for tol in range(len(tol_mlqmc))])\n    \n    return cost\n</pre> # Execute the convergence test def execute_convergence_test(smoothness=1, lengthscale=1):          # Convergence test for MLMC     tol_mlmc, n_samp_mlmc = test_convergence(MLEllipticPDE, qp.IIDStdUniform(32), qp.CubMLMCCont, verbose=False)          # Convergence test for MLQMC     tol_mlqmc, n_samp_mlqmc = test_convergence(MLEllipticPDE, qp.Lattice(32,replications=8), qp.CubMLQMCCont, verbose=False, n_init=32)          # Compute cost per level     max_levels = max(max([len(n_samp) for n_samp in n_samp_mlmc]), max([len(n_samp) for n_samp in n_samp_mlqmc]))     cost_per_level = np.array([2**level + int(2**(level-1)) for level in range(max_levels)])     cost_per_level = cost_per_level/cost_per_level[-1]          # Compute total cost for each tolerance and store the result     cost = {}     cost[\"mc\"] = (tol_mlmc, [n_samp_mlmc[tol][0] for tol in range(len(tol_mlmc))]) # where we assume V[Q_0] = V[Q_L]     cost[\"qmc\"] = (tol_mlqmc, [n_samp_mlqmc[tol][0] for tol in range(len(tol_mlqmc))]) # where we assume V[Q_0] = V[Q_L]     cost[\"mlmc\"] = (tol_mlmc, [sum([n_samp*cost_per_level[j] for j, n_samp in enumerate(n_samp_mlmc[tol])]) for tol in range(len(tol_mlmc))])     cost[\"mlqmc\"] = (tol_mlqmc, [sum([n_samp*cost_per_level[j] for j, n_samp in enumerate(n_samp_mlqmc[tol])]) for tol in range(len(tol_mlqmc))])          return cost In\u00a0[27]: Copied! <pre># Plot the result\ndef plot_convergence(cost):\n    fig, ax = plt.subplots(figsize=(8, 6))\n    ax.plot(cost[\"mc\"][0], cost[\"mc\"][1], marker=\"o\", label=\"MC\")\n    ax.plot(cost[\"qmc\"][0], cost[\"qmc\"][1], marker=\"o\", label=\"QMC\")\n    ax.plot(cost[\"mlmc\"][0], cost[\"mlmc\"][1], marker=\"o\", label=\"MLMC\")\n    ax.plot(cost[\"mlqmc\"][0], cost[\"mlqmc\"][1], marker=\"o\", label=\"MLQMC\")\n    ax.legend(frameon=False)\n    ax.set_xscale(\"log\")\n    ax.set_yscale(\"log\")\n    ax.set_xlabel(r\"error tolerance $\\varepsilon$\")\n    ax.set_ylabel(r\"equivalent \\# model evaluations at finest level\")\n    plt.show()\n</pre> # Plot the result def plot_convergence(cost):     fig, ax = plt.subplots(figsize=(8, 6))     ax.plot(cost[\"mc\"][0], cost[\"mc\"][1], marker=\"o\", label=\"MC\")     ax.plot(cost[\"qmc\"][0], cost[\"qmc\"][1], marker=\"o\", label=\"QMC\")     ax.plot(cost[\"mlmc\"][0], cost[\"mlmc\"][1], marker=\"o\", label=\"MLMC\")     ax.plot(cost[\"mlqmc\"][0], cost[\"mlqmc\"][1], marker=\"o\", label=\"MLQMC\")     ax.legend(frameon=False)     ax.set_xscale(\"log\")     ax.set_yscale(\"log\")     ax.set_xlabel(r\"error tolerance $\\varepsilon$\")     ax.set_ylabel(r\"equivalent \\# model evaluations at finest level\")     plt.show() <p>This command takes a while to execute (about 1 minute on my laptop):</p> In\u00a0[28]: Copied! <pre>plot_convergence(execute_convergence_test())\n</pre> plot_convergence(execute_convergence_test()) <p>The benefit of the low-discrepancy point set depends on the smoothness of the random field: the smoother the random field, the better. Here's an example for a Gaussian random field with a smaller smoothness $\\nu=1/2$ and smaller length scale $\\lambda=1/3$.</p> In\u00a0[29]: Copied! <pre>smoothness = 1/2\nlengthscale = 1/3\n</pre> smoothness = 1/2 lengthscale = 1/3 In\u00a0[30]: Copied! <pre>n = 256\nx, w, v = get_eigenpairs(n, smoothness=smoothness, lengthscale=lengthscale)\nfig, ax = plt.subplots(figsize=(6, 4))\nfor _ in range(10):\n    ax.plot(x, evaluate(w, v))\nax.set_xlim(0, 1)\nax.set_xlabel(r\"$x$\")\nax.set_ylabel(r\"$\\log(a(x, \\cdot))$\")\nplt.show()\n</pre> n = 256 x, w, v = get_eigenpairs(n, smoothness=smoothness, lengthscale=lengthscale) fig, ax = plt.subplots(figsize=(6, 4)) for _ in range(10):     ax.plot(x, evaluate(w, v)) ax.set_xlim(0, 1) ax.set_xlabel(r\"$x$\") ax.set_ylabel(r\"$\\log(a(x, \\cdot))$\") plt.show() In\u00a0[31]: Copied! <pre>plot_convergence(execute_convergence_test(lengthscale=lengthscale, smoothness=smoothness))\n</pre> plot_convergence(execute_convergence_test(lengthscale=lengthscale, smoothness=smoothness)) <p>While the multilevel quasi-Monte Carlo method is still the fastest method, the asymptotic cost complexity of the QMC-based methods reduces to approximately the same rate as the MC-based methods.</p> <p>The benefits of the multilevel methods over single-level methods will be even larger for two- or three-dimensional PDE problems, since it will be even more computationally efficient to take samples on a coarse grid.</p>"},{"location":"demos/elliptic-pde/#elliptic-pde","title":"Elliptic PDE\u00b6","text":""},{"location":"demos/elliptic-pde/#1-problem-definition","title":"1. Problem definition\u00b6","text":""},{"location":"demos/elliptic-pde/#2-single-level-methods","title":"2. Single-level methods\u00b6","text":""},{"location":"demos/elliptic-pde/#3-multilevel-methods","title":"3. Multilevel methods\u00b6","text":""},{"location":"demos/elliptic-pde/#31-multilevel-quasi-monte-carlo","title":"3.1 Multilevel (quasi-)Monte Carlo\u00b6","text":""},{"location":"demos/elliptic-pde/#32-continuation-multilevel-quasi-monte-carlo","title":"3.2 Continuation multilevel (quasi-)Monte Carlo\u00b6","text":""},{"location":"demos/elliptic-pde/#4-convergence-tests","title":"4. Convergence tests\u00b6","text":""},{"location":"demos/gbm_demo/","title":"Geometric Brownian Motion","text":"<p>Larysa Matiukha and Sou-Cheng T. Choi</p> <p>Illinois Institute of Technology</p> <p>Modification date: 06/28/2025</p> <p>Creation date: 08/24/2024</p> In\u00a0[1]: Copied! <pre>import qmcpy as qp\nimport numpy as np\nimport scipy.stats as sc\nimport matplotlib.pyplot as plt\nimport ipywidgets as widgets\n</pre> import qmcpy as qp import numpy as np import scipy.stats as sc import matplotlib.pyplot as plt import ipywidgets as widgets <p>Geometric Brownian motion (GBM) is a continuous stochastic process in which the natural logarithm of its values follows a Brownian motion. $[1]$</p> <p>Mathematically, it can be defined as follows:</p> <p>$\\large{S_t = S_0 \\, e^{\\big(\\mu - \\frac{\\sigma^2}{2}\\big)  t + \\sigma W_t}}$,</p> <p>where</p> <ul> <li>$S_0$ is the initial value,</li> <li>$\\mu$ is a drift coefficient</li> <li>$\\sigma$ is difussion coefficient</li> <li>$W_t$ is a (standard) Brownian motion.</li> </ul> <p>GBM is commonly used to model stock prices and options payoffs.</p> <p>Geometric Brownian Motion in QMCPy inherits from BrownianMotion class $[2, 3]$.</p> <p>Let's explore the constructor and sample generation methods through the built-in help documentation:</p> In\u00a0[2]: Copied! <pre>help(qp.GeometricBrownianMotion.__init__)\n</pre> help(qp.GeometricBrownianMotion.__init__) <pre>Help on function __init__ in module qmcpy.true_measure.geometric_brownian_motion:\n\n__init__(self, sampler, t_final=1, initial_value=1, drift=0, diffusion=1, decomp_type='PCA')\n    GeometricBrownianMotion(t) = initial_value * exp[(drift - 0.5 * diffusion) * t\n                                                     + \\sqrt{diffusion} * StandardBrownianMotion(t)]\n\n    Args:\n        sampler (DiscreteDistribution/TrueMeasure): A discrete distribution or true measure.\n        t_final (float): End time for the geometric Brownian motion, non-negative.\n        initial_value (float): Positive initial value of the process.\n        drift (float): Drift coefficient.\n        diffusion (float): Diffusion coefficient, positive.\n        decomp_type (str): Method of decomposition, either \"PCA\" or \"Cholesky\".\n\n    Note: diffusion is $\\sigma^2$, where $\\sigma$ is volatility.\n\n</pre> In\u00a0[3]: Copied! <pre>help(qp.GeometricBrownianMotion.gen_samples)\n</pre> help(qp.GeometricBrownianMotion.gen_samples) <pre>Help on function gen_samples in module qmcpy.true_measure.abstract_true_measure:\n\ngen_samples(self, n=None, n_min=None, n_max=None, return_weights=False, warn=True)\n\n</pre> <p>Now let's create a simple GBM instance and generate sample paths to see the class in action:</p> In\u00a0[4]: Copied! <pre>gbm = qp.GeometricBrownianMotion(qp.Lattice(2)) \ngbm\n</pre> gbm = qp.GeometricBrownianMotion(qp.Lattice(2))  gbm Out[4]: <pre>GeometricBrownianMotion (AbstractTrueMeasure)\n    time_vec        [0.5 1. ]\n    drift           0\n    diffusion       1\n    mean_gbm        [1. 1.]\n    covariance_gbm  [[0.649 0.649]\n                     [0.649 1.718]]\n    decomp_type     PCA</pre> In\u00a0[5]: Copied! <pre>gbm.gen_samples(n=4) # generates four 2-dimensional samples\n</pre> gbm.gen_samples(n=4) # generates four 2-dimensional samples Out[5]: <pre>array([[1.58827541, 1.14170358],\n       [0.42462055, 0.41507014],\n       [0.2630158 , 0.10817803],\n       [1.83631636, 0.40513132]])</pre> In\u00a0[6]: Copied! <pre># Generate GBM samples for theoretical validation\nS0, mu, sigma, T, n_samples = 100.0, 0.05, 0.20, 1.0, 2**12\ndiffusion = sigma**2\nsampler = qp.Lattice(5, seed=42)\nqp_gbm = qp.GeometricBrownianMotion(sampler, t_final=T, initial_value=S0, drift=mu, diffusion=diffusion)\npaths = qp_gbm.gen_samples(n_samples)\nS_T = paths[:, -1]  # Final values only\n\n# Calculate theoretical vs empirical sample moments\ntheo_mean = S0 * np.exp(mu * T)\ntheo_var = S0**2 * np.exp(2*mu*T) * (np.exp(diffusion * T) - 1)\nqp_emp_mean = np.mean(S_T)\nqp_emp_var = np.var(S_T, ddof=1) \nprint(f\"Mean: {qp_emp_mean:.3f} (theoretical: {theo_mean:.3f})\")\nprint(f\"Variance: {qp_emp_var:.3f} (theoretical: {theo_var:.3f})\")\nqp_gbm\n</pre> # Generate GBM samples for theoretical validation S0, mu, sigma, T, n_samples = 100.0, 0.05, 0.20, 1.0, 2**12 diffusion = sigma**2 sampler = qp.Lattice(5, seed=42) qp_gbm = qp.GeometricBrownianMotion(sampler, t_final=T, initial_value=S0, drift=mu, diffusion=diffusion) paths = qp_gbm.gen_samples(n_samples) S_T = paths[:, -1]  # Final values only  # Calculate theoretical vs empirical sample moments theo_mean = S0 * np.exp(mu * T) theo_var = S0**2 * np.exp(2*mu*T) * (np.exp(diffusion * T) - 1) qp_emp_mean = np.mean(S_T) qp_emp_var = np.var(S_T, ddof=1)  print(f\"Mean: {qp_emp_mean:.3f} (theoretical: {theo_mean:.3f})\") print(f\"Variance: {qp_emp_var:.3f} (theoretical: {theo_var:.3f})\") qp_gbm <pre>Mean: 105.127 (theoretical: 105.127)\nVariance: 449.776 (theoretical: 451.029)\n</pre> Out[6]: <pre>GeometricBrownianMotion (AbstractTrueMeasure)\n    time_vec        [0.2 0.4 0.6 0.8 1. ]\n    drift           0.050\n    diffusion       0.040\n    mean_gbm        [101.005 102.02  103.045 104.081 105.127]\n    covariance_gbm  [[ 81.943  82.767  83.599  84.439  85.288]\n                     [ 82.767 167.869 169.556 171.26  172.981]\n                     [ 83.599 169.556 257.923 260.516 263.134]\n                     [ 84.439 171.26  260.516 352.258 355.798]\n                     [ 85.288 172.981 263.134 355.798 451.029]]\n    decomp_type     PCA</pre> <p>Below we compare Brownian motion and geometric Brownian motion using the same parameters: <code>drift</code> = 0, <code>diffusion</code> = 1, <code>initial_value</code> = 1.</p> <p>First, let's define a utility function that will help us visualize GBM paths with different samplers and parameters:</p> In\u00a0[7]: Copied! <pre>def plot_paths(motion_type, sampler, t_final, initial_value, drift, diffusion, n):\n    if motion_type.upper() == 'BM':\n        motion = qp.BrownianMotion(sampler, t_final, initial_value, drift, diffusion)\n        title = f'Realizations of Brownian Motion using {type(sampler).__name__} points'\n        ylabel = 'W(t)'\n    elif motion_type.upper() == 'GBM':\n        motion = qp.GeometricBrownianMotion(sampler, t_final, initial_value, drift, diffusion)\n        title = f'Realizations of Geometric Brownian Motion using {type(sampler).__name__} points'\n        ylabel = 'S(t)'\n    else:\n        raise ValueError(\"motion_type must be 'BM' or 'GBM'\")\n    \n    t = motion.gen_samples(n)\n    initial_values = np.full((n, 1), motion.initial_value)\n    t_w_init = np.hstack((initial_values, t))\n    tvec_w_0 = np.hstack(([0], motion.time_vec))\n\n    plt.figure(figsize=(7, 4));\n    plt.plot(tvec_w_0, t_w_init.T); \n    plt.title(title);\n    plt.xlabel('t');\n    plt.ylabel(ylabel);\n    plt.xlim([tvec_w_0[0], tvec_w_0[-1]]);\n    plt.show();\n</pre> def plot_paths(motion_type, sampler, t_final, initial_value, drift, diffusion, n):     if motion_type.upper() == 'BM':         motion = qp.BrownianMotion(sampler, t_final, initial_value, drift, diffusion)         title = f'Realizations of Brownian Motion using {type(sampler).__name__} points'         ylabel = 'W(t)'     elif motion_type.upper() == 'GBM':         motion = qp.GeometricBrownianMotion(sampler, t_final, initial_value, drift, diffusion)         title = f'Realizations of Geometric Brownian Motion using {type(sampler).__name__} points'         ylabel = 'S(t)'     else:         raise ValueError(\"motion_type must be 'BM' or 'GBM'\")          t = motion.gen_samples(n)     initial_values = np.full((n, 1), motion.initial_value)     t_w_init = np.hstack((initial_values, t))     tvec_w_0 = np.hstack(([0], motion.time_vec))      plt.figure(figsize=(7, 4));     plt.plot(tvec_w_0, t_w_init.T);      plt.title(title);     plt.xlabel('t');     plt.ylabel(ylabel);     plt.xlim([tvec_w_0[0], tvec_w_0[-1]]);     plt.show(); <p>Paths of the driftless Brownian motion should fluctuate symmetrically around the initial value (y = 1) and can take negative values, while those of Geometric Brownian Motion remain strictly positive.</p> In\u00a0[8]: Copied! <pre># Compare Brownian Motion and Geometric Brownian Motion using the unified plotting function\nn = 16\nsampler = qp.Lattice(2**7)\nplot_paths('BM', sampler, t_final=1, initial_value=1, drift=0, diffusion=1, n=n)\nplot_paths('GBM', sampler, t_final=1, initial_value=1, drift=0, diffusion=1, n=n)\n</pre> # Compare Brownian Motion and Geometric Brownian Motion using the unified plotting function n = 16 sampler = qp.Lattice(2**7) plot_paths('BM', sampler, t_final=1, initial_value=1, drift=0, diffusion=1, n=n) plot_paths('GBM', sampler, t_final=1, initial_value=1, drift=0, diffusion=1, n=n) <p>Now, using <code>plot_gbm_paths</code>, we generate 32 GBM paths to model stock price, $S(t)$, with initial value $S_0$ = 50, drift coeffient, $\\mu = 0.1$, diffusion coefficient $\\sigma = 0.2$ using IID points.</p> In\u00a0[9]: Copied! <pre>gbm_iid = plot_paths('GBM', qp.IIDStdUniform(2**8), t_final=5, initial_value=50, drift=0.1, diffusion=0.2, n=32)\n</pre> gbm_iid = plot_paths('GBM', qp.IIDStdUniform(2**8), t_final=5, initial_value=50, drift=0.1, diffusion=0.2, n=32) <p>Using the same parameter values as in example above, we generate 32 GBM paths to model stock price using low-discrepancy lattice points:</p> In\u00a0[10]: Copied! <pre>gbm_lattice = plot_paths('GBM', qp.Lattice(2**8), t_final=5, initial_value=50, drift=0.1, diffusion=0.2, n=32)\n</pre> gbm_lattice = plot_paths('GBM', qp.Lattice(2**8), t_final=5, initial_value=50, drift=0.1, diffusion=0.2, n=32) <p>Next, we define a more sophisticated visualization function that combines path plotting with statistical analysis by showing both the GBM trajectories and the distribution of final values:</p> In\u00a0[11]: Copied! <pre>def plot_gbm_paths_with_distribution(N, sampler, t_final, initial_value, drift, diffusion,n):\n    gbm = qp.GeometricBrownianMotion(sampler, t_final=t_final, initial_value=initial_value, drift=drift, diffusion=diffusion)\n    gbm_path = gbm.gen_samples(2**n)\n    \n    fig, ax = plt.subplots(figsize=(14, 7))\n    T = max(gbm.time_vec)\n    \n    # Plot GBM paths\n    ax.plot(gbm.time_vec, gbm_path.T, lw=0.75, alpha=0.7, color='skyblue')\n    \n    # Set up main plot\n    ax.set_title(f'Geometric Brownian Motion Paths\\n{N} Simulations, T = {T}, $\\mu$ = {drift:.1f}, $\\sigma$ = {diffusion:.1f}, using {type(sampler).__name__} points')\n    ax.set_xlabel(r'$t$')\n    ax.set_ylabel(r'$S(t)$')\n    ax.set_ylim(bottom=0)\n    ax.set_xlim(0, T)\n    \n    # Add histogram\n    final_values = gbm_path[:, -1]\n    hist_ax = ax.inset_axes([1.05, 0., 0.5, 1])\n    hist_ax.hist(final_values, bins=20, density=True, alpha=0.5, color='skyblue', orientation='horizontal')\n    \n    # Add theoretical lognormal PDF\n    shape, _, scale = sc.lognorm.fit(final_values, floc=0)\n    x = np.linspace(0, max(final_values), 1000)\n    pdf = sc.lognorm.pdf(x, shape, loc=0, scale=scale)\n    hist_ax.plot(pdf, x, 'r-', lw=2, label='Lognormal PDF')\n    \n    # Finalize histogram\n    hist_ax.set_title(f'E[$S_T$] = {np.mean(final_values):.2f}', pad=20)\n    hist_ax.axhline(np.mean(final_values), color='blue', linestyle='--', lw=1.5, label=r'$E[S_T]$')\n    hist_ax.set_yticks([])\n    hist_ax.set_xlabel('Density')\n    hist_ax.legend()\n    hist_ax.set_ylim(bottom=0)\n    \n    plt.tight_layout()  \n    plt.show();\n</pre>  def plot_gbm_paths_with_distribution(N, sampler, t_final, initial_value, drift, diffusion,n):     gbm = qp.GeometricBrownianMotion(sampler, t_final=t_final, initial_value=initial_value, drift=drift, diffusion=diffusion)     gbm_path = gbm.gen_samples(2**n)          fig, ax = plt.subplots(figsize=(14, 7))     T = max(gbm.time_vec)          # Plot GBM paths     ax.plot(gbm.time_vec, gbm_path.T, lw=0.75, alpha=0.7, color='skyblue')          # Set up main plot     ax.set_title(f'Geometric Brownian Motion Paths\\n{N} Simulations, T = {T}, $\\mu$ = {drift:.1f}, $\\sigma$ = {diffusion:.1f}, using {type(sampler).__name__} points')     ax.set_xlabel(r'$t$')     ax.set_ylabel(r'$S(t)$')     ax.set_ylim(bottom=0)     ax.set_xlim(0, T)          # Add histogram     final_values = gbm_path[:, -1]     hist_ax = ax.inset_axes([1.05, 0., 0.5, 1])     hist_ax.hist(final_values, bins=20, density=True, alpha=0.5, color='skyblue', orientation='horizontal')          # Add theoretical lognormal PDF     shape, _, scale = sc.lognorm.fit(final_values, floc=0)     x = np.linspace(0, max(final_values), 1000)     pdf = sc.lognorm.pdf(x, shape, loc=0, scale=scale)     hist_ax.plot(pdf, x, 'r-', lw=2, label='Lognormal PDF')          # Finalize histogram     hist_ax.set_title(f'E[$S_T$] = {np.mean(final_values):.2f}', pad=20)     hist_ax.axhline(np.mean(final_values), color='blue', linestyle='--', lw=1.5, label=r'$E[S_T]$')     hist_ax.set_yticks([])     hist_ax.set_xlabel('Density')     hist_ax.legend()     hist_ax.set_ylim(bottom=0)          plt.tight_layout()       plt.show(); In\u00a0[12]: Copied! <pre>eps = np.finfo(float).eps\nslider_style = {'handle_color': 'blue'}\n\n@widgets.interact\ndef f(n=widgets.IntSlider(min=0, max=8, step=1, value=7, style=slider_style),\n      t_final=widgets.FloatSlider(min=eps, max=10, step=0.1, value=5.0, style=slider_style),\n      initial_value=widgets.FloatSlider(min=eps, max=100, step=0.1, value=40, style=slider_style),\n      drift=widgets.FloatSlider(min=-2, max=2, step=0.1, value=0.1, style=slider_style),\n      diffusion=widgets.FloatSlider(min=eps, max=4, step=0.1, value=0.2, style=slider_style),\n      sampler=widgets.Dropdown(options=['IIDStdUniform', 'Lattice','Halton','Sobol'], value='IIDStdUniform', description='Sampler')\n):\n      # Create sampler instance\n      if sampler == 'IIDStdUniform':\n            sampler_instance = qp.IIDStdUniform(2**n, seed=7)\n      elif sampler == 'Lattice':\n            sampler_instance = qp.Lattice(2**n, seed=7)\n      elif sampler == 'Halton':\n            sampler_instance = qp.Halton(2**n, seed=7)\n      elif sampler == 'Sobol':\n            sampler_instance = qp.Sobol(2**n, seed=7)\n\n      # Call plotting function with error handling\n      plot_gbm_paths_with_distribution(2**n, sampler_instance, t_final=t_final, \n                                    initial_value=initial_value, drift=drift, \n                                    diffusion=diffusion, n=n)\n</pre> eps = np.finfo(float).eps slider_style = {'handle_color': 'blue'}  @widgets.interact def f(n=widgets.IntSlider(min=0, max=8, step=1, value=7, style=slider_style),       t_final=widgets.FloatSlider(min=eps, max=10, step=0.1, value=5.0, style=slider_style),       initial_value=widgets.FloatSlider(min=eps, max=100, step=0.1, value=40, style=slider_style),       drift=widgets.FloatSlider(min=-2, max=2, step=0.1, value=0.1, style=slider_style),       diffusion=widgets.FloatSlider(min=eps, max=4, step=0.1, value=0.2, style=slider_style),       sampler=widgets.Dropdown(options=['IIDStdUniform', 'Lattice','Halton','Sobol'], value='IIDStdUniform', description='Sampler') ):       # Create sampler instance       if sampler == 'IIDStdUniform':             sampler_instance = qp.IIDStdUniform(2**n, seed=7)       elif sampler == 'Lattice':             sampler_instance = qp.Lattice(2**n, seed=7)       elif sampler == 'Halton':             sampler_instance = qp.Halton(2**n, seed=7)       elif sampler == 'Sobol':             sampler_instance = qp.Sobol(2**n, seed=7)        # Call plotting function with error handling       plot_gbm_paths_with_distribution(2**n, sampler_instance, t_final=t_final,                                      initial_value=initial_value, drift=drift,                                      diffusion=diffusion, n=n) <pre>interactive(children=(IntSlider(value=7, description='n', max=8, style=SliderStyle(handle_color='blue')), Floa\u2026</pre> In\u00a0[13]: Copied! <pre>try:\n    import QuantLib as ql\nexcept ModuleNotFoundError:\n    !pip install -q QuantLib==1.38\n</pre> try:     import QuantLib as ql except ModuleNotFoundError:     !pip install -q QuantLib==1.38 In\u00a0[14]: Copied! <pre>import QuantLib as ql\nimport time\n\ndef generate_quantlib_paths(initial_value, mu, sigma, maturity, n_steps, n_paths):\n    \"\"\"Generate GBM paths using QuantLib\"\"\"\n    process = ql.GeometricBrownianMotionProcess(initial_value, mu, sigma)\n    rng = ql.GaussianRandomSequenceGenerator(ql.UniformRandomSequenceGenerator(n_steps, ql.UniformRandomGenerator()))\n    sequence_gen = ql.GaussianPathGenerator(process, maturity, n_steps, rng, False)\n    start_time = time.time()\n    paths = np.zeros((n_paths, n_steps + 1))\n    for i in range(n_paths):\n        sample_path = sequence_gen.next().value()\n        paths[i, :] = np.array([sample_path[j] for j in range(n_steps + 1)])\n    generation_time = time.time() - start_time\n    return paths, generation_time\n\ndef generate_qmcpy_paths(initial_value, mu, diffusion, maturity, n_steps, n_paths):\n    \"\"\"Generate GBM paths using QMCPy\"\"\"\n    sampler = qp.Sobol(n_steps, seed=42)\n    gbm = qp.GeometricBrownianMotion(sampler, t_final=maturity, initial_value=initial_value, drift=mu, diffusion=diffusion)\n    start_time = time.time()\n    paths = gbm.gen_samples(n_paths)\n    generation_time = time.time() - start_time\n    return paths, generation_time, gbm\n\ndef compute_theoretical_covariance(S0, mu, sigma, t1, t2):\n    \"\"\"Compute theoretical covariance matrix for GBM at two time points\"\"\"\n    return np.array([\n        [S0**2 * np.exp(2*mu*t1) * (np.exp(sigma**2 * t1) - 1), \n         S0**2 * np.exp(mu*(t1+t2)) * (np.exp(sigma**2 * t1) - 1)],\n        [S0**2 * np.exp(mu*(t1+t2)) * (np.exp(sigma**2 * t1) - 1), \n         S0**2 * np.exp(2*mu*t2) * (np.exp(sigma**2 * t2) - 1)]\n    ])\n\ndef extract_covariance_samples(paths, n_steps, is_quantlib=True):\n    \"\"\"Extract samples at two time points and compute covariance matrix\"\"\"\n    if is_quantlib:\n        idx1, idx2 = int(0.5 * n_steps), n_steps\n        samples_t1, samples_t2 = paths[:, idx1], paths[:, idx2]\n    else:  # QMCPy\n        idx1, idx2 = int(0.5 * (n_steps - 1)), n_steps - 1\n        samples_t1, samples_t2 = paths[:, idx1], paths[:, idx2]\n    return np.cov(np.vstack((samples_t1, samples_t2)))\n\n# Parameters for GBM comparison\nparams_ql = {'initial_value': 100, 'mu': 0.05, 'sigma': 0.2, 'maturity': 1.0, 'n_steps': 252, 'n_paths': 2**14}\nparams_qp = {'initial_value': 100, 'mu': 0.05, 'diffusion': 0.2**2, 'maturity': 1.0, 'n_steps': 252, 'n_paths': 2**14}\n\n# Generate paths for both libraries\nquantlib_paths, quantlib_time = generate_quantlib_paths(**params_ql)\nqmcpy_paths, qmcpy_time, qp_gbm = generate_qmcpy_paths(**params_qp)\n\n# Compute covariance matrices\nquantlib_cov = extract_covariance_samples(quantlib_paths, params_ql['n_steps'], is_quantlib=True)\nqmcpy_cov = extract_covariance_samples(qmcpy_paths, params_qp['n_steps'], is_quantlib=False)\ntheoretical_cov = compute_theoretical_covariance(params_ql['initial_value'], params_ql['mu'], params_ql['sigma'], 0.5, 1.0)\n\n# Final value statistics\nquantlib_final, qmcpy_final = quantlib_paths[:, -1], qmcpy_paths[:, -1]\ntheoretical_mean = params_ql['initial_value'] * np.exp(params_ql['mu'] * params_ql['maturity'])\ntheoretical_std = np.sqrt(params_ql['initial_value']**2 * np.exp(2*params_ql['mu']*params_ql['maturity']) * \n                         (np.exp(params_ql['sigma']**2 * params_ql['maturity']) - 1))\n\n# Display results\nprint(\"COMPARISON: QuantLib vs QMCPy GeometricBrownianMotion\")\nprint(\"=\"*55)\nprint(f\"\\nQuantLib sample covariance matrix:\\n{quantlib_cov}\")\nprint(f\"\\nQMCPy sample covariance matrix:\\n{qmcpy_cov}\")\nprint(f\"\\nTheoretical covariance matrix:\\n{theoretical_cov}\")\n\nprint(\"\\nFINAL VALUE STATISTICS (t=1 year)\")\nprint(\"-\"*35)\nfor name, final_vals in [(\"QuantLib\", quantlib_final), (\"QMCPy\", qmcpy_final)]:\n    print(f\"{name:&lt;12} Mean: {np.mean(final_vals):.2f}, Empirical Std: {np.std(final_vals, ddof=1):.2f}\")\nprint(f\"{'Theoretical':&lt;12} Mean: {theoretical_mean:.2f}, Theoretical Std: {theoretical_std:.2f}\")\n\nprint(\"\\nPERFORMANCE COMPARISON\")\nprint(\"-\"*25)\nprint(f\"QuantLib generation time: {quantlib_time:.3f} seconds\")\nprint(f\"QMCPy generation time:    {qmcpy_time:.3f} seconds\")\nspeedup = quantlib_time / qmcpy_time if quantlib_time &gt; qmcpy_time else qmcpy_time / quantlib_time\nfaster_lib = \"QMCPy\" if quantlib_time &gt; qmcpy_time else \"QuantLib\"\nprint(f\"{faster_lib} is {speedup:.1f}x faster\")\n\n# Store variables for visualization cell (extract individual values from params)\ncov_matrix, qmcpy_cov_matrix = quantlib_cov, qmcpy_cov\npaths, qmcpy_paths = quantlib_paths, qmcpy_paths\ninitial_value = params_ql['initial_value']\nmu = params_ql['mu']\nsigma = params_ql['sigma']\nmaturity = params_ql['maturity']\nn_steps = params_ql['n_steps']\n</pre> import QuantLib as ql import time  def generate_quantlib_paths(initial_value, mu, sigma, maturity, n_steps, n_paths):     \"\"\"Generate GBM paths using QuantLib\"\"\"     process = ql.GeometricBrownianMotionProcess(initial_value, mu, sigma)     rng = ql.GaussianRandomSequenceGenerator(ql.UniformRandomSequenceGenerator(n_steps, ql.UniformRandomGenerator()))     sequence_gen = ql.GaussianPathGenerator(process, maturity, n_steps, rng, False)     start_time = time.time()     paths = np.zeros((n_paths, n_steps + 1))     for i in range(n_paths):         sample_path = sequence_gen.next().value()         paths[i, :] = np.array([sample_path[j] for j in range(n_steps + 1)])     generation_time = time.time() - start_time     return paths, generation_time  def generate_qmcpy_paths(initial_value, mu, diffusion, maturity, n_steps, n_paths):     \"\"\"Generate GBM paths using QMCPy\"\"\"     sampler = qp.Sobol(n_steps, seed=42)     gbm = qp.GeometricBrownianMotion(sampler, t_final=maturity, initial_value=initial_value, drift=mu, diffusion=diffusion)     start_time = time.time()     paths = gbm.gen_samples(n_paths)     generation_time = time.time() - start_time     return paths, generation_time, gbm  def compute_theoretical_covariance(S0, mu, sigma, t1, t2):     \"\"\"Compute theoretical covariance matrix for GBM at two time points\"\"\"     return np.array([         [S0**2 * np.exp(2*mu*t1) * (np.exp(sigma**2 * t1) - 1),           S0**2 * np.exp(mu*(t1+t2)) * (np.exp(sigma**2 * t1) - 1)],         [S0**2 * np.exp(mu*(t1+t2)) * (np.exp(sigma**2 * t1) - 1),           S0**2 * np.exp(2*mu*t2) * (np.exp(sigma**2 * t2) - 1)]     ])  def extract_covariance_samples(paths, n_steps, is_quantlib=True):     \"\"\"Extract samples at two time points and compute covariance matrix\"\"\"     if is_quantlib:         idx1, idx2 = int(0.5 * n_steps), n_steps         samples_t1, samples_t2 = paths[:, idx1], paths[:, idx2]     else:  # QMCPy         idx1, idx2 = int(0.5 * (n_steps - 1)), n_steps - 1         samples_t1, samples_t2 = paths[:, idx1], paths[:, idx2]     return np.cov(np.vstack((samples_t1, samples_t2)))  # Parameters for GBM comparison params_ql = {'initial_value': 100, 'mu': 0.05, 'sigma': 0.2, 'maturity': 1.0, 'n_steps': 252, 'n_paths': 2**14} params_qp = {'initial_value': 100, 'mu': 0.05, 'diffusion': 0.2**2, 'maturity': 1.0, 'n_steps': 252, 'n_paths': 2**14}  # Generate paths for both libraries quantlib_paths, quantlib_time = generate_quantlib_paths(**params_ql) qmcpy_paths, qmcpy_time, qp_gbm = generate_qmcpy_paths(**params_qp)  # Compute covariance matrices quantlib_cov = extract_covariance_samples(quantlib_paths, params_ql['n_steps'], is_quantlib=True) qmcpy_cov = extract_covariance_samples(qmcpy_paths, params_qp['n_steps'], is_quantlib=False) theoretical_cov = compute_theoretical_covariance(params_ql['initial_value'], params_ql['mu'], params_ql['sigma'], 0.5, 1.0)  # Final value statistics quantlib_final, qmcpy_final = quantlib_paths[:, -1], qmcpy_paths[:, -1] theoretical_mean = params_ql['initial_value'] * np.exp(params_ql['mu'] * params_ql['maturity']) theoretical_std = np.sqrt(params_ql['initial_value']**2 * np.exp(2*params_ql['mu']*params_ql['maturity']) *                           (np.exp(params_ql['sigma']**2 * params_ql['maturity']) - 1))  # Display results print(\"COMPARISON: QuantLib vs QMCPy GeometricBrownianMotion\") print(\"=\"*55) print(f\"\\nQuantLib sample covariance matrix:\\n{quantlib_cov}\") print(f\"\\nQMCPy sample covariance matrix:\\n{qmcpy_cov}\") print(f\"\\nTheoretical covariance matrix:\\n{theoretical_cov}\")  print(\"\\nFINAL VALUE STATISTICS (t=1 year)\") print(\"-\"*35) for name, final_vals in [(\"QuantLib\", quantlib_final), (\"QMCPy\", qmcpy_final)]:     print(f\"{name:&lt;12} Mean: {np.mean(final_vals):.2f}, Empirical Std: {np.std(final_vals, ddof=1):.2f}\") print(f\"{'Theoretical':&lt;12} Mean: {theoretical_mean:.2f}, Theoretical Std: {theoretical_std:.2f}\")  print(\"\\nPERFORMANCE COMPARISON\") print(\"-\"*25) print(f\"QuantLib generation time: {quantlib_time:.3f} seconds\") print(f\"QMCPy generation time:    {qmcpy_time:.3f} seconds\") speedup = quantlib_time / qmcpy_time if quantlib_time &gt; qmcpy_time else qmcpy_time / quantlib_time faster_lib = \"QMCPy\" if quantlib_time &gt; qmcpy_time else \"QuantLib\" print(f\"{faster_lib} is {speedup:.1f}x faster\")  # Store variables for visualization cell (extract individual values from params) cov_matrix, qmcpy_cov_matrix = quantlib_cov, qmcpy_cov paths, qmcpy_paths = quantlib_paths, qmcpy_paths initial_value = params_ql['initial_value'] mu = params_ql['mu'] sigma = params_ql['sigma'] maturity = params_ql['maturity'] n_steps = params_ql['n_steps'] <pre>COMPARISON: QuantLib vs QMCPy GeometricBrownianMotion\n=======================================================\n\nQuantLib sample covariance matrix:\n[[214.48159054 220.5858228 ]\n [220.5858228  452.69883602]]\n\nQMCPy sample covariance matrix:\n[[212.42271836 217.75632634]\n [217.75632634 450.8703067 ]]\n\nTheoretical covariance matrix:\n[[212.37084878 217.74704241]\n [217.74704241 451.02880782]]\n\nFINAL VALUE STATISTICS (t=1 year)\n-----------------------------------\nQuantLib     Mean: 105.12, Empirical Std: 21.28\nQMCPy        Mean: 105.13, Empirical Std: 21.23\nTheoretical  Mean: 105.13, Theoretical Std: 21.24\n\nPERFORMANCE COMPARISON\n-------------------------\nQuantLib generation time: 0.774 seconds\nQMCPy generation time:    0.454 seconds\nQMCPy is 1.7x faster\n</pre> In\u00a0[15]: Copied! <pre># Visualization \ndef plot_paths_on_axis(ax, time_vec, paths_data, title, color, n_plot=50):\n    \"\"\"Helper function to plot GBM paths on a given axis\"\"\"\n    for i in range(min(n_plot, paths_data.shape[0])):\n        ax.plot(time_vec, paths_data[i, :], alpha=0.2 + 0.3 * (i / n_plot), color=color, linewidth=0.5)\n    ax.set_title(title, fontsize=12, fontweight='bold')\n    ax.set_xlabel('Time')\n    ax.set_ylabel('Stock Price')\n    ax.grid(True, alpha=0.3)\n\n# Create comparison visualization with 2x2 subplots\nfig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(11, 8))\nplot_data = [ (ax1, np.linspace(0, maturity, n_steps + 1), paths, 'QuantLib GBM Paths (Sample)', 'blue'),\n              (ax2, np.linspace(maturity/n_steps, maturity, n_steps), qmcpy_paths, 'QMCPy GBM Paths (Sample)', 'red') ]\nfor ax, time_grid, data, title, color in plot_data:\n    plot_paths_on_axis(ax, time_grid, data, title, color)\n\n# Final value distributions\nfor final_vals, color, label in [(quantlib_final, 'blue', 'QuantLib'), (qmcpy_final, 'red', 'QMCPy')]:\n    ax3.hist(final_vals, bins=50, alpha=0.6, color=color, label=label, density=True)\nax3.set_title('Final Value Distributions (t=1)', fontsize=11, fontweight='bold')\nax3.set_xlabel('Stock Price')\nax3.set_ylabel('Density')\nax3.legend()\nax3.grid(True, alpha=0.3)\n\n# QMCPy covariance matrix heatmap\nim = ax4.imshow(qp_gbm.covariance_gbm, cmap='viridis', aspect='equal') # origin='lower'\nax4.set_title('QMCPy Covariance Matrix Heatmap', fontsize=11, fontweight='bold')\nax4.set_xlabel('Time')\nax4.set_ylabel('Time')\n\n# Set custom tick labels using time_vec\ntime_ticks = np.arange(0, len(qp_gbm.time_vec), max(1, len(qp_gbm.time_vec)//5))  # Show ~5 ticks\nax4.set_xticks(time_ticks)\nax4.set_yticks(time_ticks)\nax4.set_xticklabels([f'{qp_gbm.time_vec[i]:.1f}' for i in time_ticks])\nax4.set_yticklabels([f'{qp_gbm.time_vec[i]:.1f}' for i in time_ticks])\ncbar = plt.colorbar(im, ax=ax4, shrink=0.8)   # Add colorbar\ncbar.set_label('Covariance Value', rotation=270, labelpad=20)\n\nplt.tight_layout()\nplt.show();\n</pre> # Visualization  def plot_paths_on_axis(ax, time_vec, paths_data, title, color, n_plot=50):     \"\"\"Helper function to plot GBM paths on a given axis\"\"\"     for i in range(min(n_plot, paths_data.shape[0])):         ax.plot(time_vec, paths_data[i, :], alpha=0.2 + 0.3 * (i / n_plot), color=color, linewidth=0.5)     ax.set_title(title, fontsize=12, fontweight='bold')     ax.set_xlabel('Time')     ax.set_ylabel('Stock Price')     ax.grid(True, alpha=0.3)  # Create comparison visualization with 2x2 subplots fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(11, 8)) plot_data = [ (ax1, np.linspace(0, maturity, n_steps + 1), paths, 'QuantLib GBM Paths (Sample)', 'blue'),               (ax2, np.linspace(maturity/n_steps, maturity, n_steps), qmcpy_paths, 'QMCPy GBM Paths (Sample)', 'red') ] for ax, time_grid, data, title, color in plot_data:     plot_paths_on_axis(ax, time_grid, data, title, color)  # Final value distributions for final_vals, color, label in [(quantlib_final, 'blue', 'QuantLib'), (qmcpy_final, 'red', 'QMCPy')]:     ax3.hist(final_vals, bins=50, alpha=0.6, color=color, label=label, density=True) ax3.set_title('Final Value Distributions (t=1)', fontsize=11, fontweight='bold') ax3.set_xlabel('Stock Price') ax3.set_ylabel('Density') ax3.legend() ax3.grid(True, alpha=0.3)  # QMCPy covariance matrix heatmap im = ax4.imshow(qp_gbm.covariance_gbm, cmap='viridis', aspect='equal') # origin='lower' ax4.set_title('QMCPy Covariance Matrix Heatmap', fontsize=11, fontweight='bold') ax4.set_xlabel('Time') ax4.set_ylabel('Time')  # Set custom tick labels using time_vec time_ticks = np.arange(0, len(qp_gbm.time_vec), max(1, len(qp_gbm.time_vec)//5))  # Show ~5 ticks ax4.set_xticks(time_ticks) ax4.set_yticks(time_ticks) ax4.set_xticklabels([f'{qp_gbm.time_vec[i]:.1f}' for i in time_ticks]) ax4.set_yticklabels([f'{qp_gbm.time_vec[i]:.1f}' for i in time_ticks]) cbar = plt.colorbar(im, ax=ax4, shrink=0.8)   # Add colorbar cbar.set_label('Covariance Value', rotation=270, labelpad=20)  plt.tight_layout() plt.show();  <p>QMCPy vs QuantLib: Both libraries produce statistically equivalent GBM simulations that match theoretical values. QMCPy typically runs 2-4 times faster due to vectorized operations, making it excellent for research and high-performance applications. QuantLib remains the industry standard for production systems requiring comprehensive derivatives support.</p> In\u00a0[16]: Copied! <pre># If you need different samplers, create efficiently\nbase_params = {'t_final': 1, 'initial_value': 100, 'drift': 0.05, 'diffusion': 0.2}\n\n# Create different instances (still user-facing API)\nsobol_gbm = qp.GeometricBrownianMotion(qp.Sobol(252), **base_params)\nlattice_gbm = qp.GeometricBrownianMotion(qp.Lattice(252), **base_params)\nhalton_gbm = qp.GeometricBrownianMotion(qp.Halton(252), **base_params)\n</pre> # If you need different samplers, create efficiently base_params = {'t_final': 1, 'initial_value': 100, 'drift': 0.05, 'diffusion': 0.2}  # Create different instances (still user-facing API) sobol_gbm = qp.GeometricBrownianMotion(qp.Sobol(252), **base_params) lattice_gbm = qp.GeometricBrownianMotion(qp.Lattice(252), **base_params) halton_gbm = qp.GeometricBrownianMotion(qp.Halton(252), **base_params)"},{"location":"demos/gbm_demo/#geometric-brownian-motion-demo","title":"Geometric Brownian Motion Demo\u00b6","text":""},{"location":"demos/gbm_demo/#gbm-object-in-qmcpy","title":"GBM object in QMCPy\u00b6","text":""},{"location":"demos/gbm_demo/#log-normality-property","title":"Log-Normality Property\u00b6","text":"<p>At any time $t &gt; 0$, $S_t$ follows a log-normal distribution with expected value and variance as follows (see Section 3.2 in $[1]$):</p> <ul> <li>$E[S_t] = S_0 e^{\\mu t}$</li> <li>$\\text{Var}[S_t] = S_0^2 e^{2\\mu t}(e^{\\sigma^2 t} - 1)$</li> </ul> <p>Let's validate these theoretical properties by generating a large number of GBM samples and comparing the empirical moments with the theoretical values. Note that the theoretical values match the last values in <code>qp_gbm.mean_gbm</code> and <code>qp_gbm.covariance_gbm</code> for the final time point.</p>"},{"location":"demos/gbm_demo/#gmb-vs-brownian-motion","title":"GMB vs Brownian Motion\u00b6","text":""},{"location":"demos/gbm_demo/#gbm-using-low-discrepancy-lattice-sequence-distrubtion","title":"GBM Using Low-Discrepancy Lattice Sequence Distrubtion\u00b6","text":""},{"location":"demos/gbm_demo/#interactive-visualization","title":"Interactive Visualization\u00b6","text":"<p>The following code defines a set of sliders to control parameters for simulating paths of GBM. It sets the machine epsilon (eps) as the minimum value for <code>initial_value</code>,  <code>t_final</code>, and <code>diffusion</code>, ensuring they are always positive.  The function <code>plot_gbm_paths_with_distribution</code> then visualizes the GBM paths based on the specified parameters in the left subplot and fits a lognormal distribution to the histogram of the data values at the final time point in the right subplot.</p>"},{"location":"demos/gbm_demo/#quantlib-vs-qmcpy-comparison","title":"QuantLib vs QMCPy Comparison\u00b6","text":"<p>In this section, we compare QMCPy's GeometricBrownianMotion implementation with the industry-standard QuantLib library [6] to validate its accuracy and performance.</p>"},{"location":"demos/gbm_demo/#references","title":"References\u00b6","text":"<ul> <li>$[1]$ Paul Glasserman, P. (2003) Monte Carlo Methods in Financial Engineering. Springer, 2nd edition</li> <li>$[2]$ Choi, Sou-Cheng T. and Hickernell, Fred J. and Jagadeeswaran, Rathinavel and McCourt, Michael J. and Sorokin, Aleksei G. (2022) Quasi-Monte Carlo Software, In Alexander Keller, editor, Monte Carlo and Quasi-Monte Carlo Methods Springer International Publishing</li> <li>$[3]$ S.-C. T. Choi and F. J. Hickernell and R. Jagadeeswaran and M. McCourt and A. Sorokin. (2023) QMCPy: A quasi-Monte Carlo Python Library (versions 1--1.6.2)</li> <li>$[4]$ Hull, J. C. (2017) Options, Futures, and Other Derivatives. Pearson, 10th edition</li> <li>$[5]$ Sheldon. Ross. (2014) Introduction to Probability Models. Academic Press, 11th edition</li> <li>$[6]$ QuantLib Development Team. QuantLib: A free/open-source library for quantitative finance, Version 1.38, https://www.quantlib.org</li> </ul>"},{"location":"demos/gbm_demo/#appendix-covariance-matrix-derivation-for-geometric-brownian-motion","title":"Appendix: Covariance Matrix Derivation for Geometric Brownian Motion\u00b6","text":"<p>Here we derive the covariance matrix of $(S(t_1), \\ldots , S(t_n))$ for one-dimensional Geometric Brownian Motion defined as $S(t) = S_0 \\, e^{\\big(\\mu - \\frac{\\sigma^2}{2}\\big)  t + \\sigma W(t)}$, where $W(t)$ is a standard one-dimensional Brownian motion.</p> <ol> <li><p>Recall the definition of covariance: $$\\text{Cov}(S(t_i), S(t_j)) = E[S(t_i)S(t_j)] - E[S(t_i)]E[S(t_j)].$$</p> </li> <li><p>Calculate the product of expectations: The expected value of $S(t)$ is $E[S(t)] = S_0 e^{\\mu t}$. Therefore, the product of the expectations is: $$E[S(t_i)]E[S(t_j)] = (S_0 e^{\\mu t_i})(S_0 e^{\\mu t_j}) = S_0^2 e^{\\mu(t_i + t_j)}$$</p> </li> <li><p>Calculate the expectation of the product, $E[S(t_i)S(t_j)]$: $$\\begin{aligned} S(t_i)S(t_j) &amp;= S_0 e^{(\\mu - \\frac{\\sigma^2}{2})t_i + \\sigma W(t_i)} \\cdot S_0 e^{(\\mu - \\frac{\\sigma^2}{2})t_j + \\sigma W(t_j)} \\\\ &amp;= S_0^2 \\exp\\left( (\\mu - \\frac{\\sigma^2}{2})(t_i + t_j) + \\sigma \\left(W(t_i) + W(t_j) \\right) \\right) \\end{aligned}$$ The exponent is a normal random variable. Let's call it $Y$: $$Y = (\\mu - \\frac{\\sigma^2}{2})(t_i + t_j) + \\sigma\\left(W(t_i) + W(t_j)\\right)$$ To find the expectation of $e^Y$, we use the property that if $Y \\sim N(\\text{mean}, \\text{variance})$, then $E[e^Y] = e^{\\text{mean} + \\frac{1}{2}\\text{variance}}$.</p> <ul> <li><p>The mean of $Y$ is $E[Y] = E[(\\mu - \\frac{\\sigma^2}{2})(t_i + t_j) + \\sigma(W(t_i) + W(t_j))] = (\\mu - \\frac{\\sigma^2}{2})(t_i + t_j).$</p> </li> <li><p>The variance of $Y$ is $$\\begin{aligned}   \\text{Var}(Y) &amp;= \\text{Var}[(\\mu - \\frac{\\sigma^2}{2})(t_i + t_j) + \\sigma(W(t_i) + W(t_j))] \\\\   &amp;= \\text{Var}[\\sigma(W(t_i) + W(t_j))] \\\\   &amp;= \\sigma^2 \\text{Var}(W(t_i) + W(t_j)) \\\\   &amp;= \\sigma^2(\\text{Var}(W(t_i)) + \\text{Var}(W(t_j)) + 2\\text{Cov}(W(t_i), W(t_j))) \\\\   &amp;= \\sigma^2(t_i + t_j + 2\\min(t_i, t_j))    \\end{aligned}$$ Now we can compute $E[S(t_i)S(t_j)] = S_0^2 E[e^Y]$: $$\\begin{aligned} E[S(t_i)S(t_j)] &amp;= S_0^2 \\exp\\left( E[Y] + \\frac{1}{2}\\text{Var}(Y) \\right) \\\\ &amp;= S_0^2 \\exp\\left( (\\mu - \\frac{\\sigma^2}{2})(t_i + t_j) + \\frac{1}{2}\\sigma^2 \\left(t_i + t_j + 2\\min(t_i, t_j)\\right) \\right) \\end{aligned}$$ Simplifying the exponent: $$\\begin{aligned} &amp;\\mu(t_i + t_j) - \\frac{\\sigma^2}{2}(t_i+t_j) + \\frac{\\sigma^2}{2}(t_i+t_j) + \\sigma^2\\min(t_i,t_j) \\\\ &amp;= \\mu(t_i + t_j) + \\sigma^2\\min(t_i,t_j) \\end{aligned}$$ So, the final expression for the expectation of the product is: $$E[S(t_i)S(t_j)] = S_0^2 e^{\\mu(t_i + t_j) + \\sigma^2\\min(t_i, t_j)}$$</p> </li> </ul> </li> <li><p>Combine the terms to get the covariance: $$\\begin{aligned} \\text{Cov}(S(t_i), S(t_j)) &amp;= S_0^2 e^{\\mu(t_i + t_j) + \\sigma^2\\min(t_i, t_j)} - S_0^2 e^{\\mu(t_i + t_j)} \\\\ &amp;= S_0^2 e^{\\mu(t_i + t_j)} \\left(e^{\\sigma^2 \\min(t_i, t_j)} - 1\\right). \\end{aligned}$$</p> </li> </ol>"},{"location":"demos/iris/","title":"Sensitivity Analysis for the Iris Dataset","text":"In\u00a0[2]: Copied! <pre>from numpy import *\nfrom qmcpy import *\nimport pandas as pd\nfrom sklearn.datasets import load_iris\nfrom sklearn.tree import DecisionTreeClassifier,plot_tree\nfrom sklearn.model_selection import train_test_split\nfrom skopt import gp_minimize\nfrom matplotlib import pyplot\n</pre> from numpy import * from qmcpy import * import pandas as pd from sklearn.datasets import load_iris from sklearn.tree import DecisionTreeClassifier,plot_tree from sklearn.model_selection import train_test_split from skopt import gp_minimize from matplotlib import pyplot In\u00a0[3]: Copied! <pre>data = load_iris()\nprint(data['DESCR'])\n</pre> data = load_iris() print(data['DESCR']) <pre>.. _iris_dataset:\n\nIris plants dataset\n--------------------\n\n**Data Set Characteristics:**\n\n    :Number of Instances: 150 (50 in each of three classes)\n    :Number of Attributes: 4 numeric, predictive attributes and the class\n    :Attribute Information:\n        - sepal length in cm\n        - sepal width in cm\n        - petal length in cm\n        - petal width in cm\n        - class:\n                - Iris-Setosa\n                - Iris-Versicolour\n                - Iris-Virginica\n                \n    :Summary Statistics:\n\n    ============== ==== ==== ======= ===== ====================\n                    Min  Max   Mean    SD   Class Correlation\n    ============== ==== ==== ======= ===== ====================\n    sepal length:   4.3  7.9   5.84   0.83    0.7826\n    sepal width:    2.0  4.4   3.05   0.43   -0.4194\n    petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\n    petal width:    0.1  2.5   1.20   0.76    0.9565  (high!)\n    ============== ==== ==== ======= ===== ====================\n\n    :Missing Attribute Values: None\n    :Class Distribution: 33.3% for each of 3 classes.\n    :Creator: R.A. Fisher\n    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\n    :Date: July, 1988\n\nThe famous Iris database, first used by Sir R.A. Fisher. The dataset is taken\nfrom Fisher's paper. Note that it's the same as in R, but not as in the UCI\nMachine Learning Repository, which has two wrong data points.\n\nThis is perhaps the best known database to be found in the\npattern recognition literature.  Fisher's paper is a classic in the field and\nis referenced frequently to this day.  (See Duda &amp; Hart, for example.)  The\ndata set contains 3 classes of 50 instances each, where each class refers to a\ntype of iris plant.  One class is linearly separable from the other 2; the\nlatter are NOT linearly separable from each other.\n\n|details-start|\n**References**\n|details-split|\n\n- Fisher, R.A. \"The use of multiple measurements in taxonomic problems\"\n  Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\n  Mathematical Statistics\" (John Wiley, NY, 1950).\n- Duda, R.O., &amp; Hart, P.E. (1973) Pattern Classification and Scene Analysis.\n  (Q327.D83) John Wiley &amp; Sons.  ISBN 0-471-22361-1.  See page 218.\n- Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\n  Structure and Classification Rule for Recognition in Partially Exposed\n  Environments\".  IEEE Transactions on Pattern Analysis and Machine\n  Intelligence, Vol. PAMI-2, No. 1, 67-71.\n- Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\n  on Information Theory, May 1972, 431-433.\n- See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\n  conceptual clustering system finds 3 classes in the data.\n- Many, many more ...\n\n|details-end|\n</pre> In\u00a0[4]: Copied! <pre>x = data['data']\ny = data['target']\nfeature_names = data['feature_names']\ndf = pd.DataFrame(hstack((x,y[:,None])),columns=feature_names+['iris type'])\nprint('df shape:',df.shape)\ntarget_names = data['target_names']\niris_type_map = {i:target_names[i] for i in range(len(target_names))}\nprint('iris species map:',iris_type_map)\ndf.head()\n</pre> x = data['data'] y = data['target'] feature_names = data['feature_names'] df = pd.DataFrame(hstack((x,y[:,None])),columns=feature_names+['iris type']) print('df shape:',df.shape) target_names = data['target_names'] iris_type_map = {i:target_names[i] for i in range(len(target_names))} print('iris species map:',iris_type_map) df.head() <pre>df shape: (150, 5)\niris species map: {0: 'setosa', 1: 'versicolor', 2: 'virginica'}\n</pre> Out[4]: sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) iris type 0 5.1 3.5 1.4 0.2 0.0 1 4.9 3.0 1.4 0.2 0.0 2 4.7 3.2 1.3 0.2 0.0 3 4.6 3.1 1.5 0.2 0.0 4 5.0 3.6 1.4 0.2 0.0 In\u00a0[5]: Copied! <pre>xt,xv,yt,yv = train_test_split(x,y,test_size=1/3,random_state=7)\nprint('training data   (xt) shape: %s'%str(xt.shape))\nprint('training labels (yt) shape: %s'%str(yt.shape))\nprint('testing data    (xv) shape: %s'%str(xv.shape))\nprint('testing labels  (yv) shape: %s'%str(yv.shape))\n</pre> xt,xv,yt,yv = train_test_split(x,y,test_size=1/3,random_state=7) print('training data   (xt) shape: %s'%str(xt.shape)) print('training labels (yt) shape: %s'%str(yt.shape)) print('testing data    (xv) shape: %s'%str(xv.shape)) print('testing labels  (yv) shape: %s'%str(yv.shape)) <pre>training data   (xt) shape: (100, 4)\ntraining labels (yt) shape: (100,)\ntesting data    (xv) shape: (50, 4)\ntesting labels  (yv) shape: (50,)\n</pre> In\u00a0[6]: Copied! <pre>hp_domain = [\n    {'name':'max_depth', 'bounds':[1,8]},\n    {'name':'min_weight_fraction_leaf', 'bounds':[0,.5]}]\nhpnames = [param['name'] for param in hp_domain]\nhp_lb = array([param['bounds'][0] for param in hp_domain])\nhp_ub = array([param['bounds'][1] for param in hp_domain])\nd = len(hp_domain)\ndef get_dt_accuracy(hparams):\n    accuracies = zeros(len(hparams))\n    for i,hparam in enumerate(hparams):\n        kwargs = {hp_domain[j]['name']:hparam[j] for j in range(d)}\n        kwargs['max_depth'] = int(kwargs['max_depth'])\n        dt = DecisionTreeClassifier(random_state=7,**kwargs).fit(xt,yt)\n        yhat = dt.predict(xv)\n        accuracies[i] = mean(yhat==yv)\n    return accuracies\ncf = CustomFun(\n    true_measure = Uniform(DigitalNetB2(d,seed=7),lower_bound=hp_lb,upper_bound=hp_ub),\n    g = get_dt_accuracy,\n    parallel=False)\n</pre> hp_domain = [     {'name':'max_depth', 'bounds':[1,8]},     {'name':'min_weight_fraction_leaf', 'bounds':[0,.5]}] hpnames = [param['name'] for param in hp_domain] hp_lb = array([param['bounds'][0] for param in hp_domain]) hp_ub = array([param['bounds'][1] for param in hp_domain]) d = len(hp_domain) def get_dt_accuracy(hparams):     accuracies = zeros(len(hparams))     for i,hparam in enumerate(hparams):         kwargs = {hp_domain[j]['name']:hparam[j] for j in range(d)}         kwargs['max_depth'] = int(kwargs['max_depth'])         dt = DecisionTreeClassifier(random_state=7,**kwargs).fit(xt,yt)         yhat = dt.predict(xv)         accuracies[i] = mean(yhat==yv)     return accuracies cf = CustomFun(     true_measure = Uniform(DigitalNetB2(d,seed=7),lower_bound=hp_lb,upper_bound=hp_ub),     g = get_dt_accuracy,     parallel=False) In\u00a0[7]: Copied! <pre>avg_accuracy,data_avg_accuracy = CubQMCNetG(cf,abs_tol=1e-4).integrate()\ndata_avg_accuracy\n</pre> avg_accuracy,data_avg_accuracy = CubQMCNetG(cf,abs_tol=1e-4).integrate() data_avg_accuracy Out[7]: <pre>Data (Data)\n    solution        0.787\n    comb_bound_low  0.787\n    comb_bound_high 0.787\n    comb_bound_diff 1.63e-04\n    comb_flags      1\n    n_total         2^(14)\n    n               2^(14)\n    time_integrate  13.243\nCubQMCNetG (AbstractStoppingCriterion)\n    abs_tol         1.00e-04\n    rel_tol         0\n    n_init          2^(10)\n    n_limit         2^(35)\nCustomFun (AbstractIntegrand)\nUniform (AbstractTrueMeasure)\n    lower_bound     [1 0]\n    upper_bound     [8.  0.5]\nDigitalNetB2 (AbstractLDDiscreteDistribution)\n    d               2^(1)\n    replications    1\n    randomize       LMS_DS\n    gen_mats_source joe_kuo.6.21201.txt\n    order           NATURAL\n    t               63\n    alpha           1\n    n_limit         2^(32)\n    entropy         7</pre> <p>Here we find the average accuracy to be 78.7% using $2^{14}$ samples.</p> In\u00a0[8]: Copied! <pre>si = SensitivityIndices(cf)\nsolution_importance,data_importance = CubQMCNetG(si,abs_tol=2.5e-2).integrate()\ndata_importance\n</pre> si = SensitivityIndices(cf) solution_importance,data_importance = CubQMCNetG(si,abs_tol=2.5e-2).integrate() data_importance Out[8]: <pre>Data (Data)\n    solution        [[0.159 0.748]\n                     [0.253 0.837]]\n    comb_bound_low  [[0.144 0.726]\n                     [0.235 0.818]]\n    comb_bound_high [[0.174 0.769]\n                     [0.272 0.855]]\n    comb_bound_diff [[0.03  0.042]\n                     [0.037 0.037]]\n    comb_flags      [[ True  True]\n                     [ True  True]]\n    n_total         2^(13)\n    n               [[[4096 8192]\n                      [4096 8192]\n                      [4096 8192]]\n                    \n                     [[2048 8192]\n                      [2048 8192]\n                      [2048 8192]]]\n    time_integrate  23.404\nCubQMCNetG (AbstractStoppingCriterion)\n    abs_tol         0.025\n    rel_tol         0\n    n_init          2^(10)\n    n_limit         2^(35)\nSensitivityIndices (AbstractIntegrand)\n    indices         [[ True False]\n                     [False  True]]\nUniform (AbstractTrueMeasure)\n    lower_bound     [1 0]\n    upper_bound     [8.  0.5]\nDigitalNetB2 (AbstractLDDiscreteDistribution)\n    d               2^(1)\n    replications    1\n    randomize       LMS_DS\n    gen_mats_source joe_kuo.6.21201.txt\n    order           NATURAL\n    t               63\n    alpha           1\n    n_limit         2^(32)\n    entropy         7</pre> In\u00a0[9]: Copied! <pre>print('closed sensitivity indices: %s'%str(solution_importance[0].squeeze()))\nprint('total sensitivity indices: %s'%str(solution_importance[1].squeeze()))\n</pre> print('closed sensitivity indices: %s'%str(solution_importance[0].squeeze())) print('total sensitivity indices: %s'%str(solution_importance[1].squeeze())) <pre>closed sensitivity indices: [0.15864786 0.74759984]\ntotal sensitivity indices: [0.25307821 0.83668445]\n</pre> <p>Looking closer at the output, we see that the second hyperparameter (<code>min_weight_fraction_leaf</code>) is more important than the first one (<code>max_depth</code>). The closed sensitivity indices measure how much that hyperparameter contributes to testing accuracy variance. The total sensitivity indices measure how much that hyperparameter, or any subset of hyperparameters containing that one contributes to testing accuracy variance. For example, the first closed sensitivity index approximates the variability attributable to {<code>max_depth</code>} while the first total sensitivity index approximates the variability attributable to both {<code>max_depth</code>} and {<code>max_depth</code>,<code>min_weight_fraction_leaf</code>}.</p> In\u00a0[12]: Copied! <pre>def marginal(x,compute_flags,xpts,bools,not_bools):\n    n,_ = x.shape\n    x2 = zeros((n,d),dtype=float)\n    x2[:,bools] = x\n    y = zeros((n,len(xpts)),dtype=float)\n    for k,xpt in enumerate(xpts):\n        if not compute_flags[k]: continue\n        x2[:,not_bools] = xpt\n        y[:,k] = get_dt_accuracy(x2)\n    return y.T\n</pre> def marginal(x,compute_flags,xpts,bools,not_bools):     n,_ = x.shape     x2 = zeros((n,d),dtype=float)     x2[:,bools] = x     y = zeros((n,len(xpts)),dtype=float)     for k,xpt in enumerate(xpts):         if not compute_flags[k]: continue         x2[:,not_bools] = xpt         y[:,k] = get_dt_accuracy(x2)     return y.T In\u00a0[13]: Copied! <pre>fig,ax = pyplot.subplots(nrows=1,ncols=2,figsize=(8,4))\nnticks = 32\nxpts01 = linspace(0,1,nticks)\nfor i in range(2):\n    xpts = xpts01*(hp_ub[i]-hp_lb[i])+hp_lb[i]\n    bools = array([True if j not in [i] else False for j in range(d)])\n    def marginal_i(x,compute_flags): return marginal(x,compute_flags,xpts,bools,~bools)\n    cf = CustomFun(\n         true_measure = Uniform(DigitalNetB2(1,seed=7),lower_bound=hp_lb[bools],upper_bound=hp_ub[bools]),\n         g = marginal_i,\n         dimension_indv = len(xpts),\n         parallel=False)\n    sol,data = CubQMCNetG(cf,abs_tol=5e-2).integrate()\n    ax[i].plot(xpts,sol,'-o',color='m')\n    ax[i].fill_between(xpts,data.comb_bound_high,data.comb_bound_low,color='c',alpha=.5)\n    ax[i].set_xlabel(hpnames[i].replace('_',' '))\n    ax[i].set_ylabel('accuracy')\n</pre> fig,ax = pyplot.subplots(nrows=1,ncols=2,figsize=(8,4)) nticks = 32 xpts01 = linspace(0,1,nticks) for i in range(2):     xpts = xpts01*(hp_ub[i]-hp_lb[i])+hp_lb[i]     bools = array([True if j not in [i] else False for j in range(d)])     def marginal_i(x,compute_flags): return marginal(x,compute_flags,xpts,bools,~bools)     cf = CustomFun(          true_measure = Uniform(DigitalNetB2(1,seed=7),lower_bound=hp_lb[bools],upper_bound=hp_ub[bools]),          g = marginal_i,          dimension_indv = len(xpts),          parallel=False)     sol,data = CubQMCNetG(cf,abs_tol=5e-2).integrate()     ax[i].plot(xpts,sol,'-o',color='m')     ax[i].fill_between(xpts,data.comb_bound_high,data.comb_bound_low,color='c',alpha=.5)     ax[i].set_xlabel(hpnames[i].replace('_',' '))     ax[i].set_ylabel('accuracy') In\u00a0[14]: Copied! <pre>x0 = data_avg_accuracy.xfull*(hp_ub-hp_lb)+hp_lb\ny0 = -data_avg_accuracy.yfull.squeeze()\nprint('best result before BO is %d%% accuracy'%(-100*y0.min()))\nresult = gp_minimize(\n    func = lambda hparams: get_dt_accuracy(atleast_2d(hparams)).squeeze().item(),\n    dimensions = [(l,u) for l,u in zip(hp_lb,hp_ub)],\n    n_calls = 32,\n    n_initial_points = 0,\n    x0 = x0[:128].tolist(),\n    y0 = y0[:128].tolist(),\n    random_state = 7)\nxbo_best = result.x\nybo_best = -result.fun\nprint('best result from BO is %d%% accuracy'%(100*ybo_best))\nxbo = array(result.x_iters)\nybo = -array(result.func_vals)\n</pre> x0 = data_avg_accuracy.xfull*(hp_ub-hp_lb)+hp_lb y0 = -data_avg_accuracy.yfull.squeeze() print('best result before BO is %d%% accuracy'%(-100*y0.min())) result = gp_minimize(     func = lambda hparams: get_dt_accuracy(atleast_2d(hparams)).squeeze().item(),     dimensions = [(l,u) for l,u in zip(hp_lb,hp_ub)],     n_calls = 32,     n_initial_points = 0,     x0 = x0[:128].tolist(),     y0 = y0[:128].tolist(),     random_state = 7) xbo_best = result.x ybo_best = -result.fun print('best result from BO is %d%% accuracy'%(100*ybo_best)) xbo = array(result.x_iters) ybo = -array(result.func_vals) <pre>best result before BO is 94% accuracy\nbest result from BO is 94% accuracy\n</pre> In\u00a0[15]: Copied! <pre>best_kwargs = {name:val for name,val in zip(hpnames,xbo_best)}\nbest_kwargs['max_depth'] = int(best_kwargs['max_depth'])\nprint(best_kwargs)\ndt = DecisionTreeClassifier(random_state=7,**best_kwargs).fit(xt,yt)\nyhat = dt.predict(xv)\naccuracy = mean(yhat==yv)\nprint('best decision tree accuracy: %.1f%%'%(100*accuracy))\nfig = pyplot.figure(figsize=(10,15))\nplot_tree(dt,feature_names=feature_names,class_names=target_names,filled=True);\n</pre> best_kwargs = {name:val for name,val in zip(hpnames,xbo_best)} best_kwargs['max_depth'] = int(best_kwargs['max_depth']) print(best_kwargs) dt = DecisionTreeClassifier(random_state=7,**best_kwargs).fit(xt,yt) yhat = dt.predict(xv) accuracy = mean(yhat==yv) print('best decision tree accuracy: %.1f%%'%(100*accuracy)) fig = pyplot.figure(figsize=(10,15)) plot_tree(dt,feature_names=feature_names,class_names=target_names,filled=True); <pre>{'max_depth': 6, 'min_weight_fraction_leaf': 0.007127984848298143}\nbest decision tree accuracy: 94.0%\n</pre> In\u00a0[18]: Copied! <pre>xfeatures = df.to_numpy()\nxfeatures_low = xfeatures[:,:-1].min(0)\nxfeatures_high = xfeatures[:,:-1].max(0)\nd_features = len(xfeatures_low)\ndef dt_pp(t): return dt.predict_proba(t).T\ncf = CustomFun(\n    true_measure = Uniform(DigitalNetB2(d_features,seed=7),\n        lower_bound = xfeatures_low,\n        upper_bound = xfeatures_high),\n    g = dt_pp,\n    dimension_indv = 3,\n    parallel = False)\nsi_cf = SobolIndices(cf,indices=\"all\")\nsolution,data = CubQMCNetG(si_cf,abs_tol=1e-3,n_init=2**10).integrate()\ndata\n</pre> xfeatures = df.to_numpy() xfeatures_low = xfeatures[:,:-1].min(0) xfeatures_high = xfeatures[:,:-1].max(0) d_features = len(xfeatures_low) def dt_pp(t): return dt.predict_proba(t).T cf = CustomFun(     true_measure = Uniform(DigitalNetB2(d_features,seed=7),         lower_bound = xfeatures_low,         upper_bound = xfeatures_high),     g = dt_pp,     dimension_indv = 3,     parallel = False) si_cf = SobolIndices(cf,indices=\"all\") solution,data = CubQMCNetG(si_cf,abs_tol=1e-3,n_init=2**10).integrate() data Out[18]: <pre>Data (Data)\n    solution        [[[0.    0.049 0.05 ]\n                      [0.    0.    0.   ]\n                      [1.    0.301 0.279]\n                      ...\n                      [0.    0.396 0.409]\n                      [1.    0.999 0.999]\n                      [1.    0.775 0.768]]\n                    \n                     [[0.    0.225 0.232]\n                      [0.    0.    0.   ]\n                      [1.    0.605 0.592]\n                      ...\n                      [0.    0.699 0.721]\n                      [1.    0.999 0.999]\n                      [1.    0.951 0.95 ]]]\n    comb_bound_low  [[[0.    0.048 0.05 ]\n                      [0.    0.    0.   ]\n                      [0.999 0.301 0.278]\n                      ...\n                      [0.    0.395 0.408]\n                      [0.999 0.999 0.999]\n                      [0.999 0.775 0.767]]\n                    \n                     [[0.    0.224 0.231]\n                      [0.    0.    0.   ]\n                      [0.999 0.604 0.591]\n                      ...\n                      [0.    0.698 0.721]\n                      [0.999 0.999 0.999]\n                      [0.999 0.95  0.949]]]\n    comb_bound_high [[[0.    0.05  0.051]\n                      [0.    0.    0.   ]\n                      [1.    0.302 0.279]\n                      ...\n                      [0.    0.397 0.41 ]\n                      [1.    1.    1.   ]\n                      [1.    0.776 0.769]]\n                    \n                     [[0.    0.225 0.233]\n                      [0.    0.    0.   ]\n                      [1.    0.605 0.593]\n                      ...\n                      [0.    0.699 0.722]\n                      [1.    1.    1.   ]\n                      [1.    0.952 0.951]]]\n    comb_bound_diff [[[0.    0.001 0.001]\n                      [0.    0.    0.   ]\n                      [0.001 0.001 0.001]\n                      ...\n                      [0.    0.002 0.002]\n                      [0.001 0.001 0.001]\n                      [0.001 0.001 0.002]]\n                    \n                     [[0.    0.002 0.002]\n                      [0.    0.    0.   ]\n                      [0.001 0.001 0.002]\n                      ...\n                      [0.    0.001 0.001]\n                      [0.001 0.001 0.001]\n                      [0.001 0.002 0.002]]]\n    comb_flags      [[[ True  True  True]\n                      [ True  True  True]\n                      [ True  True  True]\n                      ...\n                      [ True  True  True]\n                      [ True  True  True]\n                      [ True  True  True]]\n                    \n                     [[ True  True  True]\n                      [ True  True  True]\n                      [ True  True  True]\n                      ...\n                      [ True  True  True]\n                      [ True  True  True]\n                      [ True  True  True]]]\n    n_total         2^(18)\n    n               [[[[  1024  65536  65536]\n                       [  1024   1024   1024]\n                       [  8192 131072 131072]\n                       ...\n                       [  1024 131072 131072]\n                       [  8192  65536  32768]\n                       [  8192 262144 131072]]\n                    \n                      [[  1024  65536  65536]\n                       [  1024   1024   1024]\n                       [  8192 131072 131072]\n                       ...\n                       [  1024 131072 131072]\n                       [  8192  65536  32768]\n                       [  8192 262144 131072]]\n                    \n                      [[  1024  65536  65536]\n                       [  1024   1024   1024]\n                       [  8192 131072 131072]\n                       ...\n                       [  1024 131072 131072]\n                       [  8192  65536  32768]\n                       [  8192 262144 131072]]]\n                    \n                    \n                     [[[  1024  32768  32768]\n                       [  1024   1024   1024]\n                       [  8192 131072  65536]\n                       ...\n                       [  1024 131072 131072]\n                       [  8192  65536  32768]\n                       [  8192 131072 131072]]\n                    \n                      [[  1024  32768  32768]\n                       [  1024   1024   1024]\n                       [  8192 131072  65536]\n                       ...\n                       [  1024 131072 131072]\n                       [  8192  65536  32768]\n                       [  8192 131072 131072]]\n                    \n                      [[  1024  32768  32768]\n                       [  1024   1024   1024]\n                       [  8192 131072  65536]\n                       ...\n                       [  1024 131072 131072]\n                       [  8192  65536  32768]\n                       [  8192 131072 131072]]]]\n    time_integrate  8.221\nCubQMCNetG (AbstractStoppingCriterion)\n    abs_tol         0.001\n    rel_tol         0\n    n_init          2^(10)\n    n_limit         2^(35)\nSensitivityIndices (AbstractIntegrand)\n    indices         [[ True False False False]\n                     [False  True False False]\n                     [False False  True False]\n                     ...\n                     [ True  True False  True]\n                     [ True False  True  True]\n                     [False  True  True  True]]\nUniform (AbstractTrueMeasure)\n    lower_bound     [4.3 2.  1.  0.1]\n    upper_bound     [7.9 4.4 6.9 2.5]\nDigitalNetB2 (AbstractLDDiscreteDistribution)\n    d               2^(2)\n    replications    1\n    randomize       LMS_DS\n    gen_mats_source joe_kuo.6.21201.txt\n    order           NATURAL\n    t               63\n    alpha           1\n    n_limit         2^(32)\n    entropy         7</pre> <p>While the solution looks unwieldy, it has quite a natural interpretation. The first axis determines whether we are looking at a closed (index 0) or total (index 1) sensitivity index as before. The second axis indexes the subset of features we are testing. The third and final axis is length 3 for the 3 class probabilities we are interested in. For example, <code>solution[0,2,2]</code> looks at the closed sensitivity index of our index 2 feature (petal length) for our index 2 probability (virginica) AKA how important is petal length alone to determining if an Iris is virginica.</p> <p>The results indicate that setosa Irises can be completely determined based on petal length while the versicolor and virginica Irises can be completely determined by looking at both petal length and petal width. Interestingly sepal length and sepal width do not contribute significantly to determining species.</p> <p>These insights are not surprising or especially insightful for a decision tree where the tree structure indicates importance and the scores may even be computed directly. However, for more complicated models and datasets, this analysis pipeline may provide advanced insight into both hyperparameter tuning and feature importance.</p> In\u00a0[19]: Copied! <pre>print('solution shape:',solution.shape,'\\n')\nsi_closed = solution[0]\nsi_total = solution[1]\nprint('SI Closed')\nprint(si_closed,'\\n')\nprint('SI Total')\nprint(si_total)\n</pre> print('solution shape:',solution.shape,'\\n') si_closed = solution[0] si_total = solution[1] print('SI Closed') print(si_closed,'\\n') print('SI Total') print(si_total) <pre>solution shape: (2, 14, 3) \n\nSI Closed\n[[0.         0.04908409 0.05049225]\n [0.         0.         0.        ]\n [0.99967739 0.30125221 0.27859386]\n [0.         0.32358608 0.33400856]\n [0.         0.04908409 0.05049225]\n [0.99967739 0.45170218 0.43419784]\n [0.         0.3959954  0.40918357]\n [0.99967739 0.30125221 0.27859386]\n [0.         0.32358608 0.33400856]\n [0.99967739 0.77513307 0.7682392 ]\n [0.99967739 0.45170218 0.43419784]\n [0.         0.3959954  0.40918357]\n [0.99967739 0.9993052  0.99928672]\n [0.99967739 0.77513307 0.7682392 ]] \n\nSI Total\n[[0.         0.22459549 0.23200145]\n [0.         0.         0.        ]\n [0.99957471 0.60450383 0.59188948]\n [0.         0.54868564 0.56664366]\n [0.         0.22459549 0.23200145]\n [0.99957471 0.67654515 0.6661002 ]\n [0.         0.69852711 0.72138276]\n [0.99957471 0.60450383 0.59188948]\n [0.         0.54868564 0.56664366]\n [0.99957471 0.95117562 0.9496174 ]\n [0.99957471 0.67654515 0.6661002 ]\n [0.         0.69852711 0.72138276]\n [0.99957471 0.99944024 0.99945703]\n [0.99957471 0.95117562 0.9496174 ]]\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"demos/iris/#ml-sensitivity-indices","title":"ML Sensitivity Indices\u00b6","text":"<p>This notebook demonstrates QMCPy's support for vectorized sensitivity index computation. We preview this functionality by performing classification of Iris species using a decision tree. The computed sensitivity indices provide insight into input subset importance for a classic machine learning problem.</p>"},{"location":"demos/iris/#load-data","title":"Load Data\u00b6","text":"<p>We begin by reading in the Iris dataset and providing some basic summary statistics. Our goal will be to predict the Iris class (Setosa, Versicolour, or Virginica) based on Iris attributes (sepal length, sepal width, petal length, and petal width).</p>"},{"location":"demos/iris/#importance-of-decision-tree-hyperparameters","title":"Importance of Decision Tree Hyperparameters\u00b6","text":"<p>We would like to predict Iris species using a Decision Tree (DT) classifier. When initializing a DT, we arrive at the question of how to set hyperparameters such as tree depth or the minimum weight fraction for each leaf. These hyperparameters can greatly effect classification accuracy, so it is worthwhile to consider their importance to determining classification performance.</p> <p>Note that while this notebook uses decision trees and the Iris dataset, the methodology is directly applicable to other datasets and models.</p> <p>We begin this exploration by setting up a hyperparameter domain in which to uniformly sample DT hyperparameter configurations. A helper function and its tie into QMCPy are also created.</p>"},{"location":"demos/iris/#average-accuracy","title":"Average Accuracy\u00b6","text":"<p>Our first goal will be to find the average DT accuracy across the hyperparameter domain. To do so, we perform quasi-Monte Carlo numerical integration to approximate the mean testing accuracy.</p>"},{"location":"demos/iris/#sensitivity-indices","title":"Sensitivity Indices\u00b6","text":"<p>Next, we wish to quantify how important individual hyperparameters are to determining testing accuracy. To do this, we compute the sensitivity indices of our hyperparameters. In QMCPy we use the <code>SensitivityIndices</code> class to compute these sensitivity indices.</p>"},{"location":"demos/iris/#marginals","title":"Marginals\u00b6","text":"<p>We may also use QMCPy's support for vectorized quasi-Monte Carlo to compute marginal distributions. This is relatively straightforward to do for the Uniform true measure used here, but caution should be taken when adapting these techniques to distributions without independent marginals.</p>"},{"location":"demos/iris/#bayesian-optimization-of-hyperparameters","title":"Bayesian Optimization of Hyperparameters\u00b6","text":"<p>Having explored DT hyperparameter importance, we are now ready to construct our optimal DT. We already have quite a bit of data relating hyperparameter settings to testing accuracy, so we may simply select the best configuration and call this an optimal DT. However, if we are looking to squeeze out even more performance, we may choose to perform Bayesian Optimization which incorporates our past metadata. Sample code is provided below despite not finding an improved configuration for this problem.</p>"},{"location":"demos/iris/#best-decision-tree-analysis","title":"Best Decision Tree Analysis\u00b6","text":"<p>Below we print the configuration that rested in the best DT. We also print the optimal accuracy achieved (at this configuration) and visualize the branches of this tree.</p>"},{"location":"demos/iris/#feature-importance","title":"Feature Importance\u00b6","text":"<p>With the optimal DT in hand, we may now question how important the Irises features are in determining the class/species. To answer this question, we again perform sensitivity analysis, but this time we select a uniform measure over the domain of Iris features. Our output which we wish to quantify the variance of is now a length 3 vector of class probabilities. How variable is each species classification as a function of each Iris feature?</p>"},{"location":"demos/lattice_random_generator/","title":"2023 Random Lattice Generating Vectors","text":"In\u00a0[16]: Copied! <pre>import qmcpy as qp\nimport numpy as np  #basic numerical routines in Python\nimport time  #timing routines\nfrom matplotlib import pyplot;  #plotting\n\npyplot.rc('font', size=16)  #set defaults so that the plots are readable\npyplot.rc('axes', titlesize=16)\npyplot.rc('axes', labelsize=16)\npyplot.rc('xtick', labelsize=16)\npyplot.rc('ytick', labelsize=16)\npyplot.rc('legend', fontsize=16)\npyplot.rc('figure', titlesize=16)\n\n#a helpful plotting method to show increasing numbers of points\ndef plot_successive_points(distrib,ld_name,first_n=64,n_cols=1,pt_clr='bgkcmy',\n                           xlim=[0,1],ylim=[0,1],coord1 = 0,coord2 = 1):\n  fig,ax = pyplot.subplots(nrows=1,ncols=n_cols,figsize=(5*n_cols,5.5))\n  if n_cols==1: ax = [ax]\n  last_n = first_n*(2**n_cols)\n  points = distrib.gen_samples(n=last_n)\n  for i in range(n_cols):\n    n = first_n\n    nstart = 0\n    for j in range(i+1):\n      n = first_n*(2**j)\n      ax[i].scatter(points[nstart:n,coord1],points[nstart:n,coord2],color=pt_clr[j])\n      nstart = n\n    ax[i].set_title('n = %d'%n)\n    ax[i].set_xlim(xlim); ax[i].set_xticks(xlim); ax[i].set_xlabel('$x_{i,%d}$'%(coord1+1))\n    ax[i].set_ylim(ylim); ax[i].set_yticks(ylim); ax[i].set_ylabel('$x_{i,%d}$'%(coord2+1))\n    ax[i].set_aspect((xlim[1]-xlim[0])/(ylim[1]-ylim[0]))\n  fig.suptitle('%s Points'%ld_name)\n</pre> import qmcpy as qp import numpy as np  #basic numerical routines in Python import time  #timing routines from matplotlib import pyplot;  #plotting  pyplot.rc('font', size=16)  #set defaults so that the plots are readable pyplot.rc('axes', titlesize=16) pyplot.rc('axes', labelsize=16) pyplot.rc('xtick', labelsize=16) pyplot.rc('ytick', labelsize=16) pyplot.rc('legend', fontsize=16) pyplot.rc('figure', titlesize=16)  #a helpful plotting method to show increasing numbers of points def plot_successive_points(distrib,ld_name,first_n=64,n_cols=1,pt_clr='bgkcmy',                            xlim=[0,1],ylim=[0,1],coord1 = 0,coord2 = 1):   fig,ax = pyplot.subplots(nrows=1,ncols=n_cols,figsize=(5*n_cols,5.5))   if n_cols==1: ax = [ax]   last_n = first_n*(2**n_cols)   points = distrib.gen_samples(n=last_n)   for i in range(n_cols):     n = first_n     nstart = 0     for j in range(i+1):       n = first_n*(2**j)       ax[i].scatter(points[nstart:n,coord1],points[nstart:n,coord2],color=pt_clr[j])       nstart = n     ax[i].set_title('n = %d'%n)     ax[i].set_xlim(xlim); ax[i].set_xticks(xlim); ax[i].set_xlabel('$x_{i,%d}$'%(coord1+1))     ax[i].set_ylim(ylim); ax[i].set_yticks(ylim); ax[i].set_ylabel('$x_{i,%d}$'%(coord2+1))     ax[i].set_aspect((xlim[1]-xlim[0])/(ylim[1]-ylim[0]))   fig.suptitle('%s Points'%ld_name) In\u00a0[17]: Copied! <pre>lat = qp.Lattice()\nhelp(lat.__init__)\n</pre> lat = qp.Lattice() help(lat.__init__) <pre>Help on method __init__ in module qmcpy.discrete_distribution.lattice.lattice:\n\n__init__(dimension=1, replications=None, seed=None, randomize='SHIFT', generating_vector='kuo.lattice-33002-1024-1048576.9125.txt', order='NATURAL', m_max=None) method of qmcpy.discrete_distribution.lattice.lattice.Lattice instance\n    Args:\n        dimension (Union[int,np.ndarray]): Dimension of the generator.\n    \n            - If an `int` is passed in, use generating vector components at indices 0,...,`dimension`-1.\n            - If an `np.ndarray` is passed in, use generating vector components at these indices.\n        \n        replications (int): Number of independent randomizations.\n        seed (Union[None,int,np.random.SeedSeq): Seed the random number generator for reproducibility.\n        randomize (str): Options are \n            \n            - `'SHIFT'`: Random shift.\n            - `'FALSE'`: No randomization. In this case the first point will be the origin. \n        \n        generating_vector (Union[str,np.ndarray,int]: Specify the generating vector.\n            \n            - A `str` should be the name (or path) of a file from the LDData repo at [https://github.com/QMCSoftware/LDData/tree/main/lattice](https://github.com/QMCSoftware/LDData/tree/main/lattice).\n            - A `np.ndarray` of integers with shape $(d,)$ or $(r,d)$ where $d$ is the number of dimensions and $r$ is the number of replications.  \n                Must supply `m_max` where $2^{m_\\mathrm{max}}$ is the max number of supported samples. \n            - An `int`, call it $M$, \n            gives the random generating vector $(1,v_1,\\dots,v_{d-1})^T$ \n            where $d$ is the dimension and $v_i$ are randomly selected from $\\{3,5,\\dots,2^M-1\\}$ uniformly and independently.  \n            We require require $1 &lt; M &lt; 27$. \n    \n        order (str): `'LINEAR'`, `'NATURAL'`, or `'GRAY'` ordering. See the doctest example above.\n        m_max (int): $2^{m_\\mathrm{max}}$ is the maximum number of supported samples.\n\n</pre> In\u00a0[18]: Copied! <pre>help(lat.gen_samples)\n</pre> help(lat.gen_samples) <pre>Help on method gen_samples in module qmcpy.discrete_distribution.abstract_discrete_distribution:\n\ngen_samples(n=None, n_min=None, n_max=None, return_binary=False, warn=True) method of qmcpy.discrete_distribution.lattice.lattice.Lattice instance\n\n</pre> In\u00a0[19]: Copied! <pre>lat = qp.Lattice(dimension = 2,randomize= True, generating_vector=21, seed = 120) \nprint(\"Basic information of the lattice:\")\nprint(lat)\n\nprint(\"\\nA sample lattice generated by the random generating vector: \")\n\nn = 16 #number of points in the sample\nprint(lat.gen_samples(n))\n</pre> lat = qp.Lattice(dimension = 2,randomize= True, generating_vector=21, seed = 120)  print(\"Basic information of the lattice:\") print(lat)  print(\"\\nA sample lattice generated by the random generating vector: \")  n = 16 #number of points in the sample print(lat.gen_samples(n)) <pre>Basic information of the lattice:\nLattice (AbstractLDDiscreteDistribution)\n    d               2^(1)\n    replications    1\n    randomize       SHIFT\n    gen_vec_source  random\n    order           NATURAL\n    n_limit         2^(21)\n    entropy         120\n\nA sample lattice generated by the random generating vector: \n[[0.34548142 0.46736834]\n [0.84548142 0.96736834]\n [0.59548142 0.21736834]\n [0.09548142 0.71736834]\n [0.47048142 0.84236834]\n [0.97048142 0.34236834]\n [0.72048142 0.59236834]\n [0.22048142 0.09236834]\n [0.40798142 0.15486834]\n [0.90798142 0.65486834]\n [0.65798142 0.90486834]\n [0.15798142 0.40486834]\n [0.53298142 0.52986834]\n [0.03298142 0.02986834]\n [0.78298142 0.27986834]\n [0.28298142 0.77986834]]\n</pre> In\u00a0[20]: Copied! <pre>#Here the sample sizes are prime numbers\nlat = qp.Lattice(dimension=2,generating_vector= 16,seed = 136, order=\"GRAY\")\n    \nprimes = [64,256,512,1024]\n\nfor i in range(len(primes)):\n    plot_successive_points(distrib = lat,ld_name = \"Lattice\",first_n=primes[i],pt_clr=\"bgcmr\"[i])\n</pre> #Here the sample sizes are prime numbers lat = qp.Lattice(dimension=2,generating_vector= 16,seed = 136, order=\"GRAY\")      primes = [64,256,512,1024]  for i in range(len(primes)):     plot_successive_points(distrib = lat,ld_name = \"Lattice\",first_n=primes[i],pt_clr=\"bgcmr\"[i])  In\u00a0[21]: Copied! <pre>import warnings\nwarnings.simplefilter('ignore')\n\nd = 5  #coded as parameters so that \ntol = 1E-3 #you can change here and propagate them through this example\n\ndata_random = qp.CubQMCLatticeG(qp.Keister(qp.Gaussian(qp.Lattice(d,generating_vector = 26), mean = 0, covariance = 1/2)), abs_tol = tol).integrate()[1]\ndata_default = qp.CubQMCLatticeG(qp.Keister(qp.Gaussian(qp.Lattice(d), mean = 0, covariance = 1/2)),  abs_tol = tol).integrate()[1]\nprint(\"Integration data from a random lattice generator:\")\nprint(data_random)\nprint(\"\\nIntegration data from the default lattice generator:\")\nprint(data_default)\n</pre> import warnings warnings.simplefilter('ignore')  d = 5  #coded as parameters so that  tol = 1E-3 #you can change here and propagate them through this example  data_random = qp.CubQMCLatticeG(qp.Keister(qp.Gaussian(qp.Lattice(d,generating_vector = 26), mean = 0, covariance = 1/2)), abs_tol = tol).integrate()[1] data_default = qp.CubQMCLatticeG(qp.Keister(qp.Gaussian(qp.Lattice(d), mean = 0, covariance = 1/2)),  abs_tol = tol).integrate()[1] print(\"Integration data from a random lattice generator:\") print(data_random) print(\"\\nIntegration data from the default lattice generator:\") print(data_default)   <pre>Integration data from a random lattice generator:\nData (Data)\n    solution        1.135\n    comb_bound_low  1.135\n    comb_bound_high 1.136\n    comb_bound_diff 0.001\n    comb_flags      1\n    n_total         2^(17)\n    n               2^(17)\n    time_integrate  0.153\nCubQMCLatticeG (AbstractStoppingCriterion)\n    abs_tol         0.001\n    rel_tol         0\n    n_init          2^(10)\n    n_limit         2^(30)\nKeister (AbstractIntegrand)\nGaussian (AbstractTrueMeasure)\n    mean            0\n    covariance      2^(-1)\n    decomp_type     PCA\n    transform       Gaussian (AbstractTrueMeasure)\n                        mean            0\n                        covariance      2^(-1)\n                        decomp_type     PCA\nLattice (AbstractLDDiscreteDistribution)\n    d               5\n    replications    1\n    randomize       SHIFT\n    gen_vec_source  random\n    order           NATURAL\n    n_limit         2^(26)\n    entropy         253523607185012067649885865695647709066\n\nIntegration data from the default lattice generator:\nData (Data)\n    solution        1.135\n    comb_bound_low  1.135\n    comb_bound_high 1.136\n    comb_bound_diff 0.001\n    comb_flags      1\n    n_total         2^(17)\n    n               2^(17)\n    time_integrate  0.147\nCubQMCLatticeG (AbstractStoppingCriterion)\n    abs_tol         0.001\n    rel_tol         0\n    n_init          2^(10)\n    n_limit         2^(30)\nKeister (AbstractIntegrand)\nGaussian (AbstractTrueMeasure)\n    mean            0\n    covariance      2^(-1)\n    decomp_type     PCA\n    transform       Gaussian (AbstractTrueMeasure)\n                        mean            0\n                        covariance      2^(-1)\n                        decomp_type     PCA\nLattice (AbstractLDDiscreteDistribution)\n    d               5\n    replications    1\n    randomize       SHIFT\n    gen_vec_source  kuo.lattice-33002-1024-1048576.9125.txt\n    order           NATURAL\n    n_limit         2^(20)\n    entropy         122490702083376388603057539026669171278\n</pre> In\u00a0[\u00a0]: Copied! <pre>#mean vs. median plot\nimport qmcpy as qp\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\n\nd = 2\nN_min = 6\nN_max = 18\nN_list = 2**np.arange(N_min,N_max)\nr = 11\nnum_trials = 25\n\n\nerror_median = np.zeros(N_max - N_min) \nerror_mean = np.zeros(N_max - N_min) \nerror_mean_onegen = np.zeros(N_max - N_min) \nfor i in range(num_trials):\n    y_median = []\n    y_mean = []\n    y_mean_one_gen = []\n    print(\"%d, \"%i,end='',flush=True)\n    list_of_keister_objects_random = []\n    list_of_keister_objects_default = []\n    y_randomized_list = []\n    y_default_list = []\n    for k in range(r):\n        lattice = qp.Lattice(generating_vector = 26,dimension=d)\n        keister = qp.Keister(lattice)\n        list_of_keister_objects_random.append(keister)\n        x = keister.discrete_distrib.gen_samples(N_list.max())\n        y = keister.f(x)\n        y_randomized_list.append(y)\n        keister = qp.Keister(qp.Lattice(d))\n        list_of_keister_objects_default.append(keister)\n        x = keister.discrete_distrib.gen_samples(N_list.max())\n        y = keister.f(x)    \n        y_default_list.append(y) \n            \n    for N in N_list:\n\n        y_median.append(np.median([np.mean(y[:N]) for y in y_randomized_list]))\n        y_mean_one_gen.append(np.mean([np.mean(y[:N]) for y in y_default_list]))\n        y_mean.append(np.mean([np.mean(y[:N]) for y in y_randomized_list]))\n\n    answer = keister.exact_integ(d)\n    error_median += abs(answer-y_median)\n    error_mean += abs(answer-y_mean)\n    error_mean_onegen += abs(answer-y_mean_one_gen)\nprint()\n\nerror_median /= num_trials\nerror_mean /= num_trials\nerror_mean_onegen /= num_trials\n\nplt.loglog(N_list,error_median,label = \"median of means\")\nplt.loglog(N_list,error_mean,label = \"mean of means\")\nplt.loglog(N_list,error_mean_onegen,label = \"mean of random shifts\")\nplt.xlabel(\"sample size\")\nplt.ylabel(\"error\")\nplt.title(\"Comparison of lattice generators\")\nplt.legend()\n# plt.savefig(\"./meanvsmedian.png\")\n</pre> #mean vs. median plot import qmcpy as qp import numpy as np import matplotlib.pyplot as plt    d = 2 N_min = 6 N_max = 18 N_list = 2**np.arange(N_min,N_max) r = 11 num_trials = 25   error_median = np.zeros(N_max - N_min)  error_mean = np.zeros(N_max - N_min)  error_mean_onegen = np.zeros(N_max - N_min)  for i in range(num_trials):     y_median = []     y_mean = []     y_mean_one_gen = []     print(\"%d, \"%i,end='',flush=True)     list_of_keister_objects_random = []     list_of_keister_objects_default = []     y_randomized_list = []     y_default_list = []     for k in range(r):         lattice = qp.Lattice(generating_vector = 26,dimension=d)         keister = qp.Keister(lattice)         list_of_keister_objects_random.append(keister)         x = keister.discrete_distrib.gen_samples(N_list.max())         y = keister.f(x)         y_randomized_list.append(y)         keister = qp.Keister(qp.Lattice(d))         list_of_keister_objects_default.append(keister)         x = keister.discrete_distrib.gen_samples(N_list.max())         y = keister.f(x)             y_default_list.append(y)                   for N in N_list:          y_median.append(np.median([np.mean(y[:N]) for y in y_randomized_list]))         y_mean_one_gen.append(np.mean([np.mean(y[:N]) for y in y_default_list]))         y_mean.append(np.mean([np.mean(y[:N]) for y in y_randomized_list]))      answer = keister.exact_integ(d)     error_median += abs(answer-y_median)     error_mean += abs(answer-y_mean)     error_mean_onegen += abs(answer-y_mean_one_gen) print()  error_median /= num_trials error_mean /= num_trials error_mean_onegen /= num_trials  plt.loglog(N_list,error_median,label = \"median of means\") plt.loglog(N_list,error_mean,label = \"mean of means\") plt.loglog(N_list,error_mean_onegen,label = \"mean of random shifts\") plt.xlabel(\"sample size\") plt.ylabel(\"error\") plt.title(\"Comparison of lattice generators\") plt.legend() # plt.savefig(\"./meanvsmedian.png\")  <pre>0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, \n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"demos/lattice_random_generator/#random-lattice-generators-are-not-bad","title":"Random Lattice Generators Are Not Bad\u00b6","text":""},{"location":"demos/lattice_random_generator/#lattice-declaration-and-the-gen_samples-function","title":"Lattice Declaration and the gen_samples function\u00b6","text":""},{"location":"demos/lattice_random_generator/#driver-code","title":"Driver code\u00b6","text":""},{"location":"demos/lattice_random_generator/#plots-of-lattices","title":"Plots of lattices\u00b6","text":""},{"location":"demos/lattice_random_generator/#integration","title":"Integration\u00b6","text":""},{"location":"demos/lattice_random_generator/#runtime-comparison-between-random-generator-and-hard-conded-generator","title":"Runtime comparison between random generator and hard-conded generator\u00b6","text":""},{"location":"demos/lattice_random_generator/#mean-vs-median-as-a-function-of-the-sample-size-plot","title":"Mean vs. Median as a function of the sample size plot\u00b6","text":""},{"location":"demos/lebesgue_integration/","title":"Lebesgue Integration","text":"In\u00a0[26]: Copied! <pre>from qmcpy import *\nfrom numpy import *\n</pre> from qmcpy import * from numpy import * In\u00a0[27]: Copied! <pre>abs_tol = .01\ndim = 1\na = 0\nb = 2\ntrue_value = 8./3\n</pre> abs_tol = .01 dim = 1 a = 0 b = 2 true_value = 8./3 In\u00a0[28]: Copied! <pre># Lebesgue Measure\nintegrand = CustomFun(\n    true_measure = Lebesgue(Uniform(Halton(dim, seed=7, replications=32),lower_bound=a, upper_bound=b)),\n    g = lambda x: (x**2).sum(-1))\nsolution,data = CubQMCCLT(integrand, abs_tol=abs_tol).integrate()\nprint('y = %.3f'%solution)\nerror = abs((solution-true_value))\nif error&gt;abs_tol:\n    raise Exception(\"Not within error tolerance\")\n</pre> # Lebesgue Measure integrand = CustomFun(     true_measure = Lebesgue(Uniform(Halton(dim, seed=7, replications=32),lower_bound=a, upper_bound=b)),     g = lambda x: (x**2).sum(-1)) solution,data = CubQMCCLT(integrand, abs_tol=abs_tol).integrate() print('y = %.3f'%solution) error = abs((solution-true_value)) if error&gt;abs_tol:     raise Exception(\"Not within error tolerance\") <pre>y = 2.667\n</pre> In\u00a0[29]: Copied! <pre># Uniform Measure\nintegrand = CustomFun(\n    true_measure = Uniform(Halton(dim, seed=7, replications=32), lower_bound=a, upper_bound=b),\n    g = lambda x: (2*(x**2)).sum(-1))\nsolution,data = CubQMCCLT(integrand, abs_tol=abs_tol).integrate()\nprint('y = %.3f'%solution)\nerror = abs((solution-true_value))\nif error&gt;abs_tol:\n    raise Exception(\"Not within error tolerance\")\n</pre> # Uniform Measure integrand = CustomFun(     true_measure = Uniform(Halton(dim, seed=7, replications=32), lower_bound=a, upper_bound=b),     g = lambda x: (2*(x**2)).sum(-1)) solution,data = CubQMCCLT(integrand, abs_tol=abs_tol).integrate() print('y = %.3f'%solution) error = abs((solution-true_value)) if error&gt;abs_tol:     raise Exception(\"Not within error tolerance\") <pre>y = 2.667\n</pre> In\u00a0[30]: Copied! <pre>abs_tol = .001\ndim = 2\na = array([1.,2.])\nb = array([2.,4.])\ntrue_value = ((a[0]**3-b[0]**3)*(a[1]-b[1])+(a[0]-b[0])*(a[1]**3-b[1]**3))/3\nprint('Answer = %.5f'%true_value)\n</pre> abs_tol = .001 dim = 2 a = array([1.,2.]) b = array([2.,4.]) true_value = ((a[0]**3-b[0]**3)*(a[1]-b[1])+(a[0]-b[0])*(a[1]**3-b[1]**3))/3 print('Answer = %.5f'%true_value) <pre>Answer = 23.33333\n</pre> In\u00a0[31]: Copied! <pre># Lebesgue Measure\nintegrand = CustomFun(\n    true_measure = Lebesgue(Uniform(DigitalNetB2(dim, seed=7, replications=32), lower_bound=a, upper_bound=b)), \n    g = lambda x: (x**2).sum(-1))\nsolution,data = CubQMCCLT(integrand, abs_tol=abs_tol).integrate()\nprint('y = %.5f'%solution)\nerror = abs((solution-true_value))\nif error&gt;abs_tol:\n    raise Exception(\"Not within error tolerance\")\n</pre> # Lebesgue Measure integrand = CustomFun(     true_measure = Lebesgue(Uniform(DigitalNetB2(dim, seed=7, replications=32), lower_bound=a, upper_bound=b)),      g = lambda x: (x**2).sum(-1)) solution,data = CubQMCCLT(integrand, abs_tol=abs_tol).integrate() print('y = %.5f'%solution) error = abs((solution-true_value)) if error&gt;abs_tol:     raise Exception(\"Not within error tolerance\") <pre>y = 23.33336\n</pre> In\u00a0[32]: Copied! <pre># Uniform Measure\nintegrand = CustomFun(\n    true_measure = Uniform(DigitalNetB2(dim, seed=17, replications=32), lower_bound=a, upper_bound=b),\n    g = lambda x: (b-a).prod()*(x**2).sum(-1))\nsolution,data = CubQMCCLT(integrand, abs_tol=abs_tol).integrate()\nprint('y = %.5f'%solution)\nerror = abs((solution-true_value))\nif error&gt;abs_tol:\n    raise Exception(\"Not within error tolerance\")\n</pre> # Uniform Measure integrand = CustomFun(     true_measure = Uniform(DigitalNetB2(dim, seed=17, replications=32), lower_bound=a, upper_bound=b),     g = lambda x: (b-a).prod()*(x**2).sum(-1)) solution,data = CubQMCCLT(integrand, abs_tol=abs_tol).integrate() print('y = %.5f'%solution) error = abs((solution-true_value)) if error&gt;abs_tol:     raise Exception(\"Not within error tolerance\") <pre>y = 23.33333\n</pre> In\u00a0[33]: Copied! <pre>abs_tol = .0001\ndim = 1\na = 3\nb = 5\ntrue_value = -0.87961 \n</pre> abs_tol = .0001 dim = 1 a = 3 b = 5 true_value = -0.87961  In\u00a0[34]: Copied! <pre># Lebesgue Measure\nintegrand = CustomFun(\n    true_measure = Lebesgue(Uniform(Lattice(dim, randomize=True, seed=7),a,b)), \n    g = lambda x: (sin(x)/log(x)).sum(-1))\nsolution,data = CubQMCLatticeG(integrand, abs_tol=abs_tol).integrate()\nprint('y = %.3f'%solution)\nerror = abs((solution-true_value))\nif error&gt;abs_tol:\n    raise Exception(\"Not within error tolerance\")\n</pre> # Lebesgue Measure integrand = CustomFun(     true_measure = Lebesgue(Uniform(Lattice(dim, randomize=True, seed=7),a,b)),      g = lambda x: (sin(x)/log(x)).sum(-1)) solution,data = CubQMCLatticeG(integrand, abs_tol=abs_tol).integrate() print('y = %.3f'%solution) error = abs((solution-true_value)) if error&gt;abs_tol:     raise Exception(\"Not within error tolerance\") <pre>y = -0.880\n</pre> In\u00a0[35]: Copied! <pre>abs_tol = .1\ndim = 2\ntrue_value = pi\n</pre> abs_tol = .1 dim = 2 true_value = pi In\u00a0[36]: Copied! <pre>integrand = CustomFun(\n    true_measure = Lebesgue(Gaussian(Lattice(dim,seed=7))),\n    g = lambda x: exp(-x**2).prod(1))\nsolution,data = CubQMCLatticeG(integrand,abs_tol=abs_tol).integrate()\nprint('y = %.3f'%solution)\nerror = abs((solution-true_value))\nif error&gt;abs_tol:\n    raise Exception(\"Not within error tolerance\")\n</pre> integrand = CustomFun(     true_measure = Lebesgue(Gaussian(Lattice(dim,seed=7))),     g = lambda x: exp(-x**2).prod(1)) solution,data = CubQMCLatticeG(integrand,abs_tol=abs_tol).integrate() print('y = %.3f'%solution) error = abs((solution-true_value)) if error&gt;abs_tol:     raise Exception(\"Not within error tolerance\") <pre>y = 3.142\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"demos/lebesgue_integration/#qmcpy-for-lebesgue-integration","title":"QMCPy for Lebesgue Integration\u00b6","text":"<p>This notebook will give examples of how to use QMCPy for integration problems that not are defined in terms of a standard measure. i.e. Uniform or Gaussian.</p>"},{"location":"demos/lebesgue_integration/#sample-problem-1","title":"Sample Problem 1\u00b6","text":"<p>$$y  = \\int_{[0,2]} x^2 dx = 2\\int_{[0,2]} \\frac{x^2}{2} dx$$</p> <p>where the middle expression may be viewed as WRT the Lebesgue measure and the right expression may be viewed as WRT the uniform measure</p>"},{"location":"demos/lebesgue_integration/#sample-problem-2","title":"Sample Problem 2\u00b6","text":"<p>$$y = \\int_{[a,b]^d} ||x||_2^2 dx = \\Pi_{i=1}^d (b_i-a_i)\\int_{[a,b]^d} ||x||_2^2 \\; [ \\Pi_{i=1}^d (b_i-a_i)]^{-1} dx$$</p> <p>where the middle expression may be viewed as WRT the Lebesgue measure and the right expression may be viewed as WRT the uniform measure</p>"},{"location":"demos/lebesgue_integration/#sample-problem-3","title":"Sample Problem 3\u00b6","text":"<p>Integral that cannot be done in terms of any standard mathematical functions</p> <p>$$y = \\int_{[a,b]} \\frac{\\sin{x}}{\\log{x}} dx$$</p> <p>where the middle expression may be viewed as WRT the Lebesgue measure and the right expression may be viewed as WRT the uniform measure</p> <p>Mathematica Code: <code>Integrate[Sin[x]/Log[x], {x,a,b}]</code></p>"},{"location":"demos/lebesgue_integration/#sample-problem-4","title":"Sample Problem 4\u00b6","text":"<p>Integral over $\\mathbb{R}^d$ $$y = \\int_{\\mathbb{R}^2} e^{-||x||_2^2} dx$$</p>"},{"location":"demos/linear-scrambled-halton/","title":"Halton Points and their Randomizations","text":"<p>In Halton, each dimension has a prime number associated to it: 2 is associated to dimension 1, 3 to dimension 2, 5 to dimension 3 and so on. These prime numbers are referred as bases.</p> <p>Based on the bases, a different scrambling matrix is generated for each dimension where the lower triangle is random between 0 and (base - 1), the diagonal is random between 1 and (base - 1), and the upper triangle has all zeros. For each dimension, we convert the indices to their log base representations, compute their dot product with the scrambling matrix, and convert them to decimal (base 10) to generate our samples.</p> <p>Based on the bases, a different vector is generated for each dimension which is random between 0 and (base - 1). For each dimension, we convert the indices to their log base representations, add them to the vector , and convert them to decimal (base 10) to generate our samples.</p> <p>In this option, we implement the Linear Matrix Scrambling of Halton but before converting to base 10, we apply the Digital Shift to the scrambled coefficients (the dot product of the log base representations of the indices with the scrambling matrix), and then convert to base 10.</p> In\u00a0[1]: Copied! <pre>import qmcpy as qp\n</pre> import qmcpy as qp In\u00a0[2]: Copied! <pre>help(qp.Halton().__init__)\n</pre> help(qp.Halton().__init__) <pre>Help on method __init__ in module qmcpy.discrete_distribution.halton:\n\n__init__(dimension=1, replications=None, seed=None, randomize='LMS_PERM', t=63, n_lim=4294967296, t_lms=None) method of qmcpy.discrete_distribution.halton.Halton instance\n    Args:\n        dimension (Union[int,np.ndarray]): Dimension of the generator.\n    \n            - If an `int` is passed in, use generating vector components at indices 0,...,`dimension`-1.\n            - If an `np.ndarray` is passed in, use generating vector components at these indices.\n        \n        replications (int): Number of independent randomizations of a pointset.\n        seed (Union[None,int,np.random.SeedSeq): Seed the random number generator for reproducibility.\n        randomize (str): Options are\n            \n            - `'LMS_PERM'`: Linear matrix scramble with digital shift.\n            - `'LMS_DS'`: Linear matrix scramble with permutation.\n            - `'LMS'`: Linear matrix scramble only.\n            - `'PERM'`: Permutation scramble only.\n            - `'DS'`: Digital shift only.\n            - `'NUS'`: Nested uniform scrambling.\n            - `'QRNG'`: Deterministic permutation scramble and random digital shift from QRNG [1] (with `generalize=True`). Does *not* support replications&gt;1.\n            - `'FALSE'`: No randomization. In this case the first point will be the origin. \n        t (int): Number of bits in integer represetation of points *after* randomization. The number of bits in the generating matrices is inferred based on the largest value.\n        n_lim (int): Maximum number of compatible points, determines the number of rows in the generating matrices.\n\n</pre> In\u00a0[3]: Copied! <pre>help(qp.Halton().gen_samples)\n</pre> help(qp.Halton().gen_samples) <pre>Help on method gen_samples in module qmcpy.discrete_distribution.abstract_discrete_distribution:\n\ngen_samples(n=None, n_min=None, n_max=None, return_binary=False, warn=True) method of qmcpy.discrete_distribution.halton.Halton instance\n\n</pre> In\u00a0[4]: Copied! <pre>dimension = 2\nlms_halton = qp.Halton(dimension, randomize= 'LMS') # Linear Matrix Scrambling\nds_halton = qp.Halton(dimension, randomize= 'DS') # Digital Shift\nlms_ds_halton = qp.Halton(dimension, randomize= 'LMS_DS') # Linear Matrix Scrambling Combined with Digital Shift\n</pre> dimension = 2 lms_halton = qp.Halton(dimension, randomize= 'LMS') # Linear Matrix Scrambling ds_halton = qp.Halton(dimension, randomize= 'DS') # Digital Shift lms_ds_halton = qp.Halton(dimension, randomize= 'LMS_DS') # Linear Matrix Scrambling Combined with Digital Shift <p>Here the difference between the three is shown by printing some samples:</p> In\u00a0[5]: Copied! <pre>print(\"Samples Generated by Linear Matrix Scrambling: \")\nprint(lms_halton.gen_samples(8,warn=False))\nprint(\"\\nSamples Generated by Digital Shift: \")\nprint(ds_halton.gen_samples(8))\nprint(\"\\nSamples Generated by Linear Matrix Scrambling Combined with Digital Shift: \")\nprint(lms_ds_halton.gen_samples(8))\n</pre> print(\"Samples Generated by Linear Matrix Scrambling: \") print(lms_halton.gen_samples(8,warn=False)) print(\"\\nSamples Generated by Digital Shift: \") print(ds_halton.gen_samples(8)) print(\"\\nSamples Generated by Linear Matrix Scrambling Combined with Digital Shift: \") print(lms_ds_halton.gen_samples(8)) <pre>Samples Generated by Linear Matrix Scrambling: \n[[0.         0.        ]\n [0.66488583 0.4953392 ]\n [0.25933582 0.9904689 ]\n [0.90756417 0.21615921]\n [0.16785788 0.5632913 ]\n [0.50313275 0.72102387]\n [0.40862172 0.28368836]\n [0.76047682 0.44152192]]\n\nSamples Generated by Digital Shift: \n[[0.06134401 0.4803077 ]\n [0.56134401 0.81364103]\n [0.31134401 0.14697436]\n [0.81134401 0.59141881]\n [0.18634401 0.92475214]\n [0.68634401 0.25808548]\n [0.43634401 0.36919659]\n [0.93634401 0.70252992]]\n\nSamples Generated by Linear Matrix Scrambling Combined with Digital Shift: \n[[0.43966325 0.00409093]\n [0.63714101 0.72179898]\n [0.08269944 0.44092915]\n [0.77633116 0.32543015]\n [0.35612274 0.90047472]\n [0.53412944 0.60725921]\n [0.24433632 0.17094181]\n [0.92618907 0.8776755 ]]\n</pre> <p>Here the difference between the three is shown by plotting with more samples:</p> In\u00a0[6]: Copied! <pre>fig1,ax1 = qp.plot_proj(lms_halton, n = 2**5)\nfig2,ax2 = qp.plot_proj(ds_halton,n = 2**5)\nfig3,ax3 = qp.plot_proj(lms_ds_halton,n = 2**5)\n</pre> fig1,ax1 = qp.plot_proj(lms_halton, n = 2**5) fig2,ax2 = qp.plot_proj(ds_halton,n = 2**5) fig3,ax3 = qp.plot_proj(lms_ds_halton,n = 2**5) <pre>/Users/alegresor/Desktop/QMCSoftware/qmcpy/discrete_distribution/halton.py:260: UserWarning: Without randomization, the first Halton point is the origin\n  warnings.warn(\"Without randomization, the first Halton point is the origin\")\n</pre> <p>Here we show a 3-dimensional scrambled Halton:</p> In\u00a0[7]: Copied! <pre>dimension = 3\nlms_halton = qp.Halton(dimension, randomize= 'LMS')\nfig,ax = qp.plot_proj(lms_halton, n = 2**5, d_horizontal = range(dimension), d_vertical = range(dimension),math_ind = False)\n</pre> dimension = 3 lms_halton = qp.Halton(dimension, randomize= 'LMS') fig,ax = qp.plot_proj(lms_halton, n = 2**5, d_horizontal = range(dimension), d_vertical = range(dimension),math_ind = False) <pre>/Users/alegresor/Desktop/QMCSoftware/qmcpy/discrete_distribution/halton.py:260: UserWarning: Without randomization, the first Halton point is the origin\n  warnings.warn(\"Without randomization, the first Halton point is the origin\")\n</pre> <p>Here we show a 4-dimensional scrambled Halton with successively increasing number of points. The initial points are in blue.  The next additional points are in orange. The final additional points are in green.</p> In\u00a0[8]: Copied! <pre>dimension = 4\nlms_halton = qp.Halton(dimension, randomize= 'LMS')\nfig,ax = qp.plot_proj(lms_halton, n = [2**5,2**6,2**7], d_horizontal = range(dimension), d_vertical = range(dimension),math_ind = False,marker_size = 15)\n</pre> dimension = 4 lms_halton = qp.Halton(dimension, randomize= 'LMS') fig,ax = qp.plot_proj(lms_halton, n = [2**5,2**6,2**7], d_horizontal = range(dimension), d_vertical = range(dimension),math_ind = False,marker_size = 15) <pre>/Users/alegresor/Desktop/QMCSoftware/qmcpy/discrete_distribution/halton.py:260: UserWarning: Without randomization, the first Halton point is the origin\n  warnings.warn(\"Without randomization, the first Halton point is the origin\")\n</pre> <p>Here we show a 3-dimensional Halton with Digital Shift:</p> In\u00a0[9]: Copied! <pre>dimension = 3\nds_halton = qp.Halton(dimension, randomize= 'DS')\nfig,ax = qp.plot_proj(ds_halton, n = 2**5, d_horizontal = range(dimension), d_vertical = range(dimension),math_ind = False)\n</pre> dimension = 3 ds_halton = qp.Halton(dimension, randomize= 'DS') fig,ax = qp.plot_proj(ds_halton, n = 2**5, d_horizontal = range(dimension), d_vertical = range(dimension),math_ind = False) <p>Here we show a 4-dimensional Halton with Digital Shift with successively increasing number of points. The initial points are in blue.  The next additional points are in orange. The final additional points are in green.</p> In\u00a0[10]: Copied! <pre>dimension = 4\nds_halton = qp.Halton(dimension, randomize= 'DS')\nfig,ax = qp.plot_proj(ds_halton, n = [2**5,2**6,2**7], d_horizontal = range(dimension), d_vertical = range(dimension),math_ind = False,marker_size = 15)\n</pre> dimension = 4 ds_halton = qp.Halton(dimension, randomize= 'DS') fig,ax = qp.plot_proj(ds_halton, n = [2**5,2**6,2**7], d_horizontal = range(dimension), d_vertical = range(dimension),math_ind = False,marker_size = 15) <p>Here we show a 3-dimensional Halton with Linear Matrix Scrambling Combined with Digital Shift:</p> In\u00a0[11]: Copied! <pre>dimension = 3\nlms_ds_halton = qp.Halton(dimension, randomize='LMS_DS')\nfig,ax = qp.plot_proj(lms_ds_halton, n = 2**5, d_horizontal = range(dimension), d_vertical = range(dimension),math_ind = False)\n</pre> dimension = 3 lms_ds_halton = qp.Halton(dimension, randomize='LMS_DS') fig,ax = qp.plot_proj(lms_ds_halton, n = 2**5, d_horizontal = range(dimension), d_vertical = range(dimension),math_ind = False) <p>Here we show a 4-dimensional Halton with Linear Matrix Scrambling combined with Digital Shift with successively increasing number of points. The initial points are in blue.  The next additional points are in orange. The final additional points are in green.</p> In\u00a0[12]: Copied! <pre>dimension = 4\nlms_ds_halton = qp.Halton(dimension, randomize='LMS_DS')\nfig,ax = qp.plot_proj(lms_ds_halton, n = [2**5,2**6,2**7], d_horizontal = range(dimension), d_vertical = range(dimension),math_ind = False,marker_size = 15)\n</pre> dimension = 4 lms_ds_halton = qp.Halton(dimension, randomize='LMS_DS') fig,ax = qp.plot_proj(lms_ds_halton, n = [2**5,2**6,2**7], d_horizontal = range(dimension), d_vertical = range(dimension),math_ind = False,marker_size = 15) In\u00a0[14]: Copied! <pre>from time import time\ndimension = 25\nsamples = 1000\nrand_options = ['QRNG','LMS', 'DS', 'LMS_DS', 'OWEN']\nfor i in range(len(rand_options)):\n    t_start = time()\n    rand_halton = qp.Halton(dimension,randomize = rand_options[i]).gen_samples(samples,warn=False)\n    t_end = time()\n    print(\"Time to generate samples for \" + rand_options[i] + \" = \" + str(t_end - t_start))\n</pre> from time import time dimension = 25 samples = 1000 rand_options = ['QRNG','LMS', 'DS', 'LMS_DS', 'OWEN'] for i in range(len(rand_options)):     t_start = time()     rand_halton = qp.Halton(dimension,randomize = rand_options[i]).gen_samples(samples,warn=False)     t_end = time()     print(\"Time to generate samples for \" + rand_options[i] + \" = \" + str(t_end - t_start)) <pre>Time to generate samples for QRNG = 0.01398015022277832\nTime to generate samples for LMS = 0.02975296974182129\nTime to generate samples for DS = 0.016762971878051758\nTime to generate samples for LMS_DS = 0.03149604797363281\nTime to generate samples for OWEN = 10.350087881088257\n</pre>"},{"location":"demos/linear-scrambled-halton/#linear-matrix-scrambling-and-digital-shift-for-halton","title":"Linear Matrix Scrambling and Digital Shift for Halton\u00b6","text":""},{"location":"demos/linear-scrambled-halton/#here-we-explain-linear-matrix-scrambling","title":"Here we explain Linear Matrix Scrambling:\u00b6","text":""},{"location":"demos/linear-scrambled-halton/#here-we-explain-digital-shift","title":"Here we explain Digital Shift:\u00b6","text":""},{"location":"demos/linear-scrambled-halton/#here-we-explain-linear-matrix-scrambling-combined-with-digital-shift","title":"Here we explain Linear Matrix Scrambling Combined with Digital Shift:\u00b6","text":""},{"location":"demos/linear-scrambled-halton/#here-we-set-up-the-qmcpy-environment","title":"Here we set up the QMCPY environment:\u00b6","text":""},{"location":"demos/linear-scrambled-halton/#here-we-explain-the-parameters-of-the-init-function","title":"Here we explain the parameters of the init function:\u00b6","text":""},{"location":"demos/linear-scrambled-halton/#here-we-explain-the-parameters-of-the-gen_samples-function","title":"Here we explain the parameters of the gen_samples function:\u00b6","text":""},{"location":"demos/linear-scrambled-halton/#comparison-between-lms-ds-and-lms_ds","title":"Comparison between LMS, DS, and LMS_DS:\u00b6","text":""},{"location":"demos/linear-scrambled-halton/#examples-of-linear-matrix-scrambling-with-plots","title":"Examples of Linear Matrix Scrambling with plots:\u00b6","text":""},{"location":"demos/linear-scrambled-halton/#examples-of-digital-shifts-with-plots","title":"Examples of Digital Shifts with plots:\u00b6","text":""},{"location":"demos/linear-scrambled-halton/#examples-of-linear-matrix-scrambling-combined-with-digital-shift-with-plots","title":"Examples of Linear Matrix Scrambling Combined with Digital Shift with plots:\u00b6","text":""},{"location":"demos/linear-scrambled-halton/#speed-comparison-between-different-halton-randomize-options","title":"Speed Comparison Between Different Halton Randomize Options:\u00b6","text":""},{"location":"demos/nei_demo/","title":"Noisy EI","text":"In\u00a0[1]: Copied! <pre>import numpy as np\nimport qmcpy as qp\nfrom scipy.linalg import solve_triangular, cho_solve, cho_factor\nfrom scipy.stats import norm\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nlw = 3\nms = 8\n</pre> import numpy as np import qmcpy as qp from scipy.linalg import solve_triangular, cho_solve, cho_factor from scipy.stats import norm import matplotlib.pyplot as plt %matplotlib inline  lw = 3 ms = 8 <p>We make some fake data and consider the sequential decision-making problem of trying to optimize the function depicted below.</p> In\u00a0[2]: Copied! <pre>def yf(x):\n    return np.cos(10 * x) * np.exp(.2 * x) + np.exp(-5 * (x - .4) ** 2)\n\nxplt = np.linspace(0, 1, 300)\nyplt = yf(xplt)\n\nx = np.array([.1, .2, .4, .7, .9])\ny = yf(x)\nv = np.array([.001, .05, .01, .1, .4])\n\nplt.plot(xplt, yplt, linewidth=lw)\nplt.plot(x, y, 'o', markersize=ms, color='orange')\nplt.errorbar(x, y, yerr=2 * np.sqrt(v), marker='', linestyle='', color='orange', linewidth=3)\nplt.title('Sample data with noise');\n</pre> def yf(x):     return np.cos(10 * x) * np.exp(.2 * x) + np.exp(-5 * (x - .4) ** 2)  xplt = np.linspace(0, 1, 300) yplt = yf(xplt)  x = np.array([.1, .2, .4, .7, .9]) y = yf(x) v = np.array([.001, .05, .01, .1, .4])  plt.plot(xplt, yplt, linewidth=lw) plt.plot(x, y, 'o', markersize=ms, color='orange') plt.errorbar(x, y, yerr=2 * np.sqrt(v), marker='', linestyle='', color='orange', linewidth=3) plt.title('Sample data with noise'); <p>We can build a zero mean Gaussian process model to this data, observed under noise.  Below are plots of the posterior distribution.  We use the Gaussian (square exponential) kernel as our prior covariance belief.</p> <p>This kernel has a shape parameter, the Gaussian process has a global variance, which are both chosen fixed for simplicity.  The <code>fudge_factor</code> which is added here to prevent ill-conditioning for a large matrix.</p> <p>Notice the higher uncertainty in the posterior in locations where the observed noise is greater.</p> In\u00a0[3]: Copied! <pre>def gaussian_kernel(x, z, e, pv):\n    return pv * np.exp(-e ** 2 * (x[:, None] - z[None, :]) ** 2)\n\nshape_parameter = 4.1\nprocess_variance = .9\nfudge_factor = 1e-10\n\nkernel_prior_data = gaussian_kernel(x, x, shape_parameter, process_variance)\nkernel_cross_matrix = gaussian_kernel(xplt, x, shape_parameter, process_variance)\nkernel_prior_plot = gaussian_kernel(xplt, xplt, shape_parameter, process_variance)\n\nprior_cholesky = np.linalg.cholesky(kernel_prior_data + np.diag(v))\npartial_cardinal_functions = solve_triangular(prior_cholesky, kernel_cross_matrix.T, lower=True)\nposterior_covariance = kernel_prior_plot - np.dot(partial_cardinal_functions.T, partial_cardinal_functions)\nposterior_cholesky = np.linalg.cholesky(posterior_covariance + fudge_factor * np.eye(len(xplt)))\n\nfull_cardinal_functions = solve_triangular(prior_cholesky.T, partial_cardinal_functions, lower=False)\nposterior_mean = np.dot(full_cardinal_functions.T, y)\n\nnum_posterior_draws = 123\nnormal_draws = np.random.normal(size=(num_posterior_draws, len(xplt)))\nposterior_draws = posterior_mean[:, None] + np.dot(posterior_cholesky, normal_draws.T)\n\nplt.plot(xplt, posterior_draws, alpha=.1, color='r')\nplt.plot(xplt, posterior_mean, color='k', linewidth=lw)\nplt.errorbar(x, y, yerr=2 * np.sqrt(v), marker='', linestyle='', color='orange', linewidth=3);\n</pre> def gaussian_kernel(x, z, e, pv):     return pv * np.exp(-e ** 2 * (x[:, None] - z[None, :]) ** 2)  shape_parameter = 4.1 process_variance = .9 fudge_factor = 1e-10  kernel_prior_data = gaussian_kernel(x, x, shape_parameter, process_variance) kernel_cross_matrix = gaussian_kernel(xplt, x, shape_parameter, process_variance) kernel_prior_plot = gaussian_kernel(xplt, xplt, shape_parameter, process_variance)  prior_cholesky = np.linalg.cholesky(kernel_prior_data + np.diag(v)) partial_cardinal_functions = solve_triangular(prior_cholesky, kernel_cross_matrix.T, lower=True) posterior_covariance = kernel_prior_plot - np.dot(partial_cardinal_functions.T, partial_cardinal_functions) posterior_cholesky = np.linalg.cholesky(posterior_covariance + fudge_factor * np.eye(len(xplt)))  full_cardinal_functions = solve_triangular(prior_cholesky.T, partial_cardinal_functions, lower=False) posterior_mean = np.dot(full_cardinal_functions.T, y)  num_posterior_draws = 123 normal_draws = np.random.normal(size=(num_posterior_draws, len(xplt))) posterior_draws = posterior_mean[:, None] + np.dot(posterior_cholesky, normal_draws.T)  plt.plot(xplt, posterior_draws, alpha=.1, color='r') plt.plot(xplt, posterior_mean, color='k', linewidth=lw) plt.errorbar(x, y, yerr=2 * np.sqrt(v), marker='', linestyle='', color='orange', linewidth=3); <p>First we take a look at the EI quantity by itself which, despite having a closed form, we will approximate using basic Monte Carlo below.  The closed form is very preferable, but not applicable in all situations.</p> <p>Expected improvement is just the expectation (under the posterior distribution) of the improvement beyond the current best value.  If we were trying to maximize this function that we are studying then improvement would be defined as</p> <p>$$I(x) = (Y_x|\\mathcal{D} - y^*)_+,$$</p> <p>the positive part of the gap between the model $Y_x|\\mathcal{D}$ and the current highest value $y^*=\\max\\{y_1,\\ldots,y_N\\}$.  Since $Y_x|\\mathcal{D}$ is a random variable (normally distributed because we have a Gaussian process model), we generally study the expected value of this, which is plotted below.  Written as an integral, this would look like</p> <p>$$\\mathrm{EI}(x) = \\int_{-\\infty}^\\infty (y - y^*)_+\\, p_{Y_x|\\mathcal{D}}(y)\\; \\text{d}y$$</p> <p>NOTE: This quantity is written for maximization here, but most of the literature is concerned with minimization.  We can rewrite this if needed, but the math is essentially the same.</p> <p>This $EI$ quantity is referred to as an acquisition function, a function which defines the utility associated with sampling at a given point.  For each acquisition function, there is a balance between exploration and exploitation (as is the focus of most topics involving sequential decision-making under uncertainty).</p> In\u00a0[4]: Copied! <pre>improvement_draws = np.fmax(posterior_draws - max(y), 0)\nplt.plot(xplt, improvement_draws, alpha=.1, color='#96CA4F', linewidth=lw)\nplt.ylabel('improvement draws')\nax2 = plt.gca().twinx()\nax2.plot(xplt, np.mean(improvement_draws, axis=1), color='#A23D97', linewidth=lw)\nax2.set_ylabel('expected improvement');\n</pre> improvement_draws = np.fmax(posterior_draws - max(y), 0) plt.plot(xplt, improvement_draws, alpha=.1, color='#96CA4F', linewidth=lw) plt.ylabel('improvement draws') ax2 = plt.gca().twinx() ax2.plot(xplt, np.mean(improvement_draws, axis=1), color='#A23D97', linewidth=lw) ax2.set_ylabel('expected improvement'); <p>The NEI quantity is then computed using multiple EI computations (each using a different posterior GP draw) computed without noise.  In this computation below, we will use the closed form of EI, to speed up the computation -- it is possible to execute the same strategy as above, though.</p> <p>This computation is vectorized so as to compute for multiple $x$ locations at the same time. The algorithm from the Facebook paper is written for only a single location.  We are omitting the constraints aspect of their paper because the problem can be considered without that.  To define the integral, though, we need some more definitions/notation.</p> <p>First, we need to define $\\mathrm{EI}(x;\\mathbf{y}, \\mathcal{X}, \\boldsymbol{\\epsilon})$ to be the expected improvement at a location $x$, given the $N$ values stored in the vector $\\mathbf{y}$ having been evaluated with noise $\\boldsymbol{\\epsilon}$ at the points $\\mathcal{X}$,</p> <p>$$ \\mathbf{y}=\\begin{pmatrix}y_1\\\\\\vdots\\\\y_N\\end{pmatrix},\\qquad \\mathcal{X}=\\{\\mathbf{x}_1,\\ldots,\\mathbf{x}_N\\},\\qquad \\boldsymbol{\\epsilon}=\\begin{pmatrix}\\epsilon_1\\\\\\vdots\\\\\\epsilon_N\\end{pmatrix}. $$</p> <p>The noise is assumed to be $\\epsilon_i\\sim\\mathcal{N}(0, \\sigma^2)$ for some fixed $\\sigma^2$.  The noise need not actually be homoscedastic, but it is a standard assumption. We encapsulate this information in $\\mathcal{D}=\\{\\mathbf{y},\\mathcal{X},\\boldsymbol{\\epsilon}\\}$.  This is omitted from the earlier notation, because the data would be fixed.</p> <p>The point of NEI though is to deal with noisy observed values (EI, itself, is notorious for not dealing with noisy data very well).  It does this by considering a variety of posterior draws at the locations in $\\mathcal{X}$.  These have distribution</p> <p>$$ Y_{\\mathcal{X}}|\\mathcal{D}=Y_{\\mathcal{X}}|\\mathbf{y}, \\mathcal{X}, \\boldsymbol{\\epsilon}\\sim \\mathcal{N}\\left(\\mathsf{K}(\\mathsf{K}+\\mathsf{E})^{-1}\\mathbf{y}, \\mathsf{K} - \\mathsf{K}(\\mathsf{K}+\\mathsf{E})^{-1}\\mathsf{K}\\right), $$</p> <p>where</p> <p>$$ \\mathbf{k}(x)=\\begin{pmatrix}K(x,x_1)\\\\\\vdots\\\\K(x,x_N)\\end{pmatrix},\\qquad \\mathsf{K}=\\begin{pmatrix} K(x_1,x_1)&amp;\\cdots&amp;K(x_1, x_N)\\\\&amp;\\vdots&amp;\\\\K(x_N,x_1)&amp;\\cdots&amp;K(x_N, x_N) \\end{pmatrix}=\\begin{pmatrix}\\mathbf{k}(x_1)^T\\\\\\vdots\\\\\\mathbf{k}(x_N)^T\\end{pmatrix},\\qquad \\mathsf{E}=\\begin{pmatrix}\\epsilon_1&amp;&amp;\\\\&amp;\\ddots&amp;\\\\&amp;&amp;\\epsilon_N\\end{pmatrix} $$</p> <p>In practice, unless noise has actually been measured at each point, it would be common to simply plug in $\\epsilon_1=\\ldots=\\epsilon_N=\\sigma^2$.  The term <code>noisy_predictions_at_data</code> below is drawn from this distribution (though in a standard iid fashion, not a more awesome QMC fashion).</p> <p>The EI integral, although approximated earlier using Monte Carlo, can actually be written in closed form.  We do so below to also solidify our newer notation:</p> <p>$$ \\mathrm{EI}(x;\\mathbf{y}, \\mathcal{X}, \\boldsymbol{\\epsilon}) = \\int_{-\\infty}^\\infty (y - y^*)_+\\, p_{Y_x|\\mathbf{y}, \\mathcal{X}, \\boldsymbol{\\epsilon}}(y)\\; \\text{d}y = s(z\\Phi(z)+\\phi(z)) $$</p> <p>where $\\phi$ and $\\Phi$ are the standard normal pdf and cdf, and</p> <p>$$ \\mu=\\mathbf{k}(x)^T(\\mathsf{K}+\\mathsf{E})^{-1}\\mathbf{y},\\qquad s^2 = K(x, x)-\\mathbf{k}(x)^T(\\mathsf{K}+\\mathsf{E})^{-1}\\mathbf{k}(x),\\qquad z=(\\mu - y^*)/s. $$</p> <p>It is very important to remember that these quantities are functions of $\\mathbf{y},\\mathcal{X},\\boldsymbol{\\epsilon}$ despite the absence of those quantities in the notation.</p> <p>The goal of the NEI integral is to simulate many possible random realizations of what could actually be the truth at the locations $\\mathcal{X}$ and then run a noiseless EI computation over each of those realizations.  The average of these outcomes is the NEI quantity.  This would look like:</p> <p>$$ \\mathrm{NEI}(x) = \\int_{\\mathbf{f}\\in\\mathbb{R}^N} \\mathrm{EI}(x;\\mathbf{f}, \\mathcal{X}, 0)\\, p_{Y_{\\mathcal{X}}|\\mathbf{y},\\mathcal{X},\\boldsymbol{\\epsilon}}(\\mathbf{f})\\;\\text{d}\\mathbf{f} $$</p> <p>NOTE: There are ways to do this computation in a more vectorized fashion, so it would more likely be a loop involving chunks of MC elements at a time.  Just so you know.</p> In\u00a0[5]: Copied! <pre>num_draws_at_data = 109\n# These draws are done through QMC in the FB paper\nnormal_draws_at_data = np.random.normal(size=(num_draws_at_data, len(x)))\n\npartial_cardinal_functions_at_data = solve_triangular(prior_cholesky, kernel_prior_data.T, lower=True)\nposterior_covariance_at_data = kernel_prior_data - np.dot(partial_cardinal_functions_at_data.T, partial_cardinal_functions_at_data)\nposterior_cholesky_at_data = np.linalg.cholesky(posterior_covariance_at_data + fudge_factor * np.eye(len(x)))\n\nnoisy_predictions_at_data = y[:, None] + np.dot(posterior_cholesky_at_data, normal_draws_at_data.T)\n\nprior_cholesky_noiseless = np.linalg.cholesky(kernel_prior_data)\npartial_cardinal_functions = solve_triangular(prior_cholesky_noiseless, kernel_cross_matrix.T, lower=True)\nfull_cardinal_functions = solve_triangular(prior_cholesky.T, partial_cardinal_functions, lower=False)\npointwise_sd = np.sqrt(np.fmax(process_variance - np.sum(partial_cardinal_functions ** 2, axis=0), 1e-100))\n\nall_noiseless_eis = []\nfor draw in noisy_predictions_at_data.T:\n    posterior_mean = np.dot(full_cardinal_functions.T, draw)\n    \n    z = (posterior_mean - max(y)) / pointwise_sd\n    ei = pointwise_sd * (z * norm.cdf(z) + norm.pdf(z))\n    \n    all_noiseless_eis.append(ei)\n\nall_noiseless_eis = np.array(all_noiseless_eis)\n\nplt.plot(xplt, all_noiseless_eis.T, alpha=.1, color='#96CA4F', linewidth=lw)\nplt.ylabel('expected improvement draws', color='#96CA4F')\nax2 = plt.gca().twinx()\nax2.plot(xplt, np.mean(all_noiseless_eis, axis=0), color='#A23D97', linewidth=lw)\nax2.set_ylabel('noisy expected improvement', color='#A23D97');\n</pre> num_draws_at_data = 109 # These draws are done through QMC in the FB paper normal_draws_at_data = np.random.normal(size=(num_draws_at_data, len(x)))  partial_cardinal_functions_at_data = solve_triangular(prior_cholesky, kernel_prior_data.T, lower=True) posterior_covariance_at_data = kernel_prior_data - np.dot(partial_cardinal_functions_at_data.T, partial_cardinal_functions_at_data) posterior_cholesky_at_data = np.linalg.cholesky(posterior_covariance_at_data + fudge_factor * np.eye(len(x)))  noisy_predictions_at_data = y[:, None] + np.dot(posterior_cholesky_at_data, normal_draws_at_data.T)  prior_cholesky_noiseless = np.linalg.cholesky(kernel_prior_data) partial_cardinal_functions = solve_triangular(prior_cholesky_noiseless, kernel_cross_matrix.T, lower=True) full_cardinal_functions = solve_triangular(prior_cholesky.T, partial_cardinal_functions, lower=False) pointwise_sd = np.sqrt(np.fmax(process_variance - np.sum(partial_cardinal_functions ** 2, axis=0), 1e-100))  all_noiseless_eis = [] for draw in noisy_predictions_at_data.T:     posterior_mean = np.dot(full_cardinal_functions.T, draw)          z = (posterior_mean - max(y)) / pointwise_sd     ei = pointwise_sd * (z * norm.cdf(z) + norm.pdf(z))          all_noiseless_eis.append(ei)  all_noiseless_eis = np.array(all_noiseless_eis)  plt.plot(xplt, all_noiseless_eis.T, alpha=.1, color='#96CA4F', linewidth=lw) plt.ylabel('expected improvement draws', color='#96CA4F') ax2 = plt.gca().twinx() ax2.plot(xplt, np.mean(all_noiseless_eis, axis=0), color='#A23D97', linewidth=lw) ax2.set_ylabel('noisy expected improvement', color='#A23D97'); <p>Even the EI integral, which does have a closed form, might better be considered in a QMC fashion because of interesting use cases.  We are going to reconsider the same problem from above, but here we am not looking to maximize the function -- we want to find the \"level set\" associated with the value $y=1$.  Below you can see how the different outcome might look.</p> <p>In this case, the quantity of relevance is not exactly an integral, but it is a function of this posterior mean and standard deviation, which might need to be estimated through an integral (rather than the closed form, which we do have for a GP situation).</p> In\u00a0[6]: Copied! <pre>fig, axes = plt.subplots(1, 3, figsize=(14, 4))\n\nax = axes[0]\nax.plot(xplt, yplt, linewidth=lw)\nax.plot(x, y, 'o', markersize=ms, color='orange')\nax.errorbar(x, y, yerr=2 * np.sqrt(v), marker='', linestyle='', color='orange', linewidth=3)\nax.set_title('Sample data with noise')\nax.set_ylim(-2.4, 2.4)\n\nax = axes[1]\nax.plot(xplt, posterior_draws, alpha=.1, color='r')\nax.plot(xplt, posterior_mean, color='k', linewidth=lw)\nax.set_title('Posterior draws')\nax.set_ylim(-2.4, 2.4)\n\nax = axes[2]\nposterior_mean_distance_from_1 = np.mean(np.abs(posterior_draws - 1), axis=1)\nposterior_standard_deviation = np.std(posterior_draws, axis=1)\nlevel_set_expected_improvement = norm.cdf(-posterior_mean_distance_from_1 / posterior_standard_deviation)\nax.plot(xplt, level_set_expected_improvement, color='#A23D97', linewidth=lw)\nax.set_title('level set expected improvement')\n\nplt.tight_layout();\n</pre> fig, axes = plt.subplots(1, 3, figsize=(14, 4))  ax = axes[0] ax.plot(xplt, yplt, linewidth=lw) ax.plot(x, y, 'o', markersize=ms, color='orange') ax.errorbar(x, y, yerr=2 * np.sqrt(v), marker='', linestyle='', color='orange', linewidth=3) ax.set_title('Sample data with noise') ax.set_ylim(-2.4, 2.4)  ax = axes[1] ax.plot(xplt, posterior_draws, alpha=.1, color='r') ax.plot(xplt, posterior_mean, color='k', linewidth=lw) ax.set_title('Posterior draws') ax.set_ylim(-2.4, 2.4)  ax = axes[2] posterior_mean_distance_from_1 = np.mean(np.abs(posterior_draws - 1), axis=1) posterior_standard_deviation = np.std(posterior_draws, axis=1) level_set_expected_improvement = norm.cdf(-posterior_mean_distance_from_1 / posterior_standard_deviation) ax.plot(xplt, level_set_expected_improvement, color='#A23D97', linewidth=lw) ax.set_title('level set expected improvement')  plt.tight_layout(); In\u00a0[7]: Copied! <pre>q = 5  # number of \"next points\" to be considered simultaneously\nnext_x = np.array([0.158,  0.416,  0.718,  0.935,  0.465])\n\ndef compute_qei(next_x, mc_strat, num_posterior_draws):\n    q = len(next_x)\n    \n    kernel_prior_data = gaussian_kernel(x, x, shape_parameter, process_variance)\n    kernel_cross_matrix = gaussian_kernel(next_x, x, shape_parameter, process_variance)\n    kernel_prior_plot = gaussian_kernel(next_x, next_x, shape_parameter, process_variance)\n    prior_cholesky = np.linalg.cholesky(kernel_prior_data + np.diag(v))\n    \n    partial_cardinal_functions = solve_triangular(prior_cholesky, kernel_cross_matrix.T, lower=True)\n    posterior_covariance = kernel_prior_plot - np.dot(partial_cardinal_functions.T, partial_cardinal_functions)\n    posterior_cholesky = np.linalg.cholesky(posterior_covariance + fudge_factor * np.eye(q))\n    \n    full_cardinal_functions = solve_triangular(prior_cholesky.T, partial_cardinal_functions, lower=False)\n    posterior_mean = np.dot(full_cardinal_functions.T, y)\n        \n    if mc_strat == 'numpy':\n        normal_draws = np.random.normal(size=(num_posterior_draws, q))\n    elif mc_strat == 'lattice':\n        g =  qp.Gaussian(qp.Lattice(dimension=q, randomize=True))\n        normal_draws = g.gen_samples(n=num_posterior_draws)\n    else:\n        g =  qp.Gaussian(qp.IIDStdUniform(dimension=q))\n        normal_draws = g.gen_samples(n = num_posterior_draws)\n    posterior_draws = posterior_mean[:, None] + np.dot(posterior_cholesky, normal_draws.T)\n    \n    return np.mean(np.fmax(np.max(posterior_draws[:, :num_posterior_draws] - max(y), axis=0), 0))\n</pre> q = 5  # number of \"next points\" to be considered simultaneously next_x = np.array([0.158,  0.416,  0.718,  0.935,  0.465])  def compute_qei(next_x, mc_strat, num_posterior_draws):     q = len(next_x)          kernel_prior_data = gaussian_kernel(x, x, shape_parameter, process_variance)     kernel_cross_matrix = gaussian_kernel(next_x, x, shape_parameter, process_variance)     kernel_prior_plot = gaussian_kernel(next_x, next_x, shape_parameter, process_variance)     prior_cholesky = np.linalg.cholesky(kernel_prior_data + np.diag(v))          partial_cardinal_functions = solve_triangular(prior_cholesky, kernel_cross_matrix.T, lower=True)     posterior_covariance = kernel_prior_plot - np.dot(partial_cardinal_functions.T, partial_cardinal_functions)     posterior_cholesky = np.linalg.cholesky(posterior_covariance + fudge_factor * np.eye(q))          full_cardinal_functions = solve_triangular(prior_cholesky.T, partial_cardinal_functions, lower=False)     posterior_mean = np.dot(full_cardinal_functions.T, y)              if mc_strat == 'numpy':         normal_draws = np.random.normal(size=(num_posterior_draws, q))     elif mc_strat == 'lattice':         g =  qp.Gaussian(qp.Lattice(dimension=q, randomize=True))         normal_draws = g.gen_samples(n=num_posterior_draws)     else:         g =  qp.Gaussian(qp.IIDStdUniform(dimension=q))         normal_draws = g.gen_samples(n = num_posterior_draws)     posterior_draws = posterior_mean[:, None] + np.dot(posterior_cholesky, normal_draws.T)          return np.mean(np.fmax(np.max(posterior_draws[:, :num_posterior_draws] - max(y), axis=0), 0)) In\u00a0[8]: Copied! <pre>num_posterior_draws_to_test = 2 ** np.arange(4, 17)\ntrials = 10\n\nvals = {}\nfor mc_strat in ('numpy', 'iid', 'lattice'):\n    vals[mc_strat] = []\n\n    for num_posterior_draws in num_posterior_draws_to_test:\n        qei_estimate = 0.\n        for trial in range(trials):\n            qei_estimate += compute_qei(next_x, mc_strat, num_posterior_draws)\n        avg_qei_estimate = qei_estimate/float(trials)\n        vals[mc_strat].append(avg_qei_estimate)\n\n    vals[mc_strat] = np.array(vals[mc_strat])\n#reference_answer = compute_qei(next_x, 'lattice', 2 ** 7 * max(num_posterior_draws_to_test))\nreference_answer = compute_qei(next_x, 'lattice', 2 ** 20)\n</pre> num_posterior_draws_to_test = 2 ** np.arange(4, 17) trials = 10  vals = {} for mc_strat in ('numpy', 'iid', 'lattice'):     vals[mc_strat] = []      for num_posterior_draws in num_posterior_draws_to_test:         qei_estimate = 0.         for trial in range(trials):             qei_estimate += compute_qei(next_x, mc_strat, num_posterior_draws)         avg_qei_estimate = qei_estimate/float(trials)         vals[mc_strat].append(avg_qei_estimate)      vals[mc_strat] = np.array(vals[mc_strat]) #reference_answer = compute_qei(next_x, 'lattice', 2 ** 7 * max(num_posterior_draws_to_test)) reference_answer = compute_qei(next_x, 'lattice', 2 ** 20) In\u00a0[9]: Copied! <pre>for name, results in vals.items():\n    plt.loglog(num_posterior_draws_to_test, abs(results - reference_answer), label=name)\nplt.loglog(num_posterior_draws_to_test, .05 * num_posterior_draws_to_test ** -.5, '--k', label='$O(N^{-1/2})$')\nplt.loglog(num_posterior_draws_to_test, .3 * num_posterior_draws_to_test ** -1.0, '-.k', label='$O(N^{-1})$')\nplt.xlabel('N - number of points')\nplt.ylabel('Accuracy')\nplt.legend(loc='lower left');\n</pre> for name, results in vals.items():     plt.loglog(num_posterior_draws_to_test, abs(results - reference_answer), label=name) plt.loglog(num_posterior_draws_to_test, .05 * num_posterior_draws_to_test ** -.5, '--k', label='$O(N^{-1/2})$') plt.loglog(num_posterior_draws_to_test, .3 * num_posterior_draws_to_test ** -1.0, '-.k', label='$O(N^{-1})$') plt.xlabel('N - number of points') plt.ylabel('Accuracy') plt.legend(loc='lower left'); <p>This is very similar to what the FB paper talked about, and we think exactly the kind of thing we should be emphasizing in our discussions in a potential blog post which talks about BO applications of QMC.</p> <p>Such a blog post is something that we would be happy to write up, by the way.</p> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"demos/nei_demo/#nei-noisy-expected-improvement-demo","title":"NEI (Noisy Expected Improvement) Demo\u00b6","text":"<p>You can also look at the Botorch implementation, but that requires a lot more understanding of code which involves Pytorch.  So we tried to put a simple example together here.</p>"},{"location":"demos/nei_demo/#goal","title":"Goal\u00b6","text":"<p>What would be really great would be if we could compute integrals like the EI integral or the NEI integral using QMC. If there are opportunities to use the latest research to adaptively study tolerance and truncate, that would be absolutely amazing.</p> <p>We put the NEI example up first because the FB crew has already done a great job showing how QMC can play a role.  But, as you can see, NEI is more complicated than EI, and also not yet as popular in the community (though that may change).</p>"},{"location":"demos/nei_demo/#bonus-stuff","title":"Bonus stuff\u00b6","text":""},{"location":"demos/nei_demo/#computation-of-the-qei-quantity-using-qmcpy","title":"Computation of the QEI quantity using <code>qmcpy</code>\u00b6","text":"<p>NEI is an important quantity, but there are other quantities as well which could be considered relevant demonstrations of higher dimensional integrals.</p> <p>One such quantity is a computation involving $q$ \"next points\" to sample in a BO process; in the standard formulation this quantity might involve just $q=1$, but $q&gt;1$ is also of interest for batched evaluation in parallel.</p> <p>This quantity is defined as</p> <p>$$ \\mathrm{EI}_q(x_1, \\ldots, x_q;\\mathbf{y}, \\mathcal{X}, \\boldsymbol{\\epsilon}) = \\int_{\\mathbb{R}^q} \\max_{1\\leq i\\leq q}\\left[{(y_i - y^*)_+}\\right]\\, p_{Y_{x_1,\\ldots, x_q}|\\mathbf{y}, \\mathcal{X}, \\boldsymbol{\\epsilon}}(y_1, \\ldots, y_q)\\; \\text{d}y_1\\cdots\\text{d}y_q $$</p> <p>The example we are considering here is with $q=5$ but this quantity could be made larger.  Each of these QEI computations (done in a vectorized fashion in production) would be needed in an optimization loop (likely powered by CMAES or some other high dimensional non-convex optimization tool).  This optimization problem would take place in a $qd$ dimensional space, which is one aspect which usually prevents $q$ from being too large.</p> <p>Note that some of this will look much more confusing in $d&gt;1$, but it is written here in a simplified version.</p>"},{"location":"demos/plot_proj_function/","title":"Plotting Points Automatically","text":"<p>This notebook demonstrates the different usages of the plot_proj function for Discrete Distribution and True Measure</p> <p>A Discrete Distribution or True Measure object with d dimensions has a maximum of $d\\times(d-1)$ dimensional pairings (for e.g: [2,3] and [3,2] are being considered seperate parings). The plot_proj function plots all or a subset of all the possible dimension pairings using the parameters d_vertical and d_horizontal and can also display extensibility based on the parameter n. Extensibility occurs when we want to plot the Discrete Distribution or True Measure Object with successively increasing numbers of points. Each set of points is displayed in a different color. This extensibility helps us see how the space fills up.</p> In\u00a0[1]: Copied! <pre>import qmcpy as qp\n</pre> import qmcpy as qp In\u00a0[2]: Copied! <pre>help(qp.plot_proj)\n</pre> help(qp.plot_proj) <pre>Help on function plot_proj in module qmcpy.util.plot_functions:\n\nplot_proj(sampler, n=64, d_horizontal=1, d_vertical=2, math_ind=True, marker_size=5, figfac=5, fig_title='Projection of Samples', axis_pad=0, want_grid=True, font_family='sans-serif', where_title=1, **kwargs)\n    Args:\n        sampler: the Discrete Distribution or the True Measure Object to be plotted\n        n (int or list): the number of samples or a list of samples(used for extensibility) to be plotted. \n            Default value is 64\n        d_horizontal (int or list): the dimension or list of dimensions to be plotted on the horizontal axes. \n            Default value is 1 (1st dimension).\n        d_vertical (int or list): the dimension or list of dimensions to be plotted on the vertical axes. \n            Default value is 2 (2nd dimension).\n        math_ind : setting it true will enable user to pass in math indices. \n            Default value is true, so user is required to pass in math indices.\n        marker_size: the marker size in points**2(typographic points are 1/72 in.).\n            Default value is 5.\n        figfac: the figure size factor. Default value is 5.\n        fig_title: the title of the figure. Default value is 'Projection of Samples'\n        axis_pad: the padding of the axis so that points on the boundaries can be seen. Default value is 0.\n        want_grid: setting it true will enable grid on the plot. Default value is true.\n        font_family: the font family of the plot. Default value is \"sans-serif\".\n        where_title: the position of the title on the plot. Default value is 1.\n        **kwargs : Any extra features the user would like to see in the plots\n\n</pre> <p>Here we show a two dimensional projection of an IID Object</p> <p>We have given a more descriptive title.</p> In\u00a0[3]: Copied! <pre>d = 2\niid = qp.IIDStdUniform(d)\nfig,ax = qp.plot_proj(iid, n = 2**7, fig_title =\"IID samples\")\n</pre> d = 2 iid = qp.IIDStdUniform(d) fig,ax = qp.plot_proj(iid, n = 2**7, fig_title =\"IID samples\") <p>Here we show a two dimensional projection of an LD Halton object with successively increasing numbers of points.  The initial points are in blue. The next additional points are in orange. The final additional points are in green :</p> <p>If we want to adjust the title more tightly, we can with the where_title parameter</p> In\u00a0[4]: Copied! <pre>d = 4\nhalton = qp.Halton(d)\nfig,ax = qp.plot_proj(halton, n = [2**5, 2**6, 2**7],fig_title =\"Halton samples\",where_title = 0.95)\n</pre> d = 4 halton = qp.Halton(d) fig,ax = qp.plot_proj(halton, n = [2**5, 2**6, 2**7],fig_title =\"Halton samples\",where_title = 0.95) <p>Here we show a four dimensional projection of a LD Digital Net Object : We need to adjust the placement of the title.</p> <p>We also turned off the grid</p> In\u00a0[5]: Copied! <pre>d = 4\nnet = qp.DigitalNetB2(d)\nfig, ax = qp.plot_proj(net, n = 2**7, d_horizontal = range(d), d_vertical = range(d), math_ind = False, marker_size = 15, want_grid = False)\n</pre> d = 4 net = qp.DigitalNetB2(d) fig, ax = qp.plot_proj(net, n = 2**7, d_horizontal = range(d), d_vertical = range(d), math_ind = False, marker_size = 15, want_grid = False) <p>Here we show certain specified dimensional projections (dimensions 1 and 3 on the x axis, dimensions 2 and 4 on the y axis) of a LD Digital Net Object:</p> In\u00a0[6]: Copied! <pre>d = 4\nnet = qp.DigitalNetB2(d)\nfig, ax = qp.plot_proj(net, n = 2**7, d_horizontal = [1,3], d_vertical = [2,4])\n</pre> d = 4 net = qp.DigitalNetB2(d) fig, ax = qp.plot_proj(net, n = 2**7, d_horizontal = [1,3], d_vertical = [2,4]) <p>Here we show a five dimensional projection of a LD Lattice object with successively increasing numbers of points.  The initial points are in blue. The next additional points are in orange. The final additional points are in green :</p> <p>So that points near the boundary can be seen, we add some padding</p> In\u00a0[7]: Copied! <pre>d = 5\nlattice = qp.Lattice(d,generating_vector=\"https://raw.githubusercontent.com/QMCSoftware/LDData/refs/heads/kuo_lattice_dnet/lattice/kuo.lattice-33002-1024-1048576.9125.txt\")\nfig, ax = qp.plot_proj(lattice, n = [2**6, 2**7, 2**8], d_horizontal = range(d), d_vertical = range(d), math_ind = False, marker_size = 15, axis_pad = 0.05)\n</pre> d = 5 lattice = qp.Lattice(d,generating_vector=\"https://raw.githubusercontent.com/QMCSoftware/LDData/refs/heads/kuo_lattice_dnet/lattice/kuo.lattice-33002-1024-1048576.9125.txt\") fig, ax = qp.plot_proj(lattice, n = [2**6, 2**7, 2**8], d_horizontal = range(d), d_vertical = range(d), math_ind = False, marker_size = 15, axis_pad = 0.05) <p>Here we show a two dimensional projection of a Gaussian Object and how the axes returned by the plot_proj function can be manipulated by adding a horizontal and vertical line to denote the x and y axis respectively:</p> In\u00a0[8]: Copied! <pre>d = 2\niid = qp.IIDStdUniform(d)\niid_gaussian = qp.Gaussian(iid,mean =[2,4],covariance=[[9,4],[4,5]])\nfig,ax = qp.plot_proj(iid_gaussian, n = 2**7)\nax[0,0].axvline(x=0,color= 'k',alpha=.25); #adding vertical line\nax[0,0].axhline(y=0,color= 'k',alpha=.25); #adding horizontal line\n</pre> d = 2 iid = qp.IIDStdUniform(d) iid_gaussian = qp.Gaussian(iid,mean =[2,4],covariance=[[9,4],[4,5]]) fig,ax = qp.plot_proj(iid_gaussian, n = 2**7) ax[0,0].axvline(x=0,color= 'k',alpha=.25); #adding vertical line ax[0,0].axhline(y=0,color= 'k',alpha=.25); #adding horizontal line <p>Here we show a two dimensional projection of a Gaussian Object with successively increasing numbers of points.  The initial points are in blue. The next additional points are in orange. The final additional points are in green :</p> In\u00a0[9]: Copied! <pre>d = 2\niid = qp.IIDStdUniform(d)\niid_gaussian = qp.Gaussian(iid,mean =[2,4],covariance=[[9,4],[4,5]])\nfig,ax = qp.plot_proj(iid_gaussian, n = [2**5,2**6,2**7], fig_title = \"Gaussian Distribution\")\n</pre> d = 2 iid = qp.IIDStdUniform(d) iid_gaussian = qp.Gaussian(iid,mean =[2,4],covariance=[[9,4],[4,5]]) fig,ax = qp.plot_proj(iid_gaussian, n = [2**5,2**6,2**7], fig_title = \"Gaussian Distribution\") <p>Here we show a four dimensional projection of a SciPyWrapper Object:</p> In\u00a0[10]: Copied! <pre>import scipy.stats\nd = 4\nnet = qp.DigitalNetB2(d)\nnet_sci_py_wrapper = qp.SciPyWrapper(net,[scipy.stats.beta(a=1,b=5),scipy.stats.gamma(a=1),scipy.stats.uniform(loc = 1,scale = 2),scipy.stats.norm(loc = 3, scale = 4)])\nfig, ax = qp.plot_proj(net_sci_py_wrapper, n = 2**7, d_horizontal = range(d), d_vertical = range(d), math_ind= False, marker_size = 15)\n</pre> import scipy.stats d = 4 net = qp.DigitalNetB2(d) net_sci_py_wrapper = qp.SciPyWrapper(net,[scipy.stats.beta(a=1,b=5),scipy.stats.gamma(a=1),scipy.stats.uniform(loc = 1,scale = 2),scipy.stats.norm(loc = 3, scale = 4)]) fig, ax = qp.plot_proj(net_sci_py_wrapper, n = 2**7, d_horizontal = range(d), d_vertical = range(d), math_ind= False, marker_size = 15) <p>Here we show certain specified dimensional projections (dimensions 1 and 3 on the x axis, dimensions 2 and 4 on the y axis) of a SciPyWrapper Object:</p> In\u00a0[11]: Copied! <pre>import scipy.stats\nd = 4\nnet = qp.DigitalNetB2(d)\nnet_sci_py_wrapper = qp.SciPyWrapper(net,[scipy.stats.beta(a=1,b=5),scipy.stats.gamma(a=1),scipy.stats.uniform(loc = 1,scale = 2),scipy.stats.norm(loc = 3, scale = 4)])\nfig, ax = qp.plot_proj(net_sci_py_wrapper, n = 2**7, d_horizontal = [1,3], d_vertical = [2,4])\n</pre> import scipy.stats d = 4 net = qp.DigitalNetB2(d) net_sci_py_wrapper = qp.SciPyWrapper(net,[scipy.stats.beta(a=1,b=5),scipy.stats.gamma(a=1),scipy.stats.uniform(loc = 1,scale = 2),scipy.stats.norm(loc = 3, scale = 4)]) fig, ax = qp.plot_proj(net_sci_py_wrapper, n = 2**7, d_horizontal = [1,3], d_vertical = [2,4]) <p>Here we show a four dimensional projection of a Uniform object with successively increasing numbers of points.  The initial points are in blue. The next additional points are in orange. The final additional points are in green :</p> In\u00a0[12]: Copied! <pre>d = 4\nhalton = qp.Halton(d)\nhalton_uniform = qp.Uniform(halton,lower_bound=[1,2,3,4],upper_bound=[5,7,9,11])\nfig, ax = qp.plot_proj(halton_uniform, n = [2**6, 2**7, 2**8], d_horizontal = range(d), d_vertical = range(d), math_ind = False, marker_size = 15)\n</pre> d = 4 halton = qp.Halton(d) halton_uniform = qp.Uniform(halton,lower_bound=[1,2,3,4],upper_bound=[5,7,9,11]) fig, ax = qp.plot_proj(halton_uniform, n = [2**6, 2**7, 2**8], d_horizontal = range(d), d_vertical = range(d), math_ind = False, marker_size = 15) In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"demos/plot_proj_function/#the-qmcpy-plot-projection-function-for-discrete-distribution-and-true-measure","title":"The QMCPY Plot Projection Function for Discrete Distribution and True Measure\u00b6","text":""},{"location":"demos/plot_proj_function/#here-we-set-up-the-qmcpy-environment-enabling-us-to-utilize-this-function","title":"Here we set up the QMCPY environment enabling us to utilize this function:\u00b6","text":""},{"location":"demos/plot_proj_function/#here-we-explain-the-parameters-of-the-plot_proj-function","title":"Here we explain the parameters of the plot_proj function:\u00b6","text":""},{"location":"demos/plot_proj_function/#the-following-examples-show-plotting-of-different-discrete-distribution-objects","title":"The following examples show plotting of different Discrete Distribution Objects:\u00b6","text":""},{"location":"demos/plot_proj_function/#the-following-examples-show-plotting-of-different-true-measure-objects","title":"The following examples show plotting of different True Measure Objects:\u00b6","text":""},{"location":"demos/pricing_options/","title":"Pricing Options","text":"In\u00a0[36]: Copied! <pre># Import necessary packages\nimport qmcpy as qp\nimport numpy as np\nfrom scipy import stats\nimport matplotlib.pyplot as plt\nimport time\n</pre> # Import necessary packages import qmcpy as qp import numpy as np from scipy import stats import matplotlib.pyplot as plt import time In\u00a0[37]: Copied! <pre>initPrice = 120 # initial stock price\ninterest = 0.02 # risk-free interest rate\nvol = 0.5 # volatility\nstrike = 130 # strike price\ntfinal = 1/4 # mature time\nd = 12 # number of observations\nabsTol = 0.05 # absolute tolerance of a nickel\nrelTol = 0 # zero relative tolerance\nsampleSize = 10**6 # number of smaple size\n</pre> initPrice = 120 # initial stock price interest = 0.02 # risk-free interest rate vol = 0.5 # volatility strike = 130 # strike price tfinal = 1/4 # mature time d = 12 # number of observations absTol = 0.05 # absolute tolerance of a nickel relTol = 0 # zero relative tolerance sampleSize = 10**6 # number of smaple size  In\u00a0[38]: Copied! <pre>EuroCall = qp.FinancialOption(\n    qp.IIDStdUniform(dimension=d,seed=7),\n    option=\"EUROPEAN\",\n    volatility=vol,\n    start_price=initPrice,\n    strike_price=strike,\n    interest_rate=interest,\n    t_final=tfinal,\n    call_put=\"call\")\ny = EuroCall(sampleSize)\nprint(\"The exact price of this European Call Option is \",f\"{EuroCall.get_exact_value():.4f}\")\nprint(\"After generate \", sampleSize,\"iid points, the price of estimation of the fair price is\",f\"{y.mean():.4f}\")\n</pre> EuroCall = qp.FinancialOption(     qp.IIDStdUniform(dimension=d,seed=7),     option=\"EUROPEAN\",     volatility=vol,     start_price=initPrice,     strike_price=strike,     interest_rate=interest,     t_final=tfinal,     call_put=\"call\") y = EuroCall(sampleSize) print(\"The exact price of this European Call Option is \",f\"{EuroCall.get_exact_value():.4f}\") print(\"After generate \", sampleSize,\"iid points, the price of estimation of the fair price is\",f\"{y.mean():.4f}\") <pre>The exact price of this European Call Option is  8.2779\nAfter generate  1000000 iid points, the price of estimation of the fair price is 8.2718\n</pre> In\u00a0[39]: Copied! <pre>ArithMeanCall = qp.FinancialOption(\n    qp.IIDStdUniform(dimension=d,seed=7),\n    option=\"ASIAN\",\n    volatility=vol,\n    start_price=initPrice,\n    strike_price=strike,\n    interest_rate=interest,\n    t_final=tfinal,\n    call_put=\"call\",\n    asian_mean = 'arithmetic')\ny = ArithMeanCall(sampleSize)\nprint(\"After generate \", sampleSize,\"iid points, the price of this Arithmetic Mean Call Option is\",f\"{y.mean():.4f}\")\n</pre> ArithMeanCall = qp.FinancialOption(     qp.IIDStdUniform(dimension=d,seed=7),     option=\"ASIAN\",     volatility=vol,     start_price=initPrice,     strike_price=strike,     interest_rate=interest,     t_final=tfinal,     call_put=\"call\",     asian_mean = 'arithmetic') y = ArithMeanCall(sampleSize) print(\"After generate \", sampleSize,\"iid points, the price of this Arithmetic Mean Call Option is\",f\"{y.mean():.4f}\") <pre>After generate  1000000 iid points, the price of this Arithmetic Mean Call Option is 3.3856\n</pre> <p>The price of the Asian arithmetic mean call option is smaller than the price of the European call option.</p> <p>We may also price the Asian arithmetic mean put option as follows:</p> In\u00a0[40]: Copied! <pre>ArithMeanPut = qp.FinancialOption(\n    qp.IIDStdUniform(dimension=d,seed=7),\n    option=\"ASIAN\",\n    volatility=vol,\n    start_price=initPrice,\n    strike_price=strike,\n    interest_rate=interest,\n    t_final=tfinal,\n    call_put=\"put\",\n    asian_mean = 'arithmetic')\ny = ArithMeanPut(sampleSize)\nprint(\"After generate \", sampleSize,\"iid points, the price of this Arithmetic Mean Put Option is \",f\"{y.mean():.4f}\")\n</pre> ArithMeanPut = qp.FinancialOption(     qp.IIDStdUniform(dimension=d,seed=7),     option=\"ASIAN\",     volatility=vol,     start_price=initPrice,     strike_price=strike,     interest_rate=interest,     t_final=tfinal,     call_put=\"put\",     asian_mean = 'arithmetic') y = ArithMeanPut(sampleSize) print(\"After generate \", sampleSize,\"iid points, the price of this Arithmetic Mean Put Option is \",f\"{y.mean():.4f}\") <pre>After generate  1000000 iid points, the price of this Arithmetic Mean Put Option is  13.0233\n</pre> <p>Note that the price is greater.  This is because one strike price is above the initial price, making the expected payoff greater.</p> <p>In the limit of continuous monitoring $d \\to \\infty$, the payoff is</p> <p>$$ \\begin{array}{rcc} &amp; \\textbf{call} &amp; \\textbf{put} \\\\ \\hline \\textbf{payoff} &amp;  \\displaystyle \\max\\biggl(\\frac 1T \\int_{0}^T S(t) \\, {\\rm d} t - K,0 \\biggr)\\mathsf{e}^{-rT} &amp;  \\displaystyle \\max\\biggl(K - \\frac 1T \\int_{0}^T S(t) \\, {\\rm d} t,0 \\biggr)\\mathsf{e}^{-rT}  \\end{array}  $$</p> <p>Such an option can be approximated by taking smaller time steps:</p> In\u00a0[41]: Copied! <pre>ArithMeanPut = qp.FinancialOption(\n    qp.IIDStdUniform(dimension=62,seed=7), # weekly monitoring for 3 months\n    option=\"ASIAN\",\n    volatility=vol,\n    start_price=initPrice,\n    strike_price=strike,\n    interest_rate=interest,\n    t_final=tfinal,\n    call_put=\"call\",\n    asian_mean = 'arithmetic')\ny = ArithMeanCall(sampleSize)\nprint(\"After generate \", sampleSize,\"iid points, the price of this Arithmetic Mean Call Option is\",f\"{y.mean():.4f}\")\n</pre> ArithMeanPut = qp.FinancialOption(     qp.IIDStdUniform(dimension=62,seed=7), # weekly monitoring for 3 months     option=\"ASIAN\",     volatility=vol,     start_price=initPrice,     strike_price=strike,     interest_rate=interest,     t_final=tfinal,     call_put=\"call\",     asian_mean = 'arithmetic') y = ArithMeanCall(sampleSize) print(\"After generate \", sampleSize,\"iid points, the price of this Arithmetic Mean Call Option is\",f\"{y.mean():.4f}\") <pre>After generate  1000000 iid points, the price of this Arithmetic Mean Call Option is 3.3858\n</pre> <p>The price is a bit lower, and the time is longer because more time steps are needed, which means more random variables are needed.</p> In\u00a0[42]: Copied! <pre>GeoMeanPut = qp.FinancialOption(\n    qp.IIDStdUniform(dimension=12,seed=7), #Weekly Monitoring for three months\n    option=\"ASIAN\",\n    volatility=vol,\n    start_price=initPrice,\n    strike_price=strike,\n    interest_rate=interest,\n    t_final=tfinal,\n    call_put=\"put\",\n    asian_mean='geometric',\n    asian_mean_quadrature_rule=\"RIGHT\")\ny = GeoMeanPut(sampleSize)\nprint(\"The exact price of this Geometric Asian Put Option is \",f\"{GeoMeanPut.get_exact_value():.4f}\")\nprint(\"After generate \", sampleSize,\"iid points, the price of this Geometric Mean Put Option is \",f\"{y.mean():.4f}\")\n</pre> GeoMeanPut = qp.FinancialOption(     qp.IIDStdUniform(dimension=12,seed=7), #Weekly Monitoring for three months     option=\"ASIAN\",     volatility=vol,     start_price=initPrice,     strike_price=strike,     interest_rate=interest,     t_final=tfinal,     call_put=\"put\",     asian_mean='geometric',     asian_mean_quadrature_rule=\"RIGHT\") y = GeoMeanPut(sampleSize) print(\"The exact price of this Geometric Asian Put Option is \",f\"{GeoMeanPut.get_exact_value():.4f}\") print(\"After generate \", sampleSize,\"iid points, the price of this Geometric Mean Put Option is \",f\"{y.mean():.4f}\") <pre>The exact price of this Geometric Asian Put Option is  13.7841\nAfter generate  1000000 iid points, the price of this Geometric Mean Put Option is  13.7663\n</pre> In\u00a0[43]: Copied! <pre>GeoMeanCall = qp.FinancialOption(\n    qp.IIDStdUniform(dimension=12,seed=7), #Weekly Monitoring for three months\n    option=\"ASIAN\",\n    volatility=vol,\n    start_price=initPrice,\n    strike_price=strike,\n    interest_rate=interest,\n    t_final=tfinal,\n    call_put=\"call\",\n    asian_mean='geometric',\n    asian_mean_quadrature_rule=\"RIGHT\")\ny = GeoMeanCall(sampleSize)\nprint(\"The exact price of this Geometric Asian Put Option is \",f\"{GeoMeanCall.get_exact_value():.4f}\")\nprint(\"After generate \", sampleSize,\"iid points, the price of this Geometric Mean Call Option is \",f\"{y.mean():.4f}\")\n</pre> GeoMeanCall = qp.FinancialOption(     qp.IIDStdUniform(dimension=12,seed=7), #Weekly Monitoring for three months     option=\"ASIAN\",     volatility=vol,     start_price=initPrice,     strike_price=strike,     interest_rate=interest,     t_final=tfinal,     call_put=\"call\",     asian_mean='geometric',     asian_mean_quadrature_rule=\"RIGHT\") y = GeoMeanCall(sampleSize) print(\"The exact price of this Geometric Asian Put Option is \",f\"{GeoMeanCall.get_exact_value():.4f}\") print(\"After generate \", sampleSize,\"iid points, the price of this Geometric Mean Call Option is \",f\"{y.mean():.4f}\") <pre>The exact price of this Geometric Asian Put Option is  3.5401\nAfter generate  1000000 iid points, the price of this Geometric Mean Call Option is  3.5359\n</pre> In\u00a0[44]: Copied! <pre>BarrierUpInCall = qp.FinancialOption(\n    qp.IIDStdUniform(dimension=12,seed=7), #Weekly Monitoring for three months\n    option=\"BARRIER\",\n    volatility=vol,\n    start_price=initPrice,\n    strike_price=strike,\n    interest_rate=interest,\n    t_final=tfinal,\n    call_put=\"call\",\n    barrier_in_out=\"in\",\n    barrier_price=150)\ny = BarrierUpInCall(sampleSize)\nprint(\"After generate \", sampleSize,\"iid points, the price of this Barrier UpIn Call Option is \",f\"{y.mean():.4f}\")\n</pre> BarrierUpInCall = qp.FinancialOption(     qp.IIDStdUniform(dimension=12,seed=7), #Weekly Monitoring for three months     option=\"BARRIER\",     volatility=vol,     start_price=initPrice,     strike_price=strike,     interest_rate=interest,     t_final=tfinal,     call_put=\"call\",     barrier_in_out=\"in\",     barrier_price=150) y = BarrierUpInCall(sampleSize) print(\"After generate \", sampleSize,\"iid points, the price of this Barrier UpIn Call Option is \",f\"{y.mean():.4f}\") <pre>After generate  1000000 iid points, the price of this Barrier UpIn Call Option is  7.4022\n</pre> <p>Note that this price is less than the European call option because the asset price must cross the barrier for the option to become active.</p> In\u00a0[\u00a0]: Copied! <pre>LookCall = qp.FinancialOption(\n    qp.IIDStdUniform(dimension=12,seed=7), #Weekly Monitoring for three months\n    option=\"LOOKBACK\",\n    volatility=vol,\n    start_price=initPrice,\n    strike_price=strike,\n    interest_rate=interest,\n    t_final=tfinal,\n    call_put=\"call\")\ny = LookCall(sampleSize)\nprint(\"After generate \", sampleSize,\"iid points, the price of this Lookback Call Option is \",f\"{y.mean():.4f}\")\n</pre> LookCall = qp.FinancialOption(     qp.IIDStdUniform(dimension=12,seed=7), #Weekly Monitoring for three months     option=\"LOOKBACK\",     volatility=vol,     start_price=initPrice,     strike_price=strike,     interest_rate=interest,     t_final=tfinal,     call_put=\"call\") y = LookCall(sampleSize) print(\"After generate \", sampleSize,\"iid points, the price of this Lookback Call Option is \",f\"{y.mean():.4f}\") <pre>After generate  1000000 iid points, the price of this Lookback Call Option is  17.7080\n</pre>"},{"location":"demos/pricing_options/#pricing-asian-style-options","title":"Pricing Asian Style Options\u00b6","text":"<p>In this cript we show how to use classes in QMCPy for Monte Carlo option pricing of options with Asian style payoffs and European exercise.</p> <ul> <li>The payoff depends on the whole asset price path, not only on the terminal asset price.</li> <li>The option is only exercised at expiry, unlike American options, which can be exercised at any time before expiry.</li> </ul>"},{"location":"demos/pricing_options/#european-options","title":"European Options\u00b6","text":""},{"location":"demos/pricing_options/#arithmetic-mean-options","title":"Arithmetic Mean Options\u00b6","text":"<p>The payoff of the arithmetic mean option depends on the average of the stock price, not the final stock price.  Here are the discounted payoffs:</p> <p>$$\\begin{array}{rcc}  &amp; \\textbf{call} &amp; \\textbf{put} \\\\ \\hline \\textbf{payoff} &amp;  \\displaystyle \\max\\biggl(\\frac 1d \\sum_{j=1}^d S(jT/d) - K,0 \\biggr)\\mathsf{e}^{-rT} &amp;  \\displaystyle \\max\\biggl(K - \\frac 1d \\sum_{j=1}^d S(jT/d),0 \\biggr)\\mathsf{e}^{-rT}  \\end{array} $$</p>"},{"location":"demos/pricing_options/#geometric-mean-options","title":"Geometric Mean Options\u00b6","text":"<p>One can also base the payoff on a geometric mean rather than an arithmetic mean.  Such options have a closed form solution.</p> <p>The price of a geometric mean $ \\begin{Bmatrix}  \\text{call} \\\\  \\text{put} \\end{Bmatrix}$ option is $\\begin{Bmatrix} \\le \\\\ \\ge \\end{Bmatrix}$ the price of an arithmetic mean $\\begin{Bmatrix} \\text{call} \\\\\\text{put} \\end{Bmatrix}$ option because a geometric mean is smaller than an arithmetic mean.</p>"},{"location":"demos/pricing_options/#barrier-option","title":"Barrier Option\u00b6","text":"<p>In barrier options the payoff only occurs if the asset price crosses or fails to cross a barrier, $b$</p> <p>$$ \\begin{array}{rcc}  &amp; \\textbf{up} (S(0) &lt; b) &amp; \\textbf{down} (S(0) &gt; b) \\\\ \\hline  \\textbf{in} &amp; \\text{active if } S(t) \\ge b &amp; \\text{active if } S(t) \\le  b \\\\  \\textbf{out} &amp; \\text{inactive if } S(t) \\ge b &amp; \\text{inactive if } S(t) \\le  b   \\end{array} $$</p> <p>For the barrier option with a European call type payoff, this corresponds to</p> <p>$$  \\begin{array}{rcc}  \\textbf{payoff} &amp; \\textbf{up} (S(0) &lt; b) &amp; \\textbf{down} (S(0) &gt; b) \\\\ \\hline  \\textbf{in} &amp;   1_{[b,\\infty)}(\\max_{0 \\le t \\le T} S(t)) \\max(S(T)-K,0)\\mathsf{e}^{-rT} &amp;   1_{[0,b]}(\\min_{0 \\le t \\le T} S(t)) \\max(S(T)-K,0)\\mathsf{e}^{-rT} \\\\  \\textbf{out} &amp; 1_{[0,b)}(\\max_{0 \\le t \\le T} S(t)) \\max(S(T)-K,0)\\mathsf{e}^{-rT} &amp;   1_{[b,\\infty)}(\\min_{0 \\le t \\le T} S(t)) \\max(S(T)-K,0)\\mathsf{e}^{-rT}  \\end{array} $$</p>"},{"location":"demos/pricing_options/#lookback-options","title":"Lookback Options\u00b6","text":"<p>Lookback options do not use a strike price but use the minimum or maximum asset price as their strike.  The discounted payoffs are</p> <p>$$ \\begin{array}{rcc} &amp; \\textbf{call} &amp; \\textbf{put} \\\\ \\hline  \\textbf{payoff} &amp;   \\displaystyle \\Bigl(S(T) - \\min_{0 \\le t \\le T} S(t),0 \\Bigr)\\mathsf{e}^{-rT} &amp;   \\displaystyle \\Bigl(\\max_{0 \\le t \\le T} S(t) - S(T),0 \\Bigr)\\mathsf{e}^{-rT}   \\end{array} $$</p> <p>where the values of $t$ considered for the minimum or maximum are either discrete, $0, T/d, \\dots, T$, or continuous.  Note that we would expect the prices of these options to be greater than their out of the money European counterparts.</p>"},{"location":"demos/qei-demo-for-blog/","title":"qEI","text":"In\u00a0[1]: Copied! <pre>from qmcpy import *\nimport numpy as np\nfrom scipy.linalg import solve_triangular, cho_solve, cho_factor\nfrom scipy.stats import norm\nimport matplotlib.pyplot as pyplot\n%matplotlib inline\n\nlw = 3\nms = 8\n</pre> from qmcpy import * import numpy as np from scipy.linalg import solve_triangular, cho_solve, cho_factor from scipy.stats import norm import matplotlib.pyplot as pyplot %matplotlib inline  lw = 3 ms = 8 In\u00a0[2]: Copied! <pre>def yf(x):\n    return np.cos(10 * x) * np.exp(.2 * x) + np.exp(-5 * (x - .4) ** 2)\n\nxplt = np.linspace(0, 1, 300)\nyplt = yf(xplt)\n\nx = np.array([.1, .2, .4, .7, .9])\ny = yf(x)\nv = np.array([.001, .05, .01, .1, .4])\n\npyplot.plot(xplt, yplt, linewidth=lw)\npyplot.plot(x, y, 'o', markersize=ms, color='orange')\npyplot.errorbar(x, y, yerr=2 * np.sqrt(v), marker='', linestyle='', color='orange', linewidth=lw)\npyplot.title('Sample data with noise');\n</pre> def yf(x):     return np.cos(10 * x) * np.exp(.2 * x) + np.exp(-5 * (x - .4) ** 2)  xplt = np.linspace(0, 1, 300) yplt = yf(xplt)  x = np.array([.1, .2, .4, .7, .9]) y = yf(x) v = np.array([.001, .05, .01, .1, .4])  pyplot.plot(xplt, yplt, linewidth=lw) pyplot.plot(x, y, 'o', markersize=ms, color='orange') pyplot.errorbar(x, y, yerr=2 * np.sqrt(v), marker='', linestyle='', color='orange', linewidth=lw) pyplot.title('Sample data with noise'); In\u00a0[3]: Copied! <pre>shape_parameter = 4.1\nprocess_variance = .9\nfudge_factor = 1e-10\n\ndef gaussian_kernel(x, z):\n    return process_variance * np.exp(-shape_parameter ** 2 * (x[:, None] - z[None, :]) ** 2)\n\ndef gp_posterior_params(x_to_draw):\n    n = len(x_to_draw)\n    \n    kernel_prior_data = gaussian_kernel(x, x)\n    kernel_cross_matrix = gaussian_kernel(x_to_draw, x)\n    kernel_prior_plot = gaussian_kernel(x_to_draw, x_to_draw)\n    prior_cholesky = np.linalg.cholesky(kernel_prior_data + np.diag(v))\n    \n    partial_cardinal_functions = solve_triangular(prior_cholesky, kernel_cross_matrix.T, lower=True)\n    posterior_covariance = kernel_prior_plot - np.dot(partial_cardinal_functions.T, partial_cardinal_functions) + fudge_factor * np.eye(n)\n    \n    full_cardinal_functions = solve_triangular(prior_cholesky.T, partial_cardinal_functions, lower=False)\n    posterior_mean = np.dot(full_cardinal_functions.T, y)\n    return posterior_mean,posterior_covariance\n\ndef gp_posterior_draws(x_to_draw, mc_strat, num_posterior_draws, posterior_mean, posterior_covariance):\n    q = len(x_to_draw)\n    if mc_strat == 'iid':\n        dd = IIDStdUniform(q)\n    elif mc_strat == 'lattice':\n        dd = Lattice(q)\n    elif mc_strat == 'sobol':\n        dd = Sobol(q)\n    g = Gaussian(dd,posterior_mean,posterior_covariance)\n    posterior_draws = g.gen_samples(num_posterior_draws)\n    return posterior_draws\n\ndef compute_qei(posterior_draws):\n    y_gp = np.fmax(np.max(posterior_draws.T - max(y), axis=0), 0)\n    return y_gp\n</pre> shape_parameter = 4.1 process_variance = .9 fudge_factor = 1e-10  def gaussian_kernel(x, z):     return process_variance * np.exp(-shape_parameter ** 2 * (x[:, None] - z[None, :]) ** 2)  def gp_posterior_params(x_to_draw):     n = len(x_to_draw)          kernel_prior_data = gaussian_kernel(x, x)     kernel_cross_matrix = gaussian_kernel(x_to_draw, x)     kernel_prior_plot = gaussian_kernel(x_to_draw, x_to_draw)     prior_cholesky = np.linalg.cholesky(kernel_prior_data + np.diag(v))          partial_cardinal_functions = solve_triangular(prior_cholesky, kernel_cross_matrix.T, lower=True)     posterior_covariance = kernel_prior_plot - np.dot(partial_cardinal_functions.T, partial_cardinal_functions) + fudge_factor * np.eye(n)          full_cardinal_functions = solve_triangular(prior_cholesky.T, partial_cardinal_functions, lower=False)     posterior_mean = np.dot(full_cardinal_functions.T, y)     return posterior_mean,posterior_covariance  def gp_posterior_draws(x_to_draw, mc_strat, num_posterior_draws, posterior_mean, posterior_covariance):     q = len(x_to_draw)     if mc_strat == 'iid':         dd = IIDStdUniform(q)     elif mc_strat == 'lattice':         dd = Lattice(q)     elif mc_strat == 'sobol':         dd = Sobol(q)     g = Gaussian(dd,posterior_mean,posterior_covariance)     posterior_draws = g.gen_samples(num_posterior_draws)     return posterior_draws  def compute_qei(posterior_draws):     y_gp = np.fmax(np.max(posterior_draws.T - max(y), axis=0), 0)     return y_gp In\u00a0[4]: Copied! <pre>num_posterior_draws = 2 ** 7\nNp = (25, 24)\nX, Y = np.meshgrid(np.linspace(0, 1, Np[1]), np.linspace(0, 1, Np[0]))\nxp = np.array([X.reshape(-1), Y.reshape(-1)]).T\nmu_post,sigma_cov = gp_posterior_params(xplt)\ny_draws = gp_posterior_draws(xplt, 'lattice', num_posterior_draws,mu_post,sigma_cov).T\nqei_vals = np.empty(len(xp))\nfor k, next_x in enumerate(xp):\n    mu_post,sigma_cov = gp_posterior_params(next_x)\n    gp_draws = gp_posterior_draws(next_x, 'sobol', num_posterior_draws,mu_post,sigma_cov)\n    qei_vals[k] = compute_qei(gp_draws).mean()\nZ = qei_vals.reshape(Np)\n\nfig, axes = pyplot.subplots(1, 3, figsize=(14, 4))\n\nax = axes[0]\nax.plot(xplt, yplt, linewidth=lw)\nax.plot(x, y, 'o', markersize=ms, color='orange')\nax.errorbar(x, y, yerr=2 * np.sqrt(v), marker='', linestyle='', color='orange', linewidth=lw)\nax.set_title('Sample data with noise')\nax.set_ylim((-2.3, 2.6))\n\nax = axes[1]\nax.plot(xplt, y_draws, linewidth=lw, color='b', alpha=.05)\nax.plot(x, y, 'o', markersize=ms, color='orange')\nax.errorbar(x, y, yerr=2 * np.sqrt(v), marker='', linestyle='', color='orange', linewidth=lw)\nax.set_title(f'{num_posterior_draws} GP posterior draws')\nax.set_ylim((-2.3, 2.6))\n\nax = axes[2]\nh = ax.contourf(X, Y, Z)\nax.set_xlabel('First next point')\nax.set_ylabel('Second next point')\nax.set_title('qEI for q=2 next points')\ncax = fig.colorbar(h, ax=ax)\ncax.set_label('qEI')\n\nfig.tight_layout()\n</pre> num_posterior_draws = 2 ** 7 Np = (25, 24) X, Y = np.meshgrid(np.linspace(0, 1, Np[1]), np.linspace(0, 1, Np[0])) xp = np.array([X.reshape(-1), Y.reshape(-1)]).T mu_post,sigma_cov = gp_posterior_params(xplt) y_draws = gp_posterior_draws(xplt, 'lattice', num_posterior_draws,mu_post,sigma_cov).T qei_vals = np.empty(len(xp)) for k, next_x in enumerate(xp):     mu_post,sigma_cov = gp_posterior_params(next_x)     gp_draws = gp_posterior_draws(next_x, 'sobol', num_posterior_draws,mu_post,sigma_cov)     qei_vals[k] = compute_qei(gp_draws).mean() Z = qei_vals.reshape(Np)  fig, axes = pyplot.subplots(1, 3, figsize=(14, 4))  ax = axes[0] ax.plot(xplt, yplt, linewidth=lw) ax.plot(x, y, 'o', markersize=ms, color='orange') ax.errorbar(x, y, yerr=2 * np.sqrt(v), marker='', linestyle='', color='orange', linewidth=lw) ax.set_title('Sample data with noise') ax.set_ylim((-2.3, 2.6))  ax = axes[1] ax.plot(xplt, y_draws, linewidth=lw, color='b', alpha=.05) ax.plot(x, y, 'o', markersize=ms, color='orange') ax.errorbar(x, y, yerr=2 * np.sqrt(v), marker='', linestyle='', color='orange', linewidth=lw) ax.set_title(f'{num_posterior_draws} GP posterior draws') ax.set_ylim((-2.3, 2.6))  ax = axes[2] h = ax.contourf(X, Y, Z) ax.set_xlabel('First next point') ax.set_ylabel('Second next point') ax.set_title('qEI for q=2 next points') cax = fig.colorbar(h, ax=ax) cax.set_label('qEI')  fig.tight_layout() In\u00a0[5]: Copied! <pre># parameters\nnext_x = np.array([0.158,  0.416,  0.718,  0.935,  0.465])\nnum_posterior_draws_to_test = 2 ** np.arange(4, 20)\nd = len(next_x)\nmu_post,sigma_cov = gp_posterior_params(next_x)\n</pre> # parameters next_x = np.array([0.158,  0.416,  0.718,  0.935,  0.465]) num_posterior_draws_to_test = 2 ** np.arange(4, 20) d = len(next_x) mu_post,sigma_cov = gp_posterior_params(next_x) In\u00a0[6]: Copied! <pre># get reference answer with qmcpy\nintegrand = CustomFun(\n    true_measure = Gaussian(Sobol(d),mu_post,sigma_cov),\n    g = compute_qei)\nstopping_criterion = CubQMCSobolG(integrand, abs_tol=5e-7)\nreference_answer,data = stopping_criterion.integrate()\nprint(data)\n</pre> # get reference answer with qmcpy integrand = CustomFun(     true_measure = Gaussian(Sobol(d),mu_post,sigma_cov),     g = compute_qei) stopping_criterion = CubQMCSobolG(integrand, abs_tol=5e-7) reference_answer,data = stopping_criterion.integrate() print(data) <pre>Data (Data)\n    solution        0.024\n    comb_bound_low  0.024\n    comb_bound_high 0.024\n    comb_bound_diff 8.33e-07\n    comb_flags      1\n    n_total         2^(22)\n    n               2^(22)\n    time_integrate  4.020\nCubQMCNetG (AbstractStoppingCriterion)\n    abs_tol         5.00e-07\n    rel_tol         0\n    n_init          2^(10)\n    n_limit         2^(35)\nCustomFun (AbstractIntegrand)\nGaussian (AbstractTrueMeasure)\n    mean            [ 0.807  0.371  1.204 -0.455  0.67 ]\n    covariance      [[ 1.845e-02 -2.039e-03  1.150e-04  1.219e-04 -5.985e-03]\n                     [-2.039e-03  1.355e-02  6.999e-04 -1.967e-03  2.302e-02]\n                     [ 1.150e-04  6.999e-04  8.871e-02  2.043e-02  5.757e-03]\n                     [ 1.219e-04 -1.967e-03  2.043e-02  2.995e-01 -9.482e-03]\n                     [-5.985e-03  2.302e-02  5.757e-03 -9.482e-03  6.296e-02]]\n    decomp_type     PCA\nDigitalNetB2 (AbstractLDDiscreteDistribution)\n    d               5\n    replications    1\n    randomize       LMS_DS\n    gen_mats_source joe_kuo.6.21201.txt\n    order           NATURAL\n    t               63\n    alpha           1\n    n_limit         2^(32)\n    entropy         1084764658139426821181530528540663833\n</pre> In\u00a0[7]: Copied! <pre># generate data\nnum_posterior_draws_to_test = 2 ** np.arange(4, 20)\nvals = {}\nnum_repeats = 50\nmc_strats = ('iid', 'lattice', 'sobol')\nfor mc_strat in mc_strats:\n    vals[mc_strat] = []\n    for num_posterior_draws in num_posterior_draws_to_test:\n        all_estimates = []\n        for _ in range(num_repeats):\n            y_draws = gp_posterior_draws(next_x, mc_strat, num_posterior_draws,mu_post,sigma_cov)\n            all_estimates.append(compute_qei(y_draws).mean())\n        vals[mc_strat].append(all_estimates)\n    vals[mc_strat] = np.array(vals[mc_strat])\n</pre> # generate data num_posterior_draws_to_test = 2 ** np.arange(4, 20) vals = {} num_repeats = 50 mc_strats = ('iid', 'lattice', 'sobol') for mc_strat in mc_strats:     vals[mc_strat] = []     for num_posterior_draws in num_posterior_draws_to_test:         all_estimates = []         for _ in range(num_repeats):             y_draws = gp_posterior_draws(next_x, mc_strat, num_posterior_draws,mu_post,sigma_cov)             all_estimates.append(compute_qei(y_draws).mean())         vals[mc_strat].append(all_estimates)     vals[mc_strat] = np.array(vals[mc_strat]) In\u00a0[8]: Copied! <pre>fig, ax = pyplot.subplots(1, 1, figsize=(6, 4))\n\ncolors = ('#F5811F', '#A23D97', '#00B253')\nalpha = .3\n\nfor (name, results), color in zip(vals.items(), colors):\n    bot = np.percentile(abs(results - reference_answer), 25, axis=1)\n    med = np.percentile(abs(results - reference_answer), 50, axis=1)\n    top = np.percentile(abs(results - reference_answer), 75, axis=1)\n    ax.loglog(num_posterior_draws_to_test, med, label=name, color=color)\n    ax.fill_between(num_posterior_draws_to_test, bot, top, color=color, alpha=alpha)\nax.loglog(num_posterior_draws_to_test, .1 * num_posterior_draws_to_test ** -.5, '--k', label='$O(N^{-1/2})$')\nax.loglog(num_posterior_draws_to_test, .25 * num_posterior_draws_to_test ** -1.0, '-.k', label='$O(N^{-1})$')\nax.set_xlabel('N - number of points')\nax.set_ylabel('Accuracy')\nax.legend(loc='lower left')\nax.set_title(f'Statistics from {num_repeats} runs');\n\n# plt.savefig('qei_convergence.png');\n</pre> fig, ax = pyplot.subplots(1, 1, figsize=(6, 4))  colors = ('#F5811F', '#A23D97', '#00B253') alpha = .3  for (name, results), color in zip(vals.items(), colors):     bot = np.percentile(abs(results - reference_answer), 25, axis=1)     med = np.percentile(abs(results - reference_answer), 50, axis=1)     top = np.percentile(abs(results - reference_answer), 75, axis=1)     ax.loglog(num_posterior_draws_to_test, med, label=name, color=color)     ax.fill_between(num_posterior_draws_to_test, bot, top, color=color, alpha=alpha) ax.loglog(num_posterior_draws_to_test, .1 * num_posterior_draws_to_test ** -.5, '--k', label='$O(N^{-1/2})$') ax.loglog(num_posterior_draws_to_test, .25 * num_posterior_draws_to_test ** -1.0, '-.k', label='$O(N^{-1})$') ax.set_xlabel('N - number of points') ax.set_ylabel('Accuracy') ax.legend(loc='lower left') ax.set_title(f'Statistics from {num_repeats} runs');  # plt.savefig('qei_convergence.png'); In\u00a0[9]: Copied! <pre># parameters\nnames = ['IID','Lattice','Sobol']\nepsilons = [\n      [2e-4, 5e-4, 1e-3, 2e-3, 5e-3, 1e-2], # iid nodes\n      [5e-6, 1e-5, 2e-5, 5e-5, 1e-4, 2e-4, 5e-4, 1e-3, 2e-3, 5e-3, 1e-2], # lattice\n      [5e-6, 1e-5, 2e-5, 5e-5, 1e-4, 2e-4, 5e-4, 1e-3, 2e-3, 5e-3, 1e-2]] # sobol\ntrials = 25\n# initialize time data\ntimes = {names[j]:np.zeros((len(epsilons[j]),trials),dtype=float) for j in range(len(names))}\nn_needed = {names[j]:np.zeros((len(epsilons[j]),trials),dtype=float) for j in range(len(names))}\n# run tests\nfor t in range(trials):\n  print(t)\n  for j in range(len(names)):\n    for i in range(len(epsilons[j])): \n      if j == 0:\n        sc = CubMCG(CustomFun(Gaussian(IIDStdUniform(d),mu_post,sigma_cov),compute_qei),abs_tol=epsilons[j][i],rel_tol=0)\n      elif j == 1:\n        sc = CubQMCLatticeG(CustomFun(Gaussian(Lattice(d),mu_post,sigma_cov),compute_qei),abs_tol=epsilons[j][i],rel_tol=0)\n      else:\n        sc = CubQMCSobolG(CustomFun(Gaussian(Sobol(d),mu_post,sigma_cov),compute_qei),abs_tol=epsilons[j][i],rel_tol=0)\n      solution,data = sc.integrate()\n      times[names[j]][i,t] = data.time_integrate\n      n_needed[names[j]][i,t] = data.n_total\n</pre> # parameters names = ['IID','Lattice','Sobol'] epsilons = [       [2e-4, 5e-4, 1e-3, 2e-3, 5e-3, 1e-2], # iid nodes       [5e-6, 1e-5, 2e-5, 5e-5, 1e-4, 2e-4, 5e-4, 1e-3, 2e-3, 5e-3, 1e-2], # lattice       [5e-6, 1e-5, 2e-5, 5e-5, 1e-4, 2e-4, 5e-4, 1e-3, 2e-3, 5e-3, 1e-2]] # sobol trials = 25 # initialize time data times = {names[j]:np.zeros((len(epsilons[j]),trials),dtype=float) for j in range(len(names))} n_needed = {names[j]:np.zeros((len(epsilons[j]),trials),dtype=float) for j in range(len(names))} # run tests for t in range(trials):   print(t)   for j in range(len(names)):     for i in range(len(epsilons[j])):        if j == 0:         sc = CubMCG(CustomFun(Gaussian(IIDStdUniform(d),mu_post,sigma_cov),compute_qei),abs_tol=epsilons[j][i],rel_tol=0)       elif j == 1:         sc = CubQMCLatticeG(CustomFun(Gaussian(Lattice(d),mu_post,sigma_cov),compute_qei),abs_tol=epsilons[j][i],rel_tol=0)       else:         sc = CubQMCSobolG(CustomFun(Gaussian(Sobol(d),mu_post,sigma_cov),compute_qei),abs_tol=epsilons[j][i],rel_tol=0)       solution,data = sc.integrate()       times[names[j]][i,t] = data.time_integrate       n_needed[names[j]][i,t] = data.n_total <pre>0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n</pre> In\u00a0[11]: Copied! <pre>fig,axs = pyplot.subplots(1, 3, figsize=(22, 6))\ncolors = ('#245EAB', '#A23D97', '#00B253')\nlight_colors = ('#A3DDFF', '#FFBCFF', '#4DFFA0')\nalpha = .3\ndef plot_fills(eps,data,name,color,light_color):\n    bot = np.percentile(data, 5, axis=1)\n    med = np.percentile(data, 50, axis=1)\n    top = np.percentile(data, 95, axis=1)\n    ax.loglog(eps, med, label=name, color=color)\n    ax.fill_between(eps, bot, top, color=light_color)\n    return med\nfor i,(nt_data,label) in enumerate(zip([times,n_needed],['time','n'])):\n  ax = axs[i+1]\n  # iid plot\n  eps_iid = np.array(epsilons[0])\n  data = nt_data['IID']\n  med_iid = plot_fills(eps_iid,data,'IID',colors[0],light_colors[0])\n  # lattice plot\n  eps = np.array(epsilons[1])\n  data = nt_data['Lattice']\n  med_lattice = plot_fills(eps,data,'Lattice',colors[1],light_colors[1])\n  # sobol plot\n  eps = np.array(epsilons[2])\n  data = nt_data['Sobol']\n  med_sobol = plot_fills(eps,data,'Sobol',colors[2],light_colors[2])\n  # iid bigO\n  ax.loglog(eps_iid, (med_iid[0]*eps_iid[0]**2)/(eps_iid**2), '--k', label=r'$\\mathcal{O}(1/\\epsilon^2)$')\n  # ld bigO\n  ax.loglog(eps, ((med_lattice[0]*med_sobol[0])**.5 *eps[0]) / eps , '-.k', label=r'$\\mathcal{O}(1/\\epsilon)$')\n  # metas\n  ax.set_xlabel(r'$\\epsilon$')\n  ax.set_ylabel(label)\n  ax.spines['right'].set_visible(False)\n  ax.spines['top'].set_visible(False)\n  ax.legend(loc='lower left',frameon=False)\n  ax.set_title(f'Statistics from {trials} runs')\n# plot sample data\nax = axs[0]\nax.plot(xplt, yplt, linewidth=lw)\nax.plot(x, y, 'o', markersize=ms, color='orange')\nax.errorbar(x, y, yerr=2 * np.sqrt(v), marker='', linestyle='', color='orange', linewidth=lw)\nax.set_title('Sample data with noise')\nax.set_xlim([0,1])\nax.set_xticks([0,1])\nax.set_ylim([-3, 3])\nax.set_yticks([-3,3]);\n</pre> fig,axs = pyplot.subplots(1, 3, figsize=(22, 6)) colors = ('#245EAB', '#A23D97', '#00B253') light_colors = ('#A3DDFF', '#FFBCFF', '#4DFFA0') alpha = .3 def plot_fills(eps,data,name,color,light_color):     bot = np.percentile(data, 5, axis=1)     med = np.percentile(data, 50, axis=1)     top = np.percentile(data, 95, axis=1)     ax.loglog(eps, med, label=name, color=color)     ax.fill_between(eps, bot, top, color=light_color)     return med for i,(nt_data,label) in enumerate(zip([times,n_needed],['time','n'])):   ax = axs[i+1]   # iid plot   eps_iid = np.array(epsilons[0])   data = nt_data['IID']   med_iid = plot_fills(eps_iid,data,'IID',colors[0],light_colors[0])   # lattice plot   eps = np.array(epsilons[1])   data = nt_data['Lattice']   med_lattice = plot_fills(eps,data,'Lattice',colors[1],light_colors[1])   # sobol plot   eps = np.array(epsilons[2])   data = nt_data['Sobol']   med_sobol = plot_fills(eps,data,'Sobol',colors[2],light_colors[2])   # iid bigO   ax.loglog(eps_iid, (med_iid[0]*eps_iid[0]**2)/(eps_iid**2), '--k', label=r'$\\mathcal{O}(1/\\epsilon^2)$')   # ld bigO   ax.loglog(eps, ((med_lattice[0]*med_sobol[0])**.5 *eps[0]) / eps , '-.k', label=r'$\\mathcal{O}(1/\\epsilon)$')   # metas   ax.set_xlabel(r'$\\epsilon$')   ax.set_ylabel(label)   ax.spines['right'].set_visible(False)   ax.spines['top'].set_visible(False)   ax.legend(loc='lower left',frameon=False)   ax.set_title(f'Statistics from {trials} runs') # plot sample data ax = axs[0] ax.plot(xplt, yplt, linewidth=lw) ax.plot(x, y, 'o', markersize=ms, color='orange') ax.errorbar(x, y, yerr=2 * np.sqrt(v), marker='', linestyle='', color='orange', linewidth=lw) ax.set_title('Sample data with noise') ax.set_xlim([0,1]) ax.set_xticks([0,1]) ax.set_ylim([-3, 3]) ax.set_yticks([-3,3]); In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"demos/qei-demo-for-blog/#qei-q-noisy-expected-improvement-demo-for-blog","title":"QEI (Q-Noisy Expected Improvement) Demo for Blog\u00b6","text":""},{"location":"demos/qei-demo-for-blog/#problem-setup","title":"Problem setup\u00b6","text":"<p>Here is the current data ($x$ and $y$ values with noise) from which we want to build a GP and run a Bayesian optimization.</p>"},{"location":"demos/qei-demo-for-blog/#computation-of-the-qei-quantity-using-qmcpy","title":"Computation of the qEI quantity using <code>qmcpy</code>\u00b6","text":"<p>One quantity which can appear often during BO is a computation involving $q$ \"next points\" to sample in a BO process; in the standard formulation this quantity might involve just $q=1$, but $q&gt;1$ is also of interest for batched evaluation in parallel.</p> <p>This quantity is defined as $$ \\mathrm{EI}_q(x_1, \\ldots, x_q;\\mathbf{y}, \\mathcal{X}, \\boldsymbol{\\epsilon}) = \\int_{\\mathbb{R}^q} \\max_{1\\leq i\\leq q}\\left[{(y_i - y^*)_+}\\right]\\, p_{Y_{x_1,\\ldots, x_q}|\\mathbf{y}, \\mathcal{X}, \\boldsymbol{\\epsilon}}(y_1, \\ldots, y_q)\\; \\text{d}y_1\\cdots\\text{d}y_q $$</p> <p>The example we are considering here is with $q=5$ but this quantity could be made larger.  Each of these QEI computations (done in a vectorized fashion in production) would be needed in an optimization loop (likely powered by CMAES or some other high dimensional non-convex optimization tool).  This optimization problem would take place in a $qd$ dimensional space, which is one aspect which usually prevents $q$ from being too large.</p> <p>Note that some of this will look much more confusing in $d&gt;1$, but it is written here in a simplified version.</p>"},{"location":"demos/qei-demo-for-blog/#gp-model-definition-kernel-information-and-qei-definition","title":"GP model definition (kernel information) and qEI definition\u00b6","text":""},{"location":"demos/qei-demo-for-blog/#demonstrate-the-concept-of-qei-on-2-points","title":"Demonstrate the concept of qEI on 2 points\u00b6","text":""},{"location":"demos/qei-demo-for-blog/#choose-some-set-of-next-points-against-which-to-test-the-computation","title":"Choose some set of next points against which to test the computation\u00b6","text":"<p>Here, we consider $q=5$, which is much more costly to compute than the $q=2$ demonstration above.</p> <p>Note This will take some time to run.  Use fewer <code>num_repeats</code> to reduce the cost.</p>"},{"location":"demos/qmcpy_intro/","title":"Introduction","text":"<p>Here we show three different ways to import QMCPy in a Python environment. First, we can import the package <code>qmcpy</code> under the alias <code>qp</code>.</p> In\u00a0[1]: Copied! <pre>import qmcpy as qp\nprint(qp.name, qp.__version__)\n</pre> import qmcpy as qp print(qp.name, qp.__version__) <pre>qmcpy 1.6.3c\n</pre> <p>Alternatively, we can import individual objects from 'qmcpy' as shown below.</p> In\u00a0[2]: Copied! <pre>from qmcpy.integrand import *\nfrom qmcpy.true_measure import *\nfrom qmcpy.discrete_distribution import *\nfrom qmcpy.stopping_criterion import *\n</pre> from qmcpy.integrand import * from qmcpy.true_measure import * from qmcpy.discrete_distribution import * from qmcpy.stopping_criterion import * <p>Lastly, we can import all objects from the package using an asterisk.</p> In\u00a0[3]: Copied! <pre>from qmcpy import *\n</pre> from qmcpy import * In\u00a0[4]: Copied! <pre>distribution = Lattice(dimension=2, randomize=True, seed=7)\ndistribution.gen_samples(n_min=0,n_max=4)\n</pre> distribution = Lattice(dimension=2, randomize=True, seed=7) distribution.gen_samples(n_min=0,n_max=4) Out[4]: <pre>array([[0.04386058, 0.58727432],\n       [0.54386058, 0.08727432],\n       [0.29386058, 0.33727432],\n       [0.79386058, 0.83727432]])</pre> In\u00a0[5]: Copied! <pre>from numpy.linalg import norm as norm\nfrom numpy import sqrt, array\n</pre> from numpy.linalg import norm as norm from numpy import sqrt, array <p>Our first attempt maybe to create the integrand as a Python function as follows:</p> In\u00a0[6]: Copied! <pre>def f(x): return norm(x) ** sqrt(norm(x))\n</pre> def f(x): return norm(x) ** sqrt(norm(x)) <p>It looks reasonable except that maybe the Numpy function norm is executed twice. It's okay for now. Let us quickly test if the function behaves as expected at a point value:</p> In\u00a0[7]: Copied! <pre>x = 0.01\nf(x)\n</pre> x = 0.01 f(x) Out[7]: <pre>0.6309573444801932</pre> <p>What about an array that represents $n=3$ sampling points in a two-dimensional domain, i.e., $d=2$?</p> In\u00a0[8]: Copied! <pre>x = array([[1., 0.], \n           [0., 0.01],\n           [0.04, 0.04]])\nf(x)\n</pre> x = array([[1., 0.],             [0., 0.01],            [0.04, 0.04]]) f(x) Out[8]: <pre>1.001650000560437</pre> <p>Now, the function should have returned $n=3$ real values that corresponding to each of the sampling points. Let's debug our Python function.</p> In\u00a0[9]: Copied! <pre>norm(x)\n</pre> norm(x) Out[9]: <pre>1.0016486409914407</pre> <p>Numpy's <code>norm(x)</code> is obviously a matrix norm, but we want it to be vector 2-norm that acts on each row of <code>x</code>. To that end, let's add an axis argument to the function:</p> In\u00a0[10]: Copied! <pre>norm(x, axis = 1)\n</pre> norm(x, axis = 1) Out[10]: <pre>array([1.        , 0.01      , 0.05656854])</pre> <p>Now it's working! Let's make sure that the <code>sqrt</code> function is acting on each element of the vector norm results:</p> In\u00a0[11]: Copied! <pre>sqrt(norm(x, axis = 1))\n</pre> sqrt(norm(x, axis = 1)) Out[11]: <pre>array([1.        , 0.1       , 0.23784142])</pre> <p>It is. Putting everything together, we have:</p> In\u00a0[12]: Copied! <pre>norm(x, axis = 1) ** sqrt(norm(x, axis = 1))\n</pre> norm(x, axis = 1) ** sqrt(norm(x, axis = 1)) Out[12]: <pre>array([1.        , 0.63095734, 0.50502242])</pre> <p>We have got our proper function definition now.</p> In\u00a0[13]: Copied! <pre>def myfunc(x):\n    x_norms = norm(x, axis = 1)\n    return x_norms ** sqrt(x_norms)\n</pre> def myfunc(x):     x_norms = norm(x, axis = 1)     return x_norms ** sqrt(x_norms) <p>We can now create an <code>integrand</code> instance with our <code>QuickConstruct</code> class in QMCPy and then invoke QMCPy's <code>integrate</code> function:</p> In\u00a0[14]: Copied! <pre>dim = 1\nabs_tol = .01\nintegrand = CustomFun(\n    true_measure = Uniform(IIDStdUniform(dimension=dim, seed=7)),\n    g=myfunc)\nsolution,data = CubMCCLT(integrand,abs_tol=abs_tol,rel_tol=0).integrate()\nprint(data)\n</pre> dim = 1 abs_tol = .01 integrand = CustomFun(     true_measure = Uniform(IIDStdUniform(dimension=dim, seed=7)),     g=myfunc) solution,data = CubMCCLT(integrand,abs_tol=abs_tol,rel_tol=0).integrate() print(data) <pre>Data (Data)\n    solution        0.657\n    bound_low       0.648\n    bound_high      0.667\n    bound_diff      0.019\n    n_total         3378\n    time_integrate  0.002\nCubMCCLT (AbstractStoppingCriterion)\n    abs_tol         0.010\n    rel_tol         0\n    n_init          2^(10)\n    n_limit         2^(30)\n    inflate         1.200\n    alpha           0.010\nCustomFun (AbstractIntegrand)\nUniform (AbstractTrueMeasure)\n    lower_bound     0\n    upper_bound     1\nIIDStdUniform (AbstractIIDDiscreteDistribution)\n    d               1\n    replications    1\n    entropy         7\n</pre> <p>For our integral, we know the true value. Let's check if QMCPy's solution is accurate enough:</p> In\u00a0[15]: Copied! <pre>true_sol = 0.658582  # In WolframAlpha: Integral[x**Sqrt[x], {x,0,1}]\nabs_tol = data.stopping_crit.abs_tol\nqmcpy_error = abs(true_sol - solution)\nif qmcpy_error &gt; abs_tol: raise Exception(\"Error not within bounds\")\n</pre> true_sol = 0.658582  # In WolframAlpha: Integral[x**Sqrt[x], {x,0,1}] abs_tol = data.stopping_crit.abs_tol qmcpy_error = abs(true_sol - solution) if qmcpy_error &gt; abs_tol: raise Exception(\"Error not within bounds\") <p>It's good. Shall we test the function with $d=2$ by simply changing the input parameter value of dimension for QuickConstruct?</p> In\u00a0[16]: Copied! <pre>dim = 2\nintegrand = CustomFun(\n    true_measure = Uniform(IIDStdUniform(dimension=dim, seed=7)),\n    g = myfunc)\nsolution2,data2 = CubMCCLT(integrand,abs_tol=abs_tol,rel_tol=0).integrate()\nprint(data2)\n</pre> dim = 2 integrand = CustomFun(     true_measure = Uniform(IIDStdUniform(dimension=dim, seed=7)),     g = myfunc) solution2,data2 = CubMCCLT(integrand,abs_tol=abs_tol,rel_tol=0).integrate() print(data2) <pre>Data (Data)\n    solution        0.830\n    bound_low       0.820\n    bound_high      0.840\n    bound_diff      0.020\n    n_total         5640\n    time_integrate  0.002\nCubMCCLT (AbstractStoppingCriterion)\n    abs_tol         0.010\n    rel_tol         0\n    n_init          2^(10)\n    n_limit         2^(30)\n    inflate         1.200\n    alpha           0.010\nCustomFun (AbstractIntegrand)\nUniform (AbstractTrueMeasure)\n    lower_bound     0\n    upper_bound     1\nIIDStdUniform (AbstractIIDDiscreteDistribution)\n    d               2^(1)\n    replications    1\n    entropy         7\n</pre> <p>Once again, we could test for accuracy of QMCPy with respect to the true value:</p> In\u00a0[17]: Copied! <pre>true_sol2 = 0.827606  # In WolframAlpha: Integral[Sqrt[x**2+y**2])**Sqrt[Sqrt[x**2+y**2]], {x,0,1}, {y,0,1}]\nabs_tol2 = data2.stopping_crit.abs_tol\nqmcpy_error2 = abs(true_sol2 - solution2)\nif qmcpy_error2 &gt; abs_tol2: raise Exception(\"Error not within bounds\")\n</pre> true_sol2 = 0.827606  # In WolframAlpha: Integral[Sqrt[x**2+y**2])**Sqrt[Sqrt[x**2+y**2]], {x,0,1}, {y,0,1}] abs_tol2 = data2.stopping_crit.abs_tol qmcpy_error2 = abs(true_sol2 - solution2) if qmcpy_error2 &gt; abs_tol2: raise Exception(\"Error not within bounds\") In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"demos/qmcpy_intro/#welcome-to-qmcpy","title":"Welcome to QMCPy\u00b6","text":""},{"location":"demos/qmcpy_intro/#importing-qmcpy","title":"Importing QMCPy\u00b6","text":""},{"location":"demos/qmcpy_intro/#important-notes","title":"Important Notes\u00b6","text":""},{"location":"demos/qmcpy_intro/#iid-vs-lds","title":"IID vs LDS\u00b6","text":"<p>Low discrepancy (LD) sequences such as lattice and Sobol' are not independent like IID (independent identically distributed) points.</p> <p>The code below generates 4 Sobol samples of 2 dimensions.</p>"},{"location":"demos/qmcpy_intro/#multi-dimensional-inputs","title":"Multi-Dimensional Inputs\u00b6","text":"<p>Suppose we want to create an integrand in QMCPy for evaluating the following integral:</p> <p>$$\\int_{[0,1]^d} \\|x\\|_2^{\\|x\\|_2^{1/2}} dx,$$</p> <p>where $[0,1]^d$ is the unit hypercube in $\\mathbb{R}^d$.</p> <p>The integrand is defined everywhere except at $x=0$ and hence the definite integral is also defined.</p> <p>The key in defining a Python function of an integrand in the QMCPy framework is that not only  it should be able to take one point $x \\in \\mathbb{R}^d$ and return a real value, but also that it would be able to take a set of $n$ sampling points as rows in a Numpy array of size $n \\times d$ and return an array with $n$ values evaluated at each sampling point. The following examples illustrate this point.</p>"},{"location":"demos/quickstart/","title":"Quickstart","text":"<p>Consider the problem of integrating the Keister function [2] with respect to a $d$-dimensional Gaussian measure:</p> <p>$$f(\\boldsymbol{x}) = \\pi^{d/2} \\cos(||\\boldsymbol{x}||), \\qquad \\boldsymbol{x} \\in \\mathbb{R}^d, \\qquad \\boldsymbol{X} \\sim \\mathcal{N}(\\boldsymbol{0}_d,\\mathsf{I}_d/2),   \\\\ \\mu  =  \\mathbb{E}[f(\\boldsymbol{X})] := \\int_{\\mathbb{R}^d} f(\\boldsymbol{x}) \\, \\pi^{-d/2} \\exp( - ||\\boldsymbol{x}||^2) \\,  \\rm d \\boldsymbol{x}  \\\\     =  \\int_{[0,1]^d} \\pi^{d/2}  \\cos\\left(\\sqrt{ \\frac 12 \\sum_{j=1}^d\\Phi^{-1}(x_j)}\\right)  \\, \\rm d \\boldsymbol{x},$$ where $||\\boldsymbol{x}||$ is the Euclidean norm, $\\mathsf{I}_d$ is the $d$-dimensional identity matrix, and $\\Phi$ denotes the standard normal cumulative distribution function. When $d=2$, $\\mu \\approx 1.80819$.</p> <p>The Keister function is implemented below with help from NumPy [3] in the following code snippet:</p> In\u00a0[4]: Copied! <pre>import numpy as np\ndef keister(x):\n    \"\"\"\n    x: nxd numpy ndarray\n       n samples\n       d dimensions\n\n    returns n-vector of the Keister function\n    evaluated at the n input samples\n    \"\"\"\n    d = x.shape[-1]\n    norm_x = np.sqrt((x**2).sum(-1))\n    k = np.pi**(d/2) * np.cos(norm_x)\n    return k # size n vector\n</pre> import numpy as np def keister(x):     \"\"\"     x: nxd numpy ndarray        n samples        d dimensions      returns n-vector of the Keister function     evaluated at the n input samples     \"\"\"     d = x.shape[-1]     norm_x = np.sqrt((x**2).sum(-1))     k = np.pi**(d/2) * np.cos(norm_x)     return k # size n vector <p>In addition to our Keister integrand and Gaussian true measure, we must select a discrete distribution, and a stopping criterion [4]. The stopping criterion determines the number of points at which to evaluate the integrand in order for the mean approximation to be accurate within a user-specified error tolerance, $\\varepsilon$. The discrete distribution determines the sites at which the integrand is evaluated.</p> <p>For this Keister example, we select the lattice sequence as the discrete distribution and corresponding cubature-based stopping criterion [5]. The discrete distribution, true measure, integrand, and stopping criterion are then constructed within the QMCPy framework below.</p> In\u00a0[5]: Copied! <pre>import qmcpy\nd = 2\ndiscrete_distrib = qmcpy.Lattice(dimension = d)\ntrue_measure = qmcpy.Gaussian(discrete_distrib, mean = 0, covariance = 1/2)\nintegrand = qmcpy.CustomFun(true_measure,keister)\nstopping_criterion = qmcpy.CubQMCLatticeG(integrand = integrand, abs_tol = 1e-3)\n</pre> import qmcpy d = 2 discrete_distrib = qmcpy.Lattice(dimension = d) true_measure = qmcpy.Gaussian(discrete_distrib, mean = 0, covariance = 1/2) integrand = qmcpy.CustomFun(true_measure,keister) stopping_criterion = qmcpy.CubQMCLatticeG(integrand = integrand, abs_tol = 1e-3) <p>Calling integrate on the stopping_criterion instance returns the numerical solution and a data object. Printing the data object will provide a neat summary of the integration problem. For details of the output fields, refer to the online, searchable QMCPy Documentation at https://qmcpy.readthedocs.io/.</p> In\u00a0[6]: Copied! <pre>solution, data = stopping_criterion.integrate()\nprint(data)\n</pre> solution, data = stopping_criterion.integrate() print(data) <pre>Data (Data)\n    solution        1.808\n    comb_bound_low  1.808\n    comb_bound_high 1.809\n    comb_bound_diff 0.001\n    comb_flags      1\n    n_total         2^(13)\n    n               2^(13)\n    time_integrate  0.013\nCubQMCLatticeG (AbstractStoppingCriterion)\n    abs_tol         0.001\n    rel_tol         0\n    n_init          2^(10)\n    n_limit         2^(30)\nCustomFun (AbstractIntegrand)\nGaussian (AbstractTrueMeasure)\n    mean            0\n    covariance      2^(-1)\n    decomp_type     PCA\nLattice (AbstractLDDiscreteDistribution)\n    d               2^(1)\n    replications    1\n    randomize       SHIFT\n    gen_vec_source  kuo.lattice-33002-1024-1048576.9125.txt\n    order           NATURAL\n    n_limit         2^(20)\n    entropy         331737392988868539347762820736945757498\n</pre> <p>This guide is not meant to be exhaustive but rather a quick introduction to the QMCPy framework and syntax. In an upcoming blog, we will take a closer look at low-discrepancy sequences such as the lattice sequence from the above example.</p>"},{"location":"demos/quickstart/#a-qmcpy-quick-start","title":"A QMCPy Quick Start\u00b6","text":"<p>In this tutorial, we introduce QMCPy [1]  by an example. QMCPy can be installed with pip install qmcpy or cloned from the  QMCSoftware GitHub repository.</p>"},{"location":"demos/quickstart/#references","title":"References\u00b6","text":"<ol> <li>Choi,  S.-C.  T.,  Hickernell,  F.,  McCourt,  M., Rathinavel J., &amp;  Sorokin,  A. QMCPy:  A quasi-Monte  Carlo  Python  Library. https://qmcsoftware.github.io/QMCSoftware/. 2020.</li> <li>Keister, B. D. Multidimensional Quadrature Algorithms. Computers in Physics 10, 119\u2013122 (1996).</li> <li>Oliphant,  T., Guide  to  NumPy https://ecs.wgtn.ac.nz/foswiki/pub/Support/ManualPagesAndDocumentation/numpybook.pdf (Trelgol Publishing USA, 2006).</li> <li>Hickernell, F., Choi, S.-C. T., Jiang, L. &amp; Jimenez Rugama, L. A. in WileyStatsRef-Statistics Reference Online (eds Davidian, M.et al.) (John Wiley &amp; Sons Ltd., 2018).</li> <li>Jimenez Rugama, L. A. &amp; Hickernell, F. Adaptive  Multidimensional  Integration  Based  on  Rank-1  Lattices in Monte  Carlo  and  Quasi-Monte  Carlo Methods: MCQMC, Leuven, Belgium, April 2014 (eds Cools, R. &amp; Nuyens, D.) 163.arXiv:1411.1966 (Springer-Verlag, Berlin, 2016), 407\u2013422.</li> </ol>"},{"location":"demos/ray_tracing/","title":"Ray Tracing","text":"In\u00a0[9]: Copied! <pre>from qmcpy import *\nfrom numpy import *\nfrom time import time\nfrom PIL import Image\nfrom matplotlib import pyplot\nfrom threading import Thread\n%matplotlib inline\n</pre> from qmcpy import * from numpy import * from time import time from PIL import Image from matplotlib import pyplot from threading import Thread %matplotlib inline In\u00a0[10]: Copied! <pre># constants\nEPS = 1e-8 # numerical precision error tolerance\nnorm = lambda v: sqrt(dot(v,v))\ne = array([0,0,0],dtype=float) # eye location at the orgin\n</pre> # constants EPS = 1e-8 # numerical precision error tolerance norm = lambda v: sqrt(dot(v,v)) e = array([0,0,0],dtype=float) # eye location at the orgin In\u00a0[11]: Copied! <pre>class Camera(object):\n    \"\"\" An object to render the scene. \"\"\"\n    def __init__(self, ax, scene, px=8, parallel_x_blocks=2, parallel_y_blocks=2, image_angle=90, image_dist=1):\n        \"\"\"\n        Args:\n            ax (axes): matplotlib ax to plot image on\n            scene (int): object with scene.render_px(p) method. See examples later in the notebook.\n            px (int): number of pixels in height and width. \n                Resolution = px*px.\n            parallel_x_blocks (int): number of cuts along the x axis in which to make parallel.\n            parallel_y_blocks (int): number of cuts along the y axis in which to make parallel. \n                parallel_x/y_blocks must be divisors of px. \n                Number of threads = parallel_y_blocks * parallel_x_blocks. \n        \"\"\"\n        self.ax = ax\n        self.scene = scene\n        self.px = px\n        self.fl = image_dist # distance from eye to image\n        self.i_hw = tan(image_angle/2)*self.fl # half width of the image\n        self.px_hw = self.i_hw/px # half width of a pixel\n        # set parallelization constants\n        self.p_xb = parallel_x_blocks; self.p_yb = parallel_y_blocks\n        bs_x = px/self.p_xb; bs_y = px/self.p_yb # block size for x,y\n        if bs_x%1!=0 or bs_y%1!=0: raise Exception('parallel_x/y_blocks must divide px')\n        self.bs_x = int(bs_x); self.bs_y = int(bs_y)\n    def render(self):\n        \"\"\" Render the image. \"\"\"\n        t0 = time() # start a timer\n        img = Image.new('RGB',(self.px,self.px),(0,0,0))\n        if self.p_xb==1 and self.p_yb==1:\n            # use non-parallel processing (helpful for debugging)\n            self.block_render(img,0,self.px,0,self.px)\n        else:\n            # parallel processing\n            threads = [None]*(self.p_xb*self.p_yb)\n            i_t = 0 # thread index\n            for xb in range(0,self.px,self.bs_x):\n                for yb in range(0,self.px,self.bs_y):\n                    threads[i_t] = Thread(target=self.block_render, args=(img,xb,xb+self.bs_x,yb,yb+self.bs_y))\n                    threads[i_t].start() # start threads\n                    i_t += 1\n            for i in range(len(threads)): threads[i].join() # wait for all threads to complete\n        self.ax.axis('off')\n        self.ax.imshow(asarray(img))\n        print('Render took %.1f seconds'%(time()-t0))\n    def block_render(self, img, px_x_start, px_x_end, px_y_start, px_y_end):\n        \"\"\"\n        Render a block of the image. \n        \n        Args:\n            img (PIL.Image): the image to color pixels of. \n            px_x_start (int): x index of pixel to start rendering at. \n            px_x_end (int): x index of pixel to end rendering at. \n            px_y_start (int): y index of pixel to start rendering at. \n            px_y_end (int): y index of pixel to start rendering at.\n        \"\"\"\n        for p_x in range(px_x_start,px_x_end):\n            for p_y in range(px_y_start,px_y_end):\n                p = array([-self.i_hw+2*self.i_hw*p_x/self.px,-self.i_hw+2*self.i_hw*p_y/self.px,self.fl])\n                color = self.scene.render_px(p)\n                img.putpixel((p_x,self.px-p_y-1),color)    \n</pre> class Camera(object):     \"\"\" An object to render the scene. \"\"\"     def __init__(self, ax, scene, px=8, parallel_x_blocks=2, parallel_y_blocks=2, image_angle=90, image_dist=1):         \"\"\"         Args:             ax (axes): matplotlib ax to plot image on             scene (int): object with scene.render_px(p) method. See examples later in the notebook.             px (int): number of pixels in height and width.                  Resolution = px*px.             parallel_x_blocks (int): number of cuts along the x axis in which to make parallel.             parallel_y_blocks (int): number of cuts along the y axis in which to make parallel.                  parallel_x/y_blocks must be divisors of px.                  Number of threads = parallel_y_blocks * parallel_x_blocks.          \"\"\"         self.ax = ax         self.scene = scene         self.px = px         self.fl = image_dist # distance from eye to image         self.i_hw = tan(image_angle/2)*self.fl # half width of the image         self.px_hw = self.i_hw/px # half width of a pixel         # set parallelization constants         self.p_xb = parallel_x_blocks; self.p_yb = parallel_y_blocks         bs_x = px/self.p_xb; bs_y = px/self.p_yb # block size for x,y         if bs_x%1!=0 or bs_y%1!=0: raise Exception('parallel_x/y_blocks must divide px')         self.bs_x = int(bs_x); self.bs_y = int(bs_y)     def render(self):         \"\"\" Render the image. \"\"\"         t0 = time() # start a timer         img = Image.new('RGB',(self.px,self.px),(0,0,0))         if self.p_xb==1 and self.p_yb==1:             # use non-parallel processing (helpful for debugging)             self.block_render(img,0,self.px,0,self.px)         else:             # parallel processing             threads = [None]*(self.p_xb*self.p_yb)             i_t = 0 # thread index             for xb in range(0,self.px,self.bs_x):                 for yb in range(0,self.px,self.bs_y):                     threads[i_t] = Thread(target=self.block_render, args=(img,xb,xb+self.bs_x,yb,yb+self.bs_y))                     threads[i_t].start() # start threads                     i_t += 1             for i in range(len(threads)): threads[i].join() # wait for all threads to complete         self.ax.axis('off')         self.ax.imshow(asarray(img))         print('Render took %.1f seconds'%(time()-t0))     def block_render(self, img, px_x_start, px_x_end, px_y_start, px_y_end):         \"\"\"         Render a block of the image.                   Args:             img (PIL.Image): the image to color pixels of.              px_x_start (int): x index of pixel to start rendering at.              px_x_end (int): x index of pixel to end rendering at.              px_y_start (int): y index of pixel to start rendering at.              px_y_end (int): y index of pixel to start rendering at.         \"\"\"         for p_x in range(px_x_start,px_x_end):             for p_y in range(px_y_start,px_y_end):                 p = array([-self.i_hw+2*self.i_hw*p_x/self.px,-self.i_hw+2*self.i_hw*p_y/self.px,self.fl])                 color = self.scene.render_px(p)                 img.putpixel((p_x,self.px-p_y-1),color)     In\u00a0[12]: Copied! <pre>class Plane(object):\n    def __init__(self, norm_axis, position, color):\n        \"\"\"\n        Args:\n            norm_axis (str): either 'x', 'y', or 'z'.\n            position (str): constant position of plane along the norm_axis.\n            color (tuple): length 3 tuple of rgb values.\n        \"\"\"\n        dim_dict = {'x':0,'y':1,'z':2}\n        self.d = dim_dict[norm_axis]\n        self.pos = position # self.norm_axis coordinate of the floor \n        self.color = color\n    def hit(self, o, u):\n        \"\"\"\n        Test if the beam o+tu hits the plane.\n        \n        Args:\n            o (ndarray): length 3 origin point.\n            u (ndarray): length 3 unit vector.\n        \n        Returns:\n            tuple: \n                - hit (bool): was an object hit?\n                - hit_p (ndarray): point where beam intersects object.\n                - color (tuple): length 3 tuple rgb value.\n        \"\"\"\n        k = u[self.d]\n        if k != 0:\n            t = (self.pos - o[self.d]) / u[self.d]\n            if t &gt; EPS:\n                return True, o+t*u # ray intersects the plane\n        return False, None # ray misses the plane\n    def normal(self, o, u):\n        \"\"\"\n        Get the unit normal vector to the plane at this point.\n        \n        Args:\n            o (ndarray): length 3 origin point.\n            u (ndarray): length 3 unit vector in direction of light.\n        \n        \n        Returns:\n            ndarray: length three unit normal vector.\n        \"\"\"\n        v = array([0,0,0])\n        v[self.d] = 1\n        if dot(v,u)&lt;0: \n            v[self.d] = -1\n        return v\n</pre> class Plane(object):     def __init__(self, norm_axis, position, color):         \"\"\"         Args:             norm_axis (str): either 'x', 'y', or 'z'.             position (str): constant position of plane along the norm_axis.             color (tuple): length 3 tuple of rgb values.         \"\"\"         dim_dict = {'x':0,'y':1,'z':2}         self.d = dim_dict[norm_axis]         self.pos = position # self.norm_axis coordinate of the floor          self.color = color     def hit(self, o, u):         \"\"\"         Test if the beam o+tu hits the plane.                  Args:             o (ndarray): length 3 origin point.             u (ndarray): length 3 unit vector.                  Returns:             tuple:                  - hit (bool): was an object hit?                 - hit_p (ndarray): point where beam intersects object.                 - color (tuple): length 3 tuple rgb value.         \"\"\"         k = u[self.d]         if k != 0:             t = (self.pos - o[self.d]) / u[self.d]             if t &gt; EPS:                 return True, o+t*u # ray intersects the plane         return False, None # ray misses the plane     def normal(self, o, u):         \"\"\"         Get the unit normal vector to the plane at this point.                  Args:             o (ndarray): length 3 origin point.             u (ndarray): length 3 unit vector in direction of light.                           Returns:             ndarray: length three unit normal vector.         \"\"\"         v = array([0,0,0])         v[self.d] = 1         if dot(v,u)&lt;0:              v[self.d] = -1         return v In\u00a0[13]: Copied! <pre>class Ball(object):\n    def __init__(self, center, radius, color):\n        \"\"\"\n        Args:\n            center (ndarray): length 3 center position of the ball.\n            radius (float): radius of the ball.\n            color (tuple): length 3 tuple of rgb values.\n        \"\"\"\n        self.c = center\n        self.r = radius\n        self.color = color\n    def hit(self, o, u):\n        \"\"\"\n        Test if the beam o+tu hits the ball.\n        \n        Args:\n            o (ndarray): length 3 origin point.\n            u (ndarray): length 3 unit vector.\n        \n        Returns:\n            tuple: \n                - hit (bool): was an object was hit?\n                - hit_p (ndarray): point where beam intersects object.\n                - color (tuple): length 3 tuple rgb value.\n        \"\"\"\n        q = o - self.c\n        a = dot(u,u)\n        b = 2*dot(u,q)\n        c = dot(q,q) - self.r**2\n        d = b**2 - 4*a*c\n        if d &gt; 0: # ray intersects sphere\n            tt = (-b + array([1,-1],dtype=float)*sqrt(d)) / (2**a)\n            tt = tt[tt&gt;EPS] # only want intersection from rays moving in positive direction\n            if len(tt) &gt;= 1: # at least one positive intersection\n                # beam going forward intersects ball\n                t = min(tt)\n                return True, o+t*u\n        return False, None # ray does not intersect sphere or only intersects in opposite direction\n    def normal(self, o, u):\n        \"\"\"\n        Get the unit normal vector to the sphere at thi point.\n        \n        Args:\n            o (ndarray): length 3 origin point.\n            u (ndarray): length 3 unit vector in direction of light.\n        \n        Returns:\n            ndarray: length three unit normal vector.\n        \"\"\"\n        v = (o-self.c)\n        v_u = v/norm(v)\n        return v_u\n</pre> class Ball(object):     def __init__(self, center, radius, color):         \"\"\"         Args:             center (ndarray): length 3 center position of the ball.             radius (float): radius of the ball.             color (tuple): length 3 tuple of rgb values.         \"\"\"         self.c = center         self.r = radius         self.color = color     def hit(self, o, u):         \"\"\"         Test if the beam o+tu hits the ball.                  Args:             o (ndarray): length 3 origin point.             u (ndarray): length 3 unit vector.                  Returns:             tuple:                  - hit (bool): was an object was hit?                 - hit_p (ndarray): point where beam intersects object.                 - color (tuple): length 3 tuple rgb value.         \"\"\"         q = o - self.c         a = dot(u,u)         b = 2*dot(u,q)         c = dot(q,q) - self.r**2         d = b**2 - 4*a*c         if d &gt; 0: # ray intersects sphere             tt = (-b + array([1,-1],dtype=float)*sqrt(d)) / (2**a)             tt = tt[tt&gt;EPS] # only want intersection from rays moving in positive direction             if len(tt) &gt;= 1: # at least one positive intersection                 # beam going forward intersects ball                 t = min(tt)                 return True, o+t*u         return False, None # ray does not intersect sphere or only intersects in opposite direction     def normal(self, o, u):         \"\"\"         Get the unit normal vector to the sphere at thi point.                  Args:             o (ndarray): length 3 origin point.             u (ndarray): length 3 unit vector in direction of light.                  Returns:             ndarray: length three unit normal vector.         \"\"\"         v = (o-self.c)         v_u = v/norm(v)         return v_u In\u00a0[14]: Copied! <pre>class PointLight(object):\n    \"\"\" A lamp that is a point and emits. light in all directions. \"\"\"\n    def __init__(self, position, intensity):\n        \"\"\"\n        Args:\n            position (ndarray): length 3 coordinate of light position. \n            intensity (float): intensity of the light, between 0 and 1. \n        \"\"\"\n        self.p = position\n        self.i = intensity\n</pre> class PointLight(object):     \"\"\" A lamp that is a point and emits. light in all directions. \"\"\"     def __init__(self, position, intensity):         \"\"\"         Args:             position (ndarray): length 3 coordinate of light position.              intensity (float): intensity of the light, between 0 and 1.          \"\"\"         self.p = position         self.i = intensity In\u00a0[15]: Copied! <pre>class CustomScene(object):\n    def __init__(self, objs, light, n, d, mc_type):\n        self.objs = objs\n        self.light = light\n        self.black = array([0,0,0],dtype=float) # the color black \n        self.n = n\n        self.d = d\n        self.mc_type = mc_type\n        # generate constant samples to be used for every ray tracing event\n        if self.mc_type == 'IID':\n            self.pts = IIDStdUniform(2*self.d).gen_samples(self.n)\n        elif self.mc_type == 'SOBOL':\n            self.pts = Sobol(2*self.d,order=\"GRAY\").gen_samples(self.n) \n        else: \n            raise Exception(\"mc_type must be IID or Sobol\")\n    def find_closest_obj(self,o,v):\n        \"\"\"\n        Find the closest object to point o heading in direction v\n\n        Args:\n            o (ndarray): length 3 coordinate of point we will try and find closest object to\n\n        Returns:\n            tuple: \n                hit (bool): weather any objects were hit. \n                hit_p (ndarray): length 3 coordinates of where obj was hit. \n                hit_dist (float): distance from hit_p to o. \n                hit_obj (object): the object that was hit.\n        \"\"\"\n        hit,hit_p,hit_dist,hit_obj = False,None,inf,None\n        for obj in self.objs:\n            obj_hit,obj_p = obj.hit(o,v)\n            if obj_hit:\n                v2 = obj_p-o # vector from o to object position\n                obj_dist = sqrt(dot(v2,v2))\n                if obj_dist &lt; hit_dist:\n                    hit,hit_p,hit_dist,hit_obj = True,obj_p,obj_dist,obj\n        return hit,hit_p,hit_dist,hit_obj\n    def get_obj_color(self,obj,p,l):\n        \"\"\"\n        Get the objects color at point p with light in direction l.\n\n        Args:\n            obj (object): object on which p lies\n            p (ndarray): length 3 coordinate of point on the object\n            l (ndarray): length 3 vector of direction from p to light source\n\n        Returns:\n            ndarray: length 3 RGB color\n        \"\"\"\n        n_v = obj.normal(p,l) # normal vector to obj at point p\n        color = obj.color*self.light.i*dot(n_v,l) / (norm(n_v)*norm(l))\n        return color\n    def beam_r(self,o,v,n,d,pts,nidx,didx):\n        \"\"\"\n        Recursive (Quasi-)Monte Carlo simulation of a light beam\n\n        Args:\n            o (ndarray): length 3 coordinate of current light beam position\n            v (ndarray): length 3 vector of light beam direction\n            n (ndarray): number of rays to cast when it cannot find light directly \n            d (int): remaining bounces before beam gives up\n            pts (ndarray): n samples x d dimension ndarray of samples generated by QMCPy\n            nidx (int): 2*(index of the beam)\n            didx (int): index of the sample\n        \"\"\"\n        hit,hit_p,hit_dist,hit_obj = self.find_closest_obj(o,v)\n        if hit: # an object was hit\n            l = self.light.p-hit_p # vector from position where beam hit to the lamp\n            l_dist = norm(l) # distance from hit location to lamp\n            l_u = l/l_dist # unit vector of l\n            itw,itw_p,itw_dist,itw_obj = self.find_closest_obj(hit_p,l_u) # find any object in the way\n            if itw and itw_dist&lt;= l_dist: # object between hit object and the lamp\n                if d==0:\n                    # no remaining bounces --&gt; return black (give up)\n                    return self.black\n                else:\n                    # beam has remaining bounces\n                    color_total = self.black.copy()\n                    for i in range(n):\n                        theta_0 = 2*pi*pts[nidx+i,didx]\n                        theta_1 = 2*pi*pts[nidx+i,didx+1]\n                        x = sin(theta_0)*sin(theta_1)\n                        y = sin(theta_0)*cos(theta_0)\n                        z = sin(theta_1)\n                        v_rand = array([x,y,z],dtype=float) # random direction\n                        ho_n = hit_obj.normal(hit_p,l_u)\n                        if dot(v_rand,ho_n) &lt; 0: v_rand = -v_rand # flip direction to correct hemisphere\n                        obj_color = self.get_obj_color(hit_obj,hit_p,l_u)\n                        color_total += obj_color*self.beam_r(hit_p,v_rand,n=1,d=d-1,pts=pts,nidx=nidx+i,didx=didx+2)\n                    return color_total/n # take the average of many simulations\n            else: # nothin between the object and the light source\n                # get the color based on point, normal to obj, and direction to light\n                return self.get_obj_color(hit_obj,hit_p,l_u)\n        return self.black # nothing hit --&gt; return black '\n    def render_px(self,p):\n        \"\"\"\n        Get pixel value for ball-lamp-floor scene\n\n        Args:\n            p (ndarray): length 3 array coordinates of center of pixel to render\n        \"\"\"\n        u = (p-e)/norm(p-e) # unit vector in direction of eye to pixel\n        color_0_1 = self.beam_r(e,u,n=self.n,d=self.d,pts=self.pts,nidx=0,didx=0)\n        color = (color_0_1*256).astype(int)\n        return color[0],color[1],color[2]\n</pre> class CustomScene(object):     def __init__(self, objs, light, n, d, mc_type):         self.objs = objs         self.light = light         self.black = array([0,0,0],dtype=float) # the color black          self.n = n         self.d = d         self.mc_type = mc_type         # generate constant samples to be used for every ray tracing event         if self.mc_type == 'IID':             self.pts = IIDStdUniform(2*self.d).gen_samples(self.n)         elif self.mc_type == 'SOBOL':             self.pts = Sobol(2*self.d,order=\"GRAY\").gen_samples(self.n)          else:              raise Exception(\"mc_type must be IID or Sobol\")     def find_closest_obj(self,o,v):         \"\"\"         Find the closest object to point o heading in direction v          Args:             o (ndarray): length 3 coordinate of point we will try and find closest object to          Returns:             tuple:                  hit (bool): weather any objects were hit.                  hit_p (ndarray): length 3 coordinates of where obj was hit.                  hit_dist (float): distance from hit_p to o.                  hit_obj (object): the object that was hit.         \"\"\"         hit,hit_p,hit_dist,hit_obj = False,None,inf,None         for obj in self.objs:             obj_hit,obj_p = obj.hit(o,v)             if obj_hit:                 v2 = obj_p-o # vector from o to object position                 obj_dist = sqrt(dot(v2,v2))                 if obj_dist &lt; hit_dist:                     hit,hit_p,hit_dist,hit_obj = True,obj_p,obj_dist,obj         return hit,hit_p,hit_dist,hit_obj     def get_obj_color(self,obj,p,l):         \"\"\"         Get the objects color at point p with light in direction l.          Args:             obj (object): object on which p lies             p (ndarray): length 3 coordinate of point on the object             l (ndarray): length 3 vector of direction from p to light source          Returns:             ndarray: length 3 RGB color         \"\"\"         n_v = obj.normal(p,l) # normal vector to obj at point p         color = obj.color*self.light.i*dot(n_v,l) / (norm(n_v)*norm(l))         return color     def beam_r(self,o,v,n,d,pts,nidx,didx):         \"\"\"         Recursive (Quasi-)Monte Carlo simulation of a light beam          Args:             o (ndarray): length 3 coordinate of current light beam position             v (ndarray): length 3 vector of light beam direction             n (ndarray): number of rays to cast when it cannot find light directly              d (int): remaining bounces before beam gives up             pts (ndarray): n samples x d dimension ndarray of samples generated by QMCPy             nidx (int): 2*(index of the beam)             didx (int): index of the sample         \"\"\"         hit,hit_p,hit_dist,hit_obj = self.find_closest_obj(o,v)         if hit: # an object was hit             l = self.light.p-hit_p # vector from position where beam hit to the lamp             l_dist = norm(l) # distance from hit location to lamp             l_u = l/l_dist # unit vector of l             itw,itw_p,itw_dist,itw_obj = self.find_closest_obj(hit_p,l_u) # find any object in the way             if itw and itw_dist&lt;= l_dist: # object between hit object and the lamp                 if d==0:                     # no remaining bounces --&gt; return black (give up)                     return self.black                 else:                     # beam has remaining bounces                     color_total = self.black.copy()                     for i in range(n):                         theta_0 = 2*pi*pts[nidx+i,didx]                         theta_1 = 2*pi*pts[nidx+i,didx+1]                         x = sin(theta_0)*sin(theta_1)                         y = sin(theta_0)*cos(theta_0)                         z = sin(theta_1)                         v_rand = array([x,y,z],dtype=float) # random direction                         ho_n = hit_obj.normal(hit_p,l_u)                         if dot(v_rand,ho_n) &lt; 0: v_rand = -v_rand # flip direction to correct hemisphere                         obj_color = self.get_obj_color(hit_obj,hit_p,l_u)                         color_total += obj_color*self.beam_r(hit_p,v_rand,n=1,d=d-1,pts=pts,nidx=nidx+i,didx=didx+2)                     return color_total/n # take the average of many simulations             else: # nothin between the object and the light source                 # get the color based on point, normal to obj, and direction to light                 return self.get_obj_color(hit_obj,hit_p,l_u)         return self.black # nothing hit --&gt; return black '     def render_px(self,p):         \"\"\"         Get pixel value for ball-lamp-floor scene          Args:             p (ndarray): length 3 array coordinates of center of pixel to render         \"\"\"         u = (p-e)/norm(p-e) # unit vector in direction of eye to pixel         color_0_1 = self.beam_r(e,u,n=self.n,d=self.d,pts=self.pts,nidx=0,didx=0)         color = (color_0_1*256).astype(int)         return color[0],color[1],color[2] In\u00a0[18]: Copied! <pre># create a scene\nobjs = [\n    Plane(norm_axis='y', position=-50, color=array([.75,.75,.75],dtype=float)), # floor\n    Plane(norm_axis='y', position=50,  color=array([.75,.75,.75],dtype=float)), # ceiling\n    Plane(norm_axis='x', position=50,  color=array([.75,.75,.75],dtype=float)), # right wall\n    Plane(norm_axis='x', position=-50, color=array([.75,.75,.75],dtype=float)), # left wall\n    Plane(norm_axis='z', position=150,  color=array([.75,.75,.75],dtype=float)), # back wall\n    Ball(center=array([-25,25,75],dtype=float), radius=20, color=array([1,0,0],dtype=float)), # ball\n    Ball(center=array([25,25,75],dtype=float),  radius=20, color=array([0,1,0],dtype=float)), # ball\n    Ball(center=array([0,-25,75],dtype=float),  radius=20, color=array([0,0,1],dtype=float)), # ball\n]\nlight = PointLight(position=array([0,25,0],dtype=float), intensity=1)\n# parameters\nn = 16 # number of beams\nd = 16 # max bounces of any given beam\npx = 256\n# render image\nfig,ax = pyplot.subplots(ncols=2,nrows=1,figsize=(20,10))# render scene\n# IID (MC)\nscene = CustomScene(objs,light,n,d,mc_type='IID')\ncamera = Camera(ax[0], scene, px=px, parallel_x_blocks=1, parallel_y_blocks=1, image_angle=pi/2, image_dist=1)\ncamera.render()\n# Sobol (QMC)\nscene = CustomScene(objs,light,n,d,mc_type='SOBOL')\ncamera = Camera(ax[1], scene, px=px, parallel_x_blocks=1, parallel_y_blocks=1, image_angle=pi/2, image_dist=1)\ncamera.render()\n</pre> # create a scene objs = [     Plane(norm_axis='y', position=-50, color=array([.75,.75,.75],dtype=float)), # floor     Plane(norm_axis='y', position=50,  color=array([.75,.75,.75],dtype=float)), # ceiling     Plane(norm_axis='x', position=50,  color=array([.75,.75,.75],dtype=float)), # right wall     Plane(norm_axis='x', position=-50, color=array([.75,.75,.75],dtype=float)), # left wall     Plane(norm_axis='z', position=150,  color=array([.75,.75,.75],dtype=float)), # back wall     Ball(center=array([-25,25,75],dtype=float), radius=20, color=array([1,0,0],dtype=float)), # ball     Ball(center=array([25,25,75],dtype=float),  radius=20, color=array([0,1,0],dtype=float)), # ball     Ball(center=array([0,-25,75],dtype=float),  radius=20, color=array([0,0,1],dtype=float)), # ball ] light = PointLight(position=array([0,25,0],dtype=float), intensity=1) # parameters n = 16 # number of beams d = 16 # max bounces of any given beam px = 256 # render image fig,ax = pyplot.subplots(ncols=2,nrows=1,figsize=(20,10))# render scene # IID (MC) scene = CustomScene(objs,light,n,d,mc_type='IID') camera = Camera(ax[0], scene, px=px, parallel_x_blocks=1, parallel_y_blocks=1, image_angle=pi/2, image_dist=1) camera.render() # Sobol (QMC) scene = CustomScene(objs,light,n,d,mc_type='SOBOL') camera = Camera(ax[1], scene, px=px, parallel_x_blocks=1, parallel_y_blocks=1, image_angle=pi/2, image_dist=1) camera.render() <pre>Render took 27.2 seconds\nRender took 28.4 seconds\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"demos/ray_tracing/#basic-ray-tracing","title":"Basic Ray Tracing\u00b6","text":"<ul> <li>Introduction to Ray Tracing: a Simple Method for Creating 3D Images by Scratchapixel 2.0</li> <li>Computer Graphics from scratch by Gabriel Gambetta</li> <li>Ray Tracing: Graphics for the Masses by Paul Rademacher</li> </ul>"},{"location":"demos/sample_scatter_plots/","title":"Plotting Points Manually","text":"In\u00a0[14]: Copied! <pre>from qmcpy import *\nfrom copy import deepcopy\nfrom numpy import ceil, linspace, meshgrid, zeros, array, arange, random\nfrom mpl_toolkits.mplot3d.axes3d import Axes3D\n\nimport matplotlib\n%matplotlib inline\nimport matplotlib.pyplot as plt\n\nplt.rc('font', size=16)          # controls default text sizes\nplt.rc('axes', titlesize=16)     # fontsize of the axes title\nplt.rc('axes', labelsize=16)    # fontsize of the x and y labels\nplt.rc('xtick', labelsize=16)    # fontsize of the tick labels\nplt.rc('ytick', labelsize=16)    # fontsize of the tick labels\nplt.rc('legend', fontsize=16)    # legend fontsize\nplt.rc('figure', titlesize=16)  # fontsize of the figure title\n</pre> from qmcpy import * from copy import deepcopy from numpy import ceil, linspace, meshgrid, zeros, array, arange, random from mpl_toolkits.mplot3d.axes3d import Axes3D  import matplotlib %matplotlib inline import matplotlib.pyplot as plt  plt.rc('font', size=16)          # controls default text sizes plt.rc('axes', titlesize=16)     # fontsize of the axes title plt.rc('axes', labelsize=16)    # fontsize of the x and y labels plt.rc('xtick', labelsize=16)    # fontsize of the tick labels plt.rc('ytick', labelsize=16)    # fontsize of the tick labels plt.rc('legend', fontsize=16)    # legend fontsize plt.rc('figure', titlesize=16)  # fontsize of the figure title In\u00a0[15]: Copied! <pre>n = 128\n</pre> n = 128 In\u00a0[16]: Copied! <pre>random.seed(7)\ndiscrete_distribs = [\n    IIDStdUniform(dimension=2, seed=7),\n    #IIDStdGaussian(dimension=2, seed=7),\n    #CustomIIDDistribution(lambda n: random.exponential(scale=2./3,size=(n,2)))\n]\ndd_names = [\"$\\\\mathcal{U}_2\\\\,(0,1)$\", \"$\\\\mathcal{N}_2\\\\,(0,1)$\", \"Exp(1.5)\"]\ncolors = [\"b\", \"r\", \"g\"]\nlims = [[0, 1], [-2.5, 2.5],[0,4]]\nfig, ax = plt.subplots(nrows=1, ncols=1, figsize=(16, 6))\nfor i, (dd_obj, color, lim, dd_name) in enumerate(zip(discrete_distribs, colors, lims, dd_names)):\n    samples = dd_obj.gen_samples(n)\n    ax.scatter(samples[:, 0], samples[:, 1], color=color)\n    ax.set_xlabel(\"$x_1$\")\n    ax.set_ylabel(\"$x_2$\")\n    ax.set_xlim(lim)\n    ax.set_ylim(lim)\n    ax.set_aspect(\"equal\")\n    ax.set_title(dd_name)\nfig.suptitle(\"IID Discrete Distributions\");\n</pre> random.seed(7) discrete_distribs = [     IIDStdUniform(dimension=2, seed=7),     #IIDStdGaussian(dimension=2, seed=7),     #CustomIIDDistribution(lambda n: random.exponential(scale=2./3,size=(n,2))) ] dd_names = [\"$\\\\mathcal{U}_2\\\\,(0,1)$\", \"$\\\\mathcal{N}_2\\\\,(0,1)$\", \"Exp(1.5)\"] colors = [\"b\", \"r\", \"g\"] lims = [[0, 1], [-2.5, 2.5],[0,4]] fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(16, 6)) for i, (dd_obj, color, lim, dd_name) in enumerate(zip(discrete_distribs, colors, lims, dd_names)):     samples = dd_obj.gen_samples(n)     ax.scatter(samples[:, 0], samples[:, 1], color=color)     ax.set_xlabel(\"$x_1$\")     ax.set_ylabel(\"$x_2$\")     ax.set_xlim(lim)     ax.set_ylim(lim)     ax.set_aspect(\"equal\")     ax.set_title(dd_name) fig.suptitle(\"IID Discrete Distributions\"); In\u00a0[17]: Copied! <pre>discrete_distribs = [\n    Lattice(dimension=2, randomize=True, seed=7),\n    Sobol(dimension=2, randomize=True, seed=7),\n    Halton(dimension=2,seed=7)]\ndd_names = [\"Shifted Lattice\", \"Scrambled Sobol\", \"Generalized Halton\", \"Randomized Korobov\"]\ncolors = [\"g\", \"c\", \"m\", \"r\"]\nfig, ax = plt.subplots(nrows=1, ncols=3, figsize=(16, 6))\nfor i, (dd_obj, color, dd_name) in \\\n        enumerate(zip(discrete_distribs, colors, dd_names)):\n    samples = dd_obj.gen_samples(n)\n    ax[i].scatter(samples[:, 0], samples[:, 1], color=color)\n    ax[i].set_xlabel(\"$x_1$\")\n    ax[i].set_ylabel(\"$x_2$\")\n    ax[i].set_xlim([0, 1])\n    ax[i].set_ylim([0, 1])\n    ax[i].set_aspect(\"equal\")\n    ax[i].set_title(dd_name)\nfig.suptitle(\"Low Discrepancy Discrete Distributions\")\nplt.tight_layout();\n</pre> discrete_distribs = [     Lattice(dimension=2, randomize=True, seed=7),     Sobol(dimension=2, randomize=True, seed=7),     Halton(dimension=2,seed=7)] dd_names = [\"Shifted Lattice\", \"Scrambled Sobol\", \"Generalized Halton\", \"Randomized Korobov\"] colors = [\"g\", \"c\", \"m\", \"r\"] fig, ax = plt.subplots(nrows=1, ncols=3, figsize=(16, 6)) for i, (dd_obj, color, dd_name) in \\         enumerate(zip(discrete_distribs, colors, dd_names)):     samples = dd_obj.gen_samples(n)     ax[i].scatter(samples[:, 0], samples[:, 1], color=color)     ax[i].set_xlabel(\"$x_1$\")     ax[i].set_ylabel(\"$x_2$\")     ax[i].set_xlim([0, 1])     ax[i].set_ylim([0, 1])     ax[i].set_aspect(\"equal\")     ax[i].set_title(dd_name) fig.suptitle(\"Low Discrepancy Discrete Distributions\") plt.tight_layout(); In\u00a0[18]: Copied! <pre>def plot_tm_transformed(tm_name, color, lim, measure, **kwargs):\n    fig, ax = plt.subplots(nrows=1, ncols=4, figsize=(20, 6))\n    i = 0\n    # IID Distributions\n    iid_distribs = [\n        IIDStdUniform(dimension=2, seed=7),\n        #IIDStdGaussian(dimension=2, seed=7)\n    ]\n    iid_names = [\n        \"IID $\\\\mathcal{U}\\\\,(0,1)^2$\",\n        \"IID $\\\\mathcal{N}\\\\,(0,1)^2$\"]\n    for distrib, distrib_name in zip(iid_distribs, iid_names):\n        measure_obj = measure(distrib, **kwargs)\n        samples = measure_obj.gen_samples(n)\n        ax[i].scatter(samples[:, 0], samples[:, 1], color=color)\n        i += 1\n    # Quasi Random Distributions\n    qrng_distribs = [\n        Lattice(dimension=2, randomize=True, seed=7),\n        Sobol(dimension=2, randomize=True, seed=7),\n        Halton(dimension=2, randomize=True, seed=7)]\n    qrng_names = [\"Shifted Lattice\",\n                  \"Scrambled Sobol\",\n                  \"Randomized Halton\"]\n    for distrib, distrib_name in zip(qrng_distribs, qrng_names):\n        measure_obj = measure(distrib, **kwargs)\n        samples = measure_obj.gen_samples(n_min=0,n_max=n)\n        ax[i].scatter(samples[:, 0], samples[:, 1], color=color)\n        i += 1\n    # Plot Metas\n    for i,distrib in enumerate(iid_distribs+qrng_distribs):\n        ax[i].set_xlabel(\"$x_1$\")\n        if i==0:\n            ax[i].set_ylabel(\"$x_2$\")\n        else:\n            ax[i].set_yticks([])\n        ax[i].set_xlim(lim)\n        ax[i].set_ylim(lim)\n        ax[i].set_aspect(\"equal\")\n        ax[i].set_title(type(distrib).__name__)\n    fig.suptitle(\"Transformed to %s from...\" % tm_name)\n    plt.tight_layout()\n    prefix = type(measure_obj).__name__;\n</pre> def plot_tm_transformed(tm_name, color, lim, measure, **kwargs):     fig, ax = plt.subplots(nrows=1, ncols=4, figsize=(20, 6))     i = 0     # IID Distributions     iid_distribs = [         IIDStdUniform(dimension=2, seed=7),         #IIDStdGaussian(dimension=2, seed=7)     ]     iid_names = [         \"IID $\\\\mathcal{U}\\\\,(0,1)^2$\",         \"IID $\\\\mathcal{N}\\\\,(0,1)^2$\"]     for distrib, distrib_name in zip(iid_distribs, iid_names):         measure_obj = measure(distrib, **kwargs)         samples = measure_obj.gen_samples(n)         ax[i].scatter(samples[:, 0], samples[:, 1], color=color)         i += 1     # Quasi Random Distributions     qrng_distribs = [         Lattice(dimension=2, randomize=True, seed=7),         Sobol(dimension=2, randomize=True, seed=7),         Halton(dimension=2, randomize=True, seed=7)]     qrng_names = [\"Shifted Lattice\",                   \"Scrambled Sobol\",                   \"Randomized Halton\"]     for distrib, distrib_name in zip(qrng_distribs, qrng_names):         measure_obj = measure(distrib, **kwargs)         samples = measure_obj.gen_samples(n_min=0,n_max=n)         ax[i].scatter(samples[:, 0], samples[:, 1], color=color)         i += 1     # Plot Metas     for i,distrib in enumerate(iid_distribs+qrng_distribs):         ax[i].set_xlabel(\"$x_1$\")         if i==0:             ax[i].set_ylabel(\"$x_2$\")         else:             ax[i].set_yticks([])         ax[i].set_xlim(lim)         ax[i].set_ylim(lim)         ax[i].set_aspect(\"equal\")         ax[i].set_title(type(distrib).__name__)     fig.suptitle(\"Transformed to %s from...\" % tm_name)     plt.tight_layout()     prefix = type(measure_obj).__name__; In\u00a0[19]: Copied! <pre>plot_tm_transformed(\"$\\\\mathcal{U}\\\\,(0,1)^2$\",\"b\",[0, 1],Uniform)\n</pre> plot_tm_transformed(\"$\\\\mathcal{U}\\\\,(0,1)^2$\",\"b\",[0, 1],Uniform) In\u00a0[20]: Copied! <pre>plot_tm_transformed(\"$\\\\mathcal{N}\\\\,(0,1)^2$\",\"r\",[-2.5, 2.5],Gaussian)\n</pre> plot_tm_transformed(\"$\\\\mathcal{N}\\\\,(0,1)^2$\",\"r\",[-2.5, 2.5],Gaussian) In\u00a0[21]: Copied! <pre>plot_tm_transformed(\"Discretized BrownianMotion with time_vector = [.5 , 1]\",\n                   \"g\",[-2.5, 2.5],BrownianMotion)\n</pre> plot_tm_transformed(\"Discretized BrownianMotion with time_vector = [.5 , 1]\",                    \"g\",[-2.5, 2.5],BrownianMotion) In\u00a0[22]: Copied! <pre>fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(12, 7))\nu1_a, u1_b = 2, 4\nu2_a, u2_b = 6, 8\ng1_mu, g1_var = 3, 9\ng2_mu, g2_var = 7, 9\ng_cov = 5\ndistribution = Sobol(dimension=2, randomize=True, seed=7)\nuniform_measure = Uniform(distribution,lower_bound=[u1_a, u2_a],upper_bound=[u1_b, u2_b])\ngaussian_measure = Gaussian(distribution,mean=[g1_mu, g2_mu],covariance=[[g1_var, g_cov],[g_cov,g2_var]])\n# Generate Samples and Create Scatter Plots\nfor i, (measure, color) in enumerate(zip([uniform_measure, gaussian_measure], [\"m\", \"y\"])):\n    samples = measure.gen_samples(n)\n    ax[i].scatter(samples[:, 0], samples[:, 1], color=color)\n# Plot Metas\nfor i in range(2):\n    ax[i].set_xlabel(\"$x_1$\")\n    ax[i].set_ylabel(\"$x_2$\")\n    ax[i].set_aspect(\"equal\")\nax[0].set_xlim([u1_a, u1_b])\nax[0].set_ylim([u2_a, u2_b])\nspread_g1 = ceil(3 * g1_var**.5)\nspread_g2 = ceil(3 * g2_var**.5)\nax[1].set_xlim([g1_mu - spread_g1, g1_mu + spread_g1])\nax[1].set_ylim([g2_mu - spread_g2, g2_mu + spread_g2])\nfig.suptitle(\"Shifted and Stretched Sobol Samples\")\nplt.tight_layout();\n</pre> fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(12, 7)) u1_a, u1_b = 2, 4 u2_a, u2_b = 6, 8 g1_mu, g1_var = 3, 9 g2_mu, g2_var = 7, 9 g_cov = 5 distribution = Sobol(dimension=2, randomize=True, seed=7) uniform_measure = Uniform(distribution,lower_bound=[u1_a, u2_a],upper_bound=[u1_b, u2_b]) gaussian_measure = Gaussian(distribution,mean=[g1_mu, g2_mu],covariance=[[g1_var, g_cov],[g_cov,g2_var]]) # Generate Samples and Create Scatter Plots for i, (measure, color) in enumerate(zip([uniform_measure, gaussian_measure], [\"m\", \"y\"])):     samples = measure.gen_samples(n)     ax[i].scatter(samples[:, 0], samples[:, 1], color=color) # Plot Metas for i in range(2):     ax[i].set_xlabel(\"$x_1$\")     ax[i].set_ylabel(\"$x_2$\")     ax[i].set_aspect(\"equal\") ax[0].set_xlim([u1_a, u1_b]) ax[0].set_ylim([u2_a, u2_b]) spread_g1 = ceil(3 * g1_var**.5) spread_g2 = ceil(3 * g2_var**.5) ax[1].set_xlim([g1_mu - spread_g1, g1_mu + spread_g1]) ax[1].set_ylim([g2_mu - spread_g2, g2_mu + spread_g2]) fig.suptitle(\"Shifted and Stretched Sobol Samples\") plt.tight_layout(); In\u00a0[23]: Copied! <pre>abs_tol = .3\nintegrand = Keister(IIDStdUniform(dimension=2, seed=7))\nsolution,data = CubMCCLT(integrand,abs_tol=abs_tol,rel_tol=0,n_init=16).integrate()\nprint(data)\n</pre> abs_tol = .3 integrand = Keister(IIDStdUniform(dimension=2, seed=7)) solution,data = CubMCCLT(integrand,abs_tol=abs_tol,rel_tol=0,n_init=16).integrate() print(data) <pre>Data (Data)\n    solution        1.680\n    bound_low       1.222\n    bound_high      2.137\n    bound_diff      0.915\n    n_total         63\n    time_integrate  0.001\nCubMCCLT (AbstractStoppingCriterion)\n    abs_tol         0.300\n    rel_tol         0\n    n_init          2^(4)\n    n_limit         2^(30)\n    inflate         1.200\n    alpha           0.010\nKeister (AbstractIntegrand)\nGaussian (AbstractTrueMeasure)\n    mean            0\n    covariance      2^(-1)\n    decomp_type     PCA\nIIDStdUniform (AbstractIIDDiscreteDistribution)\n    d               2^(1)\n    replications    1\n    entropy         7\n</pre> In\u00a0[24]: Copied! <pre># Constants based on running the above CLT Example\neps_list = [.5, .4, .3]\nn_list = [66, 84, 125]\nmu_hat_list = [1.8757, 1.8057, 1.8829]\n# Function Points\nnx, ny = (99, 99)\npoints_fun = zeros((nx * ny, 3))\nx = arange(.01, 1, .01)\ny = arange(.01, 1, .01)\nx_2d, y_2d = meshgrid(x, y)\npoints_fun[:, 0] = x_2d.flatten()\npoints_fun[:, 1] = y_2d.flatten()\npoints_fun[:, 2] = integrand.f(points_fun[:, :2]).squeeze()\nx_surf = points_fun[:, 0].reshape((nx, ny))\ny_surf = points_fun[:, 1].reshape((nx, ny))\nz_surf = points_fun[:, 2].reshape((nx, ny))\n# 3D Plot\nfig = plt.figure(figsize=(25, 8))\nax1 = fig.add_subplot(131, projection=\"3d\")\nax2 = fig.add_subplot(132, projection=\"3d\")\nax3 = fig.add_subplot(133, projection=\"3d\")\nfor idx, ax in enumerate([ax1, ax2, ax3]):\n    n = n_list[idx]\n    epsilon = eps_list[idx]\n    mu = mu_hat_list[idx]\n    # Surface\n    ax.plot_surface(x_surf, y_surf, z_surf, cmap=\"winter\", alpha=.2)\n    # Scatters\n    points = zeros((n, 3))\n    points[:, :2] = integrand.discrete_distrib.gen_samples(n)\n    points[:, 2] = integrand.f(points[:, :2]).squeeze()\n    ax.scatter(points[:, 0], points[:, 1], points[:, 2], color=\"r\", s=5)\n    ax.scatter(points[:, 0], points[:, 1], points[:, 2], color=\"r\", s=5)\n    ax.set_title(\"\\t$\\\\epsilon$ = %-7.1f $n$ = %-7d $\\\\hat{\\\\mu}$ = %-7.2f \"\n                 % (epsilon, n, mu), fontdict={\"fontsize\": 16})\n    # axis metas\n    n *= 2\n    ax.grid(False)\n    ax.xaxis.pane.set_edgecolor(\"black\")\n    ax.yaxis.pane.set_edgecolor(\"black\")\n    ax.set_xlabel(\"$x_1$\", fontdict={\"fontsize\": 16})\n    ax.set_ylabel(\"$x_2$\", fontdict={\"fontsize\": 16})\n    ax.set_zlabel(\"$f\\\\:(x_1,x_2)$\", fontdict={\"fontsize\": 16})\n    ax.view_init(20, 45);\n</pre> # Constants based on running the above CLT Example eps_list = [.5, .4, .3] n_list = [66, 84, 125] mu_hat_list = [1.8757, 1.8057, 1.8829] # Function Points nx, ny = (99, 99) points_fun = zeros((nx * ny, 3)) x = arange(.01, 1, .01) y = arange(.01, 1, .01) x_2d, y_2d = meshgrid(x, y) points_fun[:, 0] = x_2d.flatten() points_fun[:, 1] = y_2d.flatten() points_fun[:, 2] = integrand.f(points_fun[:, :2]).squeeze() x_surf = points_fun[:, 0].reshape((nx, ny)) y_surf = points_fun[:, 1].reshape((nx, ny)) z_surf = points_fun[:, 2].reshape((nx, ny)) # 3D Plot fig = plt.figure(figsize=(25, 8)) ax1 = fig.add_subplot(131, projection=\"3d\") ax2 = fig.add_subplot(132, projection=\"3d\") ax3 = fig.add_subplot(133, projection=\"3d\") for idx, ax in enumerate([ax1, ax2, ax3]):     n = n_list[idx]     epsilon = eps_list[idx]     mu = mu_hat_list[idx]     # Surface     ax.plot_surface(x_surf, y_surf, z_surf, cmap=\"winter\", alpha=.2)     # Scatters     points = zeros((n, 3))     points[:, :2] = integrand.discrete_distrib.gen_samples(n)     points[:, 2] = integrand.f(points[:, :2]).squeeze()     ax.scatter(points[:, 0], points[:, 1], points[:, 2], color=\"r\", s=5)     ax.scatter(points[:, 0], points[:, 1], points[:, 2], color=\"r\", s=5)     ax.set_title(\"\\t$\\\\epsilon$ = %-7.1f $n$ = %-7d $\\\\hat{\\\\mu}$ = %-7.2f \"                  % (epsilon, n, mu), fontdict={\"fontsize\": 16})     # axis metas     n *= 2     ax.grid(False)     ax.xaxis.pane.set_edgecolor(\"black\")     ax.yaxis.pane.set_edgecolor(\"black\")     ax.set_xlabel(\"$x_1$\", fontdict={\"fontsize\": 16})     ax.set_ylabel(\"$x_2$\", fontdict={\"fontsize\": 16})     ax.set_zlabel(\"$f\\\\:(x_1,x_2)$\", fontdict={\"fontsize\": 16})     ax.view_init(20, 45); In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"demos/sample_scatter_plots/#scatter-plots-of-samples","title":"Scatter Plots of Samples\u00b6","text":""},{"location":"demos/sample_scatter_plots/#iid-samples","title":"IID Samples\u00b6","text":""},{"location":"demos/sample_scatter_plots/#ld-samples","title":"LD Samples\u00b6","text":""},{"location":"demos/sample_scatter_plots/#transform-to-the-true-distribution","title":"Transform to the True Distribution\u00b6","text":"<p>Transform samples from a few discrete distributions to mimic various measures</p>"},{"location":"demos/sample_scatter_plots/#shift-and-stretch-the-true-distribution","title":"Shift and Stretch the True Distribution\u00b6","text":"<p>Transform Sobol sequences to mimic non-standard Uniform and Gaussian measures</p>"},{"location":"demos/sample_scatter_plots/#plots-samples-on-a-2d-keister-function","title":"Plots samples on a 2D Keister function\u00b6","text":""},{"location":"demos/some_true_measures/","title":"Importance Sampling with True Measures","text":"In\u00a0[1]: Copied! <pre>from qmcpy import *\nfrom scipy.special import gamma\nfrom numpy import *\n</pre> from qmcpy import * from scipy.special import gamma from numpy import * In\u00a0[2]: Copied! <pre>import matplotlib\nfrom matplotlib import pyplot\n%matplotlib inline\npyplot.rc('font', size=16)          # controls default text sizes\npyplot.rc('axes', titlesize=16)     # fontsize of the axes title\npyplot.rc('axes', labelsize=16)     # fontsize of the x and y labels\npyplot.rc('xtick', labelsize=16)    # fontsize of the tick labels\npyplot.rc('ytick', labelsize=16)    # fontsize of the tick labels\npyplot.rc('legend', fontsize=16)    # legend fontsize\npyplot.rc('figure', titlesize=16)   # fontsize of the figure title\n</pre> import matplotlib from matplotlib import pyplot %matplotlib inline pyplot.rc('font', size=16)          # controls default text sizes pyplot.rc('axes', titlesize=16)     # fontsize of the axes title pyplot.rc('axes', labelsize=16)     # fontsize of the x and y labels pyplot.rc('xtick', labelsize=16)    # fontsize of the tick labels pyplot.rc('ytick', labelsize=16)    # fontsize of the tick labels pyplot.rc('legend', fontsize=16)    # legend fontsize pyplot.rc('figure', titlesize=16)   # fontsize of the figure title In\u00a0[3]: Copied! <pre>def plt_1d(tm,lim,ax):\n    print(tm)\n    n_mesh = 100\n    ticks = linspace(*lim,n_mesh)\n    y = tm._weight(ticks[:,None])\n    ax.plot(ticks,y)\n    ax.set_xlim(lim)\n    ax.set_xlabel(\"$T$\")\n    ax.set_xticks(lim)    \n    ax.set_title(type(tm).__name__)\n</pre> def plt_1d(tm,lim,ax):     print(tm)     n_mesh = 100     ticks = linspace(*lim,n_mesh)     y = tm._weight(ticks[:,None])     ax.plot(ticks,y)     ax.set_xlim(lim)     ax.set_xlabel(\"$T$\")     ax.set_xticks(lim)         ax.set_title(type(tm).__name__) In\u00a0[4]: Copied! <pre>fig,ax = pyplot.subplots(figsize=(23,6),nrows=1,ncols=3)\nsobol = Sobol(1)\nkumaraswamy = Kumaraswamy(sobol,a=2,b=4)\nplt_1d(kumaraswamy,lim=[0,1],ax=ax[0])\nbern = BernoulliCont(sobol,lam=.75)\nplt_1d(bern,lim=[0,1],ax=ax[1])\njsu = JohnsonsSU(sobol,gamma=1,xi=2,delta=1,lam=2)\nplt_1d(jsu,lim=[-2,2],ax=ax[2])\nax[0].set_ylabel(\"Density\");\n</pre> fig,ax = pyplot.subplots(figsize=(23,6),nrows=1,ncols=3) sobol = Sobol(1) kumaraswamy = Kumaraswamy(sobol,a=2,b=4) plt_1d(kumaraswamy,lim=[0,1],ax=ax[0]) bern = BernoulliCont(sobol,lam=.75) plt_1d(bern,lim=[0,1],ax=ax[1]) jsu = JohnsonsSU(sobol,gamma=1,xi=2,delta=1,lam=2) plt_1d(jsu,lim=[-2,2],ax=ax[2]) ax[0].set_ylabel(\"Density\"); <pre>Kumaraswamy (AbstractTrueMeasure)\n    a               2^(1)\n    b               2^(2)\nBernoulliCont (AbstractTrueMeasure)\n    lam             0.750\nJohnsonsSU (AbstractTrueMeasure)\n    gamma           1\n    xi              2^(1)\n    delta           1\n    lam             2^(1)\n</pre> In\u00a0[5]: Copied! <pre>def plt_2d(tm,n,lim,ax):\n    print(tm)\n    n_mesh = 502\n    # Points\n    t = tm.gen_samples(n)\n    # PDF\n    mesh = zeros(((n_mesh)**2,3),dtype=float)\n    grid_tics = linspace(*lim,n_mesh)\n    x_mesh,y_mesh = meshgrid(grid_tics,grid_tics)\n    mesh[:,0] = x_mesh.flatten()\n    mesh[:,1] = y_mesh.flatten()\n    mesh[:,2] = tm._weight(mesh[:,:2])\n    z_mesh = mesh[:,2].reshape((n_mesh,n_mesh))\n    #   colors \n    clevel = arange(mesh[:,2].min(),mesh[:,2].max(),.025)\n    cmap = matplotlib.colors.LinearSegmentedColormap.from_list(\"\", [(.95,.95,.95),(0,0,1)]) \n    # cmap = pyplot.get_cmap('Blues')\n    #   contour + scatter plot\n    ax.contourf(x_mesh,y_mesh,z_mesh,clevel,cmap=cmap,extend='both')\n    ax.scatter(t[:,0],t[:,1],s=5,color='w')\n    #   axis\n    for nsew in ['top','bottom','left','right']: ax.spines[nsew].set_visible(False)\n    ax.xaxis.set_ticks_position('none') \n    ax.yaxis.set_ticks_position('none') \n    ax.set_aspect(1)\n    ax.set_xlim(lim)\n    ax.set_xticks(lim)\n    ax.set_ylim(lim)\n    ax.set_yticks(lim)\n    #   labels\n    ax.set_xlabel('$T_1$')\n    ax.set_ylabel('$T_2$')\n    ax.set_title('%s PDF and Random Samples'%type(tm).__name__)\n    #   metas\n    fig.tight_layout()\n</pre> def plt_2d(tm,n,lim,ax):     print(tm)     n_mesh = 502     # Points     t = tm.gen_samples(n)     # PDF     mesh = zeros(((n_mesh)**2,3),dtype=float)     grid_tics = linspace(*lim,n_mesh)     x_mesh,y_mesh = meshgrid(grid_tics,grid_tics)     mesh[:,0] = x_mesh.flatten()     mesh[:,1] = y_mesh.flatten()     mesh[:,2] = tm._weight(mesh[:,:2])     z_mesh = mesh[:,2].reshape((n_mesh,n_mesh))     #   colors      clevel = arange(mesh[:,2].min(),mesh[:,2].max(),.025)     cmap = matplotlib.colors.LinearSegmentedColormap.from_list(\"\", [(.95,.95,.95),(0,0,1)])      # cmap = pyplot.get_cmap('Blues')     #   contour + scatter plot     ax.contourf(x_mesh,y_mesh,z_mesh,clevel,cmap=cmap,extend='both')     ax.scatter(t[:,0],t[:,1],s=5,color='w')     #   axis     for nsew in ['top','bottom','left','right']: ax.spines[nsew].set_visible(False)     ax.xaxis.set_ticks_position('none')      ax.yaxis.set_ticks_position('none')      ax.set_aspect(1)     ax.set_xlim(lim)     ax.set_xticks(lim)     ax.set_ylim(lim)     ax.set_yticks(lim)     #   labels     ax.set_xlabel('$T_1$')     ax.set_ylabel('$T_2$')     ax.set_title('%s PDF and Random Samples'%type(tm).__name__)     #   metas     fig.tight_layout() In\u00a0[6]: Copied! <pre>fig,ax = pyplot.subplots(figsize=(18,6),nrows=1,ncols=3)\nsobol = Sobol(2)\nkumaraswamy = Kumaraswamy(sobol,a=[2,3],b=[3,5])\nplt_2d(kumaraswamy,n=2**8,lim=[0,1],ax=ax[0])\nbern = BernoulliCont(sobol,lam=[.25,.75])\nplt_2d(bern,n=2**8,lim=[0,1],ax=ax[1])\njsu = JohnsonsSU(sobol,gamma=[1,1],xi=[1,1],delta=[1,1],lam=[1,1])\nplt_2d(jsu,n=2**8,lim=[-2,2],ax=ax[2])\n</pre> fig,ax = pyplot.subplots(figsize=(18,6),nrows=1,ncols=3) sobol = Sobol(2) kumaraswamy = Kumaraswamy(sobol,a=[2,3],b=[3,5]) plt_2d(kumaraswamy,n=2**8,lim=[0,1],ax=ax[0]) bern = BernoulliCont(sobol,lam=[.25,.75]) plt_2d(bern,n=2**8,lim=[0,1],ax=ax[1]) jsu = JohnsonsSU(sobol,gamma=[1,1],xi=[1,1],delta=[1,1],lam=[1,1]) plt_2d(jsu,n=2**8,lim=[-2,2],ax=ax[2]) <pre>Kumaraswamy (AbstractTrueMeasure)\n    a               [2 3]\n    b               [3 5]\nBernoulliCont (AbstractTrueMeasure)\n    lam             [0.25 0.75]\nJohnsonsSU (AbstractTrueMeasure)\n    gamma           [1 1]\n    xi              [1 1]\n    delta           [1 1]\n    lam             [1 1]\n</pre> In\u00a0[9]: Copied! <pre>def compute_expected_val(tm,true_value,abs_tol):\n    if tm.d!=1: raise Exception(\"tm must be 1 dimensional for this test\")\n    cf = CustomFun(tm, g=lambda x: x[...,0])\n    sol,data = CubQMCSobolG(cf,abs_tol=abs_tol).integrate()\n    error = abs(true_value-sol)\n    if error&gt;abs_tol: \n        raise Exception(\"QMC error %.3f not within tolerance.\"%error)\n    print(\"%s integration within tolerance\"%type(tm).__name__)\n    print(\"\\tEstimated mean: %.4f\"%sol)\n    print(\"\\t%s\"%str(tm).replace('\\n','\\n\\t'))\n</pre> def compute_expected_val(tm,true_value,abs_tol):     if tm.d!=1: raise Exception(\"tm must be 1 dimensional for this test\")     cf = CustomFun(tm, g=lambda x: x[...,0])     sol,data = CubQMCSobolG(cf,abs_tol=abs_tol).integrate()     error = abs(true_value-sol)     if error&gt;abs_tol:          raise Exception(\"QMC error %.3f not within tolerance.\"%error)     print(\"%s integration within tolerance\"%type(tm).__name__)     print(\"\\tEstimated mean: %.4f\"%sol)     print(\"\\t%s\"%str(tm).replace('\\n','\\n\\t')) In\u00a0[10]: Copied! <pre>abs_tol = 1e-5\n# kumaraswamy \na,b = 2,6\nkuma = Kumaraswamy(Sobol(1),a,b)\nkuma_tv = b*gamma(1+1/a)*gamma(b)/gamma(1+1/a+b)\ncompute_expected_val(kuma,kuma_tv,abs_tol)\n# Continuous Bernoulli\nlam = .75\nbern = BernoulliCont(Sobol(1),lam=lam)\nbern_tv = 1/2 if lam==1/2 else lam/(2*lam-1)+1/(2*arctanh(1-2*lam))\ncompute_expected_val(bern,bern_tv,abs_tol)\n# Johnson's SU\n_gamma,xi,delta,lam = 1,2,3,4\njsu = JohnsonsSU(Sobol(1),gamma=_gamma,xi=xi,delta=delta,lam=lam)\njsu_tv = xi-lam*exp(1/(2*delta**2))*sinh(_gamma/delta)\ncompute_expected_val(jsu,jsu_tv,abs_tol)\n</pre> abs_tol = 1e-5 # kumaraswamy  a,b = 2,6 kuma = Kumaraswamy(Sobol(1),a,b) kuma_tv = b*gamma(1+1/a)*gamma(b)/gamma(1+1/a+b) compute_expected_val(kuma,kuma_tv,abs_tol) # Continuous Bernoulli lam = .75 bern = BernoulliCont(Sobol(1),lam=lam) bern_tv = 1/2 if lam==1/2 else lam/(2*lam-1)+1/(2*arctanh(1-2*lam)) compute_expected_val(bern,bern_tv,abs_tol) # Johnson's SU _gamma,xi,delta,lam = 1,2,3,4 jsu = JohnsonsSU(Sobol(1),gamma=_gamma,xi=xi,delta=delta,lam=lam) jsu_tv = xi-lam*exp(1/(2*delta**2))*sinh(_gamma/delta) compute_expected_val(jsu,jsu_tv,abs_tol) <pre>Kumaraswamy integration within tolerance\n\tEstimated mean: 0.3410\n\tKumaraswamy (AbstractTrueMeasure)\n\t    a               2^(1)\n\t    b               6\nBernoulliCont integration within tolerance\n\tEstimated mean: 0.5898\n\tBernoulliCont (AbstractTrueMeasure)\n\t    lam             0.750\nJohnsonsSU integration within tolerance\n\tEstimated mean: 0.5642\n\tJohnsonsSU (AbstractTrueMeasure)\n\t    gamma           1\n\t    xi              2^(1)\n\t    delta           3\n\t    lam             2^(2)\n</pre> In\u00a0[12]: Copied! <pre># compose with a Kumaraswamy transformation\nabs_tol = 1e-4\ncf = CustomFun(Uniform(Kumaraswamy(Sobol(1,seed=7),a=.5,b=.5)), g=lambda x: x[...,0])\nsol,data = CubQMCSobolG(cf,abs_tol=abs_tol).integrate() \nprint(data)\ntrue_val = .5 # expected value of standard uniform\nerror = abs(true_val-sol)\nif error&gt;abs_tol: raise Exception(\"QMC error %.3f not within tolerance.\"%error)\n</pre> # compose with a Kumaraswamy transformation abs_tol = 1e-4 cf = CustomFun(Uniform(Kumaraswamy(Sobol(1,seed=7),a=.5,b=.5)), g=lambda x: x[...,0]) sol,data = CubQMCSobolG(cf,abs_tol=abs_tol).integrate()  print(data) true_val = .5 # expected value of standard uniform error = abs(true_val-sol) if error&gt;abs_tol: raise Exception(\"QMC error %.3f not within tolerance.\"%error) <pre>Data (Data)\n    solution        0.500\n    comb_bound_low  0.500\n    comb_bound_high 0.500\n    comb_bound_diff 7.60e-05\n    comb_flags      1\n    n_total         2^(11)\n    n               2^(11)\n    time_integrate  0.004\nCubQMCNetG (AbstractStoppingCriterion)\n    abs_tol         1.00e-04\n    rel_tol         0\n    n_init          2^(10)\n    n_limit         2^(35)\nCustomFun (AbstractIntegrand)\nUniform (AbstractTrueMeasure)\n    lower_bound     0\n    upper_bound     1\n    transform       Kumaraswamy (AbstractTrueMeasure)\n                        a               2^(-1)\n                        b               2^(-1)\nDigitalNetB2 (AbstractLDDiscreteDistribution)\n    d               1\n    replications    1\n    randomize       LMS_DS\n    gen_mats_source joe_kuo.6.21201.txt\n    order           NATURAL\n    t               63\n    alpha           1\n    n_limit         2^(32)\n    entropy         7\n</pre> In\u00a0[14]: Copied! <pre># plot the above functions\nx = linspace(0,1,1000).reshape(-1,1)\nx = x[1:-1] # plotting locations\n# plot\nfig,ax = pyplot.subplots(ncols=2,figsize=(15,5))\n#    density\nrho = cf.true_measure.transform._weight(x)\ntfvals = cf.true_measure.transform._jacobian_transform_r(x,return_weights=False)\nax[0].hist(tfvals,density=True,bins=50,color='c')\nax[0].plot(x,rho,color='g',linewidth=2)\nax[0].set_title('Sampling density')\n#    functions\ngs = cf.g(x)\nfs = cf.f(x)\nax[1].plot(x,gs,color='r',label='g')\nax[1].plot(x,fs,color='b',label='f')\nax[1].legend()\nax[1].set_title('functions');\n# metas\nfor i in range(2):\n    ax[i].set_xlim([0,1])\n    ax[i].set_xticks([0,1])\n</pre> # plot the above functions x = linspace(0,1,1000).reshape(-1,1) x = x[1:-1] # plotting locations # plot fig,ax = pyplot.subplots(ncols=2,figsize=(15,5)) #    density rho = cf.true_measure.transform._weight(x) tfvals = cf.true_measure.transform._jacobian_transform_r(x,return_weights=False) ax[0].hist(tfvals,density=True,bins=50,color='c') ax[0].plot(x,rho,color='g',linewidth=2) ax[0].set_title('Sampling density') #    functions gs = cf.g(x) fs = cf.f(x) ax[1].plot(x,gs,color='r',label='g') ax[1].plot(x,fs,color='b',label='f') ax[1].legend() ax[1].set_title('functions'); # metas for i in range(2):     ax[i].set_xlim([0,1])     ax[i].set_xticks([0,1]) In\u00a0[15]: Copied! <pre># compose multiple Kumaraswamy distributions\nabs_tol = 1e-3\ndd = Sobol(1,seed=7)\nt1 = Kumaraswamy(dd,a=.5,b=.5) # first transformation\nt2 = Kumaraswamy(t1,a=5,b=2) # second transformation\ncf = CustomFun(Uniform(t2), g=lambda x:x[...,0])\nsol,data = CubQMCSobolG(cf,abs_tol=abs_tol).integrate() # expected value of standard uniform\nprint(data)\ntrue_val = .5\nerror = abs(true_val-sol)\nif error&gt;abs_tol: raise Exception(\"QMC error %.3f not within tolerance.\"%error)\n</pre> # compose multiple Kumaraswamy distributions abs_tol = 1e-3 dd = Sobol(1,seed=7) t1 = Kumaraswamy(dd,a=.5,b=.5) # first transformation t2 = Kumaraswamy(t1,a=5,b=2) # second transformation cf = CustomFun(Uniform(t2), g=lambda x:x[...,0]) sol,data = CubQMCSobolG(cf,abs_tol=abs_tol).integrate() # expected value of standard uniform print(data) true_val = .5 error = abs(true_val-sol) if error&gt;abs_tol: raise Exception(\"QMC error %.3f not within tolerance.\"%error) <pre>Data (Data)\n    solution        0.500\n    comb_bound_low  0.499\n    comb_bound_high 0.501\n    comb_bound_diff 0.002\n    comb_flags      1\n    n_total         2^(10)\n    n               2^(10)\n    time_integrate  0.002\nCubQMCNetG (AbstractStoppingCriterion)\n    abs_tol         0.001\n    rel_tol         0\n    n_init          2^(10)\n    n_limit         2^(35)\nCustomFun (AbstractIntegrand)\nUniform (AbstractTrueMeasure)\n    lower_bound     0\n    upper_bound     1\n    transform       Kumaraswamy (AbstractTrueMeasure)\n                        a               5\n                        b               2^(1)\n                        transform       Kumaraswamy (AbstractTrueMeasure)\n                                            a               2^(-1)\n                                            b               2^(-1)\nDigitalNetB2 (AbstractLDDiscreteDistribution)\n    d               1\n    replications    1\n    randomize       LMS_DS\n    gen_mats_source joe_kuo.6.21201.txt\n    order           NATURAL\n    t               63\n    alpha           1\n    n_limit         2^(32)\n    entropy         7\n</pre> In\u00a0[17]: Copied! <pre>x = linspace(0,1,1000).reshape(-1,1)\nx = x[1:-1] # plotting locations\n# plot\nfig,ax = pyplot.subplots(ncols=2,figsize=(15,5))\n#    density\ntfvals = cf.true_measure.transform._jacobian_transform_r(x,return_weights=False)\nax[0].hist(tfvals,density=True,bins=50,color='c')\nax[0].set_title('Sampling density')\n#    functions\ngs = cf.g(x)\nfs = cf.f(x)\nax[1].plot(x,gs,color='r',label='g')\nax[1].plot(x,fs,color='b',label='f')\nax[1].legend()\nfor i in range(2):\n    ax[i].set_xlim([0,1])\n    ax[i].set_xticks([0,1])\nax[1].set_title('functions');\n</pre> x = linspace(0,1,1000).reshape(-1,1) x = x[1:-1] # plotting locations # plot fig,ax = pyplot.subplots(ncols=2,figsize=(15,5)) #    density tfvals = cf.true_measure.transform._jacobian_transform_r(x,return_weights=False) ax[0].hist(tfvals,density=True,bins=50,color='c') ax[0].set_title('Sampling density') #    functions gs = cf.g(x) fs = cf.f(x) ax[1].plot(x,gs,color='r',label='g') ax[1].plot(x,fs,color='b',label='f') ax[1].legend() for i in range(2):     ax[i].set_xlim([0,1])     ax[i].set_xticks([0,1]) ax[1].set_title('functions'); In\u00a0[18]: Copied! <pre>abs_tol = 1e-4\nd = 1\n</pre> abs_tol = 1e-4 d = 1 In\u00a0[19]: Copied! <pre># standard method\nkeister_std = Keister(Sobol(d))\nsol_std,data_std = CubQMCSobolG(keister_std,abs_tol=abs_tol).integrate()\nprint(\"Standard method estimate of %.4f took %.2e seconds and %.2e samples\"%\\\n      (sol_std,data_std.time_integrate,data_std.n_total))\nprint(keister_std.true_measure)\n</pre> # standard method keister_std = Keister(Sobol(d)) sol_std,data_std = CubQMCSobolG(keister_std,abs_tol=abs_tol).integrate() print(\"Standard method estimate of %.4f took %.2e seconds and %.2e samples\"%\\       (sol_std,data_std.time_integrate,data_std.n_total)) print(keister_std.true_measure) <pre>Standard method estimate of 1.3804 took 1.71e-02 seconds and 1.64e+04 samples\nGaussian (AbstractTrueMeasure)\n    mean            0\n    covariance      2^(-1)\n    decomp_type     PCA\n</pre> In\u00a0[20]: Copied! <pre># Kumaraswamy importance sampling\ndd = Sobol(d,seed=7)\nt1 = Kumaraswamy(dd,a=.8,b=.8) # first transformation\nt2 = Gaussian(t1)\nkeister_kuma = Keister(t2)\nsol_kuma,data_kuma = CubQMCSobolG(keister_kuma,abs_tol=abs_tol).integrate() \nprint(\"Kumaraswamy IS estimate of %.4f took %.2e seconds and %.2e samples\"%\\\n      (sol_kuma,data_kuma.time_integrate,data_kuma.n_total))\nt_frac = data_kuma.time_integrate/data_std.time_integrate\nn_frac = data_kuma.n_total/data_std.n_total\nprint('That is %.1f%% of the time and %.1f%% of the samples compared to default keister.'%(t_frac*100,n_frac*100))\nprint(keister_kuma.true_measure)\n</pre> # Kumaraswamy importance sampling dd = Sobol(d,seed=7) t1 = Kumaraswamy(dd,a=.8,b=.8) # first transformation t2 = Gaussian(t1) keister_kuma = Keister(t2) sol_kuma,data_kuma = CubQMCSobolG(keister_kuma,abs_tol=abs_tol).integrate()  print(\"Kumaraswamy IS estimate of %.4f took %.2e seconds and %.2e samples\"%\\       (sol_kuma,data_kuma.time_integrate,data_kuma.n_total)) t_frac = data_kuma.time_integrate/data_std.time_integrate n_frac = data_kuma.n_total/data_std.n_total print('That is %.1f%% of the time and %.1f%% of the samples compared to default keister.'%(t_frac*100,n_frac*100)) print(keister_kuma.true_measure) <pre>Kumaraswamy IS estimate of 1.3804 took 4.00e-03 seconds and 2.05e+03 samples\nThat is 23.4% of the time and 12.5% of the samples compared to default keister.\nGaussian (AbstractTrueMeasure)\n    mean            0\n    covariance      2^(-1)\n    decomp_type     PCA\n    transform       Gaussian (AbstractTrueMeasure)\n                        mean            0\n                        covariance      1\n                        decomp_type     PCA\n                        transform       Kumaraswamy (AbstractTrueMeasure)\n                                            a               0.800\n                                            b               0.800\n</pre> In\u00a0[21]: Copied! <pre># Continuous Bernoulli importance sampling\ndd = Sobol(d,seed=7)\nt1 = BernoulliCont(dd,lam=.25) # first transformation\n#t2 = BernoulliCont(t1,lam=.75) # first transformation\nt3 = Gaussian(t1)\nkeister_cb = Keister(t3)\nsol_cb,data_cb = CubQMCSobolG(keister_cb,abs_tol=abs_tol).integrate() \nprint(\"Continuous Bernoulli IS estimate of %.4f took %.2e seconds and %.2e samples\"%\\\n      (sol_cb,data_cb.time_integrate,data_cb.n_total))\nt_frac = data_cb.time_integrate/data_std.time_integrate\nn_frac = data_cb.n_total/data_std.n_total\nprint('That is %.1f%% of the time and %.1f%% of the samples compared to default keister.'%(t_frac*100,n_frac*100))\nprint(keister_cb.true_measure)\n</pre> # Continuous Bernoulli importance sampling dd = Sobol(d,seed=7) t1 = BernoulliCont(dd,lam=.25) # first transformation #t2 = BernoulliCont(t1,lam=.75) # first transformation t3 = Gaussian(t1) keister_cb = Keister(t3) sol_cb,data_cb = CubQMCSobolG(keister_cb,abs_tol=abs_tol).integrate()  print(\"Continuous Bernoulli IS estimate of %.4f took %.2e seconds and %.2e samples\"%\\       (sol_cb,data_cb.time_integrate,data_cb.n_total)) t_frac = data_cb.time_integrate/data_std.time_integrate n_frac = data_cb.n_total/data_std.n_total print('That is %.1f%% of the time and %.1f%% of the samples compared to default keister.'%(t_frac*100,n_frac*100)) print(keister_cb.true_measure) <pre>Continuous Bernoulli IS estimate of 1.3804 took 1.28e-02 seconds and 8.19e+03 samples\nThat is 74.8% of the time and 50.0% of the samples compared to default keister.\nGaussian (AbstractTrueMeasure)\n    mean            0\n    covariance      2^(-1)\n    decomp_type     PCA\n    transform       Gaussian (AbstractTrueMeasure)\n                        mean            0\n                        covariance      1\n                        decomp_type     PCA\n                        transform       BernoulliCont (AbstractTrueMeasure)\n                                            lam             2^(-2)\n</pre> In\u00a0[22]: Copied! <pre>#  Kumaraswamy importance sampling\ndd = Sobol(d,seed=7)\nt1 = JohnsonsSU(dd,xi=2,delta=1,gamma=2,lam=1) # first transformation\nkeister_jsu = Keister(t1)\nsol_jsu,data_jsu = CubQMCSobolG(keister_jsu,abs_tol=abs_tol).integrate() \nprint(\"Kumaraswamy IS estimate of %.4f took %.2e seconds and %.2e samples\"%\\\n      (sol_jsu,data_jsu.time_integrate,data_jsu.n_total))\nt_frac = data_jsu.time_integrate/data_std.time_integrate\nn_frac = data_jsu.n_total/data_std.n_total\nprint('That is %.1f%% of the time and %.1f%% of the samples compared to default keister.'%(t_frac*100,n_frac*100))\nprint(keister_jsu.true_measure)\n</pre> #  Kumaraswamy importance sampling dd = Sobol(d,seed=7) t1 = JohnsonsSU(dd,xi=2,delta=1,gamma=2,lam=1) # first transformation keister_jsu = Keister(t1) sol_jsu,data_jsu = CubQMCSobolG(keister_jsu,abs_tol=abs_tol).integrate()  print(\"Kumaraswamy IS estimate of %.4f took %.2e seconds and %.2e samples\"%\\       (sol_jsu,data_jsu.time_integrate,data_jsu.n_total)) t_frac = data_jsu.time_integrate/data_std.time_integrate n_frac = data_jsu.n_total/data_std.n_total print('That is %.1f%% of the time and %.1f%% of the samples compared to default keister.'%(t_frac*100,n_frac*100)) print(keister_jsu.true_measure) <pre>Kumaraswamy IS estimate of 1.3804 took 9.99e-03 seconds and 8.19e+03 samples\nThat is 58.4% of the time and 50.0% of the samples compared to default keister.\nGaussian (AbstractTrueMeasure)\n    mean            0\n    covariance      2^(-1)\n    decomp_type     PCA\n    transform       JohnsonsSU (AbstractTrueMeasure)\n                        gamma           2^(1)\n                        xi              2^(1)\n                        delta           1\n                        lam             1\n</pre> In\u00a0[23]: Copied! <pre>x = linspace(0,1,1000).reshape(-1,1)\nx = x[1:-1] # plotting locations\n# plot\nfig,ax = pyplot.subplots(figsize=(10,8))\n#    functions\nfs = [keister_std.f,keister_kuma.f,keister_cb.f,keister_jsu.f]\nlabels = ['Default Keister','Kuma Keister','Cont. Bernoulli Keister',\"Johnson's SU Keister\"]\ncolors = ['m','c','r','g']\nfor f,label,color in zip(fs,labels,colors): ax.plot(x,f(x),color=color,label=label)\nax.legend()\nax.set_xlim([0,1])\nax.set_xticks([0,1])\nax.set_title('functions');\n</pre> x = linspace(0,1,1000).reshape(-1,1) x = x[1:-1] # plotting locations # plot fig,ax = pyplot.subplots(figsize=(10,8)) #    functions fs = [keister_std.f,keister_kuma.f,keister_cb.f,keister_jsu.f] labels = ['Default Keister','Kuma Keister','Cont. Bernoulli Keister',\"Johnson's SU Keister\"] colors = ['m','c','r','g'] for f,label,color in zip(fs,labels,colors): ax.plot(x,f(x),color=color,label=label) ax.legend() ax.set_xlim([0,1]) ax.set_xticks([0,1]) ax.set_title('functions'); In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"demos/some_true_measures/#some-true-measures","title":"Some True Measures\u00b6","text":"<p>In this notebook we explore some of the new, lesser-known, <code>TrueMeasure</code> instances in QMCPy. Specifically, we look at the Kumaraswamy, Continuous Bernoulli, and Johnson's $S_U$ measures.</p>"},{"location":"demos/some_true_measures/#mathematics","title":"Mathematics\u00b6","text":"<p>Denote by $f$ the one-dimensional PDF, $F$ the one-dimensional CDF, and $\\Psi=F^{-1}$ the inverse CDF transform that takes samples mimicking $\\mathcal{U}[0,1]$ to mimic the desired one-dimensional true measure. For each of these true measures we assume the dimensions are independent, so the density and CDF and computed by taking the product across dimensions and the inverse transform is applied elementwise.</p> <p>Kumaraswamy</p> <p>Parameters $a,b&gt;0$ $$f(x) = a b x^{a-1}(1-x^{a})^{b-1}$$ $$F(x) = 1-(1-x^{a})^{b}$$ $$\\Psi(x) = (1-(1-x)^{1/b})^{1/a}$$ $$\\Psi'(x) = \\frac{\\left(1-\\left(1-x\\right)^{1/b}\\right)^{1/a-1}(1-x)^{1/b-1}}{ab}$$</p> <p>Continuous Bernoulli</p> <p>Parameter $\\lambda \\in (0,1)$</p> <p>If $\\lambda=1/2$, then $$f(x) = 2\\lambda^x(1-\\lambda)^{(1-x)}$$ $$F(x) = x$$ $$\\Psi(x) = x$$ $$\\Psi'(x) = 1$$</p> <p>If $\\lambda \\neq 1/2$, then $$f(x) = \\frac{2\\tanh^{-1}(1-2\\lambda)}{1-2\\lambda} \\lambda^x(1-\\lambda)^{(1-x)}$$ $$F(x) = \\frac{\\lambda^x(1-\\lambda)^{(1-x)}+\\lambda-1}{2\\lambda-1}$$ $$\\Psi(x) = \\log\\left(\\frac{(2\\lambda-1)x-\\lambda+1}{1-\\lambda}\\right) \\,/\\, \\log\\left(\\frac{\\lambda}{1-\\lambda}\\right)$$ $$\\Psi'(x) = \\frac{1}{\\log(\\lambda/(1-\\lambda))} \\cdot \\frac{2\\lambda-1}{(2\\lambda-1)x-\\lambda+1}$$</p> <p>Johnson's $S_U$</p> <p>Parameters $\\gamma,\\xi,\\delta&gt;0,\\lambda&gt;0$ $$f(x) = \\frac{\\delta\\exp\\left(-\\frac{1}{2}\\left(\\gamma+\\delta\\sinh^{-1}\\left(\\frac{x-\\xi}{\\lambda}\\right)\\right)^2\\right)}{\\lambda\\sqrt{2\\pi\\left(1+\\left(\\frac{x-\\xi}{\\lambda}\\right)^2\\right)}}$$ $$F(x) = \\Phi\\left(\\gamma+\\delta\\sinh^{-1}\\left(\\frac{x-\\xi}{\\lambda}\\right)\\right)$$ $$\\Psi(x) = \\lambda \\sinh\\left(\\frac{\\Phi^{-1}(x)-\\gamma}{\\delta}\\right) + \\xi$$ $$\\Psi'(x) = \\frac{\\lambda}{\\delta}\\cosh\\left(\\frac{\\Phi^{-1}(x)-\\gamma}{\\delta}\\right) \\,/\\, \\phi\\left(\\Phi^{-1}(x)\\right)$$ where $\\phi$ is the standard normal PDF and $\\Phi$ is the standard normal CDF.</p>"},{"location":"demos/some_true_measures/#imports","title":"Imports\u00b6","text":""},{"location":"demos/some_true_measures/#1d-density-plot","title":"1D Density Plot\u00b6","text":""},{"location":"demos/some_true_measures/#2d-density-plot","title":"2D Density Plot\u00b6","text":""},{"location":"demos/some_true_measures/#1d-expected-values","title":"1D Expected Values\u00b6","text":""},{"location":"demos/some_true_measures/#importance-sampling-with-a-single-kumaraswamy","title":"Importance Sampling with a Single Kumaraswamy\u00b6","text":""},{"location":"demos/some_true_measures/#importance-sampling-with-2-composed-kumaraswamys","title":"Importance Sampling with 2 (Composed) Kumaraswamys\u00b6","text":""},{"location":"demos/some_true_measures/#can-we-improve-the-keister-function","title":"Can we Improve the Keister function?\u00b6","text":""},{"location":"demos/umbridge/","title":"UM-Bridge","text":"In\u00a0[1]: Copied! <pre>import umbridge\nimport qmcpy as qp\n</pre> import umbridge import qmcpy as qp In\u00a0[2]: Copied! <pre>!docker run --name muqbp -d -it -p 4243:4243 linusseelinger/benchmark-muq-beam-propagation:latest\n</pre> !docker run --name muqbp -d -it -p 4243:4243 linusseelinger/benchmark-muq-beam-propagation:latest <pre>8192780429eeace8439d0007df81db12a43215530634f5b5737bfc705135ccc5\n</pre> <p>Initialize a QMCPy sampler and distribution.</p> In\u00a0[3]: Copied! <pre>sampler = qp.DigitalNetB2(dimension=3,seed=7) # DISCRETE DISTRIBUTION\ndistribution = qp.Uniform(sampler,lower_bound=1,upper_bound=1.05) # TRUE MEASURE\n</pre> sampler = qp.DigitalNetB2(dimension=3,seed=7) # DISCRETE DISTRIBUTION distribution = qp.Uniform(sampler,lower_bound=1,upper_bound=1.05) # TRUE MEASURE <p>Initialize a UM-Bridge model and wrap it into a QMCPy compatible Integrand</p> In\u00a0[4]: Copied! <pre>model = umbridge.HTTPModel('http://localhost:4243','forward')\numbridge_config = {\"d\": sampler.d}\nintegrand = qp.UMBridgeWrapper(distribution,model,umbridge_config,parallel=False) # INTEGRAND\n</pre> model = umbridge.HTTPModel('http://localhost:4243','forward') umbridge_config = {\"d\": sampler.d} integrand = qp.UMBridgeWrapper(distribution,model,umbridge_config,parallel=False) # INTEGRAND In\u00a0[5]: Copied! <pre>x = sampler(16) # same as sampler.gen_samples(16)\ny = integrand.f(x)\nprint(y.shape)\nprint(type(y))\nprint(y.dtype)\n</pre> x = sampler(16) # same as sampler.gen_samples(16) y = integrand.f(x) print(y.shape) print(type(y)) print(y.dtype) <pre>(31, 16)\n&lt;class 'numpy.ndarray'&gt;\nfloat64\n</pre> In\u00a0[6]: Copied! <pre>qmc_stop_crit = qp.CubQMCNetG(integrand,abs_tol=2.5e-2) # QMC STOPPING CRITERION\nsolution,data = qmc_stop_crit.integrate()\nprint(data)\n</pre> qmc_stop_crit = qp.CubQMCNetG(integrand,abs_tol=2.5e-2) # QMC STOPPING CRITERION solution,data = qmc_stop_crit.integrate() print(data) <pre>Data (Data)\n    solution        [  0.      3.855  14.69  ... 898.921 935.383 971.884]\n    comb_bound_low  [  0.      3.854  14.688 ... 898.901 935.362 971.862]\n    comb_bound_high [  0.      3.855  14.691 ... 898.941 935.404 971.906]\n    comb_bound_diff [0.    0.001 0.003 ... 0.041 0.042 0.044]\n    comb_flags      [ True  True  True ...  True  True  True]\n    n_total         2^(11)\n    n               [1024 1024 1024 ... 2048 2048 2048]\n    time_integrate  9.566\nCubQMCNetG (AbstractStoppingCriterion)\n    abs_tol         0.025\n    rel_tol         0\n    n_init          2^(10)\n    n_limit         2^(35)\nUMBridgeWrapper (AbstractIntegrand)\nUniform (AbstractTrueMeasure)\n    lower_bound     1\n    upper_bound     1.050\nDigitalNetB2 (AbstractLDDiscreteDistribution)\n    d               3\n    replications    1\n    randomize       LMS_DS\n    gen_mats_source joe_kuo.6.21201.txt\n    order           NATURAL\n    t               63\n    alpha           1\n    n_limit         2^(32)\n    entropy         7\n</pre> In\u00a0[7]: Copied! <pre>from matplotlib import pyplot\nfig,ax = pyplot.subplots(figsize=(6,3))\nax.plot(solution,'-o')\nax.set_xlim([0,len(solution)-1]); ax.set_xlabel(r'$x$')\nax.set_ylim([1000,-10]);  ax.set_ylabel(r'$u(x)$');\n</pre> from matplotlib import pyplot fig,ax = pyplot.subplots(figsize=(6,3)) ax.plot(solution,'-o') ax.set_xlim([0,len(solution)-1]); ax.set_xlabel(r'$x$') ax.set_ylim([1000,-10]);  ax.set_ylabel(r'$u(x)$'); <p>QMCPy can automatically multi-threaded requests to the model by setting <code>parallel=p</code> where <code>p</code> is the number of processors used by multiprocessing.pool.ThreadPool. Setting <code>parallel=True</code> is equivalent to setting <code>parallel=os.cpu_count()</code>.</p> In\u00a0[8]: Copied! <pre>import os\nprint('Available CPUs: %d'%os.cpu_count())\n</pre> import os print('Available CPUs: %d'%os.cpu_count()) <pre>Available CPUs: 12\n</pre> In\u00a0[10]: Copied! <pre>integrand = qp.UMBridgeWrapper(distribution,model,umbridge_config,parallel=8)\nsolution,data = qp.CubQMCNetG(integrand,abs_tol=2.5e-2).integrate()\ndata\n</pre> integrand = qp.UMBridgeWrapper(distribution,model,umbridge_config,parallel=8) solution,data = qp.CubQMCNetG(integrand,abs_tol=2.5e-2).integrate() data Out[10]: <pre>Data (Data)\n    solution        [  0.      3.855  14.69  ... 898.921 935.383 971.884]\n    comb_bound_low  [  0.      3.854  14.688 ... 898.901 935.362 971.862]\n    comb_bound_high [  0.      3.855  14.691 ... 898.941 935.404 971.906]\n    comb_bound_diff [0.    0.001 0.003 ... 0.041 0.042 0.044]\n    comb_flags      [ True  True  True ...  True  True  True]\n    n_total         2^(11)\n    n               [1024 1024 1024 ... 2048 2048 2048]\n    time_integrate  4.719\nCubQMCNetG (AbstractStoppingCriterion)\n    abs_tol         0.025\n    rel_tol         0\n    n_init          2^(10)\n    n_limit         2^(35)\nUMBridgeWrapper (AbstractIntegrand)\nUniform (AbstractTrueMeasure)\n    lower_bound     1\n    upper_bound     1.050\nDigitalNetB2 (AbstractLDDiscreteDistribution)\n    d               3\n    replications    1\n    randomize       LMS_DS\n    gen_mats_source joe_kuo.6.21201.txt\n    order           NATURAL\n    t               63\n    alpha           1\n    n_limit         2^(32)\n    entropy         7</pre> In\u00a0[11]: Copied! <pre>!docker rm -f muqbp\n</pre> !docker rm -f muqbp <pre>muqbp\n</pre>"},{"location":"demos/umbridge/#um-bridge-with-qmcpy","title":"UM-Bridge with QMCPy\u00b6","text":"<p>Using QMCPy to evaluate the UM-Bridge Cantilever Beam Function and approximate the expectation with respect to a uniform random variable.</p>"},{"location":"demos/umbridge/#imports","title":"Imports\u00b6","text":""},{"location":"demos/umbridge/#start-docker-container","title":"Start Docker Container\u00b6","text":"<p>See the UM-Bridge Documentation for image options.</p>"},{"location":"demos/umbridge/#problem-setup","title":"Problem Setup\u00b6","text":""},{"location":"demos/umbridge/#model-evaluation","title":"Model Evaluation\u00b6","text":""},{"location":"demos/umbridge/#automatically-approximate-the-expectation","title":"Automatically Approximate the Expectation\u00b6","text":""},{"location":"demos/umbridge/#parallel-evaluation","title":"Parallel Evaluation\u00b6","text":""},{"location":"demos/umbridge/#shut-down-docker-image","title":"Shut Down Docker Image\u00b6","text":""},{"location":"demos/vectorized_qmc/","title":"Vectorized QMC Algorithms Tracking Fourier Coefficient Decay","text":"In\u00a0[1]: Copied! <pre>%%capture\n# @title Execute this cell to install dependancies\ntry:\n  import google.colab\n  import os\n  !pip install -q qmcpy &gt;&gt; /dev/null\n  !apt-get update &amp;&amp; apt-get install -y --no-install-recommends texlive-latex-base texlive-fonts-recommended texlive-latex-extra cm-super dvipng\nexcept:\n  pass\n\nimport matplotlib.pyplot as plt\n\nplt.rcParams.update({\n\"text.usetex\": True,\n\"font.family\": \"serif\",\n\"text.latex.preamble\": r\"\\usepackage{amsmath}\\usepackage{amssymb}\\newcommand{\\bx}{\\boldsymbol{x}}\"\n})\n</pre> %%capture # @title Execute this cell to install dependancies try:   import google.colab   import os   !pip install -q qmcpy &gt;&gt; /dev/null   !apt-get update &amp;&amp; apt-get install -y --no-install-recommends texlive-latex-base texlive-fonts-recommended texlive-latex-extra cm-super dvipng except:   pass  import matplotlib.pyplot as plt  plt.rcParams.update({ \"text.usetex\": True, \"font.family\": \"serif\", \"text.latex.preamble\": r\"\\usepackage{amsmath}\\usepackage{amssymb}\\newcommand{\\bx}{\\boldsymbol{x}}\" }) In\u00a0[2]: Copied! <pre>import qmcpy as qp\nimport numpy as np\n</pre> import qmcpy as qp import numpy as np In\u00a0[3]: Copied! <pre>from matplotlib import pyplot\n%matplotlib inline\nroot = None#'./'\n</pre> from matplotlib import pyplot %matplotlib inline root = None#'./' In\u00a0[4]: Copied! <pre>n = 2**6\ns = 10\nfig,ax = pyplot.subplots(figsize=(8,4),nrows=1,ncols=3)\nfor i,(dd,name) in enumerate(zip(\n    [qp.IIDStdUniform(2,seed=7),qp.DigitalNetB2(2,seed=7),qp.Lattice(2,seed=7)],\n    ['IID','Randomized Digital Net (LD)','Randomized Lattice (LD)'])):\n    pts = dd.gen_samples(n)\n    ax[i].scatter(pts[0:n//4,0],pts[0:n//4,1],color='k',marker='s',s=s)\n    ax[i].scatter(pts[n//4:n//2,0],pts[n//4:n//2,1],color='k',marker='o',s=s)\n    ax[i].scatter(pts[n//2:n,0],pts[n//2:n,1],color='k',marker='^',s=s)\n    ax[i].set_aspect(1)\n    ax[i].set_xlabel(r'$x_{1}$')\n    ax[i].set_ylabel(r'$x_{2}$')\n    ax[i].set_xlim([0,1])\n    ax[i].set_ylim([0,1])\n    ax[i].set_xticks([0,.25,.5,.75,1])\n    ax[i].set_xticklabels([r'$0$',r'$1/4$',r'$1/2$',r'$3/4$',r'$1$'])\n    ax[i].set_yticks([0,.25,.5,.75,1])\n    ax[i].set_yticklabels([r'$0$',r'$1/4$',r'$1/2$',r'$3/4$',r'$1$'])\n    ax[i].set_title(name)\nif root: fig.savefig(root+'ld_seqs.pdf',transparent=True)\n</pre> n = 2**6 s = 10 fig,ax = pyplot.subplots(figsize=(8,4),nrows=1,ncols=3) for i,(dd,name) in enumerate(zip(     [qp.IIDStdUniform(2,seed=7),qp.DigitalNetB2(2,seed=7),qp.Lattice(2,seed=7)],     ['IID','Randomized Digital Net (LD)','Randomized Lattice (LD)'])):     pts = dd.gen_samples(n)     ax[i].scatter(pts[0:n//4,0],pts[0:n//4,1],color='k',marker='s',s=s)     ax[i].scatter(pts[n//4:n//2,0],pts[n//4:n//2,1],color='k',marker='o',s=s)     ax[i].scatter(pts[n//2:n,0],pts[n//2:n,1],color='k',marker='^',s=s)     ax[i].set_aspect(1)     ax[i].set_xlabel(r'$x_{1}$')     ax[i].set_ylabel(r'$x_{2}$')     ax[i].set_xlim([0,1])     ax[i].set_ylim([0,1])     ax[i].set_xticks([0,.25,.5,.75,1])     ax[i].set_xticklabels([r'$0$',r'$1/4$',r'$1/2$',r'$3/4$',r'$1$'])     ax[i].set_yticks([0,.25,.5,.75,1])     ax[i].set_yticklabels([r'$0$',r'$1/4$',r'$1/2$',r'$3/4$',r'$1$'])     ax[i].set_title(name) if root: fig.savefig(root+'ld_seqs.pdf',transparent=True) In\u00a0[5]: Copied! <pre>def cantilever_beam_function(T,compute_flags): # T is (n x 3)\n    Y = np.zeros((2,len(T)),dtype=float) # (n x 2)\n    l,w,t = 100,4,2\n    T1,T2,T3 = T[:,0],T[:,1],T[:,2] # Python is indexed from 0\n    if compute_flags[0]: # compute D. x^2 is \"x**2\" in Python\n        Y[0] = 4*l**3/(T1*w*t)*np.sqrt(T2**2/t**4+T3**2/w**4)\n    if compute_flags[1]: # compute S\n        Y[1] = 600*(T2/(w*t**2)+T3/(w**2*t))\n    return Y\ntrue_measure = qp.Gaussian(\n    sampler = qp.DigitalNetB2(dimension=3,seed=7),\n    mean = [2.9e7,500,1000],\n    covariance = np.diag([(1.45e6)**2,(100)**2,(100)**2]))\nintegrand = qp.CustomFun(true_measure,\n    g = cantilever_beam_function,\n    dimension_indv = 2)\nqmc_stop_crit = qp.CubQMCNetG(integrand,\n    abs_tol = 1e-3,\n    rel_tol = 1e-6)\nsolution,data = qmc_stop_crit.integrate()\nprint(solution)\n# [2.42575885e+00 3.74999973e+04]\nprint(data)\n</pre> def cantilever_beam_function(T,compute_flags): # T is (n x 3)     Y = np.zeros((2,len(T)),dtype=float) # (n x 2)     l,w,t = 100,4,2     T1,T2,T3 = T[:,0],T[:,1],T[:,2] # Python is indexed from 0     if compute_flags[0]: # compute D. x^2 is \"x**2\" in Python         Y[0] = 4*l**3/(T1*w*t)*np.sqrt(T2**2/t**4+T3**2/w**4)     if compute_flags[1]: # compute S         Y[1] = 600*(T2/(w*t**2)+T3/(w**2*t))     return Y true_measure = qp.Gaussian(     sampler = qp.DigitalNetB2(dimension=3,seed=7),     mean = [2.9e7,500,1000],     covariance = np.diag([(1.45e6)**2,(100)**2,(100)**2])) integrand = qp.CustomFun(true_measure,     g = cantilever_beam_function,     dimension_indv = 2) qmc_stop_crit = qp.CubQMCNetG(integrand,     abs_tol = 1e-3,     rel_tol = 1e-6) solution,data = qmc_stop_crit.integrate() print(solution) # [2.42575885e+00 3.74999973e+04] print(data) <pre>[2.42570423e+00 3.75000056e+04]\nData (Data)\n    solution        [2.426e+00 3.750e+04]\n    comb_bound_low  [2.425e+00 3.750e+04]\n    comb_bound_high [2.426e+00 3.750e+04]\n    comb_bound_diff [0.001 0.073]\n    comb_flags      [ True  True]\n    n_total         2^(17)\n    n               [  2048 131072]\n    time_integrate  0.215\nCubQMCNetG (AbstractStoppingCriterion)\n    abs_tol         0.001\n    rel_tol         1.00e-06\n    n_init          2^(10)\n    n_limit         2^(35)\nCustomFun (AbstractIntegrand)\nGaussian (AbstractTrueMeasure)\n    mean            [2.9e+07 5.0e+02 1.0e+03]\n    covariance      [[2.102e+12 0.000e+00 0.000e+00]\n                     [0.000e+00 1.000e+04 0.000e+00]\n                     [0.000e+00 0.000e+00 1.000e+04]]\n    decomp_type     PCA\nDigitalNetB2 (AbstractLDDiscreteDistribution)\n    d               3\n    replications    1\n    randomize       LMS_DS\n    gen_mats_source joe_kuo.6.21201.txt\n    order           NATURAL\n    t               63\n    alpha           1\n    n_limit         2^(32)\n    entropy         7\n</pre> In\u00a0[6]: Copied! <pre>import scipy\nfrom sklearn.gaussian_process import GaussianProcessRegressor,kernels\n\nf = lambda x: np.cos(10*x)*np.exp(.2*x)+np.exp(-5*(x-.4)**2)\nxplt = np.linspace(0,1,100)\nyplt = f(xplt)\nx = np.array([.1, .2, .4, .7, .9])\ny = f(x)\nymax = y.max()\n\ngp = GaussianProcessRegressor(kernel=kernels.RBF(length_scale=1.0,length_scale_bounds=(1e-2, 1e2)),\n    n_restarts_optimizer = 16).fit(x[:,None],y)\nyhatplt,stdhatplt = gp.predict(xplt[:,None],return_std=True)\n\ntpax = 32\nx0mesh,x1mesh = np.meshgrid(np.linspace(0,1,tpax),np.linspace(0,1,tpax))\npost_mus = np.zeros((tpax,tpax,2),dtype=float)\npost_sqrtcovs = np.zeros((tpax,tpax,2,2),dtype=float)\nfor j0 in range(tpax):\n    for j1 in range(tpax):\n        candidate = np.array([[x0mesh[j0,j1]],[x1mesh[j0,j1]]])\n        post_mus[j0,j1],post_cov = gp.predict(candidate,return_cov=True)\n        evals,evecs = scipy.linalg.eig(post_cov)\n        post_sqrtcovs[j0,j1] = np.sqrt(np.maximum(evals.real,0))*evecs\n\ndef qei_acq_vec(x,compute_flags):\n    xgauss = scipy.stats.norm.ppf(x)\n    n = len(x)\n    qei_vals = np.zeros((tpax,tpax,n),dtype=float)\n    for j0 in range(tpax):\n        for j1 in range(tpax):\n            if compute_flags[j0,j1]==False: continue\n            sqrt_cov = post_sqrtcovs[j0,j1]\n            mu_post = post_mus[j0,j1]\n            for i in range(len(x)):\n                yij = sqrt_cov@xgauss[i]+mu_post\n                qei_vals[j0,j1,i] = max((yij-ymax).max(),0)\n    return qei_vals\n\nqei_acq_vec_qmcpy = qp.CustomFun(\n    true_measure = qp.Uniform(qp.DigitalNetB2(2,seed=7)),\n    g = qei_acq_vec,\n    dimension_indv = (tpax,tpax),\n    parallel=False)\nqei_vals,qei_data = qp.CubQMCNetG(qei_acq_vec_qmcpy,abs_tol=.025,rel_tol=0).integrate() # .0005\nprint(qei_data)\n\na = np.unravel_index(np.argmax(qei_vals,axis=None),qei_vals.shape)\nxnext = np.array([x0mesh[a[0],a[1]],x1mesh[a[0],a[1]]])\nfnext = f(xnext)\n</pre> import scipy from sklearn.gaussian_process import GaussianProcessRegressor,kernels  f = lambda x: np.cos(10*x)*np.exp(.2*x)+np.exp(-5*(x-.4)**2) xplt = np.linspace(0,1,100) yplt = f(xplt) x = np.array([.1, .2, .4, .7, .9]) y = f(x) ymax = y.max()  gp = GaussianProcessRegressor(kernel=kernels.RBF(length_scale=1.0,length_scale_bounds=(1e-2, 1e2)),     n_restarts_optimizer = 16).fit(x[:,None],y) yhatplt,stdhatplt = gp.predict(xplt[:,None],return_std=True)  tpax = 32 x0mesh,x1mesh = np.meshgrid(np.linspace(0,1,tpax),np.linspace(0,1,tpax)) post_mus = np.zeros((tpax,tpax,2),dtype=float) post_sqrtcovs = np.zeros((tpax,tpax,2,2),dtype=float) for j0 in range(tpax):     for j1 in range(tpax):         candidate = np.array([[x0mesh[j0,j1]],[x1mesh[j0,j1]]])         post_mus[j0,j1],post_cov = gp.predict(candidate,return_cov=True)         evals,evecs = scipy.linalg.eig(post_cov)         post_sqrtcovs[j0,j1] = np.sqrt(np.maximum(evals.real,0))*evecs  def qei_acq_vec(x,compute_flags):     xgauss = scipy.stats.norm.ppf(x)     n = len(x)     qei_vals = np.zeros((tpax,tpax,n),dtype=float)     for j0 in range(tpax):         for j1 in range(tpax):             if compute_flags[j0,j1]==False: continue             sqrt_cov = post_sqrtcovs[j0,j1]             mu_post = post_mus[j0,j1]             for i in range(len(x)):                 yij = sqrt_cov@xgauss[i]+mu_post                 qei_vals[j0,j1,i] = max((yij-ymax).max(),0)     return qei_vals  qei_acq_vec_qmcpy = qp.CustomFun(     true_measure = qp.Uniform(qp.DigitalNetB2(2,seed=7)),     g = qei_acq_vec,     dimension_indv = (tpax,tpax),     parallel=False) qei_vals,qei_data = qp.CubQMCNetG(qei_acq_vec_qmcpy,abs_tol=.025,rel_tol=0).integrate() # .0005 print(qei_data)  a = np.unravel_index(np.argmax(qei_vals,axis=None),qei_vals.shape) xnext = np.array([x0mesh[a[0],a[1]],x1mesh[a[0],a[1]]]) fnext = f(xnext) <pre>Data (Data)\n    solution        [[0.06  0.079 0.072 ... 0.06  0.061 0.067]\n                     [0.08  0.064 0.067 ... 0.064 0.065 0.071]\n                     [0.073 0.067 0.032 ... 0.032 0.033 0.039]\n                     ...\n                     [0.06  0.064 0.032 ... 0.    0.001 0.007]\n                     [0.061 0.065 0.033 ... 0.001 0.001 0.007]\n                     [0.067 0.071 0.039 ... 0.007 0.007 0.007]]\n    comb_bound_low  [[0.059 0.078 0.071 ... 0.059 0.06  0.066]\n                     [0.078 0.063 0.066 ... 0.064 0.064 0.07 ]\n                     [0.071 0.066 0.032 ... 0.032 0.033 0.038]\n                     ...\n                     [0.059 0.063 0.032 ... 0.    0.001 0.006]\n                     [0.06  0.064 0.033 ... 0.001 0.001 0.006]\n                     [0.065 0.07  0.038 ... 0.006 0.006 0.006]]\n    comb_bound_high [[0.061 0.081 0.074 ... 0.061 0.062 0.068]\n                     [0.081 0.065 0.068 ... 0.065 0.066 0.072]\n                     [0.074 0.068 0.033 ... 0.033 0.034 0.04 ]\n                     ...\n                     [0.061 0.065 0.033 ... 0.    0.001 0.008]\n                     [0.062 0.066 0.034 ... 0.001 0.001 0.008]\n                     [0.068 0.072 0.04  ... 0.008 0.008 0.008]]\n    comb_bound_diff [[0.002 0.002 0.003 ... 0.002 0.002 0.003]\n                     [0.002 0.002 0.002 ... 0.002 0.002 0.002]\n                     [0.002 0.002 0.001 ... 0.001 0.001 0.002]\n                     ...\n                     [0.002 0.002 0.001 ... 0.    0.    0.002]\n                     [0.002 0.002 0.001 ... 0.    0.    0.002]\n                     [0.003 0.002 0.002 ... 0.002 0.002 0.002]]\n    comb_flags      [[ True  True  True ...  True  True  True]\n                     [ True  True  True ...  True  True  True]\n                     [ True  True  True ...  True  True  True]\n                     ...\n                     [ True  True  True ...  True  True  True]\n                     [ True  True  True ...  True  True  True]\n                     [ True  True  True ...  True  True  True]]\n    n_total         2^(10)\n    n               [[1024 1024 1024 ... 1024 1024 1024]\n                     [1024 1024 1024 ... 1024 1024 1024]\n                     [1024 1024 1024 ... 1024 1024 1024]\n                     ...\n                     [1024 1024 1024 ... 1024 1024 1024]\n                     [1024 1024 1024 ... 1024 1024 1024]\n                     [1024 1024 1024 ... 1024 1024 1024]]\n    time_integrate  7.262\nCubQMCNetG (AbstractStoppingCriterion)\n    abs_tol         0.025\n    rel_tol         0\n    n_init          2^(10)\n    n_limit         2^(35)\nCustomFun (AbstractIntegrand)\nUniform (AbstractTrueMeasure)\n    lower_bound     0\n    upper_bound     1\nDigitalNetB2 (AbstractLDDiscreteDistribution)\n    d               2^(1)\n    replications    1\n    randomize       LMS_DS\n    gen_mats_source joe_kuo.6.21201.txt\n    order           NATURAL\n    t               63\n    alpha           1\n    n_limit         2^(32)\n    entropy         7\n</pre> In\u00a0[7]: Copied! <pre>from matplotlib import cm\nfig,ax = pyplot.subplots(nrows=1,ncols=2,figsize=(8,3.25))\nax[0].scatter(x,y,color='k',label='Query Points')\nax[0].plot(xplt,yplt,color='k',linestyle='--',label='True function',linewidth=1)\nax[0].plot(xplt,yhatplt,color='k',label='GP Mean',linewidth=1)\nax[0].fill_between(xplt,yhatplt-1.96*stdhatplt,yhatplt+1.96*stdhatplt,color='k',alpha=.25,label='95% CI')\nax[0].scatter(xnext,fnext,color='k',marker='*',s=200,zorder=10)\nax[0].set_xlim([0,1])\nax[0].set_xticks([0,1])\nax[0].set_xlabel(r'$x$')\nax[0].set_ylabel(r'$y$')\nfig.legend(labels=['data','true function','posterior mean',r'95\\% CI','next points by qEI'],loc='lower center',bbox_to_anchor=(.5,-.05),ncol=5)\ncontour = ax[1].contourf(x0mesh,x1mesh,qei_vals,cmap=cm.Greys_r)\nax[1].scatter([xnext[0]],[xnext[1]],color='k',marker='*',s=200)\nfig.colorbar(contour,ax=None,shrink=1,aspect=5)\nax[1].scatter(x0mesh.flatten(),x1mesh.flatten(),color='w',s=1)\nax[1].set_xlim([0,1])\nax[1].set_xticks([0,1])\nax[1].set_ylim([0,1])\nax[1].set_yticks([0,1])\nax[1].set_xlabel(r'$x_1$')\nax[1].set_ylabel(r'$x_2$')\nif root: fig.savefig(root+'gp.pdf',transparent=True)\n</pre> from matplotlib import cm fig,ax = pyplot.subplots(nrows=1,ncols=2,figsize=(8,3.25)) ax[0].scatter(x,y,color='k',label='Query Points') ax[0].plot(xplt,yplt,color='k',linestyle='--',label='True function',linewidth=1) ax[0].plot(xplt,yhatplt,color='k',label='GP Mean',linewidth=1) ax[0].fill_between(xplt,yhatplt-1.96*stdhatplt,yhatplt+1.96*stdhatplt,color='k',alpha=.25,label='95% CI') ax[0].scatter(xnext,fnext,color='k',marker='*',s=200,zorder=10) ax[0].set_xlim([0,1]) ax[0].set_xticks([0,1]) ax[0].set_xlabel(r'$x$') ax[0].set_ylabel(r'$y$') fig.legend(labels=['data','true function','posterior mean',r'95\\% CI','next points by qEI'],loc='lower center',bbox_to_anchor=(.5,-.05),ncol=5) contour = ax[1].contourf(x0mesh,x1mesh,qei_vals,cmap=cm.Greys_r) ax[1].scatter([xnext[0]],[xnext[1]],color='k',marker='*',s=200) fig.colorbar(contour,ax=None,shrink=1,aspect=5) ax[1].scatter(x0mesh.flatten(),x1mesh.flatten(),color='w',s=1) ax[1].set_xlim([0,1]) ax[1].set_xticks([0,1]) ax[1].set_ylim([0,1]) ax[1].set_yticks([0,1]) ax[1].set_xlabel(r'$x_1$') ax[1].set_ylabel(r'$x_2$') if root: fig.savefig(root+'gp.pdf',transparent=True) In\u00a0[8]: Copied! <pre>import pandas as pd\nfrom sklearn.model_selection import train_test_split\ndf = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/haberman/haberman.data',header=None)\ndf.columns = ['Age','1900 Year','Axillary Nodes','Survival Status']\ndf.loc[df['Survival Status']==2,'Survival Status'] = 0\nx,y = df[['Age','1900 Year','Axillary Nodes']],df['Survival Status']\nxt,xv,yt,yv = train_test_split(x,y,test_size=.33,random_state=7)\n</pre> import pandas as pd from sklearn.model_selection import train_test_split df = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/haberman/haberman.data',header=None) df.columns = ['Age','1900 Year','Axillary Nodes','Survival Status'] df.loc[df['Survival Status']==2,'Survival Status'] = 0 x,y = df[['Age','1900 Year','Axillary Nodes']],df['Survival Status'] xt,xv,yt,yv = train_test_split(x,y,test_size=.33,random_state=7) In\u00a0[9]: Copied! <pre>print(df.head(),'\\n')\nprint(df[['Age','1900 Year','Axillary Nodes']].describe(),'\\n')\nprint(df['Survival Status'].astype(str).describe())\nprint('\\ntrain samples: %d test samples: %d\\n'%(len(xt),len(xv)))\nprint('train positives %d   train negatives: %d'%(np.sum(yt==1),np.sum(yt==0)))\nprint(' test positives %d    test negatives: %d'%(np.sum(yv==1),np.sum(yv==0)))\nxt.head()\n</pre> print(df.head(),'\\n') print(df[['Age','1900 Year','Axillary Nodes']].describe(),'\\n') print(df['Survival Status'].astype(str).describe()) print('\\ntrain samples: %d test samples: %d\\n'%(len(xt),len(xv))) print('train positives %d   train negatives: %d'%(np.sum(yt==1),np.sum(yt==0))) print(' test positives %d    test negatives: %d'%(np.sum(yv==1),np.sum(yv==0))) xt.head() <pre>   Age  1900 Year  Axillary Nodes  Survival Status\n0   30         64               1                1\n1   30         62               3                1\n2   30         65               0                1\n3   31         59               2                1\n4   31         65               4                1 \n\n              Age   1900 Year  Axillary Nodes\ncount  306.000000  306.000000      306.000000\nmean    52.457516   62.852941        4.026144\nstd     10.803452    3.249405        7.189654\nmin     30.000000   58.000000        0.000000\n25%     44.000000   60.000000        0.000000\n50%     52.000000   63.000000        1.000000\n75%     60.750000   65.750000        4.000000\nmax     83.000000   69.000000       52.000000 \n\ncount     306\nunique      2\ntop         1\nfreq      225\nName: Survival Status, dtype: object\n\ntrain samples: 205 test samples: 101\n\ntrain positives 151   train negatives: 54\n test positives 74    test negatives: 27\n</pre> Out[9]: Age 1900 Year Axillary Nodes 46 41 58 0 199 57 64 1 115 49 64 10 128 50 61 0 249 63 63 0 In\u00a0[10]: Copied! <pre>blr = qp.BayesianLRCoeffs(\n    sampler = qp.DigitalNetB2(4,seed=1),\n    feature_array = xt, # np.ndarray of shape (n,d-1)\n    response_vector = yt, # np.ndarray of shape (n,)\n    prior_mean = 0, # normal prior mean = (0,0,...,0)\n    prior_covariance = 5) # normal prior covariance = 5I\nqmc_sc = qp.CubQMCNetG(blr,\n    abs_tol = .1,\n    rel_tol = .5,\n    error_fun = \"BOTH\",\n    n_limit=2**18)\nblr_coefs,blr_data = qmc_sc.integrate()\nprint(blr_data)\n# LDTransformData (AccumulateData Object)\n#     solution        [-0.004  0.13  -0.157  0.008]\n#     comb_bound_low  [-0.006  0.092 -0.205  0.007]\n#     comb_bound_high [-0.003  0.172 -0.109  0.012]\n#     comb_flags      [ True  True  True  True]\n#     n_total         2^(18)\n#     n               [[  1024.   1024. 262144.   2048.]\n#                     [  1024.   1024. 262144.   2048.]]\n#     time_integrate  2.229\n</pre> blr = qp.BayesianLRCoeffs(     sampler = qp.DigitalNetB2(4,seed=1),     feature_array = xt, # np.ndarray of shape (n,d-1)     response_vector = yt, # np.ndarray of shape (n,)     prior_mean = 0, # normal prior mean = (0,0,...,0)     prior_covariance = 5) # normal prior covariance = 5I qmc_sc = qp.CubQMCNetG(blr,     abs_tol = .1,     rel_tol = .5,     error_fun = \"BOTH\",     n_limit=2**18) blr_coefs,blr_data = qmc_sc.integrate() print(blr_data) # LDTransformData (AccumulateData Object) #     solution        [-0.004  0.13  -0.157  0.008] #     comb_bound_low  [-0.006  0.092 -0.205  0.007] #     comb_bound_high [-0.003  0.172 -0.109  0.012] #     comb_flags      [ True  True  True  True] #     n_total         2^(18) #     n               [[  1024.   1024. 262144.   2048.] #                     [  1024.   1024. 262144.   2048.]] #     time_integrate  2.229 <pre>Data (Data)\n    solution        [ 0.262 -0.043 -0.226 -1.203]\n    comb_bound_low  [ 0.185 -0.067 -0.306 -1.656]\n    comb_bound_high [ 0.347 -0.035 -0.163 -0.75 ]\n    comb_bound_diff [0.162 0.031 0.143 0.906]\n    comb_flags      [ True  True  True False]\n    n_total         2^(18)\n    n               [[  1024   1024   1024 262144]\n                     [  1024   1024   1024 262144]]\n    time_integrate  2.856\nCubQMCNetG (AbstractStoppingCriterion)\n    abs_tol         0.100\n    rel_tol         2^(-1)\n    n_init          2^(10)\n    n_limit         2^(18)\nBayesianLRCoeffs (AbstractIntegrand)\nGaussian (AbstractTrueMeasure)\n    mean            0\n    covariance      5\n    decomp_type     PCA\nDigitalNetB2 (AbstractLDDiscreteDistribution)\n    d               2^(2)\n    replications    1\n    randomize       LMS_DS\n    gen_mats_source joe_kuo.6.21201.txt\n    order           NATURAL\n    t               63\n    alpha           1\n    n_limit         2^(32)\n    entropy         1\n</pre> <pre>/Users/alegresor/Desktop/QMCSoftware/qmcpy/stopping_criterion/abstract_cub_qmc_ld_g.py:215: MaxSamplesWarning: \n                Already generated 262144 samples.\n                Trying to generate 262144 new samples would exceeds n_limit = 262144.\n                No more samples will be generated.\n                Note that error tolerances may not be satisfied. \n  warnings.warn(warning_s, MaxSamplesWarning)\n</pre> In\u00a0[11]: Copied! <pre>from sklearn.linear_model import LogisticRegression\ndef metrics(y,yhat):\n    y,yhat = np.array(y),np.array(yhat)\n    tp = np.sum((y==1)*(yhat==1))\n    tn = np.sum((y==0)*(yhat==0))\n    fp = np.sum((y==0)*(yhat==1))\n    fn = np.sum((y==1)*(yhat==0))\n    accuracy = (tp+tn)/(len(y))\n    precision = tp/(tp+fp)\n    recall = tp/(tp+fn)\n    return [accuracy,precision,recall]\n\nresults = pd.DataFrame({name:[] for name in ['method','Age','1900 Year','Axillary Nodes','Intercept','Accuracy','Precision','Recall']})\nfor i,l1_ratio in enumerate([0,.5,1]):\n    lr = LogisticRegression(random_state=7,penalty=\"elasticnet\",solver='saga',l1_ratio=l1_ratio).fit(xt,yt)\n    results.loc[i] = [r'Elastic-Net \\lambda=%.1f'%l1_ratio]+lr.coef_.squeeze().tolist()+[lr.intercept_.item()]+metrics(yv,lr.predict(xv))\n\nblr_predict = lambda x: 1/(1+np.exp(-np.array(x)@blr_coefs[:-1]-blr_coefs[-1]))&gt;=.5\nblr_train_accuracy = np.mean(blr_predict(xt)==yt)\nblr_test_accuracy = np.mean(blr_predict(xv)==yv)\nresults.loc[len(results)] = ['Bayesian']+blr_coefs.squeeze().tolist()+metrics(yv,blr_predict(xv))\n\nimport warnings\nwarnings.simplefilter('ignore',FutureWarning)\nresults.set_index('method',inplace=True)\nprint(results.head())\n#root: results.to_latex(root+'lr_table.tex',formatters={'%s'%tt:lambda v:'%.1f'%(100*v) for tt in ['accuracy','precision','recall']},float_format=\"%.2e\")\n</pre> from sklearn.linear_model import LogisticRegression def metrics(y,yhat):     y,yhat = np.array(y),np.array(yhat)     tp = np.sum((y==1)*(yhat==1))     tn = np.sum((y==0)*(yhat==0))     fp = np.sum((y==0)*(yhat==1))     fn = np.sum((y==1)*(yhat==0))     accuracy = (tp+tn)/(len(y))     precision = tp/(tp+fp)     recall = tp/(tp+fn)     return [accuracy,precision,recall]  results = pd.DataFrame({name:[] for name in ['method','Age','1900 Year','Axillary Nodes','Intercept','Accuracy','Precision','Recall']}) for i,l1_ratio in enumerate([0,.5,1]):     lr = LogisticRegression(random_state=7,penalty=\"elasticnet\",solver='saga',l1_ratio=l1_ratio).fit(xt,yt)     results.loc[i] = [r'Elastic-Net \\lambda=%.1f'%l1_ratio]+lr.coef_.squeeze().tolist()+[lr.intercept_.item()]+metrics(yv,lr.predict(xv))  blr_predict = lambda x: 1/(1+np.exp(-np.array(x)@blr_coefs[:-1]-blr_coefs[-1]))&gt;=.5 blr_train_accuracy = np.mean(blr_predict(xt)==yt) blr_test_accuracy = np.mean(blr_predict(xv)==yv) results.loc[len(results)] = ['Bayesian']+blr_coefs.squeeze().tolist()+metrics(yv,blr_predict(xv))  import warnings warnings.simplefilter('ignore',FutureWarning) results.set_index('method',inplace=True) print(results.head()) #root: results.to_latex(root+'lr_table.tex',formatters={'%s'%tt:lambda v:'%.1f'%(100*v) for tt in ['accuracy','precision','recall']},float_format=\"%.2e\") <pre>                              Age  1900 Year  Axillary Nodes  Intercept  \\\nmethod                                                                    \nElastic-Net \\lambda=0.0 -0.012279   0.034401       -0.115153   0.001990   \nElastic-Net \\lambda=0.5 -0.012041   0.034170       -0.114770   0.002025   \nElastic-Net \\lambda=1.0 -0.011803   0.033940       -0.114387   0.002061   \nBayesian                 0.262086  -0.043253       -0.225631  -1.202777   \n\n                         Accuracy  Precision    Recall  \nmethod                                                  \nElastic-Net \\lambda=0.0  0.742574   0.766667  0.932432  \nElastic-Net \\lambda=0.5  0.742574   0.766667  0.932432  \nElastic-Net \\lambda=1.0  0.742574   0.766667  0.932432  \nBayesian                 0.732673   0.737374  0.986486  \n</pre> In\u00a0[12]: Copied! <pre>a,b = 7,0.1\ndnb2 = qp.DigitalNetB2(3,seed=7)\nishigami = qp.Ishigami(dnb2,a,b)\nidxs = np.array([\n    [True,False,False],\n    [False,True,False],\n    [False,False,True],\n    [True,True,False],\n    [True,False,True],\n    [False,True,True]],dtype=bool)\nishigami_si = qp.SensitivityIndices(ishigami,idxs)\nqmc_algo = qp.CubQMCNetG(ishigami_si,abs_tol=.05)\nsolution,data = qmc_algo.integrate()\nprint(data)\nsi_closed = solution[0].squeeze()\nsi_total = solution[1].squeeze()\nci_comb_low_closed = data.comb_bound_low[0].squeeze()\nci_comb_high_closed = data.comb_bound_high[0].squeeze()\nci_comb_low_total = data.comb_bound_low[1].squeeze()\nci_comb_high_total = data.comb_bound_high[1].squeeze()\nprint(\"\\nApprox took %.1f sec and n = 2^(%d)\"%\n    (data.time_integrate,np.log2(data.n_total)))\nprint('\\t si_closed:',si_closed)\nprint('\\t si_total:',si_total)\nprint('\\t ci_comb_low_closed:',ci_comb_low_closed)\nprint('\\t ci_comb_high_closed:',ci_comb_high_closed)\nprint('\\t ci_comb_low_total:',ci_comb_low_total)\nprint('\\t ci_comb_high_total:',ci_comb_high_total)\n\ntrue_indices = qp.Ishigami._exact_sensitivity_indices(idxs,a,b)\nsi_closed_true = true_indices[0]\nsi_total_true = true_indices[1]\n</pre> a,b = 7,0.1 dnb2 = qp.DigitalNetB2(3,seed=7) ishigami = qp.Ishigami(dnb2,a,b) idxs = np.array([     [True,False,False],     [False,True,False],     [False,False,True],     [True,True,False],     [True,False,True],     [False,True,True]],dtype=bool) ishigami_si = qp.SensitivityIndices(ishigami,idxs) qmc_algo = qp.CubQMCNetG(ishigami_si,abs_tol=.05) solution,data = qmc_algo.integrate() print(data) si_closed = solution[0].squeeze() si_total = solution[1].squeeze() ci_comb_low_closed = data.comb_bound_low[0].squeeze() ci_comb_high_closed = data.comb_bound_high[0].squeeze() ci_comb_low_total = data.comb_bound_low[1].squeeze() ci_comb_high_total = data.comb_bound_high[1].squeeze() print(\"\\nApprox took %.1f sec and n = 2^(%d)\"%     (data.time_integrate,np.log2(data.n_total))) print('\\t si_closed:',si_closed) print('\\t si_total:',si_total) print('\\t ci_comb_low_closed:',ci_comb_low_closed) print('\\t ci_comb_high_closed:',ci_comb_high_closed) print('\\t ci_comb_low_total:',ci_comb_low_total) print('\\t ci_comb_high_total:',ci_comb_high_total)  true_indices = qp.Ishigami._exact_sensitivity_indices(idxs,a,b) si_closed_true = true_indices[0] si_total_true = true_indices[1] <pre>Data (Data)\n    solution        [[0.314 0.443 0.004 0.756 0.567 0.44 ]\n                     [0.582 0.443 0.245 0.984 0.567 0.694]]\n    comb_bound_low  [[0.293 0.426 0.    0.722 0.539 0.418]\n                     [0.552 0.432 0.236 0.969 0.544 0.67 ]]\n    comb_bound_high [[0.334 0.459 0.008 0.79  0.595 0.463]\n                     [0.612 0.453 0.253 1.    0.59  0.718]]\n    comb_bound_diff [[0.041 0.033 0.008 0.069 0.056 0.046]\n                     [0.06  0.021 0.017 0.031 0.045 0.048]]\n    comb_flags      [[ True  True  True  True  True  True]\n                     [ True  True  True  True  True  True]]\n    n_total         2^(10)\n    n               [[[1024 1024 1024 1024 1024 1024]\n                      [1024 1024 1024 1024 1024 1024]\n                      [1024 1024 1024 1024 1024 1024]]\n                    \n                     [[1024 1024 1024 1024 1024 1024]\n                      [1024 1024 1024 1024 1024 1024]\n                      [1024 1024 1024 1024 1024 1024]]]\n    time_integrate  0.021\nCubQMCNetG (AbstractStoppingCriterion)\n    abs_tol         0.050\n    rel_tol         0\n    n_init          2^(10)\n    n_limit         2^(35)\nSensitivityIndices (AbstractIntegrand)\n    indices         [[ True False False]\n                     [False  True False]\n                     [False False  True]\n                     [ True  True False]\n                     [ True False  True]\n                     [False  True  True]]\nUniform (AbstractTrueMeasure)\n    lower_bound     -3.142\n    upper_bound     3.142\nDigitalNetB2 (AbstractLDDiscreteDistribution)\n    d               3\n    replications    1\n    randomize       LMS_DS\n    gen_mats_source joe_kuo.6.21201.txt\n    order           NATURAL\n    t               63\n    alpha           1\n    n_limit         2^(32)\n    entropy         7\n\nApprox took 0.0 sec and n = 2^(10)\n\t si_closed: [0.3135849  0.44255327 0.00393187 0.75607985 0.56701027 0.44044942]\n\t si_total: [0.58195085 0.44252811 0.24471329 0.98433863 0.56700193 0.69402006]\n\t ci_comb_low_closed: [0.29324641 0.42598379 0.         0.72174769 0.53904217 0.4175601 ]\n\t ci_comb_high_closed: [0.33392338 0.45912275 0.00786373 0.79041202 0.59497838 0.46333875]\n\t ci_comb_low_total: [0.55205604 0.43181773 0.23639082 0.96867726 0.54432148 0.67010028]\n\t ci_comb_high_total: [0.61184567 0.45323849 0.25303576 1.         0.58968238 0.71793983]\n</pre> In\u00a0[13]: Copied! <pre>fig,ax = pyplot.subplots(figsize=(6,3))\nax.grid(False)\nfor spine in ['top','left','right','bottom']: ax.spines[spine].set_visible(False)\nwidth = .75\nax.errorbar(fmt='none',color='k',\n    x = 1-si_total_true,\n    y = np.arange(len(si_closed)),\n    xerr = 0,\n    yerr = width/2,\n    alpha = 1)\nbar_closed = ax.barh(np.arange(len(si_closed)),np.flip(si_closed),width,label='Closed SI',color='w',edgecolor='k',alpha=.75,linestyle='--')\nax.errorbar(fmt='none',color='k',\n    x = si_closed,\n    y = np.flip(np.arange(len(si_closed)))+width/4,\n    xerr = np.vstack((si_closed-ci_comb_low_closed,ci_comb_high_closed-si_closed)),\n    yerr = 0,\n    #elinewidth = 5,\n    alpha = .75)\nbar_total = ax.barh(np.arange(len(si_closed)),si_total,width,label='Total SI',color='w',alpha=.25,edgecolor='k',left=1-si_total,zorder=10,linestyle='-.')\nax.errorbar(fmt='none',color='k',\n    x = 1-si_total,\n    y = np.arange(len(si_closed))-width/4,\n    xerr = np.vstack((si_total-ci_comb_low_total,ci_comb_high_total-si_total)),\n    yerr = 0,\n    #elinewidth = 5,\n    alpha = .25)\nclosed_labels = [r'$\\underline{s}_{\\{%s\\}} = %.2f$'%(','.join([str(i+1) for i in idx]),c) for idx,c in zip(idxs[::-1],np.flip(si_closed))]\nclosed_labels[3] = ''\ntotal_labels = [r'$\\overline{s}_{\\{%s\\}} = %.2f$'%(','.join([str(i+1) for i in idx]),t) for idx,t in zip(idxs,si_total)]\nax.bar_label(bar_closed,label_type='center',labels=closed_labels)\nax.bar_label(bar_total,label_type='center',labels=total_labels)\nax.set_xlim([-.001,1.001])\nax.axvline(x=0,ymin=0,ymax=len(si_closed),color='k',alpha=.25)\nax.axvline(x=1,ymin=0,ymax=len(si_closed),color='k',alpha=.25)\nax.set_yticklabels([])\nif root: fig.savefig(root+'ishigami.pdf',transparent=True)\n</pre> fig,ax = pyplot.subplots(figsize=(6,3)) ax.grid(False) for spine in ['top','left','right','bottom']: ax.spines[spine].set_visible(False) width = .75 ax.errorbar(fmt='none',color='k',     x = 1-si_total_true,     y = np.arange(len(si_closed)),     xerr = 0,     yerr = width/2,     alpha = 1) bar_closed = ax.barh(np.arange(len(si_closed)),np.flip(si_closed),width,label='Closed SI',color='w',edgecolor='k',alpha=.75,linestyle='--') ax.errorbar(fmt='none',color='k',     x = si_closed,     y = np.flip(np.arange(len(si_closed)))+width/4,     xerr = np.vstack((si_closed-ci_comb_low_closed,ci_comb_high_closed-si_closed)),     yerr = 0,     #elinewidth = 5,     alpha = .75) bar_total = ax.barh(np.arange(len(si_closed)),si_total,width,label='Total SI',color='w',alpha=.25,edgecolor='k',left=1-si_total,zorder=10,linestyle='-.') ax.errorbar(fmt='none',color='k',     x = 1-si_total,     y = np.arange(len(si_closed))-width/4,     xerr = np.vstack((si_total-ci_comb_low_total,ci_comb_high_total-si_total)),     yerr = 0,     #elinewidth = 5,     alpha = .25) closed_labels = [r'$\\underline{s}_{\\{%s\\}} = %.2f$'%(','.join([str(i+1) for i in idx]),c) for idx,c in zip(idxs[::-1],np.flip(si_closed))] closed_labels[3] = '' total_labels = [r'$\\overline{s}_{\\{%s\\}} = %.2f$'%(','.join([str(i+1) for i in idx]),t) for idx,t in zip(idxs,si_total)] ax.bar_label(bar_closed,label_type='center',labels=closed_labels) ax.bar_label(bar_total,label_type='center',labels=total_labels) ax.set_xlim([-.001,1.001]) ax.axvline(x=0,ymin=0,ymax=len(si_closed),color='k',alpha=.25) ax.axvline(x=1,ymin=0,ymax=len(si_closed),color='k',alpha=.25) ax.set_yticklabels([]) if root: fig.savefig(root+'ishigami.pdf',transparent=True) In\u00a0[14]: Copied! <pre>fig,ax = pyplot.subplots(nrows=1,ncols=3,figsize=(8,3))\nx_1d = np.linspace(0,1,num=128)\nx_1d_mat = np.tile(x_1d,(3,1)).T\ny_1d = qp.Ishigami._exact_fu_functions(x_1d_mat,indices=[[0],[1],[2]],a=a,b=b)\nfor i in range(2):\n    ax[i].plot(x_1d,y_1d[:,i],color='k')\n    ax[i].set_xlim([0,1])\n    ax[i].set_xticks([0,1])\n    ax[i].set_xlabel(r'$x_{%d}$'%(i+1))\n    ax[i].set_title(r'$f_{\\{%d\\}} \\in [%.1f,%.1f]$'%(i+1,y_1d[:,i].min(),y_1d[:,i].max()))\nx_mesh,y_mesh = np.meshgrid(x_1d,x_1d)\nxquery = np.zeros((x_mesh.size,3))\nfor i,idx in enumerate([[1,2]]): # [[0,1],[0,2],[1,2]]\n    xquery[:,idx[0]] = x_mesh.flatten()\n    xquery[:,idx[1]] = y_mesh.flatten()\n    zquery = qp.Ishigami._exact_fu_functions(xquery,indices=[idx],a=a,b=b)\n    z_mesh = zquery.reshape(x_mesh.shape)\n    ax[2+i].contourf(x_mesh,y_mesh,z_mesh,cmap=cm.Greys_r)\n    ax[2+i].set_xlabel(r'$x_{%d}$'%(idx[0]+1))\n    ax[2+i].set_ylabel(r'$x_{%d}$'%(idx[1]+1))\n    ax[2+i].set_title(r'$f_{\\{%d,%d\\}} \\in [%.1f,%.1f]$'%(tuple([i+1 for i in idx])+(z_mesh.min(),z_mesh.max())))\n    ax[2+i].set_xlim([0,1])\n    ax[2+i].set_ylim([0,1])\n    ax[2+i].set_xticks([0,1])\n    ax[2+i].set_yticks([0,1])\nif root: fig.savefig(root+'ishigami_fu.pdf')\n</pre> fig,ax = pyplot.subplots(nrows=1,ncols=3,figsize=(8,3)) x_1d = np.linspace(0,1,num=128) x_1d_mat = np.tile(x_1d,(3,1)).T y_1d = qp.Ishigami._exact_fu_functions(x_1d_mat,indices=[[0],[1],[2]],a=a,b=b) for i in range(2):     ax[i].plot(x_1d,y_1d[:,i],color='k')     ax[i].set_xlim([0,1])     ax[i].set_xticks([0,1])     ax[i].set_xlabel(r'$x_{%d}$'%(i+1))     ax[i].set_title(r'$f_{\\{%d\\}} \\in [%.1f,%.1f]$'%(i+1,y_1d[:,i].min(),y_1d[:,i].max())) x_mesh,y_mesh = np.meshgrid(x_1d,x_1d) xquery = np.zeros((x_mesh.size,3)) for i,idx in enumerate([[1,2]]): # [[0,1],[0,2],[1,2]]     xquery[:,idx[0]] = x_mesh.flatten()     xquery[:,idx[1]] = y_mesh.flatten()     zquery = qp.Ishigami._exact_fu_functions(xquery,indices=[idx],a=a,b=b)     z_mesh = zquery.reshape(x_mesh.shape)     ax[2+i].contourf(x_mesh,y_mesh,z_mesh,cmap=cm.Greys_r)     ax[2+i].set_xlabel(r'$x_{%d}$'%(idx[0]+1))     ax[2+i].set_ylabel(r'$x_{%d}$'%(idx[1]+1))     ax[2+i].set_title(r'$f_{\\{%d,%d\\}} \\in [%.1f,%.1f]$'%(tuple([i+1 for i in idx])+(z_mesh.min(),z_mesh.max())))     ax[2+i].set_xlim([0,1])     ax[2+i].set_ylim([0,1])     ax[2+i].set_xticks([0,1])     ax[2+i].set_yticks([0,1]) if root: fig.savefig(root+'ishigami_fu.pdf') In\u00a0[15]: Copied! <pre>from sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neural_network import MLPClassifier\ndata = load_iris()\nfeature_names = data[\"feature_names\"]\nfeature_names = [fn.replace('sepal ','S')\\\n    .replace('length ','L')\\\n    .replace('petal ','P')\\\n    .replace('width ','W')\\\n    .replace('(cm)','') for fn in feature_names]\ntarget_names = data[\"target_names\"]\nxt,xv,yt,yv = train_test_split(data[\"data\"],data[\"target\"],\n    test_size = 1/3,\n    random_state = 7)\n</pre> from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split from sklearn.neural_network import MLPClassifier data = load_iris() feature_names = data[\"feature_names\"] feature_names = [fn.replace('sepal ','S')\\     .replace('length ','L')\\     .replace('petal ','P')\\     .replace('width ','W')\\     .replace('(cm)','') for fn in feature_names] target_names = data[\"target_names\"] xt,xv,yt,yv = train_test_split(data[\"data\"],data[\"target\"],     test_size = 1/3,     random_state = 7) In\u00a0[16]: Copied! <pre>mlpc = MLPClassifier(random_state=7,max_iter=1024).fit(xt,yt)\nyhat = mlpc.predict(xv)\nprint(\"accuracy: %.1f%%\"%(100*(yv==yhat).mean()))\n# accuracy: 98.0%\nsampler = qp.DigitalNetB2(4,seed=7)\ntrue_measure =  qp.Uniform(sampler,\n    lower_bound = xt.min(0),\n    upper_bound = xt.max(0))\nfun = qp.CustomFun(\n    true_measure = true_measure,\n    g = lambda x: mlpc.predict_proba(x).T,\n    dimension_indv = 3)\nsi_fun = qp.SensitivityIndices(fun,indices=\"all\")\nqmc_algo = qp.CubQMCNetG(si_fun,abs_tol=.005)\nnn_sis,nn_sis_data = qmc_algo.integrate()\n</pre> mlpc = MLPClassifier(random_state=7,max_iter=1024).fit(xt,yt) yhat = mlpc.predict(xv) print(\"accuracy: %.1f%%\"%(100*(yv==yhat).mean())) # accuracy: 98.0% sampler = qp.DigitalNetB2(4,seed=7) true_measure =  qp.Uniform(sampler,     lower_bound = xt.min(0),     upper_bound = xt.max(0)) fun = qp.CustomFun(     true_measure = true_measure,     g = lambda x: mlpc.predict_proba(x).T,     dimension_indv = 3) si_fun = qp.SensitivityIndices(fun,indices=\"all\") qmc_algo = qp.CubQMCNetG(si_fun,abs_tol=.005) nn_sis,nn_sis_data = qmc_algo.integrate() <pre>accuracy: 98.0%\n</pre> In\u00a0[17]: Copied! <pre>#print(nn_sis_data.flags_indv.shape)\n#print(nn_sis_data.flags_comb.shape)\nprint('samples: 2^(%d)'%np.log2(nn_sis_data.n_total))\nprint('time: %.1e'%nn_sis_data.time_integrate)\nprint('indices:\\n%s'%nn_sis_data.integrand.indices)\n\nimport pandas as pd\n\ndf_closed = pd.DataFrame(nn_sis[0],columns=target_names,index=[str(np.where(idx)[0]) for idx in nn_sis_data.integrand.indices])\nprint('\\nClosed Indices')\nprint(df_closed)\ndf_total = pd.DataFrame(nn_sis[1],columns=target_names,index=[str(np.where(idx)[0]) for idx in nn_sis_data.integrand.indices])\nprint('\\nTotal Indices')\ndf_closed_singletons = df_closed.loc[['[%d]'%i for i in range(4)]].T\ndf_closed_singletons['sum singletons'] = df_closed_singletons[['[%d]'%i for i in range(4)]].sum(1)\ndf_closed_singletons.columns = data['feature_names']+['sum']\ndf_closed_singletons = df_closed_singletons*100\ndf_closed_singletons\n</pre> #print(nn_sis_data.flags_indv.shape) #print(nn_sis_data.flags_comb.shape) print('samples: 2^(%d)'%np.log2(nn_sis_data.n_total)) print('time: %.1e'%nn_sis_data.time_integrate) print('indices:\\n%s'%nn_sis_data.integrand.indices)  import pandas as pd  df_closed = pd.DataFrame(nn_sis[0],columns=target_names,index=[str(np.where(idx)[0]) for idx in nn_sis_data.integrand.indices]) print('\\nClosed Indices') print(df_closed) df_total = pd.DataFrame(nn_sis[1],columns=target_names,index=[str(np.where(idx)[0]) for idx in nn_sis_data.integrand.indices]) print('\\nTotal Indices') df_closed_singletons = df_closed.loc[['[%d]'%i for i in range(4)]].T df_closed_singletons['sum singletons'] = df_closed_singletons[['[%d]'%i for i in range(4)]].sum(1) df_closed_singletons.columns = data['feature_names']+['sum'] df_closed_singletons = df_closed_singletons*100 df_closed_singletons <pre>samples: 2^(15)\ntime: 1.5e+00\nindices:\n[[ True False False False]\n [False  True False False]\n [False False  True False]\n [False False False  True]\n [ True  True False False]\n [ True False  True False]\n [ True False False  True]\n [False  True  True False]\n [False  True False  True]\n [False False  True  True]\n [ True  True  True False]\n [ True  True False  True]\n [ True False  True  True]\n [False  True  True  True]]\n\nClosed Indices\n           setosa  versicolor  virginica\n[0]      0.000000    0.067568   0.099035\n[1]      0.051979    0.002589   0.000000\n[2]      0.712711    0.326664   0.498664\n[3]      0.051251    0.018706   0.120796\n[0 1]    0.052284    0.080725   0.112412\n[0 2]    0.714204    0.462147   0.641450\n[0 3]    0.050927    0.086925   0.207602\n[1 2]    0.840864    0.433329   0.514042\n[1 3]    0.102538    0.004948   0.126151\n[2 3]    0.822405    0.583743   0.705192\n[0 1 2]  0.843055    0.571913   0.661004\n[0 1 3]  0.103351    0.103411   0.217246\n[0 2 3]  0.824347    0.816010   0.946992\n[1 2 3]  0.995490    0.739267   0.726782\n\nTotal Indices\n</pre> Out[17]: sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) sum setosa 0.000000 5.197885 71.271100 5.125145 81.594130 versicolor 6.756844 0.258851 32.666361 1.870561 41.552616 virginica 9.903452 0.000000 49.866351 12.079616 71.849419 In\u00a0[18]: Copied! <pre>nindices = len(nn_sis_data.integrand.indices)\nfig,ax = pyplot.subplots(figsize=(9,5))\nticks = np.arange(nindices)\nwidth = .25\nfor i,(alpha,species) in enumerate(zip([.25,.5,.75],data['target_names'])):\n    cvals = df_closed[species].to_numpy()\n    tvals = df_total[species].to_numpy()\n    ticks_i = ticks+i*width\n    ax.bar(ticks_i,cvals,width=width,align='edge',color='k',alpha=alpha,label=species)\n    #ax.bar(ticks_i,np.flip(tvals),width=width,align='edge',bottom=1-np.flip(tvals),color=color,alpha=.1)\nax.set_xlim([0,13+3*width])\nax.set_xticks(ticks+1.5*width)\n\n# closed_labels = [r'$\\underline{s}_{\\{%s\\}}$'%(','.join([r'\\text{%s}'%feature_names[i] for i in idx])) for idx in nn_sis_data.integrand.indices]\nclosed_labels = ['\\n'.join([feature_names[i] for i in np.where(idx)[0]]) for idx in nn_sis_data.integrand.indices]\nax.set_xticklabels(closed_labels,rotation=0)\nax.set_ylim([0,1]); ax.set_yticks([0,1])\nax.grid(False)\nfor spine in ['top','right','bottom']: ax.spines[spine].set_visible(False)\nax.legend(frameon=False,loc='lower center',bbox_to_anchor=(.5,-.2),ncol=3);\nfig.tight_layout()\nif root: fig.savefig(root+'nn_si.pdf')\n</pre> nindices = len(nn_sis_data.integrand.indices) fig,ax = pyplot.subplots(figsize=(9,5)) ticks = np.arange(nindices) width = .25 for i,(alpha,species) in enumerate(zip([.25,.5,.75],data['target_names'])):     cvals = df_closed[species].to_numpy()     tvals = df_total[species].to_numpy()     ticks_i = ticks+i*width     ax.bar(ticks_i,cvals,width=width,align='edge',color='k',alpha=alpha,label=species)     #ax.bar(ticks_i,np.flip(tvals),width=width,align='edge',bottom=1-np.flip(tvals),color=color,alpha=.1) ax.set_xlim([0,13+3*width]) ax.set_xticks(ticks+1.5*width)  # closed_labels = [r'$\\underline{s}_{\\{%s\\}}$'%(','.join([r'\\text{%s}'%feature_names[i] for i in idx])) for idx in nn_sis_data.integrand.indices] closed_labels = ['\\n'.join([feature_names[i] for i in np.where(idx)[0]]) for idx in nn_sis_data.integrand.indices] ax.set_xticklabels(closed_labels,rotation=0) ax.set_ylim([0,1]); ax.set_yticks([0,1]) ax.grid(False) for spine in ['top','right','bottom']: ax.spines[spine].set_visible(False) ax.legend(frameon=False,loc='lower center',bbox_to_anchor=(.5,-.2),ncol=3); fig.tight_layout() if root: fig.savefig(root+'nn_si.pdf')"},{"location":"demos/vectorized_qmc/#vectorized-qmc","title":"Vectorized QMC\u00b6","text":""},{"location":"demos/vectorized_qmc/#ld-sequence","title":"LD Sequence\u00b6","text":""},{"location":"demos/vectorized_qmc/#simple-example","title":"Simple Example\u00b6","text":""},{"location":"demos/vectorized_qmc/#bo-qei","title":"BO QEI\u00b6","text":"<p>See the QEI Demo in QMCPy or the BoTorch Acquisition documentation for details on Bayesian Optimization using q-Expected Improvement.</p>"},{"location":"demos/vectorized_qmc/#bayesian-logistic-regression","title":"Bayesian Logistic Regression\u00b6","text":""},{"location":"demos/vectorized_qmc/#sensitivity-indices","title":"Sensitivity Indices\u00b6","text":""},{"location":"demos/vectorized_qmc/#ishigami-function","title":"Ishigami Function\u00b6","text":""},{"location":"demos/vectorized_qmc/#neural-network","title":"Neural Network\u00b6","text":""},{"location":"demos/vectorized_qmc_bayes/","title":"Vectorized QMC Bayesian Cubature","text":"In\u00a0[1]: Copied! <pre>%%capture\n# @title Execute this cell to install dependancies\ntry:\n  import google.colab\n  import os\n  !pip install -q qmcpy &gt;&gt; /dev/null\n  !apt-get update &amp;&amp; apt-get install -y --no-install-recommends texlive-latex-base texlive-fonts-recommended texlive-latex-extra cm-super dvipng\nexcept:\n  pass\n\nimport matplotlib.pyplot as plt\n\nplt.rcParams.update({\n\"text.usetex\": True,\n\"font.family\": \"serif\",\n\"text.latex.preamble\": r\"\\usepackage{amsmath}\\usepackage{amssymb}\\newcommand{\\bx}{\\boldsymbol{x}}\"\n})\n!pip install scikit-learn\n</pre> %%capture # @title Execute this cell to install dependancies try:   import google.colab   import os   !pip install -q qmcpy &gt;&gt; /dev/null   !apt-get update &amp;&amp; apt-get install -y --no-install-recommends texlive-latex-base texlive-fonts-recommended texlive-latex-extra cm-super dvipng except:   pass  import matplotlib.pyplot as plt  plt.rcParams.update({ \"text.usetex\": True, \"font.family\": \"serif\", \"text.latex.preamble\": r\"\\usepackage{amsmath}\\usepackage{amssymb}\\newcommand{\\bx}{\\boldsymbol{x}}\" }) !pip install scikit-learn  In\u00a0[2]: Copied! <pre>import qmcpy as qp\nimport numpy as np\n</pre> import qmcpy as qp import numpy as np In\u00a0[7]: Copied! <pre>from matplotlib import pyplot\n%matplotlib inline\nroot = None#'.'\n</pre> from matplotlib import pyplot %matplotlib inline root = None#'.' In\u00a0[8]: Copied! <pre>n = 2**6\ns = 10\nfig,ax = pyplot.subplots(figsize=(8,4),nrows=1,ncols=3)\nfor i,(dd,name) in enumerate(zip(\n    [qp.IIDStdUniform(2,seed=7),qp.DigitalNetB2(2,seed=7),qp.Lattice(2,seed=7)],\n    ['IID','Randomized Digital Net (LD)','Randomized Lattice (LD)'])):\n    pts = dd.gen_samples(n)\n    ax[i].scatter(pts[0:n//4,0],pts[0:n//4,1],color='k',marker='s',s=s)\n    ax[i].scatter(pts[n//4:n//2,0],pts[n//4:n//2,1],color='k',marker='o',s=s)\n    ax[i].scatter(pts[n//2:n,0],pts[n//2:n,1],color='k',marker='^',s=s)\n    ax[i].set_aspect(1)\n    ax[i].set_xlabel(r'$x_{1}$')\n    ax[i].set_ylabel(r'$x_{2}$')\n    ax[i].set_xlim([0,1])\n    ax[i].set_ylim([0,1])\n    ax[i].set_xticks([0,.25,.5,.75,1])\n    ax[i].set_xticklabels([r'$0$',r'$1/4$',r'$1/2$',r'$3/4$',r'$1$'])\n    ax[i].set_yticks([0,.25,.5,.75,1])\n    ax[i].set_yticklabels([r'$0$',r'$1/4$',r'$1/2$',r'$3/4$',r'$1$'])\n    ax[i].set_title(name)\nif root: fig.savefig(root+'ld_seqs.pdf',transparent=True)\n</pre> n = 2**6 s = 10 fig,ax = pyplot.subplots(figsize=(8,4),nrows=1,ncols=3) for i,(dd,name) in enumerate(zip(     [qp.IIDStdUniform(2,seed=7),qp.DigitalNetB2(2,seed=7),qp.Lattice(2,seed=7)],     ['IID','Randomized Digital Net (LD)','Randomized Lattice (LD)'])):     pts = dd.gen_samples(n)     ax[i].scatter(pts[0:n//4,0],pts[0:n//4,1],color='k',marker='s',s=s)     ax[i].scatter(pts[n//4:n//2,0],pts[n//4:n//2,1],color='k',marker='o',s=s)     ax[i].scatter(pts[n//2:n,0],pts[n//2:n,1],color='k',marker='^',s=s)     ax[i].set_aspect(1)     ax[i].set_xlabel(r'$x_{1}$')     ax[i].set_ylabel(r'$x_{2}$')     ax[i].set_xlim([0,1])     ax[i].set_ylim([0,1])     ax[i].set_xticks([0,.25,.5,.75,1])     ax[i].set_xticklabels([r'$0$',r'$1/4$',r'$1/2$',r'$3/4$',r'$1$'])     ax[i].set_yticks([0,.25,.5,.75,1])     ax[i].set_yticklabels([r'$0$',r'$1/4$',r'$1/2$',r'$3/4$',r'$1$'])     ax[i].set_title(name) if root: fig.savefig(root+'ld_seqs.pdf',transparent=True) In\u00a0[11]: Copied! <pre>def cantilever_beam_function(T,compute_flags): # T is (n x 3)\n    Y = np.zeros((2,len(T)),dtype=float) # (n x 2)\n    l,w,t = 100,4,2\n    T1,T2,T3 = T[:,0],T[:,1],T[:,2] # Python is indexed from 0\n    if compute_flags[0]: # compute D. x^2 is \"x**2\" in Python\n        Y[0] = 4*l**3/(T1*w*t)*np.sqrt(T2**2/t**4+T3**2/w**4)\n    if compute_flags[1]: # compute S\n        Y[1] = 600*(T2/(w*t**2)+T3/(w**2*t))\n    return Y\ntrue_measure = qp.Gaussian(\n    sampler = qp.DigitalNetB2(dimension=3,seed=7),\n    mean = [2.9e7,500,1000],\n    covariance = np.diag([(1.45e6)**2,(100)**2,(100)**2]))\nintegrand = qp.CustomFun(true_measure,\n    g = cantilever_beam_function,\n    dimension_indv = 2)\nqmc_stop_crit = qp.CubBayesNetG(integrand,\n    abs_tol = 1e-3,\n    rel_tol = 1e-6)\nsolution,data = qmc_stop_crit.integrate()\nprint(solution)\n# [2.42575885e+00 3.74999973e+04]\nprint(data)\n</pre> def cantilever_beam_function(T,compute_flags): # T is (n x 3)     Y = np.zeros((2,len(T)),dtype=float) # (n x 2)     l,w,t = 100,4,2     T1,T2,T3 = T[:,0],T[:,1],T[:,2] # Python is indexed from 0     if compute_flags[0]: # compute D. x^2 is \"x**2\" in Python         Y[0] = 4*l**3/(T1*w*t)*np.sqrt(T2**2/t**4+T3**2/w**4)     if compute_flags[1]: # compute S         Y[1] = 600*(T2/(w*t**2)+T3/(w**2*t))     return Y true_measure = qp.Gaussian(     sampler = qp.DigitalNetB2(dimension=3,seed=7),     mean = [2.9e7,500,1000],     covariance = np.diag([(1.45e6)**2,(100)**2,(100)**2])) integrand = qp.CustomFun(true_measure,     g = cantilever_beam_function,     dimension_indv = 2) qmc_stop_crit = qp.CubBayesNetG(integrand,     abs_tol = 1e-3,     rel_tol = 1e-6) solution,data = qmc_stop_crit.integrate() print(solution) # [2.42575885e+00 3.74999973e+04] print(data) <pre>[2.42553968e+00 3.75000056e+04]\nData (Data)\n    solution        [2.426e+00 3.750e+04]\n    comb_bound_low  [2.425e+00 3.750e+04]\n    comb_bound_high [2.426e+00 3.750e+04]\n    comb_bound_diff [0.001 0.028]\n    comb_flags      [ True  True]\n    n_total         2^(17)\n    n               [  1024 131072]\n    time_integrate  1.515\nCubQMCBayesNetG (AbstractStoppingCriterion)\n    abs_tol         0.001\n    rel_tol         1.00e-06\n    n_init          2^(8)\n    n_limit         2^(22)\n    order           1\nCustomFun (AbstractIntegrand)\nGaussian (AbstractTrueMeasure)\n    mean            [2.9e+07 5.0e+02 1.0e+03]\n    covariance      [[2.102e+12 0.000e+00 0.000e+00]\n                     [0.000e+00 1.000e+04 0.000e+00]\n                     [0.000e+00 0.000e+00 1.000e+04]]\n    decomp_type     PCA\nDigitalNetB2 (AbstractLDDiscreteDistribution)\n    d               3\n    replications    1\n    randomize       LMS_DS\n    gen_mats_source joe_kuo.6.21201.txt\n    order           NATURAL\n    t               63\n    alpha           1\n    n_limit         2^(32)\n    entropy         7\n</pre> In\u00a0[12]: Copied! <pre>import scipy\nfrom sklearn.gaussian_process import GaussianProcessRegressor,kernels\n\nf = lambda x: np.cos(10*x)*np.exp(.2*x)+np.exp(-5*(x-.4)**2)\nxplt = np.linspace(0,1,100)\nyplt = f(xplt)\nx = np.array([.1, .2, .4, .7, .9])\ny = f(x)\nymax = y.max()\n\ngp = GaussianProcessRegressor(kernel=kernels.RBF(length_scale=1.0,length_scale_bounds=(1e-2, 1e2)),\n    n_restarts_optimizer = 16).fit(x[:,None],y)\nyhatplt,stdhatplt = gp.predict(xplt[:,None],return_std=True)\n\ntpax = 32\nx0mesh,x1mesh = np.meshgrid(np.linspace(0,1,tpax),np.linspace(0,1,tpax))\npost_mus = np.zeros((tpax,tpax,2),dtype=float)\npost_sqrtcovs = np.zeros((tpax,tpax,2,2),dtype=float)\nfor j0 in range(tpax):\n    for j1 in range(tpax):\n        candidate = np.array([[x0mesh[j0,j1]],[x1mesh[j0,j1]]])\n        post_mus[j0,j1],post_cov = gp.predict(candidate,return_cov=True)\n        evals,evecs = scipy.linalg.eig(post_cov)\n        post_sqrtcovs[j0,j1] = np.sqrt(np.maximum(evals.real,0))*evecs\n\ndef qei_acq_vec(x,compute_flags):\n    xgauss = scipy.stats.norm.ppf(x)\n    n = len(x)\n    qei_vals = np.zeros((tpax,tpax,n),dtype=float)\n    for j0 in range(tpax):\n        for j1 in range(tpax):\n            if compute_flags[j0,j1]==False: continue\n            sqrt_cov = post_sqrtcovs[j0,j1]\n            mu_post = post_mus[j0,j1]\n            for i in range(len(x)):\n                yij = sqrt_cov@xgauss[i]+mu_post\n                qei_vals[j0,j1,i] = max((yij-ymax).max(),0)\n    return qei_vals\n\nqei_acq_vec_qmcpy = qp.CustomFun(\n    true_measure = qp.Uniform(qp.DigitalNetB2(2,seed=7)),\n    g = qei_acq_vec,\n    dimension_indv = (tpax,tpax),\n    parallel=False)\nqei_vals,qei_data = qp.CubBayesNetG(qei_acq_vec_qmcpy,abs_tol=.025,rel_tol=0).integrate() # .0005\nprint(qei_data)\n\na = np.unravel_index(np.argmax(qei_vals,axis=None),qei_vals.shape)\nxnext = np.array([x0mesh[a[0],a[1]],x1mesh[a[0],a[1]]])\nfnext = f(xnext)\n</pre> import scipy from sklearn.gaussian_process import GaussianProcessRegressor,kernels  f = lambda x: np.cos(10*x)*np.exp(.2*x)+np.exp(-5*(x-.4)**2) xplt = np.linspace(0,1,100) yplt = f(xplt) x = np.array([.1, .2, .4, .7, .9]) y = f(x) ymax = y.max()  gp = GaussianProcessRegressor(kernel=kernels.RBF(length_scale=1.0,length_scale_bounds=(1e-2, 1e2)),     n_restarts_optimizer = 16).fit(x[:,None],y) yhatplt,stdhatplt = gp.predict(xplt[:,None],return_std=True)  tpax = 32 x0mesh,x1mesh = np.meshgrid(np.linspace(0,1,tpax),np.linspace(0,1,tpax)) post_mus = np.zeros((tpax,tpax,2),dtype=float) post_sqrtcovs = np.zeros((tpax,tpax,2,2),dtype=float) for j0 in range(tpax):     for j1 in range(tpax):         candidate = np.array([[x0mesh[j0,j1]],[x1mesh[j0,j1]]])         post_mus[j0,j1],post_cov = gp.predict(candidate,return_cov=True)         evals,evecs = scipy.linalg.eig(post_cov)         post_sqrtcovs[j0,j1] = np.sqrt(np.maximum(evals.real,0))*evecs  def qei_acq_vec(x,compute_flags):     xgauss = scipy.stats.norm.ppf(x)     n = len(x)     qei_vals = np.zeros((tpax,tpax,n),dtype=float)     for j0 in range(tpax):         for j1 in range(tpax):             if compute_flags[j0,j1]==False: continue             sqrt_cov = post_sqrtcovs[j0,j1]             mu_post = post_mus[j0,j1]             for i in range(len(x)):                 yij = sqrt_cov@xgauss[i]+mu_post                 qei_vals[j0,j1,i] = max((yij-ymax).max(),0)     return qei_vals  qei_acq_vec_qmcpy = qp.CustomFun(     true_measure = qp.Uniform(qp.DigitalNetB2(2,seed=7)),     g = qei_acq_vec,     dimension_indv = (tpax,tpax),     parallel=False) qei_vals,qei_data = qp.CubBayesNetG(qei_acq_vec_qmcpy,abs_tol=.025,rel_tol=0).integrate() # .0005 print(qei_data)  a = np.unravel_index(np.argmax(qei_vals,axis=None),qei_vals.shape) xnext = np.array([x0mesh[a[0],a[1]],x1mesh[a[0],a[1]]]) fnext = f(xnext) <pre>Data (Data)\n    solution        [[0.058 0.081 0.074 ... 0.062 0.062 0.068]\n                     [0.078 0.063 0.069 ... 0.066 0.066 0.072]\n                     [0.072 0.066 0.032 ... 0.033 0.033 0.039]\n                     ...\n                     [0.058 0.063 0.032 ... 0.    0.    0.006]\n                     [0.061 0.065 0.034 ... 0.003 0.    0.006]\n                     [0.067 0.072 0.041 ... 0.01  0.01  0.006]]\n    comb_bound_low  [[0.056 0.077 0.07  ... 0.058 0.058 0.064]\n                     [0.076 0.061 0.065 ... 0.063 0.063 0.069]\n                     [0.069 0.064 0.031 ... 0.032 0.032 0.037]\n                     ...\n                     [0.056 0.061 0.031 ... 0.    0.    0.005]\n                     [0.058 0.063 0.032 ... 0.001 0.    0.005]\n                     [0.064 0.069 0.038 ... 0.006 0.006 0.004]]\n    comb_bound_high [[0.061 0.086 0.078 ... 0.066 0.066 0.073]\n                     [0.081 0.065 0.072 ... 0.069 0.07  0.076]\n                     [0.074 0.068 0.033 ... 0.035 0.035 0.042]\n                     ...\n                     [0.061 0.065 0.032 ... 0.    0.    0.008]\n                     [0.064 0.067 0.036 ... 0.005 0.    0.008]\n                     [0.071 0.075 0.044 ... 0.014 0.014 0.007]]\n    comb_bound_diff [[0.004 0.008 0.009 ... 0.008 0.008 0.008]\n                     [0.005 0.003 0.007 ... 0.006 0.006 0.007]\n                     [0.005 0.004 0.002 ... 0.004 0.004 0.005]\n                     ...\n                     [0.004 0.003 0.002 ... 0.    0.    0.003]\n                     [0.006 0.004 0.003 ... 0.004 0.    0.003]\n                     [0.007 0.007 0.007 ... 0.008 0.008 0.003]]\n    comb_flags      [[ True  True  True ...  True  True  True]\n                     [ True  True  True ...  True  True  True]\n                     [ True  True  True ...  True  True  True]\n                     ...\n                     [ True  True  True ...  True  True  True]\n                     [ True  True  True ...  True  True  True]\n                     [ True  True  True ...  True  True  True]]\n    n_total         2^(8)\n    n               [[256 256 256 ... 256 256 256]\n                     [256 256 256 ... 256 256 256]\n                     [256 256 256 ... 256 256 256]\n                     ...\n                     [256 256 256 ... 256 256 256]\n                     [256 256 256 ... 256 256 256]\n                     [256 256 256 ... 256 256 256]]\n    time_integrate  5.308\nCubQMCBayesNetG (AbstractStoppingCriterion)\n    abs_tol         0.025\n    rel_tol         0\n    n_init          2^(8)\n    n_limit         2^(22)\n    order           1\nCustomFun (AbstractIntegrand)\nUniform (AbstractTrueMeasure)\n    lower_bound     0\n    upper_bound     1\nDigitalNetB2 (AbstractLDDiscreteDistribution)\n    d               2^(1)\n    replications    1\n    randomize       LMS_DS\n    gen_mats_source joe_kuo.6.21201.txt\n    order           NATURAL\n    t               63\n    alpha           1\n    n_limit         2^(32)\n    entropy         7\n</pre> In\u00a0[14]: Copied! <pre>from matplotlib import cm\nfig,ax = pyplot.subplots(nrows=1,ncols=2,figsize=(10,4))\nax[0].scatter(x,y,color='k',label='Query Points')\nax[0].plot(xplt,yplt,color='k',linestyle='--',label='True function',linewidth=1)\nax[0].plot(xplt,yhatplt,color='k',label='GP Mean',linewidth=1)\nax[0].fill_between(xplt,yhatplt-1.96*stdhatplt,yhatplt+1.96*stdhatplt,color='k',alpha=.25,label='95% CI')\nax[0].scatter(xnext,fnext,color='k',marker='*',s=200,zorder=10)\nax[0].set_xlim([0,1])\nax[0].set_xticks([0,1])\nax[0].set_xlabel(r'$x$')\nax[0].set_ylabel(r'$y$')\nfig.legend(labels=['data','true function','posterior mean',r'95\\% CI','next points by QEI'],loc='lower center',bbox_to_anchor=(.5,-.05),ncol=5)\ncontour = ax[1].contourf(x0mesh,x1mesh,qei_vals,cmap=cm.Greys_r)\nax[1].scatter([xnext[0]],[xnext[1]],color='k',marker='*',s=200)\nfig.colorbar(contour,ax=None,shrink=1,aspect=5)\nax[1].scatter(x0mesh.flatten(),x1mesh.flatten(),color='w',s=1)\nax[1].set_xlim([0,1])\nax[1].set_xticks([0,1])\nax[1].set_ylim([0,1])\nax[1].set_yticks([0,1])\nax[1].set_xlabel(r'$x_1$')\nax[1].set_ylabel(r'$x_2$')\nif root: fig.savefig(root+'gp.pdf',transparent=True)\n</pre> from matplotlib import cm fig,ax = pyplot.subplots(nrows=1,ncols=2,figsize=(10,4)) ax[0].scatter(x,y,color='k',label='Query Points') ax[0].plot(xplt,yplt,color='k',linestyle='--',label='True function',linewidth=1) ax[0].plot(xplt,yhatplt,color='k',label='GP Mean',linewidth=1) ax[0].fill_between(xplt,yhatplt-1.96*stdhatplt,yhatplt+1.96*stdhatplt,color='k',alpha=.25,label='95% CI') ax[0].scatter(xnext,fnext,color='k',marker='*',s=200,zorder=10) ax[0].set_xlim([0,1]) ax[0].set_xticks([0,1]) ax[0].set_xlabel(r'$x$') ax[0].set_ylabel(r'$y$') fig.legend(labels=['data','true function','posterior mean',r'95\\% CI','next points by QEI'],loc='lower center',bbox_to_anchor=(.5,-.05),ncol=5) contour = ax[1].contourf(x0mesh,x1mesh,qei_vals,cmap=cm.Greys_r) ax[1].scatter([xnext[0]],[xnext[1]],color='k',marker='*',s=200) fig.colorbar(contour,ax=None,shrink=1,aspect=5) ax[1].scatter(x0mesh.flatten(),x1mesh.flatten(),color='w',s=1) ax[1].set_xlim([0,1]) ax[1].set_xticks([0,1]) ax[1].set_ylim([0,1]) ax[1].set_yticks([0,1]) ax[1].set_xlabel(r'$x_1$') ax[1].set_ylabel(r'$x_2$') if root: fig.savefig(root+'gp.pdf',transparent=True) In\u00a0[15]: Copied! <pre>import pandas as pd\nfrom sklearn.model_selection import train_test_split\ndf = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/haberman/haberman.data',header=None)\ndf.columns = ['Age','1900 Year','Axillary Nodes','Survival Status']\ndf.loc[df['Survival Status']==2,'Survival Status'] = 0\nx,y = df[['Age','1900 Year','Axillary Nodes']],df['Survival Status']\nxt,xv,yt,yv = train_test_split(x,y,test_size=.33,random_state=7)\n</pre> import pandas as pd from sklearn.model_selection import train_test_split df = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/haberman/haberman.data',header=None) df.columns = ['Age','1900 Year','Axillary Nodes','Survival Status'] df.loc[df['Survival Status']==2,'Survival Status'] = 0 x,y = df[['Age','1900 Year','Axillary Nodes']],df['Survival Status'] xt,xv,yt,yv = train_test_split(x,y,test_size=.33,random_state=7) In\u00a0[16]: Copied! <pre>print(df.head(),'\\n')\nprint(df[['Age','1900 Year','Axillary Nodes']].describe(),'\\n')\nprint(df['Survival Status'].astype(str).describe())\nprint('\\ntrain samples: %d test samples: %d\\n'%(len(xt),len(xv)))\nprint('train positives %d   train negatives: %d'%(np.sum(yt==1),np.sum(yt==0)))\nprint(' test positives %d    test negatives: %d'%(np.sum(yv==1),np.sum(yv==0)))\nxt.head()\n</pre> print(df.head(),'\\n') print(df[['Age','1900 Year','Axillary Nodes']].describe(),'\\n') print(df['Survival Status'].astype(str).describe()) print('\\ntrain samples: %d test samples: %d\\n'%(len(xt),len(xv))) print('train positives %d   train negatives: %d'%(np.sum(yt==1),np.sum(yt==0))) print(' test positives %d    test negatives: %d'%(np.sum(yv==1),np.sum(yv==0))) xt.head() <pre>   Age  1900 Year  Axillary Nodes  Survival Status\n0   30         64               1                1\n1   30         62               3                1\n2   30         65               0                1\n3   31         59               2                1\n4   31         65               4                1 \n\n              Age   1900 Year  Axillary Nodes\ncount  306.000000  306.000000      306.000000\nmean    52.457516   62.852941        4.026144\nstd     10.803452    3.249405        7.189654\nmin     30.000000   58.000000        0.000000\n25%     44.000000   60.000000        0.000000\n50%     52.000000   63.000000        1.000000\n75%     60.750000   65.750000        4.000000\nmax     83.000000   69.000000       52.000000 \n\ncount     306\nunique      2\ntop         1\nfreq      225\nName: Survival Status, dtype: object\n\ntrain samples: 205 test samples: 101\n\ntrain positives 151   train negatives: 54\n test positives 74    test negatives: 27\n</pre> Out[16]: Age 1900 Year Axillary Nodes 46 41 58 0 199 57 64 1 115 49 64 10 128 50 61 0 249 63 63 0 In\u00a0[17]: Copied! <pre>blr = qp.BayesianLRCoeffs(\n    sampler = qp.DigitalNetB2(4,seed=1),\n    feature_array = xt, # np.ndarray of shape (n,d-1)\n    response_vector = yt, # np.ndarray of shape (n,)\n    prior_mean = 0, # normal prior mean = (0,0,...,0)\n    prior_covariance = 5)\nqmc_sc = qp.CubQMCNetG(blr,\n    abs_tol = .1,\n    rel_tol = .5,\n    error_fun = \"BOTH\",\n    n_limit=2**18)\nblr_coefs,blr_data = qmc_sc.integrate()\nprint(blr_data)\n\n# LDTransformData (AccumulateData Object)\n#     solution        [-0.004  0.13  -0.157  0.008]\n#     comb_bound_low  [-0.006  0.092 -0.205  0.007]\n#     comb_bound_high [-0.003  0.172 -0.109  0.012]\n#     comb_flags      [ True  True  True  True]\n#     n_total         2^(18)\n#     n               [[  1024.   1024. 262144.   2048.]\n#                     [  1024.   1024. 262144.   2048.]]\n#     time_integrate  2.229\n</pre> blr = qp.BayesianLRCoeffs(     sampler = qp.DigitalNetB2(4,seed=1),     feature_array = xt, # np.ndarray of shape (n,d-1)     response_vector = yt, # np.ndarray of shape (n,)     prior_mean = 0, # normal prior mean = (0,0,...,0)     prior_covariance = 5) qmc_sc = qp.CubQMCNetG(blr,     abs_tol = .1,     rel_tol = .5,     error_fun = \"BOTH\",     n_limit=2**18) blr_coefs,blr_data = qmc_sc.integrate() print(blr_data)  # LDTransformData (AccumulateData Object) #     solution        [-0.004  0.13  -0.157  0.008] #     comb_bound_low  [-0.006  0.092 -0.205  0.007] #     comb_bound_high [-0.003  0.172 -0.109  0.012] #     comb_flags      [ True  True  True  True] #     n_total         2^(18) #     n               [[  1024.   1024. 262144.   2048.] #                     [  1024.   1024. 262144.   2048.]] #     time_integrate  2.229 <pre>Data (Data)\n    solution        [ 0.262 -0.043 -0.226 -1.203]\n    comb_bound_low  [ 0.185 -0.067 -0.306 -1.656]\n    comb_bound_high [ 0.347 -0.035 -0.163 -0.75 ]\n    comb_bound_diff [0.162 0.031 0.143 0.906]\n    comb_flags      [ True  True  True False]\n    n_total         2^(18)\n    n               [[  1024   1024   1024 262144]\n                     [  1024   1024   1024 262144]]\n    time_integrate  3.175\nCubQMCNetG (AbstractStoppingCriterion)\n    abs_tol         0.100\n    rel_tol         2^(-1)\n    n_init          2^(10)\n    n_limit         2^(18)\nBayesianLRCoeffs (AbstractIntegrand)\nGaussian (AbstractTrueMeasure)\n    mean            0\n    covariance      5\n    decomp_type     PCA\nDigitalNetB2 (AbstractLDDiscreteDistribution)\n    d               2^(2)\n    replications    1\n    randomize       LMS_DS\n    gen_mats_source joe_kuo.6.21201.txt\n    order           NATURAL\n    t               63\n    alpha           1\n    n_limit         2^(32)\n    entropy         1\n</pre> <pre>/Users/alegresor/Desktop/QMCSoftware/qmcpy/stopping_criterion/abstract_cub_qmc_ld_g.py:215: MaxSamplesWarning: \n                Already generated 262144 samples.\n                Trying to generate 262144 new samples would exceeds n_limit = 262144.\n                No more samples will be generated.\n                Note that error tolerances may not be satisfied. \n  warnings.warn(warning_s, MaxSamplesWarning)\n</pre> In\u00a0[18]: Copied! <pre>from sklearn.linear_model import LogisticRegression\ndef metrics(y,yhat):\n    y,yhat = np.array(y),np.array(yhat)\n    tp = np.sum((y==1)*(yhat==1))\n    tn = np.sum((y==0)*(yhat==0))\n    fp = np.sum((y==0)*(yhat==1))\n    fn = np.sum((y==1)*(yhat==0))\n    accuracy = (tp+tn)/(len(y))\n    precision = tp/(tp+fp)\n    recall = tp/(tp+fn)\n    return [accuracy,precision,recall]\n\nresults = pd.DataFrame({name:[] for name in ['method','Age','1900 Year','Axillary Nodes','Intercept','Accuracy','Precision','Recall']})\nfor i,l1_ratio in enumerate([0,.5,1]):\n    lr = LogisticRegression(random_state=7,penalty=\"elasticnet\",solver='saga',l1_ratio=l1_ratio).fit(xt,yt)\n    results.loc[i] = [r'Elastic-Net \\lambda=%.1f'%l1_ratio]+lr.coef_.squeeze().tolist()+[lr.intercept_.item()]+metrics(yv,lr.predict(xv))\n\nblr_predict = lambda x: 1/(1+np.exp(-np.array(x)@blr_coefs[:-1]-blr_coefs[-1]))&gt;=.5\nblr_train_accuracy = np.mean(blr_predict(xt)==yt)\nblr_test_accuracy = np.mean(blr_predict(xv)==yv)\nresults.loc[len(results)] = ['Bayesian']+blr_coefs.squeeze().tolist()+metrics(yv,blr_predict(xv))\n\nimport warnings\nwarnings.simplefilter('ignore',FutureWarning)\nresults.set_index('method',inplace=True)\nprint(results.head())\n#root: results.to_latex(root+'lr_table.tex',formatters={'%s'%tt:lambda v:'%.1f'%(100*v) for tt in ['accuracy','precision','recall']},float_format=\"%.2e\")\n</pre> from sklearn.linear_model import LogisticRegression def metrics(y,yhat):     y,yhat = np.array(y),np.array(yhat)     tp = np.sum((y==1)*(yhat==1))     tn = np.sum((y==0)*(yhat==0))     fp = np.sum((y==0)*(yhat==1))     fn = np.sum((y==1)*(yhat==0))     accuracy = (tp+tn)/(len(y))     precision = tp/(tp+fp)     recall = tp/(tp+fn)     return [accuracy,precision,recall]  results = pd.DataFrame({name:[] for name in ['method','Age','1900 Year','Axillary Nodes','Intercept','Accuracy','Precision','Recall']}) for i,l1_ratio in enumerate([0,.5,1]):     lr = LogisticRegression(random_state=7,penalty=\"elasticnet\",solver='saga',l1_ratio=l1_ratio).fit(xt,yt)     results.loc[i] = [r'Elastic-Net \\lambda=%.1f'%l1_ratio]+lr.coef_.squeeze().tolist()+[lr.intercept_.item()]+metrics(yv,lr.predict(xv))  blr_predict = lambda x: 1/(1+np.exp(-np.array(x)@blr_coefs[:-1]-blr_coefs[-1]))&gt;=.5 blr_train_accuracy = np.mean(blr_predict(xt)==yt) blr_test_accuracy = np.mean(blr_predict(xv)==yv) results.loc[len(results)] = ['Bayesian']+blr_coefs.squeeze().tolist()+metrics(yv,blr_predict(xv))  import warnings warnings.simplefilter('ignore',FutureWarning) results.set_index('method',inplace=True) print(results.head()) #root: results.to_latex(root+'lr_table.tex',formatters={'%s'%tt:lambda v:'%.1f'%(100*v) for tt in ['accuracy','precision','recall']},float_format=\"%.2e\") <pre>                              Age  1900 Year  Axillary Nodes  Intercept  \\\nmethod                                                                    \nElastic-Net \\lambda=0.0 -0.012279   0.034401       -0.115153   0.001990   \nElastic-Net \\lambda=0.5 -0.012041   0.034170       -0.114770   0.002025   \nElastic-Net \\lambda=1.0 -0.011803   0.033940       -0.114387   0.002061   \nBayesian                 0.262086  -0.043253       -0.225631  -1.202777   \n\n                         Accuracy  Precision    Recall  \nmethod                                                  \nElastic-Net \\lambda=0.0  0.742574   0.766667  0.932432  \nElastic-Net \\lambda=0.5  0.742574   0.766667  0.932432  \nElastic-Net \\lambda=1.0  0.742574   0.766667  0.932432  \nBayesian                 0.732673   0.737374  0.986486  \n</pre> In\u00a0[19]: Copied! <pre>a,b = 7,0.1\ndnb2 = qp.DigitalNetB2(3,seed=7)\nishigami = qp.Ishigami(dnb2,a,b)\nidxs = np.array([\n    [True,False,False],\n    [False,True,False],\n    [False,False,True],\n    [True,True,False],\n    [True,False,True],\n    [False,True,True]],dtype=bool)\nishigami_si = qp.SensitivityIndices(ishigami,idxs)\nqmc_algo = qp.CubBayesNetG(ishigami_si,abs_tol=.05)\nsolution,data = qmc_algo.integrate()\nprint(data)\nsi_closed = solution[0].squeeze()\nsi_total = solution[1].squeeze()\nci_comb_low_closed = data.comb_bound_low[0].squeeze()\nci_comb_high_closed = data.comb_bound_high[0].squeeze()\nci_comb_low_total = data.comb_bound_low[1].squeeze()\nci_comb_high_total = data.comb_bound_high[1].squeeze()\nprint(\"\\nApprox took %.1f sec and n = 2^(%d)\"%\n    (data.time_integrate,np.log2(data.n_total)))\nprint('\\t si_closed:',si_closed)\nprint('\\t si_total:',si_total)\nprint('\\t ci_comb_low_closed:',ci_comb_low_closed)\nprint('\\t ci_comb_high_closed:',ci_comb_high_closed)\nprint('\\t ci_comb_low_total:',ci_comb_low_total)\nprint('\\t ci_comb_high_total:',ci_comb_high_total)\n\ntrue_indices = qp.Ishigami._exact_sensitivity_indices(idxs,a,b)\nsi_closed_true = true_indices[0]\nsi_total_true = true_indices[1]\n</pre> a,b = 7,0.1 dnb2 = qp.DigitalNetB2(3,seed=7) ishigami = qp.Ishigami(dnb2,a,b) idxs = np.array([     [True,False,False],     [False,True,False],     [False,False,True],     [True,True,False],     [True,False,True],     [False,True,True]],dtype=bool) ishigami_si = qp.SensitivityIndices(ishigami,idxs) qmc_algo = qp.CubBayesNetG(ishigami_si,abs_tol=.05) solution,data = qmc_algo.integrate() print(data) si_closed = solution[0].squeeze() si_total = solution[1].squeeze() ci_comb_low_closed = data.comb_bound_low[0].squeeze() ci_comb_high_closed = data.comb_bound_high[0].squeeze() ci_comb_low_total = data.comb_bound_low[1].squeeze() ci_comb_high_total = data.comb_bound_high[1].squeeze() print(\"\\nApprox took %.1f sec and n = 2^(%d)\"%     (data.time_integrate,np.log2(data.n_total))) print('\\t si_closed:',si_closed) print('\\t si_total:',si_total) print('\\t ci_comb_low_closed:',ci_comb_low_closed) print('\\t ci_comb_high_closed:',ci_comb_high_closed) print('\\t ci_comb_low_total:',ci_comb_low_total) print('\\t ci_comb_high_total:',ci_comb_high_total)  true_indices = qp.Ishigami._exact_sensitivity_indices(idxs,a,b) si_closed_true = true_indices[0] si_total_true = true_indices[1] <pre>Data (Data)\n    solution        [[0.272 0.451 0.03  0.756 0.561 0.481]\n                     [0.584 0.406 0.224 0.981 0.505 0.696]]\n    comb_bound_low  [[0.227 0.404 0.011 0.726 0.518 0.447]\n                     [0.544 0.369 0.204 0.961 0.461 0.658]]\n    comb_bound_high [[0.317 0.499 0.049 0.786 0.604 0.515]\n                     [0.624 0.442 0.244 1.    0.549 0.733]]\n    comb_bound_diff [[0.09  0.095 0.038 0.061 0.086 0.068]\n                     [0.08  0.074 0.04  0.039 0.088 0.076]]\n    comb_flags      [[ True  True  True  True  True  True]\n                     [ True  True  True  True  True  True]]\n    n_total         2^(10)\n    n               [[[ 256  256  256 1024  512  512]\n                      [ 256  256  256 1024  512  512]\n                      [ 256  256  256 1024  512  512]]\n                    \n                     [[ 512  256  256  512  256  512]\n                      [ 512  256  256  512  256  512]\n                      [ 512  256  256  512  256  512]]]\n    time_integrate  0.292\nCubQMCBayesNetG (AbstractStoppingCriterion)\n    abs_tol         0.050\n    rel_tol         0\n    n_init          2^(8)\n    n_limit         2^(22)\n    order           1\nSensitivityIndices (AbstractIntegrand)\n    indices         [[ True False False]\n                     [False  True False]\n                     [False False  True]\n                     [ True  True False]\n                     [ True False  True]\n                     [False  True  True]]\nUniform (AbstractTrueMeasure)\n    lower_bound     -3.142\n    upper_bound     3.142\nDigitalNetB2 (AbstractLDDiscreteDistribution)\n    d               3\n    replications    1\n    randomize       LMS_DS\n    gen_mats_source joe_kuo.6.21201.txt\n    order           NATURAL\n    t               63\n    alpha           1\n    n_limit         2^(32)\n    entropy         7\n\nApprox took 0.3 sec and n = 2^(10)\n\t si_closed: [0.27216675 0.45135598 0.03039673 0.7559953  0.56084862 0.48106082]\n\t si_total: [0.58380168 0.40555062 0.22413097 0.98050963 0.50473157 0.69561275]\n\t ci_comb_low_closed: [0.22703312 0.40380263 0.01145781 0.72574291 0.51804596 0.44706404]\n\t ci_comb_high_closed: [0.31730037 0.49890933 0.04933565 0.78624769 0.60365128 0.51505759]\n\t ci_comb_low_total: [0.54382805 0.36861946 0.20422796 0.96101926 0.46083634 0.65783325]\n\t ci_comb_high_total: [0.62377531 0.44248178 0.24403398 1.         0.5486268  0.73339225]\n</pre> In\u00a0[21]: Copied! <pre>fig,ax = pyplot.subplots(figsize=(8,4))\nax.grid(False)\nfor spine in ['top','left','right','bottom']: ax.spines[spine].set_visible(False)\nwidth = .75\nax.errorbar(fmt='none',color='k',\n    x = 1-si_total_true,\n    y = np.arange(len(si_closed)),\n    xerr = 0,\n    yerr = width/2,\n    alpha = 1)\nbar_closed = ax.barh(np.arange(len(si_closed)),np.flip(si_closed),width,label='Closed SI',color='w',edgecolor='k',alpha=.75,linestyle='--')\nax.errorbar(fmt='none',color='k',\n    x = si_closed,\n    y = np.flip(np.arange(len(si_closed)))+width/4,\n    xerr = np.vstack((si_closed-ci_comb_low_closed,ci_comb_high_closed-si_closed)),\n    yerr = 0,\n    #elinewidth = 5,\n    alpha = .75)\nbar_total = ax.barh(np.arange(len(si_closed)),si_total,width,label='Total SI',color='w',alpha=.25,edgecolor='k',left=1-si_total,zorder=10,linestyle='-.')\nax.errorbar(fmt='none',color='k',\n    x = 1-si_total,\n    y = np.arange(len(si_closed))-width/4,\n    xerr = np.vstack((si_total-ci_comb_low_total,ci_comb_high_total-si_total)),\n    yerr = 0,\n    #elinewidth = 5,\n    alpha = .25)\nclosed_labels = [r'$\\underline{s}_{\\{%s\\}} = %.2f$'%(','.join([str(i+1) for i in idx]),c) for idx,c in zip(idxs[::-1],np.flip(si_closed))]\nclosed_labels[3] = ''\ntotal_labels = [r'$\\overline{s}_{\\{%s\\}} = %.2f$'%(','.join([str(i+1) for i in idx]),t) for idx,t in zip(idxs,si_total)]\nax.bar_label(bar_closed,label_type='center',labels=closed_labels)\nax.bar_label(bar_total,label_type='center',labels=total_labels)\nax.set_xlim([-.001,1.001])\nax.axvline(x=0,ymin=0,ymax=len(si_closed),color='k',alpha=.25)\nax.axvline(x=1,ymin=0,ymax=len(si_closed),color='k',alpha=.25)\nax.set_yticklabels([])\nif root: fig.savefig(root+'ishigami.pdf',transparent=True)\n</pre> fig,ax = pyplot.subplots(figsize=(8,4)) ax.grid(False) for spine in ['top','left','right','bottom']: ax.spines[spine].set_visible(False) width = .75 ax.errorbar(fmt='none',color='k',     x = 1-si_total_true,     y = np.arange(len(si_closed)),     xerr = 0,     yerr = width/2,     alpha = 1) bar_closed = ax.barh(np.arange(len(si_closed)),np.flip(si_closed),width,label='Closed SI',color='w',edgecolor='k',alpha=.75,linestyle='--') ax.errorbar(fmt='none',color='k',     x = si_closed,     y = np.flip(np.arange(len(si_closed)))+width/4,     xerr = np.vstack((si_closed-ci_comb_low_closed,ci_comb_high_closed-si_closed)),     yerr = 0,     #elinewidth = 5,     alpha = .75) bar_total = ax.barh(np.arange(len(si_closed)),si_total,width,label='Total SI',color='w',alpha=.25,edgecolor='k',left=1-si_total,zorder=10,linestyle='-.') ax.errorbar(fmt='none',color='k',     x = 1-si_total,     y = np.arange(len(si_closed))-width/4,     xerr = np.vstack((si_total-ci_comb_low_total,ci_comb_high_total-si_total)),     yerr = 0,     #elinewidth = 5,     alpha = .25) closed_labels = [r'$\\underline{s}_{\\{%s\\}} = %.2f$'%(','.join([str(i+1) for i in idx]),c) for idx,c in zip(idxs[::-1],np.flip(si_closed))] closed_labels[3] = '' total_labels = [r'$\\overline{s}_{\\{%s\\}} = %.2f$'%(','.join([str(i+1) for i in idx]),t) for idx,t in zip(idxs,si_total)] ax.bar_label(bar_closed,label_type='center',labels=closed_labels) ax.bar_label(bar_total,label_type='center',labels=total_labels) ax.set_xlim([-.001,1.001]) ax.axvline(x=0,ymin=0,ymax=len(si_closed),color='k',alpha=.25) ax.axvline(x=1,ymin=0,ymax=len(si_closed),color='k',alpha=.25) ax.set_yticklabels([]) if root: fig.savefig(root+'ishigami.pdf',transparent=True) In\u00a0[22]: Copied! <pre>fig,ax = pyplot.subplots(nrows=1,ncols=3,figsize=(8,3))\nx_1d = np.linspace(0,1,num=128)\nx_1d_mat = np.tile(x_1d,(3,1)).T\ny_1d = qp.Ishigami._exact_fu_functions(x_1d_mat,indices=[[0],[1],[2]],a=a,b=b)\nfor i in range(2):\n    ax[i].plot(x_1d,y_1d[:,i],color='k')\n    ax[i].set_xlim([0,1])\n    ax[i].set_xticks([0,1])\n    ax[i].set_xlabel(r'$x_{%d}$'%(i+1))\n    ax[i].set_title(r'$f_{\\{%d\\}} \\in [%.1f,%.1f]$'%(i+1,y_1d[:,i].min(),y_1d[:,i].max()))\nx_mesh,y_mesh = np.meshgrid(x_1d,x_1d)\nxquery = np.zeros((x_mesh.size,3))\nfor i,idx in enumerate([[1,2]]): # [[0,1],[0,2],[1,2]]\n    xquery[:,idx[0]] = x_mesh.flatten()\n    xquery[:,idx[1]] = y_mesh.flatten()\n    zquery = qp.Ishigami._exact_fu_functions(xquery,indices=[idx],a=a,b=b)\n    z_mesh = zquery.reshape(x_mesh.shape)\n    ax[2+i].contourf(x_mesh,y_mesh,z_mesh,cmap=cm.Greys_r)\n    ax[2+i].set_xlabel(r'$x_{%d}$'%(idx[0]+1))\n    ax[2+i].set_ylabel(r'$x_{%d}$'%(idx[1]+1))\n    ax[2+i].set_title(r'$f_{\\{%d,%d\\}} \\in [%.1f,%.1f]$'%(tuple([i+1 for i in idx])+(z_mesh.min(),z_mesh.max())))\n    ax[2+i].set_xlim([0,1])\n    ax[2+i].set_ylim([0,1])\n    ax[2+i].set_xticks([0,1])\n    ax[2+i].set_yticks([0,1])\nif root: fig.savefig(root+'ishigami_fu.pdf')\n</pre> fig,ax = pyplot.subplots(nrows=1,ncols=3,figsize=(8,3)) x_1d = np.linspace(0,1,num=128) x_1d_mat = np.tile(x_1d,(3,1)).T y_1d = qp.Ishigami._exact_fu_functions(x_1d_mat,indices=[[0],[1],[2]],a=a,b=b) for i in range(2):     ax[i].plot(x_1d,y_1d[:,i],color='k')     ax[i].set_xlim([0,1])     ax[i].set_xticks([0,1])     ax[i].set_xlabel(r'$x_{%d}$'%(i+1))     ax[i].set_title(r'$f_{\\{%d\\}} \\in [%.1f,%.1f]$'%(i+1,y_1d[:,i].min(),y_1d[:,i].max())) x_mesh,y_mesh = np.meshgrid(x_1d,x_1d) xquery = np.zeros((x_mesh.size,3)) for i,idx in enumerate([[1,2]]): # [[0,1],[0,2],[1,2]]     xquery[:,idx[0]] = x_mesh.flatten()     xquery[:,idx[1]] = y_mesh.flatten()     zquery = qp.Ishigami._exact_fu_functions(xquery,indices=[idx],a=a,b=b)     z_mesh = zquery.reshape(x_mesh.shape)     ax[2+i].contourf(x_mesh,y_mesh,z_mesh,cmap=cm.Greys_r)     ax[2+i].set_xlabel(r'$x_{%d}$'%(idx[0]+1))     ax[2+i].set_ylabel(r'$x_{%d}$'%(idx[1]+1))     ax[2+i].set_title(r'$f_{\\{%d,%d\\}} \\in [%.1f,%.1f]$'%(tuple([i+1 for i in idx])+(z_mesh.min(),z_mesh.max())))     ax[2+i].set_xlim([0,1])     ax[2+i].set_ylim([0,1])     ax[2+i].set_xticks([0,1])     ax[2+i].set_yticks([0,1]) if root: fig.savefig(root+'ishigami_fu.pdf') In\u00a0[23]: Copied! <pre>from sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neural_network import MLPClassifier\ndata = load_iris()\nfeature_names = data[\"feature_names\"]\nfeature_names = [fn.replace('sepal ','S')\\\n    .replace('length ','L')\\\n    .replace('petal ','P')\\\n    .replace('width ','W')\\\n    .replace('(cm)','') for fn in feature_names]\ntarget_names = data[\"target_names\"]\nxt,xv,yt,yv = train_test_split(data[\"data\"],data[\"target\"],\n    test_size = 1/3,\n    random_state = 7)\n</pre> from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split from sklearn.neural_network import MLPClassifier data = load_iris() feature_names = data[\"feature_names\"] feature_names = [fn.replace('sepal ','S')\\     .replace('length ','L')\\     .replace('petal ','P')\\     .replace('width ','W')\\     .replace('(cm)','') for fn in feature_names] target_names = data[\"target_names\"] xt,xv,yt,yv = train_test_split(data[\"data\"],data[\"target\"],     test_size = 1/3,     random_state = 7) In\u00a0[25]: Copied! <pre>mlpc = MLPClassifier(random_state=7,max_iter=1024).fit(xt,yt)\nyhat = mlpc.predict(xv)\nprint(\"accuracy: %.1f%%\"%(100*(yv==yhat).mean()))\n# accuracy: 98.0%\nsampler = qp.DigitalNetB2(4,seed=7)\ntrue_measure =  qp.Uniform(sampler,\n    lower_bound = xt.min(0),\n    upper_bound = xt.max(0))\nfun = qp.CustomFun(\n    true_measure = true_measure,\n    g = lambda x: mlpc.predict_proba(x).T,\n    dimension_indv = 3)\nsi_fun = qp.SensitivityIndices(fun,indices=\"all\")\nqmc_algo = qp.CubBayesNetG(si_fun,abs_tol=.005)\nnn_sis,nn_sis_data = qmc_algo.integrate()\n</pre> mlpc = MLPClassifier(random_state=7,max_iter=1024).fit(xt,yt) yhat = mlpc.predict(xv) print(\"accuracy: %.1f%%\"%(100*(yv==yhat).mean())) # accuracy: 98.0% sampler = qp.DigitalNetB2(4,seed=7) true_measure =  qp.Uniform(sampler,     lower_bound = xt.min(0),     upper_bound = xt.max(0)) fun = qp.CustomFun(     true_measure = true_measure,     g = lambda x: mlpc.predict_proba(x).T,     dimension_indv = 3) si_fun = qp.SensitivityIndices(fun,indices=\"all\") qmc_algo = qp.CubBayesNetG(si_fun,abs_tol=.005) nn_sis,nn_sis_data = qmc_algo.integrate() <pre>accuracy: 98.0%\n</pre> In\u00a0[26]: Copied! <pre>#print(nn_sis_data.flags_indv.shape)\n#print(nn_sis_data.flags_comb.shape)\nprint('samples: 2^(%d)'%np.log2(nn_sis_data.n_total))\nprint('time: %.1e'%nn_sis_data.time_integrate)\nprint('indices:\\n%s'%nn_sis_data.integrand.indices)\n\nimport pandas as pd\n\ndf_closed = pd.DataFrame(nn_sis[0],columns=target_names,index=[str(np.where(idx)[0]) for idx in nn_sis_data.integrand.indices])\nprint('\\nClosed Indices')\nprint(df_closed)\ndf_total = pd.DataFrame(nn_sis[1],columns=target_names,index=[str(np.where(idx)[0]) for idx in nn_sis_data.integrand.indices])\nprint('\\nTotal Indices')\ndf_closed_singletons = df_closed.loc[['[%d]'%i for i in range(4)]].T\ndf_closed_singletons['sum singletons'] = df_closed_singletons[['[%d]'%i for i in range(4)]].sum(1)\ndf_closed_singletons.columns = data['feature_names']+['sum']\ndf_closed_singletons = df_closed_singletons*100\ndf_closed_singletons\n</pre> #print(nn_sis_data.flags_indv.shape) #print(nn_sis_data.flags_comb.shape) print('samples: 2^(%d)'%np.log2(nn_sis_data.n_total)) print('time: %.1e'%nn_sis_data.time_integrate) print('indices:\\n%s'%nn_sis_data.integrand.indices)  import pandas as pd  df_closed = pd.DataFrame(nn_sis[0],columns=target_names,index=[str(np.where(idx)[0]) for idx in nn_sis_data.integrand.indices]) print('\\nClosed Indices') print(df_closed) df_total = pd.DataFrame(nn_sis[1],columns=target_names,index=[str(np.where(idx)[0]) for idx in nn_sis_data.integrand.indices]) print('\\nTotal Indices') df_closed_singletons = df_closed.loc[['[%d]'%i for i in range(4)]].T df_closed_singletons['sum singletons'] = df_closed_singletons[['[%d]'%i for i in range(4)]].sum(1) df_closed_singletons.columns = data['feature_names']+['sum'] df_closed_singletons = df_closed_singletons*100 df_closed_singletons <pre>samples: 2^(14)\ntime: 4.5e+01\nindices:\n[[ True False False False]\n [False  True False False]\n [False False  True False]\n [False False False  True]\n [ True  True False False]\n [ True False  True False]\n [ True False False  True]\n [False  True  True False]\n [False  True False  True]\n [False False  True  True]\n [ True  True  True False]\n [ True  True False  True]\n [ True False  True  True]\n [False  True  True  True]]\n\nClosed Indices\n           setosa  versicolor  virginica\n[0]      0.007424    0.066607   0.099046\n[1]      0.054465    0.002571   0.002252\n[2]      0.712709    0.326649   0.498656\n[3]      0.047694    0.017899   0.116317\n[0 1]    0.054934    0.084423   0.097292\n[0 2]    0.714202    0.460625   0.641442\n[0 3]    0.047812    0.088780   0.205556\n[1 2]    0.840862    0.433312   0.514035\n[1 3]    0.102551    0.032345   0.128446\n[2 3]    0.822403    0.583336   0.705183\n[0 1 2]  0.843053    0.570898   0.660995\n[0 1 3]  0.108803    0.103398   0.216627\n[0 2 3]  0.824346    0.814563   0.946842\n[1 2 3]  0.995488    0.739790   0.726773\n\nTotal Indices\n</pre> Out[26]: sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) sum setosa 0.742441 5.446506 71.270924 4.769447 82.229319 versicolor 6.660730 0.257095 32.664932 1.789914 41.372671 virginica 9.904627 0.225186 49.865641 11.631687 71.627140 In\u00a0[27]: Copied! <pre>nindices = len(nn_sis_data.integrand.indices)\nfig,ax = pyplot.subplots(figsize=(9,5))\nticks = np.arange(nindices)\nwidth = .25\nfor i,(alpha,species) in enumerate(zip([.25,.5,.75],data['target_names'])):\n    cvals = df_closed[species].to_numpy()\n    tvals = df_total[species].to_numpy()\n    ticks_i = ticks+i*width\n    ax.bar(ticks_i,cvals,width=width,align='edge',color='k',alpha=alpha,label=species)\n    #ax.bar(ticks_i,np.flip(tvals),width=width,align='edge',bottom=1-np.flip(tvals),color=color,alpha=.1)\nax.set_xlim([0,13+3*width])\nax.set_xticks(ticks+1.5*width)\n\n# closed_labels = [r'$\\underline{s}_{\\{%s\\}}$'%(','.join([r'\\text{%s}'%feature_names[i] for i in idx])) for idx in nn_sis_data.integrand.indices]\nclosed_labels = ['\\n'.join([feature_names[i] for i in np.where(idx)[0]]) for idx in nn_sis_data.integrand.indices]\nax.set_xticklabels(closed_labels,rotation=0)\nax.set_ylim([0,1]); ax.set_yticks([0,1])\nax.grid(False)\nfor spine in ['top','right','bottom']: ax.spines[spine].set_visible(False)\nax.legend(frameon=False,loc='lower center',bbox_to_anchor=(.5,-.2),ncol=3);\nfig.tight_layout()\nif root: fig.savefig(root+'nn_si.pdf')\n</pre> nindices = len(nn_sis_data.integrand.indices) fig,ax = pyplot.subplots(figsize=(9,5)) ticks = np.arange(nindices) width = .25 for i,(alpha,species) in enumerate(zip([.25,.5,.75],data['target_names'])):     cvals = df_closed[species].to_numpy()     tvals = df_total[species].to_numpy()     ticks_i = ticks+i*width     ax.bar(ticks_i,cvals,width=width,align='edge',color='k',alpha=alpha,label=species)     #ax.bar(ticks_i,np.flip(tvals),width=width,align='edge',bottom=1-np.flip(tvals),color=color,alpha=.1) ax.set_xlim([0,13+3*width]) ax.set_xticks(ticks+1.5*width)  # closed_labels = [r'$\\underline{s}_{\\{%s\\}}$'%(','.join([r'\\text{%s}'%feature_names[i] for i in idx])) for idx in nn_sis_data.integrand.indices] closed_labels = ['\\n'.join([feature_names[i] for i in np.where(idx)[0]]) for idx in nn_sis_data.integrand.indices] ax.set_xticklabels(closed_labels,rotation=0) ax.set_ylim([0,1]); ax.set_yticks([0,1]) ax.grid(False) for spine in ['top','right','bottom']: ax.spines[spine].set_visible(False) ax.legend(frameon=False,loc='lower center',bbox_to_anchor=(.5,-.2),ncol=3); fig.tight_layout() if root: fig.savefig(root+'nn_si.pdf') In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"demos/vectorized_qmc_bayes/#vectorized-qmc-bayesian","title":"Vectorized QMC (Bayesian)\u00b6","text":""},{"location":"demos/vectorized_qmc_bayes/#ld-sequence","title":"LD Sequence\u00b6","text":""},{"location":"demos/vectorized_qmc_bayes/#simple-example","title":"Simple Example\u00b6","text":""},{"location":"demos/vectorized_qmc_bayes/#bo-qei","title":"BO QEI\u00b6","text":"<p>See the QEI Demo in QMCPy or the BoTorch Acquisition documentation for details on Bayesian Optimization using q-Expected Improvement.</p>"},{"location":"demos/vectorized_qmc_bayes/#bayesian-logistic-regression","title":"Bayesian Logistic Regression\u00b6","text":""},{"location":"demos/vectorized_qmc_bayes/#sensitivity-indices","title":"Sensitivity Indices\u00b6","text":""},{"location":"demos/vectorized_qmc_bayes/#ishigami-function","title":"Ishigami Function\u00b6","text":""},{"location":"demos/vectorized_qmc_bayes/#neural-network","title":"Neural Network\u00b6","text":""},{"location":"demos/DAKOTA_Genz/dakota_genz/","title":"DAKOTA Halton Points","text":"In\u00a0[1]: Copied! <pre>from numpy import *\nfrom qmcpy import *\nimport pandas as pd\nfrom matplotlib import pyplot\nimport tempfile\nimport os\nimport subprocess\nimport numpy as np\nimport matplotlib.pyplot as plt\nplt.rcParams.update({\n\"text.usetex\": True,\n\"font.family\": \"serif\",\n\"text.latex.preamble\": r\"\\usepackage{amsmath}\\usepackage{amssymb}\\newcommand{\\bx}{\\boldsymbol{x}}\"\n})\n%matplotlib inline\n</pre> from numpy import * from qmcpy import * import pandas as pd from matplotlib import pyplot import tempfile import os import subprocess import numpy as np import matplotlib.pyplot as plt plt.rcParams.update({ \"text.usetex\": True, \"font.family\": \"serif\", \"text.latex.preamble\": r\"\\usepackage{amsmath}\\usepackage{amssymb}\\newcommand{\\bx}{\\boldsymbol{x}}\" }) %matplotlib inline In\u00a0[2]: Copied! <pre>kinds_func = ['oscillatory','corner-peak']\nkinds_coeff = [1,2,3]\nds = 2**arange(8)\nns = 2**arange(7,19)\nds\n</pre> kinds_func = ['oscillatory','corner-peak'] kinds_coeff = [1,2,3] ds = 2**arange(8) ns = 2**arange(7,19) ds Out[2]: <pre>array([  1,   2,   4,   8,  16,  32,  64, 128])</pre> In\u00a0[\u00a0]: Copied! <pre># takes about 5.5 min to run for me\nref_sols = {}\nprint('logging: ',end='',flush=True)\nx_full = DigitalNetB2(ds.max(),seed=7).gen_samples(2**22)\nfor kind_func in kinds_func:\n    for kind_coeff in kinds_coeff:\n        tag = '%s.%d'%(kind_func,kind_coeff)\n        print('%s, '%tag,end='',flush=True)\n        mu_hats = zeros(len(ds),dtype=float)\n        for j,d in enumerate(ds):\n            genz = Genz(IIDStdUniform(d),kind_func=kind_func,kind_coeff=kind_coeff)\n            y = genz.f(x_full[:,:d])\n            mu_hats[j] = y.mean()\n        ref_sols[tag] = mu_hats\nprint()\nref_sols = pd.DataFrame(ref_sols)\nref_sols['d'] = ds\nref_sols.set_index('d',inplace=True)\nref_sols \n</pre> # takes about 5.5 min to run for me ref_sols = {} print('logging: ',end='',flush=True) x_full = DigitalNetB2(ds.max(),seed=7).gen_samples(2**22) for kind_func in kinds_func:     for kind_coeff in kinds_coeff:         tag = '%s.%d'%(kind_func,kind_coeff)         print('%s, '%tag,end='',flush=True)         mu_hats = zeros(len(ds),dtype=float)         for j,d in enumerate(ds):             genz = Genz(IIDStdUniform(d),kind_func=kind_func,kind_coeff=kind_coeff)             y = genz.f(x_full[:,:d])             mu_hats[j] = y.mean()         ref_sols[tag] = mu_hats print() ref_sols = pd.DataFrame(ref_sols) ref_sols['d'] = ds ref_sols.set_index('d',inplace=True) ref_sols  Out[\u00a0]: oscillatory.1 oscillatory.2 oscillatory.3 corner-peak.1 corner-peak.2 corner-peak.3 d 1 -0.217229 -0.217229 -0.217229 8.000000e-01 0.800000 0.800000 2 -0.350528 -0.379658 -0.217290 7.133127e-01 0.712088 0.719996 4 -0.472868 -0.472147 -0.223335 5.663464e-01 0.566334 0.589674 8 -0.545580 -0.526053 -0.276837 3.573127e-01 0.360014 0.402641 16 -0.585497 -0.558159 -0.390467 1.423258e-01 0.147353 0.185776 32 -0.606470 -0.577987 -0.492894 2.259076e-02 0.025678 0.038375 64 -0.617227 -0.590764 -0.556348 5.692663e-04 0.000879 0.001606 128 -0.622677 -0.599348 -0.591170 3.615787e-07 0.000001 0.000003 In\u00a0[\u00a0]: Copied! <pre>if os.path.isfile(\"x_full_dakota.txt\"):\n    x_full_dakota = np.loadtxt(\"x_full_dakota.txt\")\nelse:\n    print(\"please download Dakota's Halton points from https://drive.google.com/uc?id=1ljmpq3w5L4OjjdinAMSLhXBeWGW6EJ3U\")\n    # with tempfile.TemporaryDirectory() as tmp:\n    #     with open(os.path.join(tmp, \"dakota.in\"), \"w\") as io:\n    #         io.write(f\"environment\\\n    #             \\ttabular_data\\n\\\n    #             method\\\n    #             \\tfsu_quasi_mc halton\\\n    #             \\t\\tsamples = {ns.max()}\\\n    #             \\toutput silent\\n\\\n    #             variables\\\n    #             \\tcontinuous_design = {ds.max()}\\\n    #             \\tlower_bounds = {' '.join(['0.0' for _ in range(ds.max())])}\\\n    #             \\tupper_bounds = {' '.join(['1.0' for _ in range(ds.max())])}\\n\\\n    #             interface\\\n    #             \\tfork\\\n    #             \\t\\tanalysis_driver = 'dummy'\\\n    #             \\tbatch\\\n    #             \\twork_directory named 'work'\\n\\\n    #             responses\\\n    #             \\tobjective_functions = 1\\\n    #             \\tno_gradients\\\n    #             \\tno_hessians\"\n    #         )\n    #     subprocess.run([\"dakota\", \"dakota.in\"], cwd=tmp, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n    #     file = os.listdir(os.path.join(tmp, \"work\"))[0]\n    #     with open(os.path.join(tmp, \"work\", file), \"r\") as io:\n    #         lines = io.readlines()\n    #         x_full_dakota = []\n    #         for n, line in enumerate(lines):\n    #             if f\"{ds.max()} variables\" in line:\n    #                 x_full_dakota.append([float(lines[n + 1 + j].split()[0]) for j in range(ds.max())])\n    #         x_full_dakota = np.vstack(x_full_dakota)\n    #     np.savetxt(\"data/x_full_dakota.txt\",x_full_dakota)\nprint(x_full_dakota.shape)\n</pre> if os.path.isfile(\"x_full_dakota.txt\"):     x_full_dakota = np.loadtxt(\"x_full_dakota.txt\") else:     print(\"please download Dakota's Halton points from https://drive.google.com/uc?id=1ljmpq3w5L4OjjdinAMSLhXBeWGW6EJ3U\")     # with tempfile.TemporaryDirectory() as tmp:     #     with open(os.path.join(tmp, \"dakota.in\"), \"w\") as io:     #         io.write(f\"environment\\     #             \\ttabular_data\\n\\     #             method\\     #             \\tfsu_quasi_mc halton\\     #             \\t\\tsamples = {ns.max()}\\     #             \\toutput silent\\n\\     #             variables\\     #             \\tcontinuous_design = {ds.max()}\\     #             \\tlower_bounds = {' '.join(['0.0' for _ in range(ds.max())])}\\     #             \\tupper_bounds = {' '.join(['1.0' for _ in range(ds.max())])}\\n\\     #             interface\\     #             \\tfork\\     #             \\t\\tanalysis_driver = 'dummy'\\     #             \\tbatch\\     #             \\twork_directory named 'work'\\n\\     #             responses\\     #             \\tobjective_functions = 1\\     #             \\tno_gradients\\     #             \\tno_hessians\"     #         )     #     subprocess.run([\"dakota\", \"dakota.in\"], cwd=tmp, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)     #     file = os.listdir(os.path.join(tmp, \"work\"))[0]     #     with open(os.path.join(tmp, \"work\", file), \"r\") as io:     #         lines = io.readlines()     #         x_full_dakota = []     #         for n, line in enumerate(lines):     #             if f\"{ds.max()} variables\" in line:     #                 x_full_dakota.append([float(lines[n + 1 + j].split()[0]) for j in range(ds.max())])     #         x_full_dakota = np.vstack(x_full_dakota)     #     np.savetxt(\"data/x_full_dakota.txt\",x_full_dakota) print(x_full_dakota.shape) <pre>(262144, 128)\n</pre> In\u00a0[19]: Copied! <pre>n_max,d_max = ns.max(),ds.max()\npts = {\n    'IID Standard Uniform': IIDStdUniform(d_max).gen_samples(n_max),\n    'Lattice (random shift)': Lattice(d_max).gen_samples(n_max),\n    'Digital Net (random scramble + shift)': DigitalNetB2(d_max).gen_samples(n_max),\n    'Halton (QMCPy)': Halton(d_max).gen_samples(n_max,warn=False),\n    'Halton (Dakota)': x_full_dakota[:n_max,:d_max]\n}\n</pre> n_max,d_max = ns.max(),ds.max() pts = {     'IID Standard Uniform': IIDStdUniform(d_max).gen_samples(n_max),     'Lattice (random shift)': Lattice(d_max).gen_samples(n_max),     'Digital Net (random scramble + shift)': DigitalNetB2(d_max).gen_samples(n_max),     'Halton (QMCPy)': Halton(d_max).gen_samples(n_max,warn=False),     'Halton (Dakota)': x_full_dakota[:n_max,:d_max] } In\u00a0[20]: Copied! <pre>nrows = len(ds)\nncols = len(kinds_func)*len(kinds_coeff)\nprint('logging')\nfig,ax = pyplot.subplots(nrows=nrows,ncols=ncols,figsize=(ncols*5,nrows*5),sharey=True,sharex=True)\nax = ax.reshape(nrows,ncols)\ncolors = pyplot.rcParams['axes.prop_cycle'].by_key()['color'] + [\"indigo\"]\nfor v,(name,x_full) in enumerate(pts.items()):\n    print('%20s d: '%name,end='',flush=True)\n    for j,d in enumerate(ds):\n        print('%d, '%d,end='',flush=True)\n        for i1,kind_func in enumerate(kinds_func):\n            for i2,kind_coeff in enumerate(kinds_coeff):\n                i = len(kinds_coeff)*i1+i2\n                tag = '%s.%d'%(kind_func,kind_coeff)\n                genz = Genz(IIDStdUniform(d),kind_func=kind_func,kind_coeff=kind_coeff)\n                y_full = genz.f(x_full[:,:d])\n                mu_hats = array([y_full[:n].mean() for n in ns],dtype=float)\n                error = abs(mu_hats-ref_sols.loc[d,tag])\n                ax[j,i].plot(ns,error,label=name, color=colors[v])\n                if v==(len(pts)-1): ax[j,i].legend(loc='lower left')\n                if v&gt;0: continue\n                ax[j,i].set_xscale('log',base=2)\n                ax[j,i].set_yscale('log',base=10)\n                if i==0: ax[j,i].set_ylabel(r'$d=%d$\\\\$\\varepsilon = \\lvert \\mu - \\hat{\\mu} \\rvert$'%d)\n                if j==0: ax[j,i].set_title(tag)\n                if j==(len(ds)-1):\n                    ax[j,i].set_xlabel(r'$n$')\n                    ax[j,i].set_xticks(ns)\n                    ax[j,i].set_xlim([ns.min(),ns.max()])\n    print()\n</pre> nrows = len(ds) ncols = len(kinds_func)*len(kinds_coeff) print('logging') fig,ax = pyplot.subplots(nrows=nrows,ncols=ncols,figsize=(ncols*5,nrows*5),sharey=True,sharex=True) ax = ax.reshape(nrows,ncols) colors = pyplot.rcParams['axes.prop_cycle'].by_key()['color'] + [\"indigo\"] for v,(name,x_full) in enumerate(pts.items()):     print('%20s d: '%name,end='',flush=True)     for j,d in enumerate(ds):         print('%d, '%d,end='',flush=True)         for i1,kind_func in enumerate(kinds_func):             for i2,kind_coeff in enumerate(kinds_coeff):                 i = len(kinds_coeff)*i1+i2                 tag = '%s.%d'%(kind_func,kind_coeff)                 genz = Genz(IIDStdUniform(d),kind_func=kind_func,kind_coeff=kind_coeff)                 y_full = genz.f(x_full[:,:d])                 mu_hats = array([y_full[:n].mean() for n in ns],dtype=float)                 error = abs(mu_hats-ref_sols.loc[d,tag])                 ax[j,i].plot(ns,error,label=name, color=colors[v])                 if v==(len(pts)-1): ax[j,i].legend(loc='lower left')                 if v&gt;0: continue                 ax[j,i].set_xscale('log',base=2)                 ax[j,i].set_yscale('log',base=10)                 if i==0: ax[j,i].set_ylabel(r'$d=%d$\\\\$\\varepsilon = \\lvert \\mu - \\hat{\\mu} \\rvert$'%d)                 if j==0: ax[j,i].set_title(tag)                 if j==(len(ds)-1):                     ax[j,i].set_xlabel(r'$n$')                     ax[j,i].set_xticks(ns)                     ax[j,i].set_xlim([ns.min(),ns.max()])     print() <pre>logging\nIID Standard Uniform d: 1, 2, 4, 8, 16, 32, 64, 128, \nLattice (random shift) d: 1, 2, 4, 8, 16, 32, 64, 128, \nDigital Net (random scramble + shift) d: 1, 2, 4, 8, 16, 32, 64, 128, \n      Halton (QMCPy) d: 1, 2, 4, 8, 16, 32, 64, 128, \n     Halton (Dakota) d: 1, 2, 4, 8, 16, 32, 64, 128, \n</pre>"},{"location":"demos/DAKOTA_Genz/dakota_genz/#genz-function-in-dakota-and-qmcpy","title":"Genz Function in Dakota and QMCPy\u00b6","text":"<p>A QMCPy implementation and comparison of Dakota's Genz function</p>"},{"location":"demos/gaussian_diagnostics/gaussian_diagnostics/","title":"Gaussian diagnostics","text":"In\u00a0[\u00a0]: Copied! <pre>import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.optimize import fmin as fminsearch\nfrom numpy import prod, sin, cos, pi\nfrom scipy.stats import norm as gaussnorm\nfrom matplotlib import cm\nimport os \n</pre> import numpy as np import matplotlib.pyplot as plt from scipy.optimize import fmin as fminsearch from numpy import prod, sin, cos, pi from scipy.stats import norm as gaussnorm from matplotlib import cm import os  In\u00a0[\u00a0]: Copied! <pre>from qmcpy.integrand import Keister\nfrom qmcpy.discrete_distribution.lattice import Lattice\n</pre> from qmcpy.integrand import Keister from qmcpy.discrete_distribution.lattice import Lattice In\u00a0[\u00a0]: Copied! <pre># print(plt.style.available)\n# plt.style.use('./presentation.mplstyle')  # custom settings\n# plt.style.library['seaborn-darkgrid']   # Showing the style settings\nplt.style.use('seaborn-v0_8-notebook')\nROOT = os.path.dirname(os.path.realpath(__file__))\nOUTDIR = ROOT+\"/outputs/\"\n</pre> # print(plt.style.available) # plt.style.use('./presentation.mplstyle')  # custom settings # plt.style.library['seaborn-darkgrid']   # Showing the style settings plt.style.use('seaborn-v0_8-notebook') ROOT = os.path.dirname(os.path.realpath(__file__)) OUTDIR = ROOT+\"/outputs/\" In\u00a0[\u00a0]: Copied! <pre>def ObjectiveFunction(theta, order, xun, ftilde):\n    tol = 100 * np.finfo(float).eps\n    n = len(ftilde)\n    arbMean = True\n    Lambda = kernel2(theta, order, xun)\n\n    # compute RKHSnorm\n    # temp = abs(ftilde(Lambda  != 0).^ 2)./ (Lambda(Lambda != 0))\n    temp = abs(ftilde[Lambda &gt; tol] ** 2) / (Lambda[Lambda &gt; tol])\n\n    # compute loss: MLE\n    if arbMean == True:\n        RKHSnorm = sum(temp[1:]) / n\n        temp_1 = sum(temp[1:])\n    else:\n        RKHSnorm = sum(temp) / n\n        temp_1 = sum(temp)\n\n    # ignore all zero eigenvalues\n    loss1 = sum(np.log(Lambda[Lambda &gt; tol])) / n\n    loss2 = np.log(temp_1)\n    loss = (loss1 + loss2)\n    if np.imag(loss) != 0:\n        # keyboard\n        raise ('error ! : loss value is complex')\n\n    # print('L1 %1.3f L2 %1.3f L %1.3f r %1.3e theta %1.3e\\n'.format(loss1, loss2, loss, order, theta))\n    return loss, Lambda, RKHSnorm\n</pre> def ObjectiveFunction(theta, order, xun, ftilde):     tol = 100 * np.finfo(float).eps     n = len(ftilde)     arbMean = True     Lambda = kernel2(theta, order, xun)      # compute RKHSnorm     # temp = abs(ftilde(Lambda  != 0).^ 2)./ (Lambda(Lambda != 0))     temp = abs(ftilde[Lambda &gt; tol] ** 2) / (Lambda[Lambda &gt; tol])      # compute loss: MLE     if arbMean == True:         RKHSnorm = sum(temp[1:]) / n         temp_1 = sum(temp[1:])     else:         RKHSnorm = sum(temp) / n         temp_1 = sum(temp)      # ignore all zero eigenvalues     loss1 = sum(np.log(Lambda[Lambda &gt; tol])) / n     loss2 = np.log(temp_1)     loss = (loss1 + loss2)     if np.imag(loss) != 0:         # keyboard         raise ('error ! : loss value is complex')      # print('L1 %1.3f L2 %1.3f L %1.3f r %1.3e theta %1.3e\\n'.format(loss1, loss2, loss, order, theta))     return loss, Lambda, RKHSnorm In\u00a0[\u00a0]: Copied! <pre>def kernel2(theta, r, xun):\n    n = xun.shape[0]\n    m = np.arange(1, (n / 2))\n    tilde_g_h1 = m ** (-r)\n    tilde_g = np.hstack([0, tilde_g_h1, 0, tilde_g_h1[::-1]])\n    g = np.fft.fft(tilde_g)\n    temp_ = (theta / 2) * g[(xun * n).astype(int)]\n    C1 = prod(1 + temp_, 1)\n    # eigenvalues must be real : Symmetric pos definite Kernel\n    vlambda = np.real(np.fft.fft(C1))\n    return vlambda\n</pre> def kernel2(theta, r, xun):     n = xun.shape[0]     m = np.arange(1, (n / 2))     tilde_g_h1 = m ** (-r)     tilde_g = np.hstack([0, tilde_g_h1, 0, tilde_g_h1[::-1]])     g = np.fft.fft(tilde_g)     temp_ = (theta / 2) * g[(xun * n).astype(int)]     C1 = prod(1 + temp_, 1)     # eigenvalues must be real : Symmetric pos definite Kernel     vlambda = np.real(np.fft.fft(C1))     return vlambda In\u00a0[\u00a0]: Copied! <pre># gaussian random function\ndef f_rand(xpts, rfun, a, b, c, seed):\n    dim = xpts.shape[1]\n    np.random.seed(seed)  # initialize random number generator for reproducability\n    N1 = int(2 ** np.floor(16 / dim))\n    Nall = N1 ** dim\n    kvec = np.zeros([dim, Nall])  # initialize kvec\n    kvec[0, 0:N1] = range(0, N1)  # first dimension\n    Nd = N1\n    for d in range(1, dim):\n        Ndm1 = Nd\n        Nd = Nd * N1\n        kvec[0:d + 1, 0:Nd] = np.vstack([\n            np.tile(kvec[0:d, 0:Ndm1], (1, N1)),\n            np.reshape(np.tile(np.arange(0, N1), (Ndm1, 1)), (1, Nd), order=\"F\")\n        ])\n\n    kvec = kvec[:, 1: Nall]  # remove the zero wavenumber\n    whZero = np.sum(kvec == 0, axis=0)\n    abfac = a ** (dim - whZero) * b ** whZero\n    kbar = np.prod(np.maximum(kvec, 1), axis=0)\n    totfac = abfac / (kbar ** rfun)\n\n    f_c = a * np.random.randn(1, Nall - 1) * totfac\n    f_s = a * np.random.randn(1, Nall - 1) * totfac\n\n    f_0 = c + (b ** dim) * np.random.randn()\n    argx = np.matmul((2 * np.pi * xpts), kvec)\n    f_c_ = f_c * np.cos(argx)\n    f_s_ = f_s * np.sin(argx)\n    fval = f_0 + np.sum(f_c_ + f_s_, axis=1)\n    return fval\n</pre> # gaussian random function def f_rand(xpts, rfun, a, b, c, seed):     dim = xpts.shape[1]     np.random.seed(seed)  # initialize random number generator for reproducability     N1 = int(2 ** np.floor(16 / dim))     Nall = N1 ** dim     kvec = np.zeros([dim, Nall])  # initialize kvec     kvec[0, 0:N1] = range(0, N1)  # first dimension     Nd = N1     for d in range(1, dim):         Ndm1 = Nd         Nd = Nd * N1         kvec[0:d + 1, 0:Nd] = np.vstack([             np.tile(kvec[0:d, 0:Ndm1], (1, N1)),             np.reshape(np.tile(np.arange(0, N1), (Ndm1, 1)), (1, Nd), order=\"F\")         ])      kvec = kvec[:, 1: Nall]  # remove the zero wavenumber     whZero = np.sum(kvec == 0, axis=0)     abfac = a ** (dim - whZero) * b ** whZero     kbar = np.prod(np.maximum(kvec, 1), axis=0)     totfac = abfac / (kbar ** rfun)      f_c = a * np.random.randn(1, Nall - 1) * totfac     f_s = a * np.random.randn(1, Nall - 1) * totfac      f_0 = c + (b ** dim) * np.random.randn()     argx = np.matmul((2 * np.pi * xpts), kvec)     f_c_ = f_c * np.cos(argx)     f_s_ = f_s * np.sin(argx)     fval = f_0 + np.sum(f_c_ + f_s_, axis=1)     return fval In\u00a0[\u00a0]: Copied! <pre>def doPeriodTx(x, integrand, ptransform):\n    ptransform = ptransform.upper()\n    if ptransform == 'BAKER':  # Baker's transform\n        xp = 1 - 2 * abs(x - 1 / 2)\n        w = 1\n    elif ptransform == 'C0':  # C^0 transform\n        xp = 3 * x ** 2 - 2 * x ** 3\n        w = prod(6 * x * (1 - x), 1)\n    elif ptransform == 'C1':  # C^1 transform\n        xp = x ** 3 * (10 - 15 * x + 6 * x ** 2)\n        w = prod(30 * x ** 2 * (1 - x) ** 2, 1)\n    elif ptransform == 'C1SIN':  # Sidi C^1 transform\n        xp = x - sin(2 * pi * x) / (2 * pi)\n        w = prod(2 * sin(pi * x) ** 2, 1)\n    elif ptransform == 'C2SIN':  # Sidi C^2 transform\n        xp = (8 - 9 * cos(pi * x) + cos(3 * pi * x)) / 16  # psi3\n        w = prod((9 * sin(pi * x) * pi - sin(3 * pi * x) * 3 * pi) / 16, 1)  # psi3_1\n    elif ptransform == 'C3SIN':  # Sidi C^3 transform\n        xp = (12 * pi * x - 8 * sin(2 * pi * x) + sin(4 * pi * x)) / (12 * pi)  # psi4\n        w = prod((12 * pi - 8 * cos(2 * pi * x) * 2 * pi + sin(4 * pi * x) * 4 * pi) / (12 * pi), 1)  # psi4_1\n    elif ptransform == 'NONE':\n        xp = x\n        w = 1\n    else:\n        raise (f\"The {ptransform} periodization transform is not implemented\")\n    y = integrand(xp) * w\n    return y\n</pre> def doPeriodTx(x, integrand, ptransform):     ptransform = ptransform.upper()     if ptransform == 'BAKER':  # Baker's transform         xp = 1 - 2 * abs(x - 1 / 2)         w = 1     elif ptransform == 'C0':  # C^0 transform         xp = 3 * x ** 2 - 2 * x ** 3         w = prod(6 * x * (1 - x), 1)     elif ptransform == 'C1':  # C^1 transform         xp = x ** 3 * (10 - 15 * x + 6 * x ** 2)         w = prod(30 * x ** 2 * (1 - x) ** 2, 1)     elif ptransform == 'C1SIN':  # Sidi C^1 transform         xp = x - sin(2 * pi * x) / (2 * pi)         w = prod(2 * sin(pi * x) ** 2, 1)     elif ptransform == 'C2SIN':  # Sidi C^2 transform         xp = (8 - 9 * cos(pi * x) + cos(3 * pi * x)) / 16  # psi3         w = prod((9 * sin(pi * x) * pi - sin(3 * pi * x) * 3 * pi) / 16, 1)  # psi3_1     elif ptransform == 'C3SIN':  # Sidi C^3 transform         xp = (12 * pi * x - 8 * sin(2 * pi * x) + sin(4 * pi * x)) / (12 * pi)  # psi4         w = prod((12 * pi - 8 * cos(2 * pi * x) * 2 * pi + sin(4 * pi * x) * 4 * pi) / (12 * pi), 1)  # psi4_1     elif ptransform == 'NONE':         xp = x         w = 1     else:         raise (f\"The {ptransform} periodization transform is not implemented\")     y = integrand(xp) * w     return y In\u00a0[\u00a0]: Copied! <pre>def create_plots(type, vz_real, fName, dim, iii, r, rOpt, theta, thetaOpt):\n    hFigNormplot, axFigNormplot = plt.subplots()\n\n    # plt.rcParams.update({'font.size': 12})\n    # plt.rcParams.update({'lines.linewidth': 2})\n\n    # set(hFigNormplot,'defaultaxesfontsize',16,\n    #   'defaulttextfontsize',12,   # make font larger\n    #   'defaultLineLineWidth',2, 'defaultLineMarkerSize',6)\n    n = len(vz_real)\n    if type == 'normplot':\n        axFigNormplot.normplot(vz_real)\n    else:\n        q = (np.arange(1, n + 1) - 1 / 2) / n\n        stNorm = gaussnorm.ppf(q)  # norminv: quantiles of standard normal\n        axFigNormplot.scatter(stNorm, sorted(vz_real), s=20)  # marker='.',\n        axFigNormplot.plot([-3, 3], [-3, 3], marker='_', linewidth=4, color='red')\n        axFigNormplot.set_xlabel('Standard Gaussian Quantiles')\n        axFigNormplot.set_ylabel('Data Quantiles')\n\n    if theta:\n        plt_title = f'$d={dim}, n={n}, r={r:1.2f}, r_{{opt}}={rOpt:1.2f}, \\\\theta={theta:1.2f}, \\\\theta_{{opt}}={thetaOpt:1.2f}$'\n        plt_filename = f'{fName}-QQPlot_n-{n}_d-{dim}_r-{r * 100}_th-{100 * theta}_case-{iii}.jpg'\n    else:\n        plt_title = f'$d={dim}, n={n}, r_{{opt}}={rOpt:1.2f}, \\\\theta_{{opt}}={thetaOpt:1.2f}$'\n        plt_filename = f'{fName}-QQPlot_n-{n}_d-{dim}_case-{iii}.jpg'\n    axFigNormplot.set_title(plt_title)\n    hFigNormplot.savefig(OUTDIR+plt_filename)\n</pre> def create_plots(type, vz_real, fName, dim, iii, r, rOpt, theta, thetaOpt):     hFigNormplot, axFigNormplot = plt.subplots()      # plt.rcParams.update({'font.size': 12})     # plt.rcParams.update({'lines.linewidth': 2})      # set(hFigNormplot,'defaultaxesfontsize',16,     #   'defaulttextfontsize',12,   # make font larger     #   'defaultLineLineWidth',2, 'defaultLineMarkerSize',6)     n = len(vz_real)     if type == 'normplot':         axFigNormplot.normplot(vz_real)     else:         q = (np.arange(1, n + 1) - 1 / 2) / n         stNorm = gaussnorm.ppf(q)  # norminv: quantiles of standard normal         axFigNormplot.scatter(stNorm, sorted(vz_real), s=20)  # marker='.',         axFigNormplot.plot([-3, 3], [-3, 3], marker='_', linewidth=4, color='red')         axFigNormplot.set_xlabel('Standard Gaussian Quantiles')         axFigNormplot.set_ylabel('Data Quantiles')      if theta:         plt_title = f'$d={dim}, n={n}, r={r:1.2f}, r_{{opt}}={rOpt:1.2f}, \\\\theta={theta:1.2f}, \\\\theta_{{opt}}={thetaOpt:1.2f}$'         plt_filename = f'{fName}-QQPlot_n-{n}_d-{dim}_r-{r * 100}_th-{100 * theta}_case-{iii}.jpg'     else:         plt_title = f'$d={dim}, n={n}, r_{{opt}}={rOpt:1.2f}, \\\\theta_{{opt}}={thetaOpt:1.2f}$'         plt_filename = f'{fName}-QQPlot_n-{n}_d-{dim}_case-{iii}.jpg'     axFigNormplot.set_title(plt_title)     hFigNormplot.savefig(OUTDIR+plt_filename) In\u00a0[\u00a0]: Copied! <pre>def create_surf_plot(fName, lnthth, lnordord, objfun, objobj, lnParamsOpt, r, theta, iii):\n    figH, axH = plt.subplots(subplot_kw={\"projection\": \"3d\"})\n    axH.view_init(40, 30)\n    shandle = axH.plot_surface(lnthth, lnordord, objobj, cmap=cm.coolwarm,\n                               linewidth=0, antialiased=False, alpha=0.8)\n    xt = np.array([.2, 0.4, 1, 3, 7])\n    axH.set_xticks(np.log(xt))\n    axH.set_xticklabels(xt.astype(str))\n    yt = np.array([1.4, 1.6, 2, 2.6, 3.7])\n    axH.set_yticks(np.log(yt - 1))\n    axH.set_yticklabels(yt.astype(str))\n    axH.set_xlabel('$\\\\theta$')\n    axH.set_ylabel('$r$')\n\n    axH.scatter(lnParamsOpt[0], lnParamsOpt[1], objfun(lnParamsOpt) * 1.002,\n                s=200, color='orange', marker='*', alpha=0.8)\n    if theta:\n        filename = f'{fName}-ObjFun_n-{npts}_d-{dim}_r-{r * 100}_th-{100 * theta}_case-{iii}.jpg'\n    else:\n        filename = f'{fName}-ObjFun_n-{npts}_d-{dim}_case-{iii}.jpg'\n    figH.savefig(OUTDIR+filename)\n</pre> def create_surf_plot(fName, lnthth, lnordord, objfun, objobj, lnParamsOpt, r, theta, iii):     figH, axH = plt.subplots(subplot_kw={\"projection\": \"3d\"})     axH.view_init(40, 30)     shandle = axH.plot_surface(lnthth, lnordord, objobj, cmap=cm.coolwarm,                                linewidth=0, antialiased=False, alpha=0.8)     xt = np.array([.2, 0.4, 1, 3, 7])     axH.set_xticks(np.log(xt))     axH.set_xticklabels(xt.astype(str))     yt = np.array([1.4, 1.6, 2, 2.6, 3.7])     axH.set_yticks(np.log(yt - 1))     axH.set_yticklabels(yt.astype(str))     axH.set_xlabel('$\\\\theta$')     axH.set_ylabel('$r$')      axH.scatter(lnParamsOpt[0], lnParamsOpt[1], objfun(lnParamsOpt) * 1.002,                 s=200, color='orange', marker='*', alpha=0.8)     if theta:         filename = f'{fName}-ObjFun_n-{npts}_d-{dim}_r-{r * 100}_th-{100 * theta}_case-{iii}.jpg'     else:         filename = f'{fName}-ObjFun_n-{npts}_d-{dim}_case-{iii}.jpg'     figH.savefig(OUTDIR+filename) In\u00a0[\u00a0]: Copied! <pre>#\n# Minimum working example to demonstrate Gaussian diagnostics concept\n#\ndef MWE_gaussian_diagnostics_engine(whEx, dim, npts, r, fpar, nReps, nPlots):\n    whEx = whEx - 1\n    fNames = ['ExpCos', 'Keister', 'rand']\n    ptransforms = ['none', 'C1sin', 'none']\n    fName = fNames[whEx]\n    ptransform = ptransforms[whEx]\n\n    rOptAll = [0] * nRep\n    thOptAll = [0] * nRep\n\n    # parameters for random function\n    # seed = 202326\n    if whEx == 2:\n        rfun = r / 2\n        f_mean = fpar[2]\n        f_std_a = fpar[0]  # this is square root of the a in the talk\n        f_std_b = fpar[1]  # this is square root of the b in the talk\n        theta = (f_std_a / f_std_b) ** 2\n    else:\n        theta = None\n\n    for iii in range(nReps):\n        seed = np.random.randint(low=1, high=1e6)  # different each rep\n        shift = np.random.rand(1, dim)\n\n        distribution = Lattice(dimension=dim, order='linear')\n        xpts = distribution.gen_samples(n_min=0, n_max=npts, warn=False)\n        xlat = (xpts-distribution.shift)%1\n        \n        if fName == 'ExpCos':\n            integrand = lambda x: np.exp(np.sum(np.cos(2 * np.pi * x), axis=1))\n        elif fName == 'Keister':\n            keister = Keister(Lattice(dimension=dim, order='linear'))\n            integrand = lambda x: keister.f(x)\n        elif fName == 'rand':\n            integrand = lambda x: f_rand(x, rfun, f_std_a, f_std_b, f_mean, seed)\n        else:\n            print('Invalid function name')\n            return\n\n        y = doPeriodTx(xpts, integrand, ptransform)\n\n        ftilde = np.fft.fft(y)  # fourier coefficients\n        ftilde[0] = 0  # ftilde = \\mV**H(\\vf - m \\vone), subtract mean\n        if dim == 1:\n            hFigIntegrand = plt.figure()\n            plt.scatter(xpts, y, 10)\n            plt.title(f'{fName}_n-{npts}_Tx-{ptransform}')\n            hFigIntegrand.savefig(OUTDIR+f'{fName}_n-{npts}_Tx-{ptransform}_rFun-{rfun:1.2f}.jpg')\n\n        def objfun(lnParams):\n            loss, Lambda, RKHSnorm = ObjectiveFunction(np.exp(lnParams[0]), 1 + np.exp(lnParams[1]), xlat, ftilde)\n            return loss\n\n        ## Plot the objective function\n        lnthetarange = np.arange(-2, 2.2, 0.2)  # range of log(theta) for plotting\n        lnorderrange = np.arange(-1, 1.1, 0.1)  # range of log(r) for plotting\n        [lnthth, lnordord] = np.meshgrid(lnthetarange, lnorderrange)\n        objobj = np.zeros(lnthth.shape)\n        for ii in range(lnthth.shape[0]):\n            for jj in range(lnthth.shape[1]):\n                objobj[ii, jj] = objfun([lnthth[ii, jj], lnordord[ii, jj]])\n\n        objMinAppx, which = objobj.min(), objobj.argmin()\n        # [whichrow, whichcol] = ind2sub(lnthth.shape, which)\n        [whichrow, whichcol] = np.unravel_index(which, lnthth.shape)\n        lnthOptAppx = lnthth[whichrow, whichcol]\n        thetaOptAppx = np.exp(lnthOptAppx)\n        lnordOptAppx = lnordord[whichrow, whichcol]\n        orderOptAppx = 1 + np.exp(lnordOptAppx)\n        # print(objMinAppx)  # minimum objective function by brute force search\n\n        ## Optimize the objective function\n        result = fminsearch(objfun, x0=[lnthOptAppx, lnordOptAppx], xtol=1e-3, full_output=True, disp=False)\n        lnParamsOpt, objMin = result[0], result[1]\n        # print(objMin)  # minimum objective function by Nelder-Mead\n        thetaOpt = np.exp(lnParamsOpt[0])\n        rOpt = 1 + np.exp(lnParamsOpt[1])\n        rOptAll[iii] = rOpt\n        thOptAll[iii] = thetaOpt\n        print(\n            f'thetaOptAppx={thetaOptAppx:7.5f}, rOptAppx={orderOptAppx:7.5f}, '\n            f'objMinAppx={objMinAppx:7.5f}, objMin={objMin:7.5f}')\n\n        if iii &lt;= nPlots:\n            create_surf_plot(fName, lnthth, lnordord, objfun, objobj, lnParamsOpt, r, theta, iii)\n\n        vlambda = kernel2(thetaOpt, rOpt, xlat)\n        s2 = sum(abs(ftilde[2:] ** 2) / vlambda[2:]) / (npts ** 2)\n        vlambda = s2 * vlambda\n\n        # apply transform\n        # $\\vZ = \\frac 1n \\mV \\mLambda**{-\\frac 12} \\mV**H(\\vf - m \\vone)$\n        # np.fft also includes 1/n division\n        vz = np.fft.ifft(ftilde / np.sqrt(vlambda))\n        vz_real = np.real(vz)  # vz must be real as intended by the transformation\n\n        if iii &lt;= nPlots:\n            create_plots('qqplot', vz_real, fName, dim, iii, r, rOpt, theta, thetaOpt)\n\n        r_str = f\"{r: 7.5f}\" if type(r) == float else str(r)\n        theta_str = f\"{theta: 7.5f}\" if type(theta) == float else str(theta)\n        print(f'\\t r = {r_str}, rOpt = {rOpt:7.5f}, theta = {theta_str}, thetaOpt = {thetaOpt:7.5f}\\n')\n\n    return [theta, rOptAll, thOptAll, fName]\n</pre> # # Minimum working example to demonstrate Gaussian diagnostics concept # def MWE_gaussian_diagnostics_engine(whEx, dim, npts, r, fpar, nReps, nPlots):     whEx = whEx - 1     fNames = ['ExpCos', 'Keister', 'rand']     ptransforms = ['none', 'C1sin', 'none']     fName = fNames[whEx]     ptransform = ptransforms[whEx]      rOptAll = [0] * nRep     thOptAll = [0] * nRep      # parameters for random function     # seed = 202326     if whEx == 2:         rfun = r / 2         f_mean = fpar[2]         f_std_a = fpar[0]  # this is square root of the a in the talk         f_std_b = fpar[1]  # this is square root of the b in the talk         theta = (f_std_a / f_std_b) ** 2     else:         theta = None      for iii in range(nReps):         seed = np.random.randint(low=1, high=1e6)  # different each rep         shift = np.random.rand(1, dim)          distribution = Lattice(dimension=dim, order='linear')         xpts = distribution.gen_samples(n_min=0, n_max=npts, warn=False)         xlat = (xpts-distribution.shift)%1                  if fName == 'ExpCos':             integrand = lambda x: np.exp(np.sum(np.cos(2 * np.pi * x), axis=1))         elif fName == 'Keister':             keister = Keister(Lattice(dimension=dim, order='linear'))             integrand = lambda x: keister.f(x)         elif fName == 'rand':             integrand = lambda x: f_rand(x, rfun, f_std_a, f_std_b, f_mean, seed)         else:             print('Invalid function name')             return          y = doPeriodTx(xpts, integrand, ptransform)          ftilde = np.fft.fft(y)  # fourier coefficients         ftilde[0] = 0  # ftilde = \\mV**H(\\vf - m \\vone), subtract mean         if dim == 1:             hFigIntegrand = plt.figure()             plt.scatter(xpts, y, 10)             plt.title(f'{fName}_n-{npts}_Tx-{ptransform}')             hFigIntegrand.savefig(OUTDIR+f'{fName}_n-{npts}_Tx-{ptransform}_rFun-{rfun:1.2f}.jpg')          def objfun(lnParams):             loss, Lambda, RKHSnorm = ObjectiveFunction(np.exp(lnParams[0]), 1 + np.exp(lnParams[1]), xlat, ftilde)             return loss          ## Plot the objective function         lnthetarange = np.arange(-2, 2.2, 0.2)  # range of log(theta) for plotting         lnorderrange = np.arange(-1, 1.1, 0.1)  # range of log(r) for plotting         [lnthth, lnordord] = np.meshgrid(lnthetarange, lnorderrange)         objobj = np.zeros(lnthth.shape)         for ii in range(lnthth.shape[0]):             for jj in range(lnthth.shape[1]):                 objobj[ii, jj] = objfun([lnthth[ii, jj], lnordord[ii, jj]])          objMinAppx, which = objobj.min(), objobj.argmin()         # [whichrow, whichcol] = ind2sub(lnthth.shape, which)         [whichrow, whichcol] = np.unravel_index(which, lnthth.shape)         lnthOptAppx = lnthth[whichrow, whichcol]         thetaOptAppx = np.exp(lnthOptAppx)         lnordOptAppx = lnordord[whichrow, whichcol]         orderOptAppx = 1 + np.exp(lnordOptAppx)         # print(objMinAppx)  # minimum objective function by brute force search          ## Optimize the objective function         result = fminsearch(objfun, x0=[lnthOptAppx, lnordOptAppx], xtol=1e-3, full_output=True, disp=False)         lnParamsOpt, objMin = result[0], result[1]         # print(objMin)  # minimum objective function by Nelder-Mead         thetaOpt = np.exp(lnParamsOpt[0])         rOpt = 1 + np.exp(lnParamsOpt[1])         rOptAll[iii] = rOpt         thOptAll[iii] = thetaOpt         print(             f'thetaOptAppx={thetaOptAppx:7.5f}, rOptAppx={orderOptAppx:7.5f}, '             f'objMinAppx={objMinAppx:7.5f}, objMin={objMin:7.5f}')          if iii &lt;= nPlots:             create_surf_plot(fName, lnthth, lnordord, objfun, objobj, lnParamsOpt, r, theta, iii)          vlambda = kernel2(thetaOpt, rOpt, xlat)         s2 = sum(abs(ftilde[2:] ** 2) / vlambda[2:]) / (npts ** 2)         vlambda = s2 * vlambda          # apply transform         # $\\vZ = \\frac 1n \\mV \\mLambda**{-\\frac 12} \\mV**H(\\vf - m \\vone)$         # np.fft also includes 1/n division         vz = np.fft.ifft(ftilde / np.sqrt(vlambda))         vz_real = np.real(vz)  # vz must be real as intended by the transformation          if iii &lt;= nPlots:             create_plots('qqplot', vz_real, fName, dim, iii, r, rOpt, theta, thetaOpt)          r_str = f\"{r: 7.5f}\" if type(r) == float else str(r)         theta_str = f\"{theta: 7.5f}\" if type(theta) == float else str(theta)         print(f'\\t r = {r_str}, rOpt = {rOpt:7.5f}, theta = {theta_str}, thetaOpt = {thetaOpt:7.5f}\\n')      return [theta, rOptAll, thOptAll, fName] In\u00a0[\u00a0]: Copied! <pre>if __name__ == '__main__':\n\n    ## Exponential Cosine example\n    fwh = 1\n    dim = 3\n    npts = 2 ** 6\n    nRep = 20\n    nPlot = 2\n    [_, rOptAll, thOptAll, fName] = \\\n        MWE_gaussian_diagnostics_engine(fwh, dim, npts, None, None, nRep, nPlot)\n\n    ## Plot Exponential Cosine example\n    figH = plt.figure()\n    plt.scatter(rOptAll, thOptAll, s=20, color='blue')\n    # axis([4 6 0.1 10])\n    # set(gca,'yscale','log')\n    plt.title(f'$d = {dim}, n = {npts}$')\n    plt.xlabel('Inferred $r$')\n    plt.ylabel('Inferred $\\\\theta$')\n    # print('-depsc',[fName '-rthInfer-n-' int2str(npts) '-d-' \\\n    #   int2str(dim)])\n    figH.savefig(OUTDIR+f'{fName}-rthInfer-n-{npts}-d-{dim}.jpg')\n    plt.close('all')\n\n    ## Tests with random function\n    rArray = [1.5, 2, 4]\n    nrArr = len(rArray)\n    fParArray = [[0.5, 1, 2], [1, 1, 1], [1, 1, 1]]\n    nfPArr = len(fParArray)\n    fwh = 3\n    dim = 2\n    npts = 2 ** 6\n    nRep = 20\n    nPlot = 2\n    thetaAll = np.zeros((nrArr, nfPArr))\n    rOptAll = np.zeros((nrArr, nfPArr, nRep))\n    thOptAll = np.zeros((nrArr, nfPArr, nRep))\n    for jjj in range(nrArr):\n        for kkk in range(nfPArr):\n            thetaAll[jjj, kkk], rOptAll[jjj, kkk, :], thOptAll[jjj, kkk, :], fName = \\\n                MWE_gaussian_diagnostics_engine(fwh, dim, npts, rArray[jjj], fParArray[kkk], nRep, nPlot)\n    plt.close('all')\n\n    ## Plot figures for random function\n    figH, axH = plt.subplots()\n    colorArray = ['blue', 'orange', 'green', 'cyan', 'maroon', 'purple']\n    nColArray = len(colorArray)\n    for jjj in range(nrArr):\n        for kkk in range(nfPArr):\n            clrInd = np.mod(nfPArr * (jjj) + kkk, nColArray)\n            clr = colorArray[clrInd]\n            axH.scatter(rOptAll[jjj, kkk, :].reshape((nRep, 1)), thOptAll[jjj, kkk, :].reshape((nRep, 1)),\n                        marker='.', s=50, color=clr)\n            axH.scatter(rArray[jjj], thetaAll[jjj, kkk], s=50, c=clr, marker='D')\n\n    axH.set(xlim=[1, 6], ylim=[0.01, 100])\n    axH.set_yscale('log')\n    axH.set_title(f'$d = {dim}, n = {npts}$')\n    axH.set_xlabel('Inferred $r$')\n    axH.set_ylabel('Inferred $\\\\theta$')\n    figH.savefig(OUTDIR+f'{fName}-rthInfer-n-{npts}-d-{dim}.jpg')\n\n    ## Keister example\n    fwh = 2\n    dim = 3\n    npts = 2 ** 6\n    nRep = 20\n    nPlot = 2\n    _, rOptAll, thOptAll, fName = MWE_gaussian_diagnostics_engine(fwh, dim, npts, None, None, nRep, nPlot)\n\n    ## Plot Keister example\n    figH = plt.figure()\n    plt.scatter(rOptAll, thOptAll, s=20, color='blue')\n    # axis([4 6 0.5 1.5])\n    # set(gca,'yscale','log')\n    plt.xlabel('Inferred $r$')\n    plt.ylabel('Inferred $\\\\theta$')\n    plt.title(f'$d = {dim}, n = {npts}$')\n    figH.savefig(OUTDIR+f'{fName}-rthInfer-n-{npts}-d-{dim}.jpg')\n\n    ## Keister example\n    fwh = 2\n    dim = 3\n    npts = 2 ** 10\n    nRep = 20\n    nPlot = 2\n    _, rOptAll, thOptAll, fName = MWE_gaussian_diagnostics_engine(fwh, dim, npts, None, None, nRep, nPlot)\n\n    ## Plot Keister example\n    figH = plt.figure()\n    plt.scatter(rOptAll, thOptAll, s=20, color='blue')\n    # axis([4 6 0.5 1.5])\n    # set(gca,'yscale','log')\n    plt.xlabel('Inferred $r$')\n    plt.ylabel('Inferred $\\\\theta$')\n    plt.title(f'$d = {dim}, n = {npts}$')\n    figH.savefig(OUTDIR+f'{fName}-rthInfer-n-{npts}-d-{dim}.jpg')\n</pre> if __name__ == '__main__':      ## Exponential Cosine example     fwh = 1     dim = 3     npts = 2 ** 6     nRep = 20     nPlot = 2     [_, rOptAll, thOptAll, fName] = \\         MWE_gaussian_diagnostics_engine(fwh, dim, npts, None, None, nRep, nPlot)      ## Plot Exponential Cosine example     figH = plt.figure()     plt.scatter(rOptAll, thOptAll, s=20, color='blue')     # axis([4 6 0.1 10])     # set(gca,'yscale','log')     plt.title(f'$d = {dim}, n = {npts}$')     plt.xlabel('Inferred $r$')     plt.ylabel('Inferred $\\\\theta$')     # print('-depsc',[fName '-rthInfer-n-' int2str(npts) '-d-' \\     #   int2str(dim)])     figH.savefig(OUTDIR+f'{fName}-rthInfer-n-{npts}-d-{dim}.jpg')     plt.close('all')      ## Tests with random function     rArray = [1.5, 2, 4]     nrArr = len(rArray)     fParArray = [[0.5, 1, 2], [1, 1, 1], [1, 1, 1]]     nfPArr = len(fParArray)     fwh = 3     dim = 2     npts = 2 ** 6     nRep = 20     nPlot = 2     thetaAll = np.zeros((nrArr, nfPArr))     rOptAll = np.zeros((nrArr, nfPArr, nRep))     thOptAll = np.zeros((nrArr, nfPArr, nRep))     for jjj in range(nrArr):         for kkk in range(nfPArr):             thetaAll[jjj, kkk], rOptAll[jjj, kkk, :], thOptAll[jjj, kkk, :], fName = \\                 MWE_gaussian_diagnostics_engine(fwh, dim, npts, rArray[jjj], fParArray[kkk], nRep, nPlot)     plt.close('all')      ## Plot figures for random function     figH, axH = plt.subplots()     colorArray = ['blue', 'orange', 'green', 'cyan', 'maroon', 'purple']     nColArray = len(colorArray)     for jjj in range(nrArr):         for kkk in range(nfPArr):             clrInd = np.mod(nfPArr * (jjj) + kkk, nColArray)             clr = colorArray[clrInd]             axH.scatter(rOptAll[jjj, kkk, :].reshape((nRep, 1)), thOptAll[jjj, kkk, :].reshape((nRep, 1)),                         marker='.', s=50, color=clr)             axH.scatter(rArray[jjj], thetaAll[jjj, kkk], s=50, c=clr, marker='D')      axH.set(xlim=[1, 6], ylim=[0.01, 100])     axH.set_yscale('log')     axH.set_title(f'$d = {dim}, n = {npts}$')     axH.set_xlabel('Inferred $r$')     axH.set_ylabel('Inferred $\\\\theta$')     figH.savefig(OUTDIR+f'{fName}-rthInfer-n-{npts}-d-{dim}.jpg')      ## Keister example     fwh = 2     dim = 3     npts = 2 ** 6     nRep = 20     nPlot = 2     _, rOptAll, thOptAll, fName = MWE_gaussian_diagnostics_engine(fwh, dim, npts, None, None, nRep, nPlot)      ## Plot Keister example     figH = plt.figure()     plt.scatter(rOptAll, thOptAll, s=20, color='blue')     # axis([4 6 0.5 1.5])     # set(gca,'yscale','log')     plt.xlabel('Inferred $r$')     plt.ylabel('Inferred $\\\\theta$')     plt.title(f'$d = {dim}, n = {npts}$')     figH.savefig(OUTDIR+f'{fName}-rthInfer-n-{npts}-d-{dim}.jpg')      ## Keister example     fwh = 2     dim = 3     npts = 2 ** 10     nRep = 20     nPlot = 2     _, rOptAll, thOptAll, fName = MWE_gaussian_diagnostics_engine(fwh, dim, npts, None, None, nRep, nPlot)      ## Plot Keister example     figH = plt.figure()     plt.scatter(rOptAll, thOptAll, s=20, color='blue')     # axis([4 6 0.5 1.5])     # set(gca,'yscale','log')     plt.xlabel('Inferred $r$')     plt.ylabel('Inferred $\\\\theta$')     plt.title(f'$d = {dim}, n = {npts}$')     figH.savefig(OUTDIR+f'{fName}-rthInfer-n-{npts}-d-{dim}.jpg')"},{"location":"demos/gaussian_diagnostics/gaussian_diagnostics_demo/","title":"2022 Bayesian Cubature Stopping Criterion","text":"In\u00a0[1]: Copied! <pre>import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.optimize import fmin as fminsearch\nfrom numpy import prod, sin, cos, pi\nfrom scipy.stats import norm as gaussnorm\nfrom matplotlib import cm\n</pre> import numpy as np import matplotlib.pyplot as plt from scipy.optimize import fmin as fminsearch from numpy import prod, sin, cos, pi from scipy.stats import norm as gaussnorm from matplotlib import cm In\u00a0[2]: Copied! <pre>from qmcpy.integrand import Keister\nfrom qmcpy.discrete_distribution.lattice import Lattice\n</pre> from qmcpy.integrand import Keister from qmcpy.discrete_distribution.lattice import Lattice In\u00a0[4]: Copied! <pre># print(plt.style.available)\n# plt.style.use('./presentation.mplstyle')  # use custom settings\nplt.style.use('seaborn-v0_8-poster')\nOUTDIR = \"./outputs_nb/\"\n# plt.rcParams.update({'font.size': 12})\n# plt.rcParams.update({'lines.linewidth': 2})\n# plt.rcParams.update({'lines.markersize': 6})\n</pre> # print(plt.style.available) # plt.style.use('./presentation.mplstyle')  # use custom settings plt.style.use('seaborn-v0_8-poster') OUTDIR = \"./outputs_nb/\" # plt.rcParams.update({'font.size': 12}) # plt.rcParams.update({'lines.linewidth': 2}) # plt.rcParams.update({'lines.markersize': 6}) <p>Let us define the objective function. (<code>cubBayesLattice</code>) finds optimal parameters by minimizing the objective function</p> In\u00a0[5]: Copied! <pre>def ObjectiveFunction(theta, order, xun, ftilde):\n    tol = 100 * np.finfo(float).eps\n    n = len(ftilde)\n    arbMean = True\n    Lambda = kernel2(theta, order, xun)\n\n    # compute RKHSnorm\n    # temp = abs(ftilde(Lambda  != 0).^ 2)./ (Lambda(Lambda != 0))\n    temp = abs(ftilde[Lambda &gt; tol] ** 2) / (Lambda[Lambda &gt; tol])\n\n    # compute loss: MLE\n    if arbMean == True:\n        RKHSnorm = sum(temp[1:]) / n\n        temp_1 = sum(temp[1:])\n    else:\n        RKHSnorm = sum(temp) / n\n        temp_1 = sum(temp)\n\n    # ignore all zero eigenvalues\n    loss1 = sum(np.log(Lambda[Lambda &gt; tol])) / n\n    loss2 = np.log(temp_1)\n    loss = (loss1 + loss2)\n    if np.imag(loss) != 0:\n        # keyboard\n        raise('error ! : loss value is complex')\n\n    # print('L1 %1.3f L2 %1.3f L %1.3f r %1.3e theta %1.3e\\n'.format(loss1, loss2, loss, order, theta))\n    return loss, Lambda, RKHSnorm\n</pre> def ObjectiveFunction(theta, order, xun, ftilde):     tol = 100 * np.finfo(float).eps     n = len(ftilde)     arbMean = True     Lambda = kernel2(theta, order, xun)      # compute RKHSnorm     # temp = abs(ftilde(Lambda  != 0).^ 2)./ (Lambda(Lambda != 0))     temp = abs(ftilde[Lambda &gt; tol] ** 2) / (Lambda[Lambda &gt; tol])      # compute loss: MLE     if arbMean == True:         RKHSnorm = sum(temp[1:]) / n         temp_1 = sum(temp[1:])     else:         RKHSnorm = sum(temp) / n         temp_1 = sum(temp)      # ignore all zero eigenvalues     loss1 = sum(np.log(Lambda[Lambda &gt; tol])) / n     loss2 = np.log(temp_1)     loss = (loss1 + loss2)     if np.imag(loss) != 0:         # keyboard         raise('error ! : loss value is complex')      # print('L1 %1.3f L2 %1.3f L %1.3f r %1.3e theta %1.3e\\n'.format(loss1, loss2, loss, order, theta))     return loss, Lambda, RKHSnorm <p>Series approximation of the shift invariant kernel</p> In\u00a0[6]: Copied! <pre>def kernel2(theta, r, xun):\n    n = xun.shape[0]\n    m = np.arange(1, (n / 2))\n    tilde_g_h1 = m ** (-r)\n    tilde_g = np.hstack([0, tilde_g_h1, 0, tilde_g_h1[::-1]])\n    g = np.fft.fft(tilde_g)\n    temp_ = (theta / 2) * g[(xun * n).astype(int)]\n    C1 = prod(1 + temp_, 1)\n    # eigenvalues must be real : Symmetric pos definite Kernel\n    vlambda = np.real(np.fft.fft(C1))\n    return vlambda\n</pre> def kernel2(theta, r, xun):     n = xun.shape[0]     m = np.arange(1, (n / 2))     tilde_g_h1 = m ** (-r)     tilde_g = np.hstack([0, tilde_g_h1, 0, tilde_g_h1[::-1]])     g = np.fft.fft(tilde_g)     temp_ = (theta / 2) * g[(xun * n).astype(int)]     C1 = prod(1 + temp_, 1)     # eigenvalues must be real : Symmetric pos definite Kernel     vlambda = np.real(np.fft.fft(C1))     return vlambda <p>Gaussian random function</p> In\u00a0[7]: Copied! <pre>def f_rand(xpts, rfun, a, b, c, seed):\n    dim = xpts.shape[1]\n    np.random.seed(seed)  # initialize random number generator for reproducability\n    N1 = int(2 ** np.floor(16 / dim))\n    Nall = N1 ** dim\n    kvec = np.zeros([dim, Nall])  # initialize kvec\n    kvec[0, 0:N1] = range(0, N1)  # first dimension\n    Nd = N1\n    for d in range(1, dim):\n        Ndm1 = Nd\n        Nd = Nd * N1\n        kvec[0:d+1, 0:Nd] = np.vstack([\n            np.tile(kvec[0:d, 0:Ndm1], (1, N1)),\n            np.reshape(np.tile(np.arange(0, N1), (Ndm1, 1)), (1, Nd), order=\"F\")\n        ])\n\n    kvec = kvec[:, 1: Nall]  # remove the zero wavenumber\n    whZero = np.sum(kvec == 0, axis=0)\n    abfac = a ** (dim - whZero) * b ** whZero\n    kbar = np.prod(np.maximum(kvec, 1), axis=0)\n    totfac = abfac / (kbar ** rfun)\n\n    f_c = a * np.random.randn(1, Nall - 1) * totfac\n    f_s = a * np.random.randn(1, Nall - 1) * totfac\n\n    f_0 = c + (b ** dim) * np.random.randn()\n    argx = np.matmul((2 * np.pi * xpts), kvec)\n    f_c_ = f_c * np.cos(argx)\n    f_s_ = f_s * np.sin(argx)\n    fval = f_0 + np.sum(f_c_ + f_s_, axis=1)\n    return fval\n</pre> def f_rand(xpts, rfun, a, b, c, seed):     dim = xpts.shape[1]     np.random.seed(seed)  # initialize random number generator for reproducability     N1 = int(2 ** np.floor(16 / dim))     Nall = N1 ** dim     kvec = np.zeros([dim, Nall])  # initialize kvec     kvec[0, 0:N1] = range(0, N1)  # first dimension     Nd = N1     for d in range(1, dim):         Ndm1 = Nd         Nd = Nd * N1         kvec[0:d+1, 0:Nd] = np.vstack([             np.tile(kvec[0:d, 0:Ndm1], (1, N1)),             np.reshape(np.tile(np.arange(0, N1), (Ndm1, 1)), (1, Nd), order=\"F\")         ])      kvec = kvec[:, 1: Nall]  # remove the zero wavenumber     whZero = np.sum(kvec == 0, axis=0)     abfac = a ** (dim - whZero) * b ** whZero     kbar = np.prod(np.maximum(kvec, 1), axis=0)     totfac = abfac / (kbar ** rfun)      f_c = a * np.random.randn(1, Nall - 1) * totfac     f_s = a * np.random.randn(1, Nall - 1) * totfac      f_0 = c + (b ** dim) * np.random.randn()     argx = np.matmul((2 * np.pi * xpts), kvec)     f_c_ = f_c * np.cos(argx)     f_s_ = f_s * np.sin(argx)     fval = f_0 + np.sum(f_c_ + f_s_, axis=1)     return fval <p>Periodization transforms</p> In\u00a0[8]: Copied! <pre>def doPeriodTx(x, integrand, ptransform):\n    ptransform = ptransform.upper()\n    if ptransform == 'BAKER':  # Baker's transform\n        xp = 1 - 2 * abs(x - 1 / 2)\n        w = 1\n    elif ptransform == 'C0':  # C^0 transform\n        xp = 3 * x ** 2 - 2 * x ** 3\n        w = prod(6 * x * (1 - x), 1)\n    elif ptransform == 'C1':  # C^1 transform\n        xp = x ** 3 * (10 - 15 * x + 6 * x ** 2)\n        w = prod(30 * x ** 2 * (1 - x) ** 2, 1)\n    elif ptransform == 'C1SIN':  # Sidi C^1 transform\n        xp = x - sin(2 * pi * x) / (2 * pi)\n        w = prod(2 * sin(pi * x) ** 2, 1)\n    elif ptransform == 'C2SIN':  # Sidi C^2 transform\n        xp = (8 - 9 * cos(pi * x) + cos(3 * pi * x)) / 16  # psi3\n        w = prod((9 * sin(pi * x) * pi - sin(3 * pi * x) * 3 * pi) / 16, 1)  # psi3_1\n    elif ptransform == 'C3SIN':  # Sidi C^3 transform\n        xp = (12 * pi * x - 8 * sin(2 * pi * x) + sin(4 * pi * x)) / (12 * pi)  # psi4\n        w = prod((12 * pi - 8 * cos(2 * pi * x) * 2 * pi + sin(4 * pi * x) * 4 * pi) / (12 * pi), 1)  # psi4_1\n    elif ptransform == 'NONE':\n        xp = x\n        w = 1\n    else:\n        raise (f\"The {ptransform} periodization transform is not implemented\")\n    y = integrand(xp) * w\n    return y\n</pre> def doPeriodTx(x, integrand, ptransform):     ptransform = ptransform.upper()     if ptransform == 'BAKER':  # Baker's transform         xp = 1 - 2 * abs(x - 1 / 2)         w = 1     elif ptransform == 'C0':  # C^0 transform         xp = 3 * x ** 2 - 2 * x ** 3         w = prod(6 * x * (1 - x), 1)     elif ptransform == 'C1':  # C^1 transform         xp = x ** 3 * (10 - 15 * x + 6 * x ** 2)         w = prod(30 * x ** 2 * (1 - x) ** 2, 1)     elif ptransform == 'C1SIN':  # Sidi C^1 transform         xp = x - sin(2 * pi * x) / (2 * pi)         w = prod(2 * sin(pi * x) ** 2, 1)     elif ptransform == 'C2SIN':  # Sidi C^2 transform         xp = (8 - 9 * cos(pi * x) + cos(3 * pi * x)) / 16  # psi3         w = prod((9 * sin(pi * x) * pi - sin(3 * pi * x) * 3 * pi) / 16, 1)  # psi3_1     elif ptransform == 'C3SIN':  # Sidi C^3 transform         xp = (12 * pi * x - 8 * sin(2 * pi * x) + sin(4 * pi * x)) / (12 * pi)  # psi4         w = prod((12 * pi - 8 * cos(2 * pi * x) * 2 * pi + sin(4 * pi * x) * 4 * pi) / (12 * pi), 1)  # psi4_1     elif ptransform == 'NONE':         xp = x         w = 1     else:         raise (f\"The {ptransform} periodization transform is not implemented\")     y = integrand(xp) * w     return y <p>Utility function to draw qqplot or normplot</p> In\u00a0[9]: Copied! <pre>def create_quant_plot(type, vz_real, fName, dim, iii, r, rOpt, theta, thetaOpt):\n    hFigNormplot, axFigNormplot = plt.subplots()\n\n    n = len(vz_real)\n    if type == 'normplot':\n        axFigNormplot.normplot(vz_real)\n    else:\n        q = (np.arange(1, n + 1) - 1 / 2) / n\n        stNorm = gaussnorm.ppf(q)  # norminv: quantiles of standard normal\n        axFigNormplot.scatter(stNorm, sorted(vz_real), s=20)  # marker='.',\n        axFigNormplot.plot([-3, 3], [-3, 3], marker='_', linewidth=4, color='red')\n        axFigNormplot.set_xlabel('Standard Gaussian Quantiles')\n        axFigNormplot.set_ylabel('Data Quantiles')\n\n    if theta:\n        plt_title = f'$d={dim}, n={n}, r={r:1.2f}, r_{{opt}}={rOpt:1.2f}, \\\\theta={theta:1.2f}, \\\\theta_{{opt}}={thetaOpt:1.2f}$'\n        plt_filename = f'{fName}-QQPlot_n-{n}_d-{dim}_r-{r * 100}_th-{100 * theta}_case-{iii}.jpg'\n    else:\n        plt_title = f'$d={dim}, n={n}, r_{{opt}}={rOpt:1.2f}, \\\\theta_{{opt}}={thetaOpt:1.2f}$'\n        plt_filename = f'{fName}-QQPlot_n-{n}_d-{dim}_case-{iii}.jpg'\n        \n    axFigNormplot.set_title(plt_title)\n    hFigNormplot.savefig(OUTDIR+plt_filename)\n</pre> def create_quant_plot(type, vz_real, fName, dim, iii, r, rOpt, theta, thetaOpt):     hFigNormplot, axFigNormplot = plt.subplots()      n = len(vz_real)     if type == 'normplot':         axFigNormplot.normplot(vz_real)     else:         q = (np.arange(1, n + 1) - 1 / 2) / n         stNorm = gaussnorm.ppf(q)  # norminv: quantiles of standard normal         axFigNormplot.scatter(stNorm, sorted(vz_real), s=20)  # marker='.',         axFigNormplot.plot([-3, 3], [-3, 3], marker='_', linewidth=4, color='red')         axFigNormplot.set_xlabel('Standard Gaussian Quantiles')         axFigNormplot.set_ylabel('Data Quantiles')      if theta:         plt_title = f'$d={dim}, n={n}, r={r:1.2f}, r_{{opt}}={rOpt:1.2f}, \\\\theta={theta:1.2f}, \\\\theta_{{opt}}={thetaOpt:1.2f}$'         plt_filename = f'{fName}-QQPlot_n-{n}_d-{dim}_r-{r * 100}_th-{100 * theta}_case-{iii}.jpg'     else:         plt_title = f'$d={dim}, n={n}, r_{{opt}}={rOpt:1.2f}, \\\\theta_{{opt}}={thetaOpt:1.2f}$'         plt_filename = f'{fName}-QQPlot_n-{n}_d-{dim}_case-{iii}.jpg'              axFigNormplot.set_title(plt_title)     hFigNormplot.savefig(OUTDIR+plt_filename) <p>Utility function to plot the objective function and minimum</p> In\u00a0[10]: Copied! <pre>def create_surf_plot(fName, lnthth, lnordord, objfun, objobj, lnParamsOpt, r, theta, iii):\n    figH, axH = plt.subplots(subplot_kw={\"projection\": \"3d\"})\n    axH.view_init(40, 30)\n    shandle = axH.plot_surface(lnthth, lnordord, objobj, cmap=cm.coolwarm,\n                               linewidth=0, antialiased=False, alpha=0.8)\n    xt = np.array([.2, 0.4, 1, 3, 7])\n    axH.set_xticks(np.log(xt))\n    axH.set_xticklabels(xt.astype(str))\n    yt = np.array([1.4, 1.6, 2, 2.6, 3.7])\n    axH.set_yticks(np.log(yt - 1))\n    axH.set_yticklabels(yt.astype(str))\n    axH.set_xlabel('$\\\\theta$')\n    axH.set_ylabel('$r$')    \n\n    axH.scatter(lnParamsOpt[0], lnParamsOpt[1], objfun(lnParamsOpt) * 1.002,\n                s=200, color='orange', marker='*', alpha=0.8)\n    if theta:\n        filename = f'{fName}-ObjFun_n-{npts}_d-{dim}_r-{r * 100}_th-{100 * theta}_case-{iii}.jpg'\n    else:\n        filename = f'{fName}-ObjFun_n-{npts}_d-{dim}_case-{iii}.jpg'                \n    figH.savefig(OUTDIR+filename)    \n</pre> def create_surf_plot(fName, lnthth, lnordord, objfun, objobj, lnParamsOpt, r, theta, iii):     figH, axH = plt.subplots(subplot_kw={\"projection\": \"3d\"})     axH.view_init(40, 30)     shandle = axH.plot_surface(lnthth, lnordord, objobj, cmap=cm.coolwarm,                                linewidth=0, antialiased=False, alpha=0.8)     xt = np.array([.2, 0.4, 1, 3, 7])     axH.set_xticks(np.log(xt))     axH.set_xticklabels(xt.astype(str))     yt = np.array([1.4, 1.6, 2, 2.6, 3.7])     axH.set_yticks(np.log(yt - 1))     axH.set_yticklabels(yt.astype(str))     axH.set_xlabel('$\\\\theta$')     axH.set_ylabel('$r$')          axH.scatter(lnParamsOpt[0], lnParamsOpt[1], objfun(lnParamsOpt) * 1.002,                 s=200, color='orange', marker='*', alpha=0.8)     if theta:         filename = f'{fName}-ObjFun_n-{npts}_d-{dim}_r-{r * 100}_th-{100 * theta}_case-{iii}.jpg'     else:         filename = f'{fName}-ObjFun_n-{npts}_d-{dim}_case-{iii}.jpg'                     figH.savefig(OUTDIR+filename)     <p>Minimum working example to demonstrate Gaussian diagnostics concept</p> In\u00a0[13]: Copied! <pre>def gaussian_diagnostics_engine(whEx, dim, npts, r, fpar, nReps, nPlots):\n    whEx = whEx - 1\n    fNames = ['ExpCos', 'Keister', 'rand']\n    ptransforms = ['none', 'C1sin', 'none']\n    fName = fNames[whEx]\n    ptransform = ptransforms[whEx]\n\n    rOptAll = [0]*nRep\n    thOptAll = [0]*nRep\n\n    # parameters for random function\n    # seed = 202326\n    if whEx == 2:\n        rfun = r / 2\n        f_mean = fpar[2]\n        f_std_a = fpar[0]  # this is square root of the a in the talk\n        f_std_b = fpar[1]  # this is square root of the b in the talk\n        theta = (f_std_a / f_std_b) ** 2\n    else:\n        theta = None\n\n    for iii in range(nReps):\n        seed = np.random.randint(low=1, high=1e6)  # different each rep\n        shift = np.random.rand(1, dim)\n\n        distribution = Lattice(dimension=dim, order='linear')\n        xpts  = distribution.gen_samples(n_min=0, n_max=npts, warn=False)\n        xlat = (xpts-distribution.shift)%1\n        if fName == 'ExpCos':\n            integrand = lambda x: np.exp(np.sum(np.cos(2 * np.pi * x), axis=1))\n        elif fName == 'Keister':\n            keister = Keister(Lattice(dimension=dim, order='linear'))\n            integrand = lambda x: keister.f(x)\n        elif fName == 'rand':\n            integrand = lambda x: f_rand(x, rfun, f_std_a, f_std_b, f_mean, seed)\n        else:\n            print('Invalid function name')\n            return\n\n        y = doPeriodTx(xpts, integrand, ptransform)\n\n        ftilde = np.fft.fft(y)  # fourier coefficients\n        ftilde[0] = 0  # ftilde = \\mV**H(\\vf - m \\vone), subtract mean\n        if dim == 1:\n            hFigIntegrand = plt.figure()\n            plt.scatter(xpts, y, 10)\n            plt.title(f'{fName}_n-{npts}_Tx-{ptransform}')\n            hFigIntegrand.savefig(OUTDIR+f'{fName}_n-{npts}_Tx-{ptransform}_rFun-{rfun:1.2f}.png')\n\n        def objfun(lnParams):\n            loss, Lambda, RKHSnorm = ObjectiveFunction(np.exp(lnParams[0]), 1 + np.exp(lnParams[1]), xlat, ftilde)\n            return loss\n\n        ## Plot the objective function\n        lnthetarange = np.arange(-2, 2.2, 0.2)  # range of log(theta) for plotting\n        lnorderrange = np.arange(-1, 1.1, 0.1)  # range of log(r) for plotting\n        [lnthth, lnordord] = np.meshgrid(lnthetarange, lnorderrange)\n        objobj = np.zeros(lnthth.shape)\n        for ii in range(lnthth.shape[0]):\n            for jj in range(lnthth.shape[1]):\n                objobj[ii, jj] = objfun([lnthth[ii, jj], lnordord[ii, jj]])\n\n        objMinAppx, which = objobj.min(), objobj.argmin()\n        # [whichrow, whichcol] = ind2sub(lnthth.shape, which)\n        [whichrow, whichcol] = np.unravel_index(which, lnthth.shape)\n        lnthOptAppx = lnthth[whichrow, whichcol]\n        thetaOptAppx = np.exp(lnthOptAppx)\n        lnordOptAppx = lnordord[whichrow, whichcol]\n        orderOptAppx = 1 + np.exp(lnordOptAppx)\n        # print(objMinAppx)  # minimum objective function by brute force search\n\n        ## Optimize the objective function\n        result = fminsearch(objfun, x0=[lnthOptAppx, lnordOptAppx], xtol=1e-3, full_output=True, disp=False)\n        lnParamsOpt, objMin = result[0], result[1]\n        # print(objMin)  # minimum objective function by Nelder-Mead\n        thetaOpt = np.exp(lnParamsOpt[0])\n        rOpt = 1 + np.exp(lnParamsOpt[1])\n        rOptAll[iii] = rOpt\n        thOptAll[iii] = thetaOpt\n        print(f'{iii}: thetaOptAppx={thetaOptAppx:7.5f}, rOptAppx={orderOptAppx:7.5f}, '\n              f'objMinAppx={objMinAppx:7.5f}, objMin={objMin:7.5f}')\n\n        if iii &lt;= nPlots:\n            create_surf_plot(fName, lnthth, lnordord, objfun, objobj, lnParamsOpt, r, theta, iii)\n\n        vlambda = kernel2(thetaOpt, rOpt, xlat)\n        s2 = sum(abs(ftilde[2:] ** 2) / vlambda[2:]) / (npts ** 2)\n        vlambda = s2 * vlambda\n\n        # apply transform\n        # $\\vZ = \\frac 1n \\mV \\mLambda**{-\\frac 12} \\mV**H(\\vf - m \\vone)$\n        # np.fft also includes 1/n division\n        vz = np.fft.ifft(ftilde / np.sqrt(vlambda))\n        vz_real = np.real(vz)  # vz must be real as intended by the transformation\n\n        if iii &lt;= nPlots:\n            create_quant_plot('qqplot', vz_real, fName, dim, iii, r, rOpt, theta, thetaOpt)\n\n        r_str = f\"{r: 7.5f}\" if type(r) == float else str(r)\n        theta_str = f\"{theta: 7.5f}\" if type(theta) == float else str(theta)\n        print(f'\\t r = {r_str}, rOpt = {rOpt:7.5f}, theta = {theta_str}, thetaOpt = {thetaOpt:7.5f}\\n')\n\n    return [theta, rOptAll, thOptAll, fName]\n</pre> def gaussian_diagnostics_engine(whEx, dim, npts, r, fpar, nReps, nPlots):     whEx = whEx - 1     fNames = ['ExpCos', 'Keister', 'rand']     ptransforms = ['none', 'C1sin', 'none']     fName = fNames[whEx]     ptransform = ptransforms[whEx]      rOptAll = [0]*nRep     thOptAll = [0]*nRep      # parameters for random function     # seed = 202326     if whEx == 2:         rfun = r / 2         f_mean = fpar[2]         f_std_a = fpar[0]  # this is square root of the a in the talk         f_std_b = fpar[1]  # this is square root of the b in the talk         theta = (f_std_a / f_std_b) ** 2     else:         theta = None      for iii in range(nReps):         seed = np.random.randint(low=1, high=1e6)  # different each rep         shift = np.random.rand(1, dim)          distribution = Lattice(dimension=dim, order='linear')         xpts  = distribution.gen_samples(n_min=0, n_max=npts, warn=False)         xlat = (xpts-distribution.shift)%1         if fName == 'ExpCos':             integrand = lambda x: np.exp(np.sum(np.cos(2 * np.pi * x), axis=1))         elif fName == 'Keister':             keister = Keister(Lattice(dimension=dim, order='linear'))             integrand = lambda x: keister.f(x)         elif fName == 'rand':             integrand = lambda x: f_rand(x, rfun, f_std_a, f_std_b, f_mean, seed)         else:             print('Invalid function name')             return          y = doPeriodTx(xpts, integrand, ptransform)          ftilde = np.fft.fft(y)  # fourier coefficients         ftilde[0] = 0  # ftilde = \\mV**H(\\vf - m \\vone), subtract mean         if dim == 1:             hFigIntegrand = plt.figure()             plt.scatter(xpts, y, 10)             plt.title(f'{fName}_n-{npts}_Tx-{ptransform}')             hFigIntegrand.savefig(OUTDIR+f'{fName}_n-{npts}_Tx-{ptransform}_rFun-{rfun:1.2f}.png')          def objfun(lnParams):             loss, Lambda, RKHSnorm = ObjectiveFunction(np.exp(lnParams[0]), 1 + np.exp(lnParams[1]), xlat, ftilde)             return loss          ## Plot the objective function         lnthetarange = np.arange(-2, 2.2, 0.2)  # range of log(theta) for plotting         lnorderrange = np.arange(-1, 1.1, 0.1)  # range of log(r) for plotting         [lnthth, lnordord] = np.meshgrid(lnthetarange, lnorderrange)         objobj = np.zeros(lnthth.shape)         for ii in range(lnthth.shape[0]):             for jj in range(lnthth.shape[1]):                 objobj[ii, jj] = objfun([lnthth[ii, jj], lnordord[ii, jj]])          objMinAppx, which = objobj.min(), objobj.argmin()         # [whichrow, whichcol] = ind2sub(lnthth.shape, which)         [whichrow, whichcol] = np.unravel_index(which, lnthth.shape)         lnthOptAppx = lnthth[whichrow, whichcol]         thetaOptAppx = np.exp(lnthOptAppx)         lnordOptAppx = lnordord[whichrow, whichcol]         orderOptAppx = 1 + np.exp(lnordOptAppx)         # print(objMinAppx)  # minimum objective function by brute force search          ## Optimize the objective function         result = fminsearch(objfun, x0=[lnthOptAppx, lnordOptAppx], xtol=1e-3, full_output=True, disp=False)         lnParamsOpt, objMin = result[0], result[1]         # print(objMin)  # minimum objective function by Nelder-Mead         thetaOpt = np.exp(lnParamsOpt[0])         rOpt = 1 + np.exp(lnParamsOpt[1])         rOptAll[iii] = rOpt         thOptAll[iii] = thetaOpt         print(f'{iii}: thetaOptAppx={thetaOptAppx:7.5f}, rOptAppx={orderOptAppx:7.5f}, '               f'objMinAppx={objMinAppx:7.5f}, objMin={objMin:7.5f}')          if iii &lt;= nPlots:             create_surf_plot(fName, lnthth, lnordord, objfun, objobj, lnParamsOpt, r, theta, iii)          vlambda = kernel2(thetaOpt, rOpt, xlat)         s2 = sum(abs(ftilde[2:] ** 2) / vlambda[2:]) / (npts ** 2)         vlambda = s2 * vlambda          # apply transform         # $\\vZ = \\frac 1n \\mV \\mLambda**{-\\frac 12} \\mV**H(\\vf - m \\vone)$         # np.fft also includes 1/n division         vz = np.fft.ifft(ftilde / np.sqrt(vlambda))         vz_real = np.real(vz)  # vz must be real as intended by the transformation          if iii &lt;= nPlots:             create_quant_plot('qqplot', vz_real, fName, dim, iii, r, rOpt, theta, thetaOpt)          r_str = f\"{r: 7.5f}\" if type(r) == float else str(r)         theta_str = f\"{theta: 7.5f}\" if type(theta) == float else str(theta)         print(f'\\t r = {r_str}, rOpt = {rOpt:7.5f}, theta = {theta_str}, thetaOpt = {thetaOpt:7.5f}\\n')      return [theta, rOptAll, thOptAll, fName] In\u00a0[14]: Copied! <pre>fwh = 1\ndim = 3\nnpts = 2 ** 6\nnRep = 20\nnPlot = 2\n[_, rOptAll, thOptAll, fName] = \\\n    gaussian_diagnostics_engine(fwh, dim, npts, None, None, nRep, nPlot)\n\n## Plot Exponential Cosine example\nfigH = plt.figure()\nplt.scatter(rOptAll, thOptAll, s=20, color='blue')\n# axis([4 6 0.1 10])\n# set(gca,'yscale','log')\nplt.title(f'$d = {dim}, n = {npts}$')\nplt.xlabel('Inferred $r$')\nplt.ylabel('Inferred $\\\\theta$')\n# print(f'{fName}-rthInfer-n-{npts}-d-{dim}')\nfigH.savefig(OUTDIR+f'{fName}-rthInfer-n-{npts}-d-{dim}.jpg')\n</pre> fwh = 1 dim = 3 npts = 2 ** 6 nRep = 20 nPlot = 2 [_, rOptAll, thOptAll, fName] = \\     gaussian_diagnostics_engine(fwh, dim, npts, None, None, nRep, nPlot)  ## Plot Exponential Cosine example figH = plt.figure() plt.scatter(rOptAll, thOptAll, s=20, color='blue') # axis([4 6 0.1 10]) # set(gca,'yscale','log') plt.title(f'$d = {dim}, n = {npts}$') plt.xlabel('Inferred $r$') plt.ylabel('Inferred $\\\\theta$') # print(f'{fName}-rthInfer-n-{npts}-d-{dim}') figH.savefig(OUTDIR+f'{fName}-rthInfer-n-{npts}-d-{dim}.jpg') <pre>0: thetaOptAppx=0.36788, rOptAppx=3.71828, objMinAppx=8.99406, objMin=8.93753\n\t r = None, rOpt = 4.48471, theta = None, thetaOpt = 0.41416\n\n1: thetaOptAppx=0.44933, rOptAppx=3.71828, objMinAppx=9.07259, objMin=9.02278\n\t r = None, rOpt = 4.46910, theta = None, thetaOpt = 0.40931\n\n2: thetaOptAppx=0.44933, rOptAppx=3.71828, objMinAppx=8.82430, objMin=8.66711\n\t r = None, rOpt = 5.06778, theta = None, thetaOpt = 0.37222\n\n3: thetaOptAppx=0.36788, rOptAppx=3.71828, objMinAppx=8.70725, objMin=8.54519\n\t r = None, rOpt = 5.06449, theta = None, thetaOpt = 0.35315\n\n4: thetaOptAppx=0.54881, rOptAppx=3.71828, objMinAppx=9.44731, objMin=9.43752\n\t r = None, rOpt = 4.04484, theta = None, thetaOpt = 0.50257\n\n5: thetaOptAppx=0.44933, rOptAppx=3.71828, objMinAppx=9.05749, objMin=8.99965\n\t r = None, rOpt = 4.52695, theta = None, thetaOpt = 0.47314\n\n6: thetaOptAppx=0.36788, rOptAppx=3.71828, objMinAppx=8.80768, objMin=8.65659\n\t r = None, rOpt = 5.07095, theta = None, thetaOpt = 0.34964\n\n7: thetaOptAppx=0.36788, rOptAppx=3.71828, objMinAppx=8.66418, objMin=8.43411\n\t r = None, rOpt = 5.39659, theta = None, thetaOpt = 0.31540\n\n8: thetaOptAppx=0.36788, rOptAppx=3.71828, objMinAppx=8.66215, objMin=8.45730\n\t r = None, rOpt = 4.90284, theta = None, thetaOpt = 0.40416\n\n9: thetaOptAppx=0.36788, rOptAppx=3.71828, objMinAppx=8.72574, objMin=8.56897\n\t r = None, rOpt = 5.03484, theta = None, thetaOpt = 0.31357\n\n10: thetaOptAppx=0.44933, rOptAppx=3.71828, objMinAppx=9.34300, objMin=9.31896\n\t r = None, rOpt = 4.30372, theta = None, thetaOpt = 0.50097\n\n11: thetaOptAppx=0.36788, rOptAppx=3.71828, objMinAppx=8.75005, objMin=8.62916\n\t r = None, rOpt = 4.83743, theta = None, thetaOpt = 0.35090\n\n12: thetaOptAppx=0.36788, rOptAppx=3.71828, objMinAppx=8.87981, objMin=8.74778\n\t r = None, rOpt = 5.00868, theta = None, thetaOpt = 0.36687\n\n13: thetaOptAppx=0.36788, rOptAppx=3.71828, objMinAppx=8.93996, objMin=8.87062\n\t r = None, rOpt = 4.62564, theta = None, thetaOpt = 0.37758\n\n14: thetaOptAppx=0.36788, rOptAppx=3.71828, objMinAppx=8.79702, objMin=8.66612\n\t r = None, rOpt = 4.93187, theta = None, thetaOpt = 0.32310\n\n15: thetaOptAppx=0.36788, rOptAppx=3.71828, objMinAppx=8.83119, objMin=8.65661\n\t r = None, rOpt = 5.30991, theta = None, thetaOpt = 0.36354\n\n16: thetaOptAppx=0.36788, rOptAppx=3.71828, objMinAppx=8.69747, objMin=8.53796\n\t r = None, rOpt = 4.89472, theta = None, thetaOpt = 0.36814\n\n17: thetaOptAppx=0.44933, rOptAppx=3.71828, objMinAppx=9.24219, objMin=9.20810\n\t r = None, rOpt = 4.35318, theta = None, thetaOpt = 0.47006\n\n18: thetaOptAppx=0.36788, rOptAppx=3.71828, objMinAppx=8.71916, objMin=8.52738\n\t r = None, rOpt = 5.23099, theta = None, thetaOpt = 0.36578\n\n19: thetaOptAppx=0.36788, rOptAppx=3.71828, objMinAppx=8.84619, objMin=8.63422\n\t r = None, rOpt = 5.26904, theta = None, thetaOpt = 0.41594\n\n</pre> In\u00a0[15]: Copied! <pre># close all the previous plots to freeup memory\nplt.close('all')\n</pre> # close all the previous plots to freeup memory plt.close('all') In\u00a0[16]: Copied! <pre>## Tests with random function\nrArray = [1.5, 2, 4]\nnrArr = len(rArray)\nfParArray = [[0.5, 1, 2], [1, 1, 1], [1, 1, 1]]\nnfPArr = len(fParArray)\nfwh = 3\ndim = 2\nnpts = 2 ** 6\nnRep = 5  # reduced from 20 to reduce the plots\nnPlot = 2\nthetaAll = np.zeros((nrArr, nfPArr))\nrOptAll = np.zeros((nrArr, nfPArr, nRep))\nthOptAll = np.zeros((nrArr, nfPArr, nRep))\nfor jjj in range(nrArr):\n    for kkk in range(nfPArr):\n        thetaAll[jjj, kkk], rOptAll[jjj, kkk, :], thOptAll[jjj, kkk, :], fName = \\\n            gaussian_diagnostics_engine(fwh, dim, npts, rArray[jjj], fParArray[kkk], nRep, nPlot)\n</pre> ## Tests with random function rArray = [1.5, 2, 4] nrArr = len(rArray) fParArray = [[0.5, 1, 2], [1, 1, 1], [1, 1, 1]] nfPArr = len(fParArray) fwh = 3 dim = 2 npts = 2 ** 6 nRep = 5  # reduced from 20 to reduce the plots nPlot = 2 thetaAll = np.zeros((nrArr, nfPArr)) rOptAll = np.zeros((nrArr, nfPArr, nRep)) thOptAll = np.zeros((nrArr, nfPArr, nRep)) for jjj in range(nrArr):     for kkk in range(nfPArr):         thetaAll[jjj, kkk], rOptAll[jjj, kkk, :], thOptAll[jjj, kkk, :], fName = \\             gaussian_diagnostics_engine(fwh, dim, npts, rArray[jjj], fParArray[kkk], nRep, nPlot)  <pre>0: thetaOptAppx=0.24660, rOptAppx=1.40657, objMinAppx=7.14637, objMin=7.14629\n\t r =  1.50000, rOpt = 1.40165, theta =  0.25000, thetaOpt = 0.25968\n\n1: thetaOptAppx=0.54881, rOptAppx=1.36788, objMinAppx=6.86055, objMin=6.82118\n\t r =  1.50000, rOpt = 1.00000, theta =  0.25000, thetaOpt = 0.27367\n\n2: thetaOptAppx=0.81873, rOptAppx=1.49659, objMinAppx=7.29922, objMin=7.29904\n\t r =  1.50000, rOpt = 1.51253, theta =  0.25000, thetaOpt = 0.89951\n\n3: thetaOptAppx=0.20190, rOptAppx=1.36788, objMinAppx=6.85804, objMin=6.82400\n\t r =  1.50000, rOpt = 1.00334, theta =  0.25000, thetaOpt = 0.11906\n\n4: thetaOptAppx=0.36788, rOptAppx=1.36788, objMinAppx=7.08249, objMin=7.08235\n\t r =  1.50000, rOpt = 1.38827, theta =  0.25000, thetaOpt = 0.38423\n\n0: thetaOptAppx=3.32012, rOptAppx=1.36788, objMinAppx=10.63981, objMin=10.61913\n\t r =  1.50000, rOpt = 1.00000, theta =  1.00000, thetaOpt = 2.13670\n\n1: thetaOptAppx=3.32012, rOptAppx=1.81873, objMinAppx=10.63974, objMin=10.63958\n\t r =  1.50000, rOpt = 1.82252, theta =  1.00000, thetaOpt = 3.13414\n\n2: thetaOptAppx=1.00000, rOptAppx=1.60653, objMinAppx=10.31446, objMin=10.31433\n\t r =  1.50000, rOpt = 1.58466, theta =  1.00000, thetaOpt = 1.00000\n\n3: thetaOptAppx=7.38906, rOptAppx=1.36788, objMinAppx=10.77169, objMin=10.73339\n\t r =  1.50000, rOpt = 1.00000, theta =  1.00000, thetaOpt = 8.50588\n\n4: thetaOptAppx=3.32012, rOptAppx=1.36788, objMinAppx=10.90059, objMin=10.89579\n\t r =  1.50000, rOpt = 1.19371, theta =  1.00000, thetaOpt = 2.19191\n\n0: thetaOptAppx=3.32012, rOptAppx=1.36788, objMinAppx=10.57812, objMin=10.54021\n\t r =  1.50000, rOpt = 1.00000, theta =  1.00000, thetaOpt = 1.88466\n\n1: thetaOptAppx=2.22554, rOptAppx=1.36788, objMinAppx=10.38092, objMin=10.35134\n\t r =  1.50000, rOpt = 1.00000, theta =  1.00000, thetaOpt = 1.41946\n\n2: thetaOptAppx=1.00000, rOptAppx=1.60653, objMinAppx=10.49902, objMin=10.49902\n\t r =  1.50000, rOpt = 1.60511, theta =  1.00000, thetaOpt = 1.00000\n\n3: thetaOptAppx=7.38906, rOptAppx=1.36788, objMinAppx=10.61599, objMin=10.56274\n\t r =  1.50000, rOpt = 1.00000, theta =  1.00000, thetaOpt = 5933827228175301.00000\n\n4: thetaOptAppx=3.32012, rOptAppx=1.44933, objMinAppx=10.99286, objMin=10.99283\n\t r =  1.50000, rOpt = 1.44032, theta =  1.00000, thetaOpt = 3.41295\n\n0: thetaOptAppx=0.67032, rOptAppx=2.22140, objMinAppx=6.41351, objMin=6.41233\n\t r = 2, rOpt = 2.28962, theta =  0.25000, thetaOpt = 0.69375\n\n1: thetaOptAppx=0.24660, rOptAppx=1.81873, objMinAppx=5.83228, objMin=5.83186\n\t r = 2, rOpt = 1.83344, theta =  0.25000, thetaOpt = 0.22655\n\n</pre> <pre>/var/folders/rz/_ktvltjs49v_z33w0h6njx5w0000gn/T/ipykernel_70657/2271441997.py:2: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n  figH, axH = plt.subplots(subplot_kw={\"projection\": \"3d\"})\n</pre> <pre>2: thetaOptAppx=0.20190, rOptAppx=1.81873, objMinAppx=5.99832, objMin=5.99775\n\t r = 2, rOpt = 1.78303, theta =  0.25000, thetaOpt = 0.18575\n\n3: thetaOptAppx=0.30119, rOptAppx=1.74082, objMinAppx=6.18621, objMin=6.18560\n\t r = 2, rOpt = 1.77182, theta =  0.25000, thetaOpt = 0.33864\n\n4: thetaOptAppx=0.30119, rOptAppx=1.74082, objMinAppx=5.98469, objMin=5.98449\n\t r = 2, rOpt = 1.73013, theta =  0.25000, thetaOpt = 0.28315\n\n0: thetaOptAppx=0.44933, rOptAppx=1.54881, objMinAppx=9.30998, objMin=9.30986\n\t r = 2, rOpt = 1.56218, theta =  1.00000, thetaOpt = 0.44163\n\n1: thetaOptAppx=1.22140, rOptAppx=2.34986, objMinAppx=9.45655, objMin=9.45601\n\t r = 2, rOpt = 2.30027, theta =  1.00000, thetaOpt = 1.11415\n\n2: thetaOptAppx=1.49182, rOptAppx=1.90484, objMinAppx=9.80753, objMin=9.80727\n\t r = 2, rOpt = 1.88085, theta =  1.00000, thetaOpt = 1.52755\n\n3: thetaOptAppx=1.49182, rOptAppx=1.90484, objMinAppx=9.83320, objMin=9.83283\n\t r = 2, rOpt = 1.91255, theta =  1.00000, thetaOpt = 1.62186\n\n4: thetaOptAppx=2.22554, rOptAppx=2.10517, objMinAppx=9.68118, objMin=9.68072\n\t r = 2, rOpt = 2.12564, theta =  1.00000, thetaOpt = 2.09482\n\n0: thetaOptAppx=7.38906, rOptAppx=3.71828, objMinAppx=9.61269, objMin=9.45113\n</pre> <pre>/var/folders/rz/_ktvltjs49v_z33w0h6njx5w0000gn/T/ipykernel_70657/4237111989.py:93: RuntimeWarning: invalid value encountered in sqrt\n  vz = np.fft.ifft(ftilde / np.sqrt(vlambda))\n</pre> <pre>\t r = 2, rOpt = 3.53413, theta =  1.00000, thetaOpt = 14.37293\n\n1: thetaOptAppx=2.22554, rOptAppx=1.74082, objMinAppx=9.35057, objMin=9.35008\n\t r = 2, rOpt = 1.78488, theta =  1.00000, thetaOpt = 2.37197\n\n2: thetaOptAppx=2.71828, rOptAppx=1.67032, objMinAppx=10.10692, objMin=10.10650\n\t r = 2, rOpt = 1.69607, theta =  1.00000, thetaOpt = 2.58742\n\n3: thetaOptAppx=2.22554, rOptAppx=1.90484, objMinAppx=9.55155, objMin=9.55141\n\t r = 2, rOpt = 1.90988, theta =  1.00000, thetaOpt = 2.36814\n\n4: thetaOptAppx=1.49182, rOptAppx=1.44933, objMinAppx=9.91009, objMin=9.91004\n\t r = 2, rOpt = 1.45254, theta =  1.00000, thetaOpt = 1.45063\n\n0: thetaOptAppx=0.16530, rOptAppx=3.01375, objMinAppx=3.22085, objMin=3.21902\n\t r = 4, rOpt = 2.93602, theta =  0.25000, thetaOpt = 0.13681\n\n1: thetaOptAppx=0.24660, rOptAppx=3.01375, objMinAppx=3.99176, objMin=3.94911\n\t r = 4, rOpt = 3.08101, theta =  0.25000, thetaOpt = 0.30249\n\n2: thetaOptAppx=0.67032, rOptAppx=3.45960, objMinAppx=3.70233, objMin=3.70125\n\t r = 4, rOpt = 3.46039, theta =  0.25000, thetaOpt = 0.71094\n\n3: thetaOptAppx=0.67032, rOptAppx=3.71828, objMinAppx=3.76987, objMin=3.76755\n\t r = 4, rOpt = 3.62029, theta =  0.25000, thetaOpt = 0.64446\n\n4: thetaOptAppx=0.44933, rOptAppx=3.71828, objMinAppx=3.89213, objMin=3.89083\n\t r = 4, rOpt = 3.75463, theta =  0.25000, thetaOpt = 0.43057\n\n0: thetaOptAppx=1.00000, rOptAppx=3.71828, objMinAppx=7.04925, objMin=7.04883\n\t r = 4, rOpt = 3.68451, theta =  1.00000, thetaOpt = 1.00000\n\n1: thetaOptAppx=1.49182, rOptAppx=3.71828, objMinAppx=6.97971, objMin=6.95183\n\t r = 4, rOpt = 4.13146, theta =  1.00000, thetaOpt = 1.72858\n\n2: thetaOptAppx=1.22140, rOptAppx=3.71828, objMinAppx=7.23932, objMin=7.18983\n\t r = 4, rOpt = 4.27292, theta =  1.00000, thetaOpt = 1.87128\n\n3: thetaOptAppx=2.71828, rOptAppx=3.71828, objMinAppx=7.40308, objMin=7.36397\n\t r = 4, rOpt = 3.58459, theta =  1.00000, thetaOpt = 4.17137\n\n</pre> <pre>/var/folders/rz/_ktvltjs49v_z33w0h6njx5w0000gn/T/ipykernel_70657/4237111989.py:93: RuntimeWarning: invalid value encountered in sqrt\n  vz = np.fft.ifft(ftilde / np.sqrt(vlambda))\n</pre> <pre>4: thetaOptAppx=1.22140, rOptAppx=3.45960, objMinAppx=7.10667, objMin=7.10617\n\t r = 4, rOpt = 3.50364, theta =  1.00000, thetaOpt = 1.21819\n\n0: thetaOptAppx=1.22140, rOptAppx=3.71828, objMinAppx=7.16822, objMin=7.15972\n\t r = 4, rOpt = 3.97812, theta =  1.00000, thetaOpt = 1.55273\n\n1: thetaOptAppx=1.82212, rOptAppx=3.71828, objMinAppx=7.36156, objMin=7.34469\n\t r = 4, rOpt = 3.57702, theta =  1.00000, thetaOpt = 1.85305\n\n2: thetaOptAppx=1.82212, rOptAppx=3.71828, objMinAppx=7.05146, objMin=7.04365\n\t r = 4, rOpt = 3.94260, theta =  1.00000, thetaOpt = 1.72259\n\n3: thetaOptAppx=1.22140, rOptAppx=3.22554, objMinAppx=6.98588, objMin=6.97968\n\t r = 4, rOpt = 3.20799, theta =  1.00000, thetaOpt = 1.06432\n\n4: thetaOptAppx=1.22140, rOptAppx=3.71828, objMinAppx=6.77223, objMin=6.75074\n\t r = 4, rOpt = 4.11981, theta =  1.00000, thetaOpt = 1.31325\n\n</pre> In\u00a0[17]: Copied! <pre># close all the previous plots to freeup memory\nplt.close('all')\n</pre> # close all the previous plots to freeup memory plt.close('all') In\u00a0[18]: Copied! <pre>figH, axH = plt.subplots()\ncolorArray = ['blue', 'orange', 'green', 'cyan', 'maroon', 'purple']\nnColArray = len(colorArray)\nfor jjj in range(nrArr):\n    for kkk in range(nfPArr):\n        clrInd = np.mod(nfPArr * (jjj) + kkk, nColArray)\n        clr = colorArray[clrInd]\n        axH.scatter(rOptAll[jjj, kkk, :].reshape((nRep, 1)), thOptAll[jjj, kkk, :].reshape((nRep, 1)),\n                 s=50, c=clr, marker='.')\n        axH.scatter(rArray[jjj], thetaAll[jjj, kkk], s=50, c=clr, marker='D')\n\naxH.set(xlim=[1, 6], ylim=[0.01, 100])\naxH.set_yscale('log')\naxH.set_title(f'$d = {dim}, n = {npts}$')\naxH.set_xlabel('Inferred $r$')\naxH.set_ylabel('Inferred $\\\\theta$')\nfigH.savefig(OUTDIR+f'{fName}-rthInfer-n-{npts}-d-{dim}.jpg')\n</pre> figH, axH = plt.subplots() colorArray = ['blue', 'orange', 'green', 'cyan', 'maroon', 'purple'] nColArray = len(colorArray) for jjj in range(nrArr):     for kkk in range(nfPArr):         clrInd = np.mod(nfPArr * (jjj) + kkk, nColArray)         clr = colorArray[clrInd]         axH.scatter(rOptAll[jjj, kkk, :].reshape((nRep, 1)), thOptAll[jjj, kkk, :].reshape((nRep, 1)),                  s=50, c=clr, marker='.')         axH.scatter(rArray[jjj], thetaAll[jjj, kkk], s=50, c=clr, marker='D')  axH.set(xlim=[1, 6], ylim=[0.01, 100]) axH.set_yscale('log') axH.set_title(f'$d = {dim}, n = {npts}$') axH.set_xlabel('Inferred $r$') axH.set_ylabel('Inferred $\\\\theta$') figH.savefig(OUTDIR+f'{fName}-rthInfer-n-{npts}-d-{dim}.jpg') In\u00a0[19]: Copied! <pre># close all the previous plots to freeup memory\nplt.close('all')\n</pre> # close all the previous plots to freeup memory plt.close('all') In\u00a0[20]: Copied! <pre>## Keister example\nfwh = 2\ndim = 3\nnpts = 2 ** 6\nnRep = 20\nnPlot = 2\n_, rOptAll, thOptAll, fName = gaussian_diagnostics_engine(fwh, dim, npts, None, None, nRep, nPlot)\n\n## Plot Keister example\nfigH = plt.figure()\nplt.scatter(rOptAll, thOptAll, s=20, color='blue')\n# axis([4 6 0.5 1.5])\n# set(gca,'yscale','log')\nplt.xlabel('Inferred $r$')\nplt.ylabel('Inferred $\\\\theta$')\nplt.title(f'$d = {dim}, n = {npts}$')\nfigH.savefig(OUTDIR+f'{fName}-rthInfer-n-{npts}-d-{dim}.jpg')\n</pre> ## Keister example fwh = 2 dim = 3 npts = 2 ** 6 nRep = 20 nPlot = 2 _, rOptAll, thOptAll, fName = gaussian_diagnostics_engine(fwh, dim, npts, None, None, nRep, nPlot)  ## Plot Keister example figH = plt.figure() plt.scatter(rOptAll, thOptAll, s=20, color='blue') # axis([4 6 0.5 1.5]) # set(gca,'yscale','log') plt.xlabel('Inferred $r$') plt.ylabel('Inferred $\\\\theta$') plt.title(f'$d = {dim}, n = {npts}$') figH.savefig(OUTDIR+f'{fName}-rthInfer-n-{npts}-d-{dim}.jpg')  <pre>0: thetaOptAppx=0.67032, rOptAppx=3.71828, objMinAppx=10.95471, objMin=10.85287\n\t r = None, rOpt = 4.89011, theta = None, thetaOpt = 0.67160\n\n1: thetaOptAppx=0.81873, rOptAppx=3.71828, objMinAppx=10.97614, objMin=10.79973\n\t r = None, rOpt = 5.23417, theta = None, thetaOpt = 0.74064\n\n2: thetaOptAppx=0.81873, rOptAppx=3.71828, objMinAppx=11.01901, objMin=10.86699\n\t r = None, rOpt = 5.12195, theta = None, thetaOpt = 0.82888\n\n3: thetaOptAppx=0.67032, rOptAppx=3.71828, objMinAppx=11.03911, objMin=10.96269\n\t r = None, rOpt = 4.64960, theta = None, thetaOpt = 0.70302\n\n4: thetaOptAppx=0.81873, rOptAppx=3.71828, objMinAppx=11.07413, objMin=10.90736\n\t r = None, rOpt = 5.33026, theta = None, thetaOpt = 0.74945\n\n5: thetaOptAppx=0.81873, rOptAppx=3.71828, objMinAppx=11.16827, objMin=11.06970\n\t r = None, rOpt = 4.91810, theta = None, thetaOpt = 0.84299\n\n6: thetaOptAppx=0.81873, rOptAppx=3.71828, objMinAppx=11.11317, objMin=10.97738\n\t r = None, rOpt = 5.07012, theta = None, thetaOpt = 0.80554\n\n7: thetaOptAppx=0.81873, rOptAppx=3.71828, objMinAppx=11.04175, objMin=10.87923\n\t r = None, rOpt = 5.16208, theta = None, thetaOpt = 0.91717\n\n8: thetaOptAppx=0.81873, rOptAppx=3.71828, objMinAppx=11.24635, objMin=11.17003\n\t r = None, rOpt = 4.71247, theta = None, thetaOpt = 0.88444\n\n9: thetaOptAppx=0.67032, rOptAppx=3.71828, objMinAppx=10.98831, objMin=10.88986\n\t r = None, rOpt = 4.82958, theta = None, thetaOpt = 0.70734\n\n10: thetaOptAppx=0.81873, rOptAppx=3.71828, objMinAppx=11.05779, objMin=10.95596\n\t r = None, rOpt = 4.87874, theta = None, thetaOpt = 0.75186\n\n11: thetaOptAppx=0.81873, rOptAppx=3.71828, objMinAppx=11.08281, objMin=10.96752\n\t r = None, rOpt = 4.98171, theta = None, thetaOpt = 0.80117\n\n12: thetaOptAppx=0.81873, rOptAppx=3.71828, objMinAppx=11.17064, objMin=11.04885\n\t r = None, rOpt = 5.18872, theta = None, thetaOpt = 0.68626\n\n13: thetaOptAppx=0.81873, rOptAppx=3.71828, objMinAppx=10.97924, objMin=10.81975\n\t r = None, rOpt = 5.08933, theta = None, thetaOpt = 0.89380\n\n14: thetaOptAppx=0.81873, rOptAppx=3.71828, objMinAppx=11.19044, objMin=11.09012\n\t r = None, rOpt = 4.88557, theta = None, thetaOpt = 0.79560\n\n15: thetaOptAppx=0.67032, rOptAppx=3.71828, objMinAppx=10.93972, objMin=10.84403\n\t r = None, rOpt = 4.78263, theta = None, thetaOpt = 0.70404\n\n16: thetaOptAppx=0.81873, rOptAppx=3.71828, objMinAppx=11.07454, objMin=10.96320\n\t r = None, rOpt = 4.92201, theta = None, thetaOpt = 0.72613\n\n17: thetaOptAppx=0.67032, rOptAppx=3.71828, objMinAppx=10.98796, objMin=10.90210\n\t r = None, rOpt = 4.85879, theta = None, thetaOpt = 0.59679\n\n18: thetaOptAppx=0.81873, rOptAppx=3.71828, objMinAppx=11.11301, objMin=10.99340\n\t r = None, rOpt = 5.06588, theta = None, thetaOpt = 0.73588\n\n19: thetaOptAppx=0.81873, rOptAppx=3.71828, objMinAppx=11.15762, objMin=11.04832\n\t r = None, rOpt = 4.99360, theta = None, thetaOpt = 0.74271\n\n</pre> In\u00a0[21]: Copied! <pre>## Keister example\nfwh = 2\ndim = 3\nnpts = 2 ** 10\nnRep = 20\nnPlot = 2\n_, rOptAll, thOptAll, fName = gaussian_diagnostics_engine(fwh, dim, npts, None, None, nRep, nPlot)\n\n## Plot Keister example\nfigH = plt.figure()\nplt.scatter(rOptAll, thOptAll, s=20, color='blue')\n# axis([4 6 0.5 1.5])\n# set(gca,'yscale','log')\nplt.xlabel('Inferred $r$')\nplt.ylabel('Inferred $\\\\theta$')\nplt.title(f'$d = {dim}, n = {npts}$')\nfigH.savefig(OUTDIR+f'{fName}-rthInfer-n-{npts}-d-{dim}.jpg')\n</pre> ## Keister example fwh = 2 dim = 3 npts = 2 ** 10 nRep = 20 nPlot = 2 _, rOptAll, thOptAll, fName = gaussian_diagnostics_engine(fwh, dim, npts, None, None, nRep, nPlot)  ## Plot Keister example figH = plt.figure() plt.scatter(rOptAll, thOptAll, s=20, color='blue') # axis([4 6 0.5 1.5]) # set(gca,'yscale','log') plt.xlabel('Inferred $r$') plt.ylabel('Inferred $\\\\theta$') plt.title(f'$d = {dim}, n = {npts}$') figH.savefig(OUTDIR+f'{fName}-rthInfer-n-{npts}-d-{dim}.jpg') <pre>0: thetaOptAppx=1.00000, rOptAppx=3.71828, objMinAppx=10.85760, objMin=10.69482\n\t r = None, rOpt = 3.89437, theta = None, thetaOpt = 1.00000\n\n</pre> <pre>/var/folders/rz/_ktvltjs49v_z33w0h6njx5w0000gn/T/ipykernel_70657/4237111989.py:93: RuntimeWarning: invalid value encountered in sqrt\n  vz = np.fft.ifft(ftilde / np.sqrt(vlambda))\n</pre> <pre>1: thetaOptAppx=0.67032, rOptAppx=3.71828, objMinAppx=10.45233, objMin=6.74852\n\t r = None, rOpt = 7.22861, theta = None, thetaOpt = 0.67986\n\n2: thetaOptAppx=0.67032, rOptAppx=3.71828, objMinAppx=10.44838, objMin=6.69462\n\t r = None, rOpt = 7.27867, theta = None, thetaOpt = 0.76628\n\n3: thetaOptAppx=0.67032, rOptAppx=3.71828, objMinAppx=10.44846, objMin=6.67804\n\t r = None, rOpt = 7.36369, theta = None, thetaOpt = 0.72433\n\n4: thetaOptAppx=0.67032, rOptAppx=3.71828, objMinAppx=10.44802, objMin=6.67556\n\t r = None, rOpt = 7.33379, theta = None, thetaOpt = 0.75321\n\n5: thetaOptAppx=1.00000, rOptAppx=3.71828, objMinAppx=11.13038, objMin=11.11848\n\t r = None, rOpt = 3.72240, theta = None, thetaOpt = 1.00000\n\n</pre> <pre>/var/folders/rz/_ktvltjs49v_z33w0h6njx5w0000gn/T/ipykernel_70657/4237111989.py:93: RuntimeWarning: invalid value encountered in sqrt\n  vz = np.fft.ifft(ftilde / np.sqrt(vlambda))\n</pre> <pre>6: thetaOptAppx=0.67032, rOptAppx=3.71828, objMinAppx=10.45040, objMin=6.71403\n\t r = None, rOpt = 7.30300, theta = None, thetaOpt = 0.69226\n\n7: thetaOptAppx=0.67032, rOptAppx=3.71828, objMinAppx=10.45274, objMin=6.74595\n\t r = None, rOpt = 7.27447, theta = None, thetaOpt = 0.72335\n\n8: thetaOptAppx=0.67032, rOptAppx=3.71828, objMinAppx=10.45001, objMin=6.74406\n\t r = None, rOpt = 7.26377, theta = None, thetaOpt = 0.68239\n\n9: thetaOptAppx=0.81873, rOptAppx=3.71828, objMinAppx=10.52787, objMin=10.14371\n\t r = None, rOpt = 4.10478, theta = None, thetaOpt = 0.81984\n\n10: thetaOptAppx=1.00000, rOptAppx=3.71828, objMinAppx=10.59215, objMin=10.55441\n\t r = None, rOpt = 3.72814, theta = None, thetaOpt = 1.00000\n\n11: thetaOptAppx=1.22140, rOptAppx=3.45960, objMinAppx=11.24312, objMin=11.16721\n\t r = None, rOpt = 3.53017, theta = None, thetaOpt = 1.22183\n\n12: thetaOptAppx=1.00000, rOptAppx=3.71828, objMinAppx=11.06674, objMin=11.04394\n\t r = None, rOpt = 3.78815, theta = None, thetaOpt = 1.00000\n\n13: thetaOptAppx=0.13534, rOptAppx=1.36788, objMinAppx=    nan, objMin=    nan\n\t r = None, rOpt = 1.36788, theta = None, thetaOpt = 0.13534\n\n14: thetaOptAppx=0.67032, rOptAppx=3.71828, objMinAppx=10.44974, objMin=6.68183\n\t r = None, rOpt = 7.31864, theta = None, thetaOpt = 0.77082\n\n15: thetaOptAppx=0.67032, rOptAppx=3.71828, objMinAppx=10.53690, objMin=10.04346\n\t r = None, rOpt = 4.14307, theta = None, thetaOpt = 0.69386\n\n16: thetaOptAppx=1.00000, rOptAppx=3.71828, objMinAppx=10.87803, objMin=10.83007\n\t r = None, rOpt = 3.73673, theta = None, thetaOpt = 1.00000\n\n17: thetaOptAppx=0.67032, rOptAppx=3.71828, objMinAppx=10.44751, objMin=6.70502\n\t r = None, rOpt = 7.30048, theta = None, thetaOpt = 0.74603\n\n18: thetaOptAppx=0.67032, rOptAppx=3.71828, objMinAppx=10.57719, objMin=9.35705\n\t r = None, rOpt = 4.85788, theta = None, thetaOpt = 0.75327\n\n19: thetaOptAppx=0.67032, rOptAppx=3.71828, objMinAppx=10.44925, objMin=6.63290\n\t r = None, rOpt = 7.40375, theta = None, thetaOpt = 0.76246\n\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"demos/gaussian_diagnostics/gaussian_diagnostics_demo/#gaussian-diagnostics","title":"Gaussian Diagnostics\u00b6","text":"<p>Experiments to demonstrate Gaussian assumption used in <code>cubBayesLattice</code></p>"},{"location":"demos/gaussian_diagnostics/gaussian_diagnostics_demo/#example-1-exponential-of-cosine","title":"Example 1: Exponential of Cosine\u00b6","text":""},{"location":"demos/gaussian_diagnostics/gaussian_diagnostics_demo/#example-2-random-function","title":"Example 2: Random function\u00b6","text":""},{"location":"demos/gaussian_diagnostics/gaussian_diagnostics_demo/#plot-additional-figures-for-random-function","title":"Plot additional figures for random function\u00b6","text":""},{"location":"demos/gaussian_diagnostics/gaussian_diagnostics_demo/#example-3a-keister-integrand-npts-64","title":"Example 3a: Keister integrand: npts = 64\u00b6","text":""},{"location":"demos/gaussian_diagnostics/gaussian_diagnostics_demo/#example-3b-keister-integrand-npts-1024","title":"Example 3b: Keister integrand: npts = 1024\u00b6","text":""},{"location":"demos/talk_paper_demos/pydata.chi.2023/","title":"2023 PyData Chicago Talk","text":"In\u00a0[1]: Copied! <pre>import qmcpy as qp\nimport numpy as np\nimport scipy.stats\nimport pandas as pd\nimport time\nfrom matplotlib import pyplot\ncolors = pyplot.rcParams['axes.prop_cycle'].by_key()['color']\n%matplotlib inline\n</pre> import qmcpy as qp import numpy as np import scipy.stats import pandas as pd import time from matplotlib import pyplot colors = pyplot.rcParams['axes.prop_cycle'].by_key()['color'] %matplotlib inline In\u00a0[2]: Copied! <pre>iid = qp.IIDStdUniform(dimension=3)\niid.gen_samples(n=4)\n</pre> iid = qp.IIDStdUniform(dimension=3) iid.gen_samples(n=4) Out[2]: <pre>array([[0.61584953, 0.08238197, 0.79597353],\n       [0.47972561, 0.36991422, 0.4520753 ],\n       [0.9705357 , 0.20129176, 0.05264096],\n       [0.12821756, 0.27695067, 0.4200091 ]])</pre> In\u00a0[3]: Copied! <pre>iid.gen_samples(4)\n</pre> iid.gen_samples(4) Out[3]: <pre>array([[0.75988525, 0.06535736, 0.88857573],\n       [0.93750365, 0.82111122, 0.62840155],\n       [0.73705715, 0.22116385, 0.86364701],\n       [0.76327139, 0.67695714, 0.73070693]])</pre> In\u00a0[4]: Copied! <pre>iid\n</pre> iid Out[4]: <pre>IIDStdUniform (AbstractIIDDiscreteDistribution)\n    d               3\n    replications    1\n    entropy         255445230289856938445909338179220309576</pre> In\u00a0[5]: Copied! <pre>ld_lattice = qp.Lattice(3)\nld_lattice.gen_samples(4)\n</pre> ld_lattice = qp.Lattice(3) ld_lattice.gen_samples(4) Out[5]: <pre>array([[0.7323192 , 0.47466547, 0.80517287],\n       [0.2323192 , 0.97466547, 0.30517287],\n       [0.9823192 , 0.22466547, 0.55517287],\n       [0.4823192 , 0.72466547, 0.05517287]])</pre> In\u00a0[6]: Copied! <pre>ld_lattice.gen_samples(4)\n</pre> ld_lattice.gen_samples(4) Out[6]: <pre>array([[0.7323192 , 0.47466547, 0.80517287],\n       [0.2323192 , 0.97466547, 0.30517287],\n       [0.9823192 , 0.22466547, 0.55517287],\n       [0.4823192 , 0.72466547, 0.05517287]])</pre> In\u00a0[7]: Copied! <pre>ld_lattice.gen_samples(n_min=2,n_max=4)\n</pre> ld_lattice.gen_samples(n_min=2,n_max=4) Out[7]: <pre>array([[0.9823192 , 0.22466547, 0.55517287],\n       [0.4823192 , 0.72466547, 0.05517287]])</pre> In\u00a0[8]: Copied! <pre>ld_lattice\n</pre> ld_lattice Out[8]: <pre>Lattice (AbstractLDDiscreteDistribution)\n    d               3\n    replications    1\n    randomize       SHIFT\n    gen_vec_source  kuo.lattice-33002-1024-1048576.9125.txt\n    order           RADICAL INVERSE\n    n_limit         2^(20)\n    entropy         218929209270440955509681448220425620652</pre> In\u00a0[9]: Copied! <pre>n = 2**7 # Lattice and Digital Net prefer powers of 2 sample sizes\ndiscrete_distribs = {\n    'IID': qp.IIDStdUniform(2),\n    'LD Lattice': qp.Lattice(2),\n    'LD Digital Net': qp.DigitalNetB2(2),\n    'LD Halton': qp.Halton(2)}\nfig,ax = pyplot.subplots(nrows=1,ncols=len(discrete_distribs),figsize=(3*len(discrete_distribs),3))\nax = np.atleast_1d(ax)\nfor i,(name,discrete_distrib) in enumerate(discrete_distribs.items()):\n    x = discrete_distrib.gen_samples(n)\n    ax[i].scatter(x[:,0],x[:,1],s=5,color=colors[i])\n    ax[i].set_title(name)\n    ax[i].set_aspect(1)\n    ax[i].set_xlabel(r'$X_{i0}$'); ax[i].set_ylabel(r'$X_{i1}$')\n    ax[i].set_xlim([0,1]); ax[i].set_ylim([0,1])\n    ax[i].set_xticks([0,1]); ax[i].set_yticks([0,1])\n</pre> n = 2**7 # Lattice and Digital Net prefer powers of 2 sample sizes discrete_distribs = {     'IID': qp.IIDStdUniform(2),     'LD Lattice': qp.Lattice(2),     'LD Digital Net': qp.DigitalNetB2(2),     'LD Halton': qp.Halton(2)} fig,ax = pyplot.subplots(nrows=1,ncols=len(discrete_distribs),figsize=(3*len(discrete_distribs),3)) ax = np.atleast_1d(ax) for i,(name,discrete_distrib) in enumerate(discrete_distribs.items()):     x = discrete_distrib.gen_samples(n)     ax[i].scatter(x[:,0],x[:,1],s=5,color=colors[i])     ax[i].set_title(name)     ax[i].set_aspect(1)     ax[i].set_xlabel(r'$X_{i0}$'); ax[i].set_ylabel(r'$X_{i1}$')     ax[i].set_xlim([0,1]); ax[i].set_ylim([0,1])     ax[i].set_xticks([0,1]); ax[i].set_yticks([0,1]) In\u00a0[10]: Copied! <pre>m_min,m_max = 6,8\nfig,ax = pyplot.subplots(nrows=1,ncols=len(discrete_distribs),figsize=(3*len(discrete_distribs),3))\nax = np.atleast_1d(ax)\nfor i,(name,discrete_distrib) in enumerate(discrete_distribs.items()):\n    x = discrete_distrib.gen_samples(2**m_max)\n    n_min = 0\n    for m in range(m_min,m_max+1):\n        n_max = 2**m\n        ax[i].scatter(x[n_min:n_max,0],x[n_min:n_max,1],s=5,color=colors[m-m_min],label='n_min = %d, n_max = %d'%(n_min,n_max))\n        n_min = 2**m\n    ax[i].set_title(name)\n    ax[i].set_aspect(1)\n    ax[i].set_xlabel(r'$X_{i0}$'); ax[i].set_ylabel(r'$X_{i1}$')\n    ax[i].set_xlim([0,1]); ax[i].set_ylim([0,1])\n    ax[i].set_xticks([0,1]); ax[i].set_yticks([0,1])\n</pre> m_min,m_max = 6,8 fig,ax = pyplot.subplots(nrows=1,ncols=len(discrete_distribs),figsize=(3*len(discrete_distribs),3)) ax = np.atleast_1d(ax) for i,(name,discrete_distrib) in enumerate(discrete_distribs.items()):     x = discrete_distrib.gen_samples(2**m_max)     n_min = 0     for m in range(m_min,m_max+1):         n_max = 2**m         ax[i].scatter(x[n_min:n_max,0],x[n_min:n_max,1],s=5,color=colors[m-m_min],label='n_min = %d, n_max = %d'%(n_min,n_max))         n_min = 2**m     ax[i].set_title(name)     ax[i].set_aspect(1)     ax[i].set_xlabel(r'$X_{i0}$'); ax[i].set_ylabel(r'$X_{i1}$')     ax[i].set_xlim([0,1]); ax[i].set_ylim([0,1])     ax[i].set_xticks([0,1]); ax[i].set_yticks([0,1]) In\u00a0[11]: Copied! <pre>discrete_distrib = qp.DigitalNetB2(4)\nx = discrete_distrib(2**7)\nd = discrete_distrib.d\nassert d&gt;=2\nfig,ax = pyplot.subplots(nrows=d,ncols=d,figsize=(3*d,3*d))\nfor i in range(d):\n    fig.delaxes(ax[i,i])\n    for j in range(i):\n        ax[i,j].scatter(x[:,i],x[:,j],s=5)\n        fig.delaxes(ax[j,i])\n        ax[i,j].set_aspect(1)\n        ax[i,j].set_xlabel(r'$X_{i%d}$'%i); ax[i,j].set_ylabel(r'$X_{i%d}$'%j)\n        ax[i,j].set_xlim([0,1]); ax[i,j].set_ylim([0,1])\n        ax[i,j].set_xticks([0,1]); ax[i,j].set_yticks([0,1])\n</pre> discrete_distrib = qp.DigitalNetB2(4) x = discrete_distrib(2**7) d = discrete_distrib.d assert d&gt;=2 fig,ax = pyplot.subplots(nrows=d,ncols=d,figsize=(3*d,3*d)) for i in range(d):     fig.delaxes(ax[i,i])     for j in range(i):         ax[i,j].scatter(x[:,i],x[:,j],s=5)         fig.delaxes(ax[j,i])         ax[i,j].set_aspect(1)         ax[i,j].set_xlabel(r'$X_{i%d}$'%i); ax[i,j].set_ylabel(r'$X_{i%d}$'%j)         ax[i,j].set_xlim([0,1]); ax[i,j].set_ylim([0,1])         ax[i,j].set_xticks([0,1]); ax[i,j].set_yticks([0,1]) In\u00a0[12]: Copied! <pre>discrete_distrib = qp.Halton(3)\ntrue_measure = qp.Gaussian(discrete_distrib,mean=[1,2,3],covariance=[4,5,6])\ntrue_measure.gen_samples(4)\n</pre> discrete_distrib = qp.Halton(3) true_measure = qp.Gaussian(discrete_distrib,mean=[1,2,3],covariance=[4,5,6]) true_measure.gen_samples(4) Out[12]: <pre>array([[ 2.18348289,  4.27924141,  1.15343988],\n       [-0.37229914,  2.15929196,  4.43156846],\n       [ 0.92311253, -0.56539553,  1.3713884 ],\n       [ 4.69593279,  3.41246141,  4.68304706]])</pre> In\u00a0[13]: Copied! <pre>true_measure.gen_samples(n_min=2,n_max=4)\n</pre> true_measure.gen_samples(n_min=2,n_max=4) Out[13]: <pre>array([[ 0.92311253, -0.56539553,  1.3713884 ],\n       [ 4.69593279,  3.41246141,  4.68304706]])</pre> In\u00a0[14]: Copied! <pre>true_measure\n</pre> true_measure Out[14]: <pre>Gaussian (AbstractTrueMeasure)\n    mean            [1 2 3]\n    covariance      [4 5 6]\n    decomp_type     PCA</pre> In\u00a0[15]: Copied! <pre>n = 2**7\ndiscrete_distrib = qp.DigitalNetB2(2)\ntrue_measures = {\n    'Non-Standard Uniform': qp.Uniform(discrete_distrib,lower_bound=[-3,-2],upper_bound=[3,2]),\n    'Standard Gaussian': qp.Gaussian(discrete_distrib),\n    'Non-Standard Gaussian': qp.Gaussian(discrete_distrib,mean=[1,2],covariance=[[5,4],[4,9]]),\n    'SciPy Based\\nIndependent Beta-Gamma': qp.SciPyWrapper(discrete_distrib,[scipy.stats.beta(a=1,b=5),scipy.stats.gamma(a=1)])}\nfig,ax = pyplot.subplots(nrows=1,ncols=len(true_measures),figsize=(3*len(true_measures),3))\nax = np.atleast_1d(ax)\nfor i,(name,true_measure) in enumerate(true_measures.items()):\n    t = true_measure.gen_samples(n)\n    ax[i].scatter(t[:,0],t[:,1],s=5,color=colors[i])\n    ax[i].set_title(name)    \n</pre> n = 2**7 discrete_distrib = qp.DigitalNetB2(2) true_measures = {     'Non-Standard Uniform': qp.Uniform(discrete_distrib,lower_bound=[-3,-2],upper_bound=[3,2]),     'Standard Gaussian': qp.Gaussian(discrete_distrib),     'Non-Standard Gaussian': qp.Gaussian(discrete_distrib,mean=[1,2],covariance=[[5,4],[4,9]]),     'SciPy Based\\nIndependent Beta-Gamma': qp.SciPyWrapper(discrete_distrib,[scipy.stats.beta(a=1,b=5),scipy.stats.gamma(a=1)])} fig,ax = pyplot.subplots(nrows=1,ncols=len(true_measures),figsize=(3*len(true_measures),3)) ax = np.atleast_1d(ax) for i,(name,true_measure) in enumerate(true_measures.items()):     t = true_measure.gen_samples(n)     ax[i].scatter(t[:,0],t[:,1],s=5,color=colors[i])     ax[i].set_title(name)     In\u00a0[16]: Copied! <pre>n = 32\ndiscrete_distrib = qp.Lattice(365)\nbrownian_motions = {\n    'Standard Brownian Motion': qp.BrownianMotion(discrete_distrib),\n    'Drifted Brownian Motion': qp.BrownianMotion(discrete_distrib,t_final=5,initial_value=5,drift=-1,diffusion=2)}\nfig,ax = pyplot.subplots(nrows=len(brownian_motions),ncols=1,figsize=(6,3*len(brownian_motions)))\nax = np.atleast_1d(ax)\nfor i,(name,brownian_motion) in enumerate(brownian_motions.items()):\n    t = brownian_motion.gen_samples(n)\n    t_w_init = np.hstack([brownian_motion.initial_value*np.ones((n,1)),t])\n    tvec_w_0 = np.hstack([0,brownian_motion.time_vec])\n    ax[i].plot(tvec_w_0,t_w_init.T)\n    ax[i].set_xlim([tvec_w_0[0],tvec_w_0[-1]])\n    ax[i].set_title(name)\n</pre> n = 32 discrete_distrib = qp.Lattice(365) brownian_motions = {     'Standard Brownian Motion': qp.BrownianMotion(discrete_distrib),     'Drifted Brownian Motion': qp.BrownianMotion(discrete_distrib,t_final=5,initial_value=5,drift=-1,diffusion=2)} fig,ax = pyplot.subplots(nrows=len(brownian_motions),ncols=1,figsize=(6,3*len(brownian_motions))) ax = np.atleast_1d(ax) for i,(name,brownian_motion) in enumerate(brownian_motions.items()):     t = brownian_motion.gen_samples(n)     t_w_init = np.hstack([brownian_motion.initial_value*np.ones((n,1)),t])     tvec_w_0 = np.hstack([0,brownian_motion.time_vec])     ax[i].plot(tvec_w_0,t_w_init.T)     ax[i].set_xlim([tvec_w_0[0],tvec_w_0[-1]])     ax[i].set_title(name) In\u00a0[17]: Copied! <pre>def myfun(t): # define g, the ORIGINAL integrand \n    # t an (n,d) shaped np.ndarray of sample from the ORIGINAL (true) measure\n    y = t.sum(1)\n    return y # an (n,) shaped np.ndarray\ntrue_measure = qp.Gaussian(qp.Halton(5)) # LD Halton discrete distrib for QMC problem\nqp_myfun = qp.CustomFun(true_measure,myfun,parallel=False)\n</pre> def myfun(t): # define g, the ORIGINAL integrand      # t an (n,d) shaped np.ndarray of sample from the ORIGINAL (true) measure     y = t.sum(1)     return y # an (n,) shaped np.ndarray true_measure = qp.Gaussian(qp.Halton(5)) # LD Halton discrete distrib for QMC problem qp_myfun = qp.CustomFun(true_measure,myfun,parallel=False) In\u00a0[18]: Copied! <pre>x = qp_myfun.discrete_distrib.gen_samples(4) # samples from the TRANSFORMED measure\ny = qp_myfun.f(x) # evaluate the TRANSFORMED integrand at the TRANSFORMED samples\ny\n</pre> x = qp_myfun.discrete_distrib.gen_samples(4) # samples from the TRANSFORMED measure y = qp_myfun.f(x) # evaluate the TRANSFORMED integrand at the TRANSFORMED samples y Out[18]: <pre>array([1.80203064, 1.41177199, 2.76302476, 1.46425731])</pre> In\u00a0[19]: Copied! <pre>x = qp_myfun.discrete_distrib.gen_samples(2**16) # samples from the TRANSFORMED measure\ny = qp_myfun.f(x) # evaluate the TRANSFORMED integrand at the TRANSFORMED samples\nmu_hat = y.mean()\nmu_hat\n</pre> x = qp_myfun.discrete_distrib.gen_samples(2**16) # samples from the TRANSFORMED measure y = qp_myfun.f(x) # evaluate the TRANSFORMED integrand at the TRANSFORMED samples mu_hat = y.mean() mu_hat Out[19]: <pre>np.float64(-7.210079451455405e-07)</pre> In\u00a0[20]: Copied! <pre>asian_option = qp.FinancialOption(\n    sampler = qp.DigitalNetB2(52),\n    option = \"ASIAN\",\n    volatility = 1/2,\n    start_price = 30,\n    strike_price = 35,\n    interest_rate = 0.001,\n    t_final = 1,\n    call_put = 'call',\n    asian_mean = 'arithmetic')\nx = asian_option.discrete_distrib.gen_samples(2**16)\ny = asian_option.f(x)\nmu_hat = y.mean()\nmu_hat\n</pre> asian_option = qp.FinancialOption(     sampler = qp.DigitalNetB2(52),     option = \"ASIAN\",     volatility = 1/2,     start_price = 30,     strike_price = 35,     interest_rate = 0.001,     t_final = 1,     call_put = 'call',     asian_mean = 'arithmetic') x = asian_option.discrete_distrib.gen_samples(2**16) y = asian_option.f(x) mu_hat = y.mean() mu_hat Out[20]: <pre>np.float64(1.7888486351025943)</pre> In\u00a0[21]: Copied! <pre>n = 32\nkeister = qp.Keister(qp.DigitalNetB2(1))\nfig,ax = pyplot.subplots(nrows=1,ncols=2,figsize=(8,4))\nx = keister.discrete_distrib.gen_samples(n)\nt = keister.true_measure.gen_samples(n)\nf_of_x = keister.f(x).squeeze()\ng_of_t = keister.g(t).squeeze()\nassert (f_of_x==g_of_t).all()\nx_fine = np.linspace(0,1,257)[1:-1,None]\nf_of_xfine = keister.f(x_fine).squeeze()\nlb = 1.2*max(abs(t.min()),abs(t.max()))\nt_fine = np.linspace(-lb,lb,257)[:,None]\ng_of_tfine = keister.g(t_fine).squeeze()\nax[0].set_title(r'Original')\nax[0].set_xlabel(r'$T_i$'); ax[0].set_ylabel(r'$g(T_i) = g(\\Phi^{-1}(X_i))$')\nax[0].plot(t_fine.squeeze(),g_of_tfine,color=colors[0],alpha=.5)\nax[0].scatter(t.squeeze(),f_of_x,s=10,color='k')\nax[1].set_title(r'Transformed')\nax[1].set_xlabel(r'$X_i$'); ax[1].set_ylabel(r'$f(X_i)$')\nax[1].scatter(x.squeeze(),f_of_x,s=10,color='k')\nax[1].plot(x_fine.squeeze(),f_of_xfine,color=colors[1],alpha=.5);\n</pre> n = 32 keister = qp.Keister(qp.DigitalNetB2(1)) fig,ax = pyplot.subplots(nrows=1,ncols=2,figsize=(8,4)) x = keister.discrete_distrib.gen_samples(n) t = keister.true_measure.gen_samples(n) f_of_x = keister.f(x).squeeze() g_of_t = keister.g(t).squeeze() assert (f_of_x==g_of_t).all() x_fine = np.linspace(0,1,257)[1:-1,None] f_of_xfine = keister.f(x_fine).squeeze() lb = 1.2*max(abs(t.min()),abs(t.max())) t_fine = np.linspace(-lb,lb,257)[:,None] g_of_tfine = keister.g(t_fine).squeeze() ax[0].set_title(r'Original') ax[0].set_xlabel(r'$T_i$'); ax[0].set_ylabel(r'$g(T_i) = g(\\Phi^{-1}(X_i))$') ax[0].plot(t_fine.squeeze(),g_of_tfine,color=colors[0],alpha=.5) ax[0].scatter(t.squeeze(),f_of_x,s=10,color='k') ax[1].set_title(r'Transformed') ax[1].set_xlabel(r'$X_i$'); ax[1].set_ylabel(r'$f(X_i)$') ax[1].scatter(x.squeeze(),f_of_x,s=10,color='k') ax[1].plot(x_fine.squeeze(),f_of_xfine,color=colors[1],alpha=.5); In\u00a0[22]: Copied! <pre>problem_cmc = qp.FinancialOption(qp.IIDStdUniform(52),option=\"ASIAN\")\ncmc_stop_crit = qp.CubMCG(problem_cmc,abs_tol=0.025)\napprox_cmc,data_cmc = cmc_stop_crit.integrate()\ndata_cmc\n</pre> problem_cmc = qp.FinancialOption(qp.IIDStdUniform(52),option=\"ASIAN\") cmc_stop_crit = qp.CubMCG(problem_cmc,abs_tol=0.025) approx_cmc,data_cmc = cmc_stop_crit.integrate() data_cmc Out[22]: <pre>Data (Data)\n    solution        1.791\n    bound_low       1.766\n    bound_high      1.816\n    bound_diff      0.050\n    n_total         492693\n    time_integrate  1.217\nCubMCG (AbstractStoppingCriterion)\n    abs_tol         0.025\n    rel_tol         0\n    n_init          2^(10)\n    n_limit         2^(30)\n    inflate         1.200\n    alpha           0.010\n    kurtmax         1.478\nFinancialOption (AbstractIntegrand)\n    option          ASIAN\n    call_put        CALL\n    volatility      2^(-1)\n    start_price     30\n    strike_price    35\n    interest_rate   0\n    t_final         1\n    asian_mean      ARITHMETIC\nBrownianMotion (AbstractTrueMeasure)\n    time_vec        [0.019 0.038 0.058 ... 0.962 0.981 1.   ]\n    drift           0\n    mean            [0. 0. 0. ... 0. 0. 0.]\n    covariance      [[0.019 0.019 0.019 ... 0.019 0.019 0.019]\n                     [0.019 0.038 0.038 ... 0.038 0.038 0.038]\n                     [0.019 0.038 0.058 ... 0.058 0.058 0.058]\n                     ...\n                     [0.019 0.038 0.058 ... 0.962 0.962 0.962]\n                     [0.019 0.038 0.058 ... 0.962 0.981 0.981]\n                     [0.019 0.038 0.058 ... 0.962 0.981 1.   ]]\n    decomp_type     PCA\nIIDStdUniform (AbstractIIDDiscreteDistribution)\n    d               52\n    replications    1\n    entropy         97012320886264964284917995170158594466</pre> In\u00a0[23]: Copied! <pre>problem_qmc = qp.FinancialOption(qp.DigitalNetB2(52),option=\"ASIAN\")\nqmc_stop_crit = qp.CubQMCNetG(problem_qmc,abs_tol=0.025)\napprox_qmc,data_qmc = qmc_stop_crit.integrate()\ndata_qmc\n</pre> problem_qmc = qp.FinancialOption(qp.DigitalNetB2(52),option=\"ASIAN\") qmc_stop_crit = qp.CubQMCNetG(problem_qmc,abs_tol=0.025) approx_qmc,data_qmc = qmc_stop_crit.integrate() data_qmc Out[23]: <pre>Data (Data)\n    solution        1.773\n    comb_bound_low  1.749\n    comb_bound_high 1.797\n    comb_bound_diff 0.048\n    comb_flags      1\n    n_total         2^(10)\n    n               2^(10)\n    time_integrate  0.002\nCubQMCNetG (AbstractStoppingCriterion)\n    abs_tol         0.025\n    rel_tol         0\n    n_init          2^(10)\n    n_limit         2^(35)\nFinancialOption (AbstractIntegrand)\n    option          ASIAN\n    call_put        CALL\n    volatility      2^(-1)\n    start_price     30\n    strike_price    35\n    interest_rate   0\n    t_final         1\n    asian_mean      ARITHMETIC\nBrownianMotion (AbstractTrueMeasure)\n    time_vec        [0.019 0.038 0.058 ... 0.962 0.981 1.   ]\n    drift           0\n    mean            [0. 0. 0. ... 0. 0. 0.]\n    covariance      [[0.019 0.019 0.019 ... 0.019 0.019 0.019]\n                     [0.019 0.038 0.038 ... 0.038 0.038 0.038]\n                     [0.019 0.038 0.058 ... 0.058 0.058 0.058]\n                     ...\n                     [0.019 0.038 0.058 ... 0.962 0.962 0.962]\n                     [0.019 0.038 0.058 ... 0.962 0.981 0.981]\n                     [0.019 0.038 0.058 ... 0.962 0.981 1.   ]]\n    decomp_type     PCA\nDigitalNetB2 (AbstractLDDiscreteDistribution)\n    d               52\n    replications    1\n    randomize       LMS DS\n    gen_mats_source joe_kuo.6.21201.txt\n    order           RADICAL INVERSE\n    t               63\n    alpha           1\n    n_limit         2^(32)\n    entropy         273514699345978368463399322345138322896</pre> In\u00a0[24]: Copied! <pre>print('QMC took %.2f%% the time and %.2f%% the samples compared to CMC'%(\n      100*data_qmc.time_integrate/data_cmc.time_integrate,100*data_qmc.n_total/data_cmc.n_total))\n</pre> print('QMC took %.2f%% the time and %.2f%% the samples compared to CMC'%(       100*data_qmc.time_integrate/data_cmc.time_integrate,100*data_qmc.n_total/data_cmc.n_total)) <pre>QMC took 0.19% the time and 0.21% the samples compared to CMC\n</pre> In\u00a0[25]: Copied! <pre>cmc_tols = [1,.75,.5,.25,.1,.075,.05,.025]\nqmc_tols = [1,.5,.1,.05,.02,.01,.005,.002,.001]\nfig,ax = pyplot.subplots(nrows=1,ncols=2,figsize=(10,5))\nn_cmc,time_cmc = np.zeros_like(cmc_tols),np.zeros_like(cmc_tols)\nfor i,cmc_tol in enumerate(cmc_tols):\n    cmc_stop_crit = qp.CubMCG(qp.FinancialOption(qp.IIDStdUniform(52),option=\"ASIAN\"),abs_tol=cmc_tol)\n    approx_cmc,data_cmc = cmc_stop_crit.integrate()\n    n_cmc[i],time_cmc[i] = data_cmc.n_total,data_cmc.time_integrate\nax[0].plot(cmc_tols,n_cmc,'-o',color=colors[0],label=r'CMC, $\\mathcal{O}(\\varepsilon^{-2})$')\nax[1].plot(cmc_tols,time_cmc,'-o',color=colors[0])\nn_qmc,time_qmc = np.zeros_like(qmc_tols),np.zeros_like(qmc_tols)\nfor i,qmc_tol in enumerate(qmc_tols):\n    qmc_stop_crit = qp.CubQMCNetG(qp.FinancialOption(qp.DigitalNetB2(52),option=\"ASIAN\"),abs_tol=qmc_tol)\n    approx_qmc,data_qmc = qmc_stop_crit.integrate()\n    n_qmc[i],time_qmc[i] = data_qmc.n_total,data_qmc.time_integrate\nax[0].plot(qmc_tols[1:],n_qmc[1:],'-o',color=colors[1],label=r'QMC, $\\mathcal{O}(\\varepsilon^{-1})$')\nax[1].plot(qmc_tols[1:],time_qmc[1:],'-o',color=colors[1])\nax[0].set_xscale('log',base=10); ax[0].set_yscale('log',base=2)\nax[1].set_xscale('log',base=10); ax[1].set_yscale('log',base=10)\nax[0].invert_xaxis(); ax[1].invert_xaxis()\nax[0].set_xlabel(r'absolute tolerance $\\varepsilon$'); ax[1].set_xlabel(r'absolute tolerance $\\varepsilon$')\nax[0].set_ylabel(r'numer of samples $n$'); ax[1].set_ylabel('integration time')\nax[0].legend(loc='upper left');\n</pre> cmc_tols = [1,.75,.5,.25,.1,.075,.05,.025] qmc_tols = [1,.5,.1,.05,.02,.01,.005,.002,.001] fig,ax = pyplot.subplots(nrows=1,ncols=2,figsize=(10,5)) n_cmc,time_cmc = np.zeros_like(cmc_tols),np.zeros_like(cmc_tols) for i,cmc_tol in enumerate(cmc_tols):     cmc_stop_crit = qp.CubMCG(qp.FinancialOption(qp.IIDStdUniform(52),option=\"ASIAN\"),abs_tol=cmc_tol)     approx_cmc,data_cmc = cmc_stop_crit.integrate()     n_cmc[i],time_cmc[i] = data_cmc.n_total,data_cmc.time_integrate ax[0].plot(cmc_tols,n_cmc,'-o',color=colors[0],label=r'CMC, $\\mathcal{O}(\\varepsilon^{-2})$') ax[1].plot(cmc_tols,time_cmc,'-o',color=colors[0]) n_qmc,time_qmc = np.zeros_like(qmc_tols),np.zeros_like(qmc_tols) for i,qmc_tol in enumerate(qmc_tols):     qmc_stop_crit = qp.CubQMCNetG(qp.FinancialOption(qp.DigitalNetB2(52),option=\"ASIAN\"),abs_tol=qmc_tol)     approx_qmc,data_qmc = qmc_stop_crit.integrate()     n_qmc[i],time_qmc[i] = data_qmc.n_total,data_qmc.time_integrate ax[0].plot(qmc_tols[1:],n_qmc[1:],'-o',color=colors[1],label=r'QMC, $\\mathcal{O}(\\varepsilon^{-1})$') ax[1].plot(qmc_tols[1:],time_qmc[1:],'-o',color=colors[1]) ax[0].set_xscale('log',base=10); ax[0].set_yscale('log',base=2) ax[1].set_xscale('log',base=10); ax[1].set_yscale('log',base=10) ax[0].invert_xaxis(); ax[1].invert_xaxis() ax[0].set_xlabel(r'absolute tolerance $\\varepsilon$'); ax[1].set_xlabel(r'absolute tolerance $\\varepsilon$') ax[0].set_ylabel(r'numer of samples $n$'); ax[1].set_ylabel('integration time') ax[0].legend(loc='upper left'); In\u00a0[26]: Copied! <pre>qmc_stop_crit = qp.CubQMCCLT(\n    integrand = qp.CustomFun(\n        true_measure = qp.Uniform(sampler=qp.Halton(3,replications=32),lower_bound=0,upper_bound=np.pi),\n        g = lambda t: np.stack([np.cos(t).prod(-1),np.sin(t).prod(-1)],axis=0),\n        dimension_indv = 2),\n    abs_tol=.0001)\napprox,data = qmc_stop_crit.integrate()\ndata\n</pre> qmc_stop_crit = qp.CubQMCCLT(     integrand = qp.CustomFun(         true_measure = qp.Uniform(sampler=qp.Halton(3,replications=32),lower_bound=0,upper_bound=np.pi),         g = lambda t: np.stack([np.cos(t).prod(-1),np.sin(t).prod(-1)],axis=0),         dimension_indv = 2),     abs_tol=.0001) approx,data = qmc_stop_crit.integrate() data Out[26]: <pre>Data (Data)\n    solution        [1.006e-05 2.580e-01]\n    comb_bound_low  [-5.012e-05  2.579e-01]\n    comb_bound_high [7.024e-05 2.581e-01]\n    comb_bound_diff [0. 0.]\n    comb_flags      [ True  True]\n    n_total         262144\n    n               [262144 131072]\n    n_rep           [8192 4096]\n    time_integrate  0.348\nCubQMCRepStudentT (AbstractStoppingCriterion)\n    inflate         1\n    alpha           0.010\n    abs_tol         1.00e-04\n    rel_tol         0\n    n_init          2^(8)\n    n_limit         2^(30)\nCustomFun (AbstractIntegrand)\nUniform (AbstractTrueMeasure)\n    lower_bound     0\n    upper_bound     3.142\nHalton (AbstractLDDiscreteDistribution)\n    d               3\n    replications    2^(5)\n    randomize       LMS DP\n    t               63\n    n_limit         2^(32)\n    entropy         339443966251781763705539893307306305063</pre> In\u00a0[27]: Copied! <pre>class CovIntegrand(qp.integrand.Integrand):\n    def __init__(self, sampler):\n        self.sampler = sampler\n        self.true_measure = qp.Gaussian(sampler,mean=1)\n        super(CovIntegrand,self).__init__(dimension_indv=3,dimension_comb=(),parallel=False)\n    def g(self, t):\n        P = t.prod(1) # P\n        S = t.sum(1) # S\n        PS = P*S #PS\n        y = np.stack([PS,P,S],axis=0)\n        return y\n    def bound_fun(self, low, high):\n        comb_low = low[0]-max(low[1]*low[2],low[1]*high[2],high[1]*low[2],high[1]*high[2])\n        comb_high = high[0]-min(low[1]*low[2],low[1]*high[2],high[1]*low[2],high[1]*high[2])\n        return comb_low,comb_high\n    def dependency(self, comb_flag):\n        return np.tile(comb_flag,3)\napprox,data = qp.CubQMCLatticeG(CovIntegrand(qp.Lattice(10)),rel_tol=.025).integrate()\ndata\n</pre> class CovIntegrand(qp.integrand.Integrand):     def __init__(self, sampler):         self.sampler = sampler         self.true_measure = qp.Gaussian(sampler,mean=1)         super(CovIntegrand,self).__init__(dimension_indv=3,dimension_comb=(),parallel=False)     def g(self, t):         P = t.prod(1) # P         S = t.sum(1) # S         PS = P*S #PS         y = np.stack([PS,P,S],axis=0)         return y     def bound_fun(self, low, high):         comb_low = low[0]-max(low[1]*low[2],low[1]*high[2],high[1]*low[2],high[1]*high[2])         comb_high = high[0]-min(low[1]*low[2],low[1]*high[2],high[1]*low[2],high[1]*high[2])         return comb_low,comb_high     def dependency(self, comb_flag):         return np.tile(comb_flag,3) approx,data = qp.CubQMCLatticeG(CovIntegrand(qp.Lattice(10)),rel_tol=.025).integrate() data Out[27]: <pre>Data (Data)\n    solution        10.074\n    comb_bound_low  9.840\n    comb_bound_high 10.320\n    comb_bound_diff 0.480\n    comb_flags      1\n    n_total         2^(20)\n    n               [1048576 1048576 1048576]\n    time_integrate  0.686\nCubQMCLatticeG (AbstractStoppingCriterion)\n    abs_tol         0.010\n    rel_tol         0.025\n    n_init          2^(10)\n    n_limit         2^(30)\nCovIntegrand (AbstractIntegrand)\nGaussian (AbstractTrueMeasure)\n    mean            1\n    covariance      1\n    decomp_type     PCA\nLattice (AbstractLDDiscreteDistribution)\n    d               10\n    replications    1\n    randomize       SHIFT\n    gen_vec_source  kuo.lattice-33002-1024-1048576.9125.txt\n    order           RADICAL INVERSE\n    n_limit         2^(20)\n    entropy         21765430054726836764349893853647822716</pre> In\u00a0[28]: Copied! <pre>from sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neural_network import MLPClassifier\ndata = load_iris()\nog_feature_names = data[\"feature_names\"]\nfeature_names = [fn.replace('sepal ','S')\\\n    .replace('length ','L')\\\n    .replace('petal ','P')\\\n    .replace('width ','W')\\\n    .replace('(cm)','') for fn in og_feature_names]\ntarget_names = data[\"target_names\"]\nxt,xv,yt,yv = train_test_split(data[\"data\"],data[\"target\"],\n    test_size = 1/3,\n    random_state = 7)\npd.DataFrame(np.hstack([data['data'],data['target'][:,None]]),columns=og_feature_names+['species']).iloc[[0,1,90,91,140,141]]\n</pre> from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split from sklearn.neural_network import MLPClassifier data = load_iris() og_feature_names = data[\"feature_names\"] feature_names = [fn.replace('sepal ','S')\\     .replace('length ','L')\\     .replace('petal ','P')\\     .replace('width ','W')\\     .replace('(cm)','') for fn in og_feature_names] target_names = data[\"target_names\"] xt,xv,yt,yv = train_test_split(data[\"data\"],data[\"target\"],     test_size = 1/3,     random_state = 7) pd.DataFrame(np.hstack([data['data'],data['target'][:,None]]),columns=og_feature_names+['species']).iloc[[0,1,90,91,140,141]] Out[28]: sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) species 0 5.1 3.5 1.4 0.2 0.0 1 4.9 3.0 1.4 0.2 0.0 90 5.5 2.6 4.4 1.2 1.0 91 6.1 3.0 4.6 1.4 1.0 140 6.7 3.1 5.6 2.4 2.0 141 6.9 3.1 5.1 2.3 2.0 In\u00a0[29]: Copied! <pre>mlpc = MLPClassifier(random_state=7,max_iter=1024).fit(xt,yt)\nyhat = mlpc.predict(xv)\nprint(\"accuracy: %.1f%%\"%(100*(yv==yhat).mean()))\n# accuracy: 98.0%\nsampler = qp.DigitalNetB2(4,seed=7)\ntrue_measure =  qp.Uniform(sampler,\n    lower_bound = xt.min(0),\n    upper_bound = xt.max(0))\nfun = qp.CustomFun(\n    true_measure = true_measure,\n    g = lambda x: mlpc.predict_proba(x).T,\n    dimension_indv = 3)\nsi_fun = qp.SensitivityIndices(fun,indices=\"all\")\nqmc_algo = qp.CubQMCNetG(si_fun,abs_tol=.005)\nnn_sis,nn_sis_data = qmc_algo.integrate()\n</pre> mlpc = MLPClassifier(random_state=7,max_iter=1024).fit(xt,yt) yhat = mlpc.predict(xv) print(\"accuracy: %.1f%%\"%(100*(yv==yhat).mean())) # accuracy: 98.0% sampler = qp.DigitalNetB2(4,seed=7) true_measure =  qp.Uniform(sampler,     lower_bound = xt.min(0),     upper_bound = xt.max(0)) fun = qp.CustomFun(     true_measure = true_measure,     g = lambda x: mlpc.predict_proba(x).T,     dimension_indv = 3) si_fun = qp.SensitivityIndices(fun,indices=\"all\") qmc_algo = qp.CubQMCNetG(si_fun,abs_tol=.005) nn_sis,nn_sis_data = qmc_algo.integrate() <pre>accuracy: 98.0%\n</pre> In\u00a0[30]: Copied! <pre>#print(nn_sis_data.flags_indv.shape)\n#print(nn_sis_data.flags_comb.shape)\nprint('samples: 2^(%d)'%np.log2(nn_sis_data.n_total))\nprint('time: %.1e'%nn_sis_data.time_integrate)\nprint('indices:\\n%s'%nn_sis_data.integrand.indices)\n\nimport pandas as pd\n\ndf_closed = pd.DataFrame(nn_sis[0],columns=target_names,index=[str(np.where(idx)[0]) for idx in nn_sis_data.integrand.indices])\nprint('\\nClosed Indices')\nprint(df_closed)\ndf_total = pd.DataFrame(nn_sis[1],columns=target_names,index=[str(np.where(idx)[0]) for idx in nn_sis_data.integrand.indices])\nprint('\\nTotal Indices')\ndf_closed_singletons = df_closed.loc[['[%d]'%i for i in range(4)]].T\ndf_closed_singletons['sum singletons'] = df_closed_singletons[['[%d]'%i for i in range(4)]].sum(1)\ndf_closed_singletons.columns = data['feature_names']+['sum']\ndf_closed_singletons = df_closed_singletons*100\ndf_closed_singletons\n</pre> #print(nn_sis_data.flags_indv.shape) #print(nn_sis_data.flags_comb.shape) print('samples: 2^(%d)'%np.log2(nn_sis_data.n_total)) print('time: %.1e'%nn_sis_data.time_integrate) print('indices:\\n%s'%nn_sis_data.integrand.indices)  import pandas as pd  df_closed = pd.DataFrame(nn_sis[0],columns=target_names,index=[str(np.where(idx)[0]) for idx in nn_sis_data.integrand.indices]) print('\\nClosed Indices') print(df_closed) df_total = pd.DataFrame(nn_sis[1],columns=target_names,index=[str(np.where(idx)[0]) for idx in nn_sis_data.integrand.indices]) print('\\nTotal Indices') df_closed_singletons = df_closed.loc[['[%d]'%i for i in range(4)]].T df_closed_singletons['sum singletons'] = df_closed_singletons[['[%d]'%i for i in range(4)]].sum(1) df_closed_singletons.columns = data['feature_names']+['sum'] df_closed_singletons = df_closed_singletons*100 df_closed_singletons <pre>samples: 2^(15)\ntime: 6.6e-01\nindices:\n[[ True False False False]\n [False  True False False]\n [False False  True False]\n [False False False  True]\n [ True  True False False]\n [ True False  True False]\n [ True False False  True]\n [False  True  True False]\n [False  True False  True]\n [False False  True  True]\n [ True  True  True False]\n [ True  True False  True]\n [ True False  True  True]\n [False  True  True  True]]\n\nClosed Indices\n           setosa  versicolor  virginica\n[0]      0.001323    0.068645   0.077325\n[1]      0.063749    0.026565   0.004784\n[2]      0.713825    0.325072   0.497800\n[3]      0.052967    0.025579   0.120317\n[0 1]    0.063925    0.091300   0.085151\n[0 2]    0.715316    0.460314   0.637738\n[0 3]    0.053469    0.092601   0.205639\n[1 2]    0.841655    0.431035   0.513277\n[1 3]    0.110739    0.039410   0.131264\n[2 3]    0.822910    0.583282   0.703142\n[0 1 2]  0.843726    0.570076   0.658272\n[0 1 3]  0.112798    0.104804   0.215817\n[0 2 3]  0.825330    0.815263   0.945267\n[1 2 3]  0.995864    0.739588   0.728499\n\nTotal Indices\n</pre> Out[30]: sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) sum setosa 0.132343 6.374860 71.382498 5.296674 83.186375 versicolor 6.864536 2.656530 32.507230 2.557911 44.586208 virginica 7.732521 0.478364 49.779978 12.031725 70.022587 In\u00a0[31]: Copied! <pre>nindices = len(nn_sis_data.integrand.indices)\nfig,ax = pyplot.subplots(figsize=(9,5))\nticks = np.arange(nindices)\nwidth = .25\nfor i,(alpha,species) in enumerate(zip([.25,.5,.75],data['target_names'])):\n    cvals = df_closed[species].to_numpy()\n    tvals = df_total[species].to_numpy()\n    ticks_i = ticks+i*width\n    ax.bar(ticks_i,cvals,width=width,align='edge',color='k',alpha=alpha,label=species)\n    #ax.bar(ticks_i,np.flip(tvals),width=width,align='edge',bottom=1-np.flip(tvals),color=color,alpha=.1)\nax.set_xlim([0,13+3*width])\nax.set_xticks(ticks+1.5*width)\n\n# closed_labels = [r'$\\underline{s}_{\\{%s\\}}$'%(','.join([r'\\text{%s}'%feature_names[i] for i in idx])) for idx in nn_sis_data.integrand.indices]\nclosed_labels = ['\\n'.join([feature_names[i] for i in np.where(idx)[0]]) for idx in nn_sis_data.integrand.indices]\nax.set_xticklabels(closed_labels,rotation=0)\nax.set_ylim([0,1]); ax.set_yticks([0,1])\nax.grid(False)\nfor spine in ['top','right','bottom']: ax.spines[spine].set_visible(False)\nax.legend(frameon=False,loc='lower center',bbox_to_anchor=(.5,-.2),ncol=3);\nfig.tight_layout()\n</pre> nindices = len(nn_sis_data.integrand.indices) fig,ax = pyplot.subplots(figsize=(9,5)) ticks = np.arange(nindices) width = .25 for i,(alpha,species) in enumerate(zip([.25,.5,.75],data['target_names'])):     cvals = df_closed[species].to_numpy()     tvals = df_total[species].to_numpy()     ticks_i = ticks+i*width     ax.bar(ticks_i,cvals,width=width,align='edge',color='k',alpha=alpha,label=species)     #ax.bar(ticks_i,np.flip(tvals),width=width,align='edge',bottom=1-np.flip(tvals),color=color,alpha=.1) ax.set_xlim([0,13+3*width]) ax.set_xticks(ticks+1.5*width)  # closed_labels = [r'$\\underline{s}_{\\{%s\\}}$'%(','.join([r'\\text{%s}'%feature_names[i] for i in idx])) for idx in nn_sis_data.integrand.indices] closed_labels = ['\\n'.join([feature_names[i] for i in np.where(idx)[0]]) for idx in nn_sis_data.integrand.indices] ax.set_xticklabels(closed_labels,rotation=0) ax.set_ylim([0,1]); ax.set_yticks([0,1]) ax.grid(False) for spine in ['top','right','bottom']: ax.spines[spine].set_visible(False) ax.legend(frameon=False,loc='lower center',bbox_to_anchor=(.5,-.2),ncol=3); fig.tight_layout() In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"demos/talk_paper_demos/pydata.chi.2023/#monte-carlo-for-vector-functions-of-integrals","title":"Monte Carlo for Vector Functions of Integrals\u00b6","text":"<p>Demo Accompanying Aleksei Sorokin's PyData Chicago 2023 Talk</p>"},{"location":"demos/talk_paper_demos/pydata.chi.2023/#monte-carlo-problem","title":"Monte Carlo Problem\u00b6","text":"<p>$$\\text{True Mean} = \\mu = \\mathbb{E}[g(T)] = \\mathbb{E}[f(X)] = \\int_{[0,1]^d} f(x) \\mathrm{d} x \\approx \\frac{1}{n} \\sum_{i=0}^{n-1} f(X_i) = \\hat{\\mu} = \\text{Sample Mean}$$</p> <ul> <li>$T$, original measure on $\\mathcal{T}$</li> <li>$g: \\mathcal{T} \\to \\mathbb{R}$, original integrand</li> <li>$X \\sim \\mathcal{U}[0,1]^d$, transformed measure</li> <li>$f: [0,1]^d \\to \\mathbb{R}$, transformed integrand</li> </ul>"},{"location":"demos/talk_paper_demos/pydata.chi.2023/#python-setup","title":"Python Setup\u00b6","text":""},{"location":"demos/talk_paper_demos/pydata.chi.2023/#discrete-distribution","title":"Discrete Distribution\u00b6","text":"<p>Generate sampling locations $X_0,\\dots,X_{n-1} \\sim \\mathcal{U}[0,1]^d$</p>"},{"location":"demos/talk_paper_demos/pydata.chi.2023/#independent-identically-distributed-iid-points-for-crude-monte-carlo-cmc","title":"Independent Identically Distributed (IID) Points for Crude Monte Carlo (CMC)\u00b6","text":""},{"location":"demos/talk_paper_demos/pydata.chi.2023/#low-discrepancy-ld-points-for-quasi-monte-carlo-qmc","title":"Low Discrepancy (LD) Points for Quasi-Monte Carlo (QMC)\u00b6","text":""},{"location":"demos/talk_paper_demos/pydata.chi.2023/#visuals","title":"Visuals\u00b6","text":""},{"location":"demos/talk_paper_demos/pydata.chi.2023/#iid-vs-ld-points","title":"IID vs LD Points\u00b6","text":""},{"location":"demos/talk_paper_demos/pydata.chi.2023/#ld-space-filling-extensibility","title":"LD Space Filling Extensibility\u00b6","text":""},{"location":"demos/talk_paper_demos/pydata.chi.2023/#high-dimensional-pairs-plotting","title":"High Dimensional Pairs Plotting\u00b6","text":""},{"location":"demos/talk_paper_demos/pydata.chi.2023/#true-measure","title":"True Measure\u00b6","text":"<p>Define $T$, facilitate transform from original integrand $g$ to transformed integrand $f$</p>"},{"location":"demos/talk_paper_demos/pydata.chi.2023/#visuals","title":"Visuals\u00b6","text":""},{"location":"demos/talk_paper_demos/pydata.chi.2023/#some-true-measure-samplings","title":"Some True Measure Samplings\u00b6","text":""},{"location":"demos/talk_paper_demos/pydata.chi.2023/#brownian-motion","title":"Brownian Motion\u00b6","text":""},{"location":"demos/talk_paper_demos/pydata.chi.2023/#integrand","title":"Integrand\u00b6","text":"<p>Define original integrand $g$, store transformed integrand $f$</p>"},{"location":"demos/talk_paper_demos/pydata.chi.2023/#wrap-your-function-into-qmcpy","title":"Wrap your Function into QMCPy\u00b6","text":"<p>Our simple example $$g(T) = T_0+T_1+\\dots+T_{d-1}, \\qquad T \\sim \\mathcal{N}(0,I_d)$$ $$f(X) = g(\\Phi^{-1}(X)), \\qquad \\Phi \\text{ standard normal CDF}$$ $$\\mathbb{E}[f(X)] = \\mathbb{E}[g(T)] = 0$$</p>"},{"location":"demos/talk_paper_demos/pydata.chi.2023/#evalute-the-automatically-transformed-integrand","title":"Evalute the Automatically Transformed Integrand\u00b6","text":""},{"location":"demos/talk_paper_demos/pydata.chi.2023/#manual-qmc-approximation","title":"Manual QMC Approximation\u00b6","text":"<p>Note that when doing importance sampling the below doesn't work. In that case we need to take a specially weighted sum instead instead of the equally weighted sum as done below.</p>"},{"location":"demos/talk_paper_demos/pydata.chi.2023/#predefined-integrands","title":"Predefined Integrands\u00b6","text":"<p>Many more integrands detailed at https://qmcpy.readthedocs.io/en/master/algorithms.html#integrand-class</p> <p>Integrands contain their true measure definition, so the user only needs to pass in a sampler. Samplers are often just discrete distributions.</p>"},{"location":"demos/talk_paper_demos/pydata.chi.2023/#visual-transformation","title":"Visual Transformation\u00b6","text":""},{"location":"demos/talk_paper_demos/pydata.chi.2023/#stopping-criterion","title":"Stopping Criterion\u00b6","text":"<p>Adaptively increase $n$ until $\\lvert \\mu - \\hat{\\mu} \\rvert &lt; \\varepsilon$ where $\\varepsilon$ is a user defined tolerance.</p> <p>The stopping criterion should match the discrete distribution e.g. IID CMC stopping criterion for IID points, QMC Lattice stopping criterion for LD Lattice points, QMC digital net stopping criterion for LD digital net points, etc.</p>"},{"location":"demos/talk_paper_demos/pydata.chi.2023/#iid-cmc-algorithm","title":"IID CMC Algorithm\u00b6","text":""},{"location":"demos/talk_paper_demos/pydata.chi.2023/#ld-qmc-algorithm","title":"LD QMC Algorithm\u00b6","text":""},{"location":"demos/talk_paper_demos/pydata.chi.2023/#visual-cmc-vs-ld","title":"Visual CMC vs LD\u00b6","text":""},{"location":"demos/talk_paper_demos/pydata.chi.2023/#vectorized-stopping-criterion","title":"Vectorized Stopping Criterion\u00b6","text":"<p>Many more examples available at https://github.com/QMCSoftware/QMCSoftware/blob/master/demos/vectorized_qmc.ipynb</p>"},{"location":"demos/talk_paper_demos/pydata.chi.2023/#vector-of-expectations","title":"Vector of Expectations\u00b6","text":"<p>As a simple example, lets compute $\\mathbb{E}[\\cos(T_0)\\cdots\\cos(T_{d-1})]$ and $\\mathbb{E}[\\sin(T_0)\\cdots\\sin(T_{d-1})]$ where $T \\sim \\mathcal{U}[0,\\pi]^d$</p>"},{"location":"demos/talk_paper_demos/pydata.chi.2023/#covariance","title":"Covariance\u00b6","text":"<p>In a simple example, let $T \\sim \\mathcal{N}(1,I_d)$ and compute the covariance of $P = T_0\\cdots T_{d-1}$ and $S = T_0+\\dots+T_{d-1}$ so that $$\\mathrm{Cov}[P,S] = \\mathbb{E}[PS]-\\mathbb{E}[P]\\mathbb{E}[S] = \\mu_0-\\mu_1\\mu_2$$ Theoretically we have $\\mathrm{Cov}[P,S] = 2d-(1)(d) = d$</p>"},{"location":"demos/talk_paper_demos/pydata.chi.2023/#sensitiviy-indices","title":"Sensitiviy Indices\u00b6","text":"<p>See Appendix A of Art Owen's Monte Carlo Book</p> <p>In the following example, we fit a neural network to Iris flower features and try to classify the Iris species. For each set of features, the classifier provides a probability of belonging to each species, a length 3 vector. We quantify the sensitiviy of this classificaiton probability to Iris features, assuming features are uniformly distributed throughout the feature domain.</p>"},{"location":"demos/talk_paper_demos/ACMTOMS_Sorokin_2025/acm_toms_sorokin_2025/","title":"2025 ACM-TOMS Paper","text":"In\u00a0[1]: Copied! <pre>import qmcpy as qp\nimport numpy as np\nimport timeit\nfrom collections import OrderedDict\nimport os\nimport scipy.stats\nimport time\nimport torch\nimport sympy\nimport gc\nimport itertools\n</pre> import qmcpy as qp import numpy as np import timeit from collections import OrderedDict import os import scipy.stats import time import torch import sympy import gc import itertools In\u00a0[2]: Copied! <pre>import matplotlib \nfrom matplotlib import pyplot\nfrom tueplots import cycler\nfrom tueplots.bundles import probnum2025\nfrom tueplots.constants import markers\nfrom tueplots.constants.color import palettes\nmatplotlib.rcParams['figure.dpi'] = 256\n_golden = (1 + 5 ** 0.5) / 2\nMW1 = 240/72\nMW2 = 500/72\nMH1 = MW1/_golden\nMH2 = MW2/_golden\nCOLORS = palettes.tue_plot\nMARKERS = markers.o_sized\npyplot.rcParams.update(probnum2025())\npyplot.rcParams.update(cycler.cycler(color=COLORS,marker=MARKERS))\n</pre> import matplotlib  from matplotlib import pyplot from tueplots import cycler from tueplots.bundles import probnum2025 from tueplots.constants import markers from tueplots.constants.color import palettes matplotlib.rcParams['figure.dpi'] = 256 _golden = (1 + 5 ** 0.5) / 2 MW1 = 240/72 MW2 = 500/72 MH1 = MW1/_golden MH2 = MW2/_golden COLORS = palettes.tue_plot MARKERS = markers.o_sized pyplot.rcParams.update(probnum2025()) pyplot.rcParams.update(cycler.cycler(color=COLORS,marker=MARKERS)) In\u00a0[3]: Copied! <pre>%%time \nlattice = qp.Lattice(\n    dimension = 52,\n    randomize = \"shift\", # for unrandomized lattice set randomize = None\n    replications = 16, # R\n    order = \"radical inverse\", # also supports \"linear\"\n    seed = None, # pass integer seed for reproducibility\n    generating_vector = \"mps.exod2_base2_m20_CKN.txt\")\nx = lattice(2**16) # a numpy.ndarray with shape 16 x 65536 x 52\n</pre> %%time  lattice = qp.Lattice(     dimension = 52,     randomize = \"shift\", # for unrandomized lattice set randomize = None     replications = 16, # R     order = \"radical inverse\", # also supports \"linear\"     seed = None, # pass integer seed for reproducibility     generating_vector = \"mps.exod2_base2_m20_CKN.txt\") x = lattice(2**16) # a numpy.ndarray with shape 16 x 65536 x 52 <pre>CPU times: user 36 ms, sys: 20.8 ms, total: 56.8 ms\nWall time: 61.1 ms\n</pre> In\u00a0[50]: Copied! <pre>%%time\ndnb2 = qp.DigitalNetB2(\n    dimension = 52, \n    randomize = \"LMS DS\", # Matousek's LMS then a digital shift\n    # other options [\"NUS\", \"DS\", \"LMS\", None]\n    t = 64, # number of LMS bits i.e. number of rows in S_j\n    alpha = 2, # interlacing factor for higher order digital nets\n    replications = 16, # R\n    order = \"radical inverse\", # also supports \"Gray code\"\n    seed = None, # pass integer seed for reproducibility\n    generating_matrices = \"joe_kuo.6.21201.txt\")\nx = dnb2(2**16) # a numpy.ndarray with shape 16 x 65536 x 52\n</pre> %%time dnb2 = qp.DigitalNetB2(     dimension = 52,      randomize = \"LMS DS\", # Matousek's LMS then a digital shift     # other options [\"NUS\", \"DS\", \"LMS\", None]     t = 64, # number of LMS bits i.e. number of rows in S_j     alpha = 2, # interlacing factor for higher order digital nets     replications = 16, # R     order = \"radical inverse\", # also supports \"Gray code\"     seed = None, # pass integer seed for reproducibility     generating_matrices = \"joe_kuo.6.21201.txt\") x = dnb2(2**16) # a numpy.ndarray with shape 16 x 65536 x 52 <pre>CPU times: user 370 ms, sys: 119 ms, total: 489 ms\nWall time: 493 ms\n</pre> In\u00a0[5]: Copied! <pre>%%time \nhalton = qp.Halton(\n    dimension = 52, \n    randomize = \"LMS DP\", # Matousek's LMS then a digital permutation\n    # other options [\"LMS DS\", \"LMS\", \"DP\", \"DS\", \"NUS\", \"QRNG\", None]\n    t = 64, # number of LMS digits i.e. number of rows in S_j\n    replications = 16, # R\n    seed = None) # pass integer seed for reproducibility\nx = halton(2**10) # a numpy.ndarray with shape 16 x 1024 x 52\n</pre> %%time  halton = qp.Halton(     dimension = 52,      randomize = \"LMS DP\", # Matousek's LMS then a digital permutation     # other options [\"LMS DS\", \"LMS\", \"DP\", \"DS\", \"NUS\", \"QRNG\", None]     t = 64, # number of LMS digits i.e. number of rows in S_j     replications = 16, # R     seed = None) # pass integer seed for reproducibility x = halton(2**10) # a numpy.ndarray with shape 16 x 1024 x 52 <pre>CPU times: user 339 ms, sys: 63.9 ms, total: 403 ms\nWall time: 405 ms\n</pre> In\u00a0[10]: Copied! <pre>d = 3 # dimension \nm = 10 # will generate 2^m points\nn = 2**m # number of points\nlattice = qp.Lattice(d) # default to radical inverse order\nkernel = qp.KernelShiftInvar(\n    d, # dimension \n    alpha = [1,2,3], # per dimension smoothness parameters\n    lengthscales = [1, 1/2, 1/4]) # per dimension product weights\nx = lattice(n_min=0,n_max=n) # shape=(n,d) lattice points\ny = np.random.rand(n) # shape=(n,) random uniforms\n# fast matrix multiplication and linear system solve\nk1 = kernel(x,x[0]) # shape=(n,) first column of Gram matrix\nlam = np.sqrt(n)*qp.fftbr(k1) # vector of eigenvalues\nyt = qp.fftbr(y)\nu = qp.ifftbr(yt*lam) # fast matrix multiplication \nv = qp.ifftbr(yt/lam) # fast linear system solve\n# efficient fast transform updates\nynew = np.random.rand(n) # shape=(n,) new random uniforms\nomega = qp.omega_fftbr(m) # shape=(n,)\nytnew = qp.fftbr(ynew) # shape=(n,)\nytfull = np.concatenate([yt+omega*ytnew,yt-omega*ytnew])/np.sqrt(2) # shape=(2n,)\n</pre> d = 3 # dimension  m = 10 # will generate 2^m points n = 2**m # number of points lattice = qp.Lattice(d) # default to radical inverse order kernel = qp.KernelShiftInvar(     d, # dimension      alpha = [1,2,3], # per dimension smoothness parameters     lengthscales = [1, 1/2, 1/4]) # per dimension product weights x = lattice(n_min=0,n_max=n) # shape=(n,d) lattice points y = np.random.rand(n) # shape=(n,) random uniforms # fast matrix multiplication and linear system solve k1 = kernel(x,x[0]) # shape=(n,) first column of Gram matrix lam = np.sqrt(n)*qp.fftbr(k1) # vector of eigenvalues yt = qp.fftbr(y) u = qp.ifftbr(yt*lam) # fast matrix multiplication  v = qp.ifftbr(yt/lam) # fast linear system solve # efficient fast transform updates ynew = np.random.rand(n) # shape=(n,) new random uniforms omega = qp.omega_fftbr(m) # shape=(n,) ytnew = qp.fftbr(ynew) # shape=(n,) ytfull = np.concatenate([yt+omega*ytnew,yt-omega*ytnew])/np.sqrt(2) # shape=(2n,) In\u00a0[11]: Copied! <pre># slow matrix multiplication and linear system solve\nkmat = kernel(x[:,None,:],x[None,:,:]) # shape=(n,n)\nu_slow = kmat@y # matrix multiplication\nv_slow = np.linalg.solve(kmat,y) # solve a linear system\n# verify correctness\nassert np.allclose(u,u_slow)\nassert np.allclose(v,v_slow)\n# get next samples \nxnew = lattice(n_min=n,n_max=2*n) # shape=(n,d) new lattice points\nk1new = kernel(xnew,x[0]) # shape=(n,) new values in the first column\n# inefficient fast transform update \nk1full = np.concatenate([k1,k1new]) # shape=(2*n,) full first column\nlamfull_inefficient = np.sqrt(2*n)*qp.fftbr(k1full) # shape=(2*n,) full eigenvalues\nyfull = np.concatenate([y,ynew]) # shape=(2*n,) full random values\nytfull_inefficient = qp.fftbr(yfull) # shape=(2*n,) full transformed points\n# efficient fast transform updates\nlamnew = np.sqrt(n)*qp.fftbr(k1new) # shape=(n,) new eigenvalues\nlamfull = np.hstack([lam+omega*lamnew,lam-omega*lamnew])\n# verify correctness\nassert np.allclose(lamfull,lamfull_inefficient)\nassert np.allclose(ytfull,ytfull_inefficient)\n</pre> # slow matrix multiplication and linear system solve kmat = kernel(x[:,None,:],x[None,:,:]) # shape=(n,n) u_slow = kmat@y # matrix multiplication v_slow = np.linalg.solve(kmat,y) # solve a linear system # verify correctness assert np.allclose(u,u_slow) assert np.allclose(v,v_slow) # get next samples  xnew = lattice(n_min=n,n_max=2*n) # shape=(n,d) new lattice points k1new = kernel(xnew,x[0]) # shape=(n,) new values in the first column # inefficient fast transform update  k1full = np.concatenate([k1,k1new]) # shape=(2*n,) full first column lamfull_inefficient = np.sqrt(2*n)*qp.fftbr(k1full) # shape=(2*n,) full eigenvalues yfull = np.concatenate([y,ynew]) # shape=(2*n,) full random values ytfull_inefficient = qp.fftbr(yfull) # shape=(2*n,) full transformed points # efficient fast transform updates lamnew = np.sqrt(n)*qp.fftbr(k1new) # shape=(n,) new eigenvalues lamfull = np.hstack([lam+omega*lamnew,lam-omega*lamnew]) # verify correctness assert np.allclose(lamfull,lamfull_inefficient) assert np.allclose(ytfull,ytfull_inefficient) In\u00a0[12]: Copied! <pre>d = 3 # dimension \nm = 10 # will generate 2^m points\nn = 2**m # number of points\ndnb2 = qp.DigitalNetB2(d) # default to radical inverse order\nkernel = qp.KernelDigShiftInvar(\n    d, # dimension \n    t = dnb2.t, # number of bits in integer representation of points\n    alpha = [1,2,3], # per dimension smoothness parameters\n    lengthscales = [1, 1/2, 1/4]) # per dimension product weights\nx = dnb2(n_min=0,n_max=n) # shape=(n,d) digital net\ny = np.random.rand(n) # shape=(n,) random uniforms\n# fast matrix multiplication and linear system solve\nk1 = kernel(x,x[0]) # shape=(n,) first column of Gram matrix\nlam = np.sqrt(n)*qp.fwht(k1) # vector of eigenvalues\nyt = qp.fwht(y)\nu = qp.fwht(yt*lam) # fast matrix multiplication \nv = qp.fwht(yt/lam) # fast linear system solve\n# efficient fast transform updates\nynew = np.random.rand(n) # shape=(n,) new random uniforms\nomega = qp.omega_fwht(m) # shape=(n,)\nytnew = qp.fwht(ynew) # shape=(n,)\nytfull = np.concatenate([yt+omega*ytnew,yt-omega*ytnew])/np.sqrt(2) # shape=(2n,)\n</pre> d = 3 # dimension  m = 10 # will generate 2^m points n = 2**m # number of points dnb2 = qp.DigitalNetB2(d) # default to radical inverse order kernel = qp.KernelDigShiftInvar(     d, # dimension      t = dnb2.t, # number of bits in integer representation of points     alpha = [1,2,3], # per dimension smoothness parameters     lengthscales = [1, 1/2, 1/4]) # per dimension product weights x = dnb2(n_min=0,n_max=n) # shape=(n,d) digital net y = np.random.rand(n) # shape=(n,) random uniforms # fast matrix multiplication and linear system solve k1 = kernel(x,x[0]) # shape=(n,) first column of Gram matrix lam = np.sqrt(n)*qp.fwht(k1) # vector of eigenvalues yt = qp.fwht(y) u = qp.fwht(yt*lam) # fast matrix multiplication  v = qp.fwht(yt/lam) # fast linear system solve # efficient fast transform updates ynew = np.random.rand(n) # shape=(n,) new random uniforms omega = qp.omega_fwht(m) # shape=(n,) ytnew = qp.fwht(ynew) # shape=(n,) ytfull = np.concatenate([yt+omega*ytnew,yt-omega*ytnew])/np.sqrt(2) # shape=(2n,) In\u00a0[13]: Copied! <pre># slow matrix multiplication and linear system solve\nkmat = kernel(x[:,None,:],x[None,:,:]) # shape=(n,n)\nu_slow = kmat@y # matrix multiplication\nv_slow = np.linalg.solve(kmat,y) # solve a linear system\n# verify correctness\nassert np.allclose(u,u_slow)\nassert np.allclose(v,v_slow)\n# get next samples \nxnew = dnb2(n_min=n,n_max=2*n) # shape=(n,d) new digital net points\nk1new = kernel(xnew,x[0]) # shape=(n,) new values in the first column\n# inefficient fast transform update \nk1full = np.concatenate([k1,k1new]) # shape=(2*n,) full first column\nlamfull_inefficient = np.sqrt(2*n)*qp.fwht(k1full) # shape=(2*n,) full eigenvalues\nyfull = np.concatenate([y,ynew]) # shape=(2*n,) full random values\nytfull_inefficient = qp.fwht(yfull) # shape=(2*n,) full transformed points\n# efficient fast transform updates\nlamnew = np.sqrt(n)*qp.fwht(k1new) # shape=(n,) new eigenvalues\nlamfull = np.hstack([lam+omega*lamnew,lam-omega*lamnew])\n# verify correctness\nassert np.allclose(lamfull,lamfull_inefficient)\nassert np.allclose(ytfull,ytfull_inefficient)\n</pre> # slow matrix multiplication and linear system solve kmat = kernel(x[:,None,:],x[None,:,:]) # shape=(n,n) u_slow = kmat@y # matrix multiplication v_slow = np.linalg.solve(kmat,y) # solve a linear system # verify correctness assert np.allclose(u,u_slow) assert np.allclose(v,v_slow) # get next samples  xnew = dnb2(n_min=n,n_max=2*n) # shape=(n,d) new digital net points k1new = kernel(xnew,x[0]) # shape=(n,) new values in the first column # inefficient fast transform update  k1full = np.concatenate([k1,k1new]) # shape=(2*n,) full first column lamfull_inefficient = np.sqrt(2*n)*qp.fwht(k1full) # shape=(2*n,) full eigenvalues yfull = np.concatenate([y,ynew]) # shape=(2*n,) full random values ytfull_inefficient = qp.fwht(yfull) # shape=(2*n,) full transformed points # efficient fast transform updates lamnew = np.sqrt(n)*qp.fwht(k1new) # shape=(n,) new eigenvalues lamfull = np.hstack([lam+omega*lamnew,lam-omega*lamnew]) # verify correctness assert np.allclose(lamfull,lamfull_inefficient) assert np.allclose(ytfull,ytfull_inefficient) In\u00a0[39]: Copied! <pre>import scipy.stats\ndef gen_corner_peak_2(x):\n    d = x.shape[-1] # x.shape=(...,n,d), e.g., (n,d) or (R,n,d) \n    c_tilde = 1/np.arange(1,d+1)**2\n    c = 0.25*c_tilde/np.sum(c_tilde)\n    y = (1+np.sum(c*x,axis=-1))**(-(d+1)) \n    return y # y.shape=(...,n), e.g., (n,) or (R,n)\nR = 10 # number of randomizations\nn = 2**15 # number of points \nd = 50 # dimension\ndnb2 = qp.DigitalNet(dimension=d, replications=R, seed=7, alpha=3)\nx = dnb2(n) # x.shape=(R,n,d)\ny = gen_corner_peak_2(x) # y.shape=(R,n) \nmuhats = np.mean(y,axis=1) # muhats.shape=(R,)\nmuhat_aggregate = np.mean(muhats) # muhat_aggregate is a scalar \nprint(muhat_aggregate)\n\"\"\" 0.014936813948394042 \"\"\"\nalpha = 0.01 # uncertainty level\nt_star = -scipy.stats.t.ppf(alpha/2,df=R-1) # quantile of Student's t \nstdhat = np.std(muhats,ddof=1) # unbiased estimate of standard deviation\nstd_error = t_star*stdhat/np.sqrt(R)\nprint(std_error)\n\"\"\" 5.247445301861484e-07 \"\"\"\nconf_int = [muhat_aggregate-std_error,muhat_aggregate+std_error]\n</pre> import scipy.stats def gen_corner_peak_2(x):     d = x.shape[-1] # x.shape=(...,n,d), e.g., (n,d) or (R,n,d)      c_tilde = 1/np.arange(1,d+1)**2     c = 0.25*c_tilde/np.sum(c_tilde)     y = (1+np.sum(c*x,axis=-1))**(-(d+1))      return y # y.shape=(...,n), e.g., (n,) or (R,n) R = 10 # number of randomizations n = 2**15 # number of points  d = 50 # dimension dnb2 = qp.DigitalNet(dimension=d, replications=R, seed=7, alpha=3) x = dnb2(n) # x.shape=(R,n,d) y = gen_corner_peak_2(x) # y.shape=(R,n)  muhats = np.mean(y,axis=1) # muhats.shape=(R,) muhat_aggregate = np.mean(muhats) # muhat_aggregate is a scalar  print(muhat_aggregate) \"\"\" 0.014936813948394042 \"\"\" alpha = 0.01 # uncertainty level t_star = -scipy.stats.t.ppf(alpha/2,df=R-1) # quantile of Student's t  stdhat = np.std(muhats,ddof=1) # unbiased estimate of standard deviation std_error = t_star*stdhat/np.sqrt(R) print(std_error) \"\"\" 5.247445301861484e-07 \"\"\" conf_int = [muhat_aggregate-std_error,muhat_aggregate+std_error] <pre>0.014936813948394042\n5.247445301861484e-07\n</pre> In\u00a0[40]: Copied! <pre>def gen_corner_peak_2(x):\n    d = x.shape[-1] # x.shape=(...,n,d), e.g., (n,d) or (R,n,d) \n    c_tilde = 1/np.arange(1,d+1)**2\n    c = 0.25*c_tilde/np.sum(c_tilde)\n    y = (1+np.sum(c*x,axis=-1))**(-(d+1)) \n    return y # y.shape=(...,n), e.g., (n,) or (R,n)\nR = 10\nd = 50 \ndnb2 = qp.DigitalNet(dimension=d, replications=R, seed=7, alpha=3)\ntrue_measure = qp.Uniform(dnb2, lower_bound=0, upper_bound=1)\nintegrand = qp.CustomFun(true_measure=true_measure, g=gen_corner_peak_2)\n# equivalent to \n# integrand = qp.Genz(dnb2, kind_func=\"CORNER PEAK\", kind_coeff=2)\nqmc_algo = qp.CubQMCRepStudentT(integrand, abs_tol=1e-4)\nsolution,data = qmc_algo.integrate() # run adaptive QMC algorithm \nprint(solution)\n\"\"\" 0.014950908095474802 \"\"\"\nconf_int = [data.comb_bound_low,data.comb_bound_high]\nstd_error = (conf_int[1]-conf_int[0])/2\nprint(std_error)\n\"\"\" 2.7968149935497788e-05 \"\"\"\nprint(data)\n\"\"\"\nData (Data)\n    solution        0.015\n    comb_bound_low  0.015\n    comb_bound_high 0.015\n    comb_bound_diff 5.59e-05\n    comb_flags      1\n    n_total         10240\n    n               10240\n    n_rep           2^(10)\n    time_integrate  0.019\nCubQMCRepStudentT (AbstractStoppingCriterion)\n    inflate         1\n    alpha           0.010\n    abs_tol         1.00e-04\n    rel_tol         0\n    n_init          2^(8)\n    n_limit         2^(30)\nCustomFun (AbstractIntegrand)\nUniform (AbstractTrueMeasure)\n    lower_bound     0\n    upper_bound     1\nDigitalNetB2 (AbstractLDDiscreteDistribution)\n    d               50\n    replications    10\n    randomize       LMS DS\n    gen_mats_source joe_kuo.6.21201.txt\n    order           RADICAL INVERSE\n    t               63\n    alpha           3\n    n_limit         2^(32)\n    entropy         7\n\"\"\"\n</pre> def gen_corner_peak_2(x):     d = x.shape[-1] # x.shape=(...,n,d), e.g., (n,d) or (R,n,d)      c_tilde = 1/np.arange(1,d+1)**2     c = 0.25*c_tilde/np.sum(c_tilde)     y = (1+np.sum(c*x,axis=-1))**(-(d+1))      return y # y.shape=(...,n), e.g., (n,) or (R,n) R = 10 d = 50  dnb2 = qp.DigitalNet(dimension=d, replications=R, seed=7, alpha=3) true_measure = qp.Uniform(dnb2, lower_bound=0, upper_bound=1) integrand = qp.CustomFun(true_measure=true_measure, g=gen_corner_peak_2) # equivalent to  # integrand = qp.Genz(dnb2, kind_func=\"CORNER PEAK\", kind_coeff=2) qmc_algo = qp.CubQMCRepStudentT(integrand, abs_tol=1e-4) solution,data = qmc_algo.integrate() # run adaptive QMC algorithm  print(solution) \"\"\" 0.014950908095474802 \"\"\" conf_int = [data.comb_bound_low,data.comb_bound_high] std_error = (conf_int[1]-conf_int[0])/2 print(std_error) \"\"\" 2.7968149935497788e-05 \"\"\" print(data) \"\"\" Data (Data)     solution        0.015     comb_bound_low  0.015     comb_bound_high 0.015     comb_bound_diff 5.59e-05     comb_flags      1     n_total         10240     n               10240     n_rep           2^(10)     time_integrate  0.019 CubQMCRepStudentT (AbstractStoppingCriterion)     inflate         1     alpha           0.010     abs_tol         1.00e-04     rel_tol         0     n_init          2^(8)     n_limit         2^(30) CustomFun (AbstractIntegrand) Uniform (AbstractTrueMeasure)     lower_bound     0     upper_bound     1 DigitalNetB2 (AbstractLDDiscreteDistribution)     d               50     replications    10     randomize       LMS DS     gen_mats_source joe_kuo.6.21201.txt     order           RADICAL INVERSE     t               63     alpha           3     n_limit         2^(32)     entropy         7 \"\"\" <pre>0.014950908095474802\n2.7968149935497788e-05\nData (Data)\n    solution        0.015\n    comb_bound_low  0.015\n    comb_bound_high 0.015\n    comb_bound_diff 5.59e-05\n    comb_flags      1\n    n_total         10240\n    n               10240\n    n_rep           2^(10)\n    time_integrate  0.008\nCubQMCRepStudentT (AbstractStoppingCriterion)\n    inflate         1\n    alpha           0.010\n    abs_tol         1.00e-04\n    rel_tol         0\n    n_init          2^(8)\n    n_limit         2^(30)\nCustomFun (AbstractIntegrand)\nUniform (AbstractTrueMeasure)\n    lower_bound     0\n    upper_bound     1\nDigitalNetB2 (AbstractLDDiscreteDistribution)\n    d               50\n    replications    10\n    randomize       LMS DS\n    gen_mats_source joe_kuo.6.21201.txt\n    order           RADICAL INVERSE\n    t               63\n    alpha           3\n    n_limit         2^(32)\n    entropy         7\n</pre> Out[40]: <pre>'\\nData (Data)\\n    solution        0.015\\n    comb_bound_low  0.015\\n    comb_bound_high 0.015\\n    comb_bound_diff 5.59e-05\\n    comb_flags      1\\n    n_total         10240\\n    n               10240\\n    n_rep           2^(10)\\n    time_integrate  0.019\\nCubQMCRepStudentT (AbstractStoppingCriterion)\\n    inflate         1\\n    alpha           0.010\\n    abs_tol         1.00e-04\\n    rel_tol         0\\n    n_init          2^(8)\\n    n_limit         2^(30)\\nCustomFun (AbstractIntegrand)\\nUniform (AbstractTrueMeasure)\\n    lower_bound     0\\n    upper_bound     1\\nDigitalNetB2 (AbstractLDDiscreteDistribution)\\n    d               50\\n    replications    10\\n    randomize       LMS DS\\n    gen_mats_source joe_kuo.6.21201.txt\\n    order           RADICAL INVERSE\\n    t               63\\n    alpha           3\\n    n_limit         2^(32)\\n    entropy         7\\n'</pre> In\u00a0[47]: Copied! <pre>m = 13 # n = 2^m\nn = 2**m # number of points\nd = 2 # dimensions\n</pre> m = 13 # n = 2^m n = 2**m # number of points d = 2 # dimensions In\u00a0[48]: Copied! <pre>pointsets = OrderedDict({\n    \"IID\": (qp.IIDStdUniform(d).gen_samples(n),{\"color\":COLORS[2]}),\n    \"Lattice Shift\": (qp.Lattice(d).gen_samples(n),{\"color\":COLORS[5]}),\n    \"Halton LMS DP\": (qp.Halton(d,randomize=\"LMS_PERM\").gen_samples(n),{\"color\":COLORS[1]}),\n    \"Halton NUS\": (qp.Halton(d,randomize=\"NUS\").gen_samples(n),{\"color\":COLORS[4]}),\n    r\"DN${}_{1}$ LMS DS\": (qp.DigitalNetB2(d,randomize=\"LMS_DS\").gen_samples(n),{\"color\":COLORS[0]}),\n    r\"DN${}_{2}$ LMS DS\": (qp.DigitalNetB2(d,randomize=\"LMS_DS\",alpha=2).gen_samples(n),{\"color\":COLORS[0]}),\n    r\"DN${}_{3}$ LMS DS\": (qp.DigitalNetB2(d,randomize=\"LMS_DS\",alpha=3).gen_samples(n),{\"color\":COLORS[0]}),\n    r\"DN${}_{4}$ LMS DS\": (qp.DigitalNetB2(d,randomize=\"LMS_DS\",alpha=4).gen_samples(n),{\"color\":COLORS[0]}),\n    r\"DN${}_{1}$ NUS\": (qp.DigitalNetB2(d,randomize=\"NUS\").gen_samples(n),{\"color\":COLORS[3]}),\n    r\"DN${}_{2}$ NUS\": (qp.DigitalNetB2(d,randomize=\"NUS\",alpha=2).gen_samples(n),{\"color\":COLORS[3]}),\n    r\"DN${}_{3}$ NUS\": (qp.DigitalNetB2(d,randomize=\"NUS\",alpha=3).gen_samples(n),{\"color\":COLORS[3]}),\n    r\"DN${}_{4}$ NUS\": (qp.DigitalNetB2(d,randomize=\"NUS\",alpha=4).gen_samples(n),{\"color\":COLORS[3]}),\n})\n</pre> pointsets = OrderedDict({     \"IID\": (qp.IIDStdUniform(d).gen_samples(n),{\"color\":COLORS[2]}),     \"Lattice Shift\": (qp.Lattice(d).gen_samples(n),{\"color\":COLORS[5]}),     \"Halton LMS DP\": (qp.Halton(d,randomize=\"LMS_PERM\").gen_samples(n),{\"color\":COLORS[1]}),     \"Halton NUS\": (qp.Halton(d,randomize=\"NUS\").gen_samples(n),{\"color\":COLORS[4]}),     r\"DN${}_{1}$ LMS DS\": (qp.DigitalNetB2(d,randomize=\"LMS_DS\").gen_samples(n),{\"color\":COLORS[0]}),     r\"DN${}_{2}$ LMS DS\": (qp.DigitalNetB2(d,randomize=\"LMS_DS\",alpha=2).gen_samples(n),{\"color\":COLORS[0]}),     r\"DN${}_{3}$ LMS DS\": (qp.DigitalNetB2(d,randomize=\"LMS_DS\",alpha=3).gen_samples(n),{\"color\":COLORS[0]}),     r\"DN${}_{4}$ LMS DS\": (qp.DigitalNetB2(d,randomize=\"LMS_DS\",alpha=4).gen_samples(n),{\"color\":COLORS[0]}),     r\"DN${}_{1}$ NUS\": (qp.DigitalNetB2(d,randomize=\"NUS\").gen_samples(n),{\"color\":COLORS[3]}),     r\"DN${}_{2}$ NUS\": (qp.DigitalNetB2(d,randomize=\"NUS\",alpha=2).gen_samples(n),{\"color\":COLORS[3]}),     r\"DN${}_{3}$ NUS\": (qp.DigitalNetB2(d,randomize=\"NUS\",alpha=3).gen_samples(n),{\"color\":COLORS[3]}),     r\"DN${}_{4}$ NUS\": (qp.DigitalNetB2(d,randomize=\"NUS\",alpha=4).gen_samples(n),{\"color\":COLORS[3]}), }) In\u00a0[49]: Copied! <pre>nrows,ncols = 3,4\nassert len(pointsets)==(nrows*ncols)\nfig,ax = pyplot.subplots(nrows=nrows,ncols=ncols,figsize=(MW2,MW2/ncols*nrows),constrained_layout=False,tight_layout=False)#figsize=(ncols*3,nrows*3))\ns = .25\nfor i,(name,(x,pltkwargs)) in enumerate(pointsets.items()):\n    ri,ci = i//ncols,i%ncols\n    ax[ri,ci].set_title(name)\n    ax[ri,ci].scatter(x[:,0],x[:,1],s=s,marker='o',edgecolor='none',**pltkwargs)#,fillstyle='full')\n    ax[ri,ci].set_xlim([0,1]); ax[ri,ci].set_xticks([0,1])\n    ax[ri,ci].set_ylim([0,1]); ax[ri,ci].set_yticks([0,1])\n    ax[ri,ci].set_aspect(1)\nfig.savefig(\"outputs/pointsets.png\",format=\"png\",dpi=1024,transparent=True,bbox_inches=\"tight\")\n</pre> nrows,ncols = 3,4 assert len(pointsets)==(nrows*ncols) fig,ax = pyplot.subplots(nrows=nrows,ncols=ncols,figsize=(MW2,MW2/ncols*nrows),constrained_layout=False,tight_layout=False)#figsize=(ncols*3,nrows*3)) s = .25 for i,(name,(x,pltkwargs)) in enumerate(pointsets.items()):     ri,ci = i//ncols,i%ncols     ax[ri,ci].set_title(name)     ax[ri,ci].scatter(x[:,0],x[:,1],s=s,marker='o',edgecolor='none',**pltkwargs)#,fillstyle='full')     ax[ri,ci].set_xlim([0,1]); ax[ri,ci].set_xticks([0,1])     ax[ri,ci].set_ylim([0,1]); ax[ri,ci].set_yticks([0,1])     ax[ri,ci].set_aspect(1) fig.savefig(\"outputs/pointsets.png\",format=\"png\",dpi=1024,transparent=True,bbox_inches=\"tight\") <pre>/var/folders/xk/w1s5c54x0zv90dgqk3vpmbsw004hmz/T/ipykernel_57279/1388457.py:3: UserWarning: The Figure parameters 'tight_layout' and 'constrained_layout' cannot be used together. Please use 'layout' parameter\n  fig,ax = pyplot.subplots(nrows=nrows,ncols=ncols,figsize=(MW2,MW2/ncols*nrows),constrained_layout=False,tight_layout=False)#figsize=(ncols*3,nrows*3))\n</pre> In\u00a0[17]: Copied! <pre>reps = 5\nm_max = 20\n</pre> reps = 5 m_max = 20 In\u00a0[18]: Copied! <pre>def time_block(pointsets_fns, rds):\n    data = {}\n    for name,(generator,pltkwargs) in pointsets_fns.items():\n        data[name] = np.nan*np.empty((len(rds),m_max+1,reps),dtype=np.float64)\n        for i,(r,d) in enumerate(rds):\n            print(\"%25s (r=%4d, d=%4d): \"%(name,r,d),end=\"\",flush=True)\n            for m in range(0,m_max+1):\n                print(\"%d, \"%m,end='',flush=True)\n                for t in range(reps):\n                    gc.collect()\n                    t0 = time.process_time()\n                    x = generator(r,2**m,d)\n                    data[name][i,m,t] = time.process_time()-t0\n                    del x\n                if np.mean(data[name][i,m])&gt;=.2: break\n            print()\n        print()\n    return data \n</pre> def time_block(pointsets_fns, rds):     data = {}     for name,(generator,pltkwargs) in pointsets_fns.items():         data[name] = np.nan*np.empty((len(rds),m_max+1,reps),dtype=np.float64)         for i,(r,d) in enumerate(rds):             print(\"%25s (r=%4d, d=%4d): \"%(name,r,d),end=\"\",flush=True)             for m in range(0,m_max+1):                 print(\"%d, \"%m,end='',flush=True)                 for t in range(reps):                     gc.collect()                     t0 = time.process_time()                     x = generator(r,2**m,d)                     data[name][i,m,t] = time.process_time()-t0                     del x                 if np.mean(data[name][i,m])&gt;=.2: break             print()         print()     return data  In\u00a0[19]: Copied! <pre>pointsets_noho_fns = OrderedDict({\n    r\"DN${}_{1}$ LMS DS\": (lambda r,n,d: qp.DigitalNetB2(d,randomize=\"LMS_DS\",replications=r).gen_samples(n),{\"color\":COLORS[0],\"marker\":MARKERS[0]}),\n    \"Halton LMS DP\": (lambda r,n,d: qp.Halton(d,randomize=\"LMS_DS\",replications=r).gen_samples(n),{\"color\":COLORS[1],\"marker\":MARKERS[1]}),\n    \"IID\": (lambda r,n,d: qp.IIDStdUniform(d,replications=r).gen_samples(n),{\"color\":COLORS[2],\"marker\":MARKERS[2]}),\n    r\"DN${}_{1}$ NUS\": (lambda r,n,d: qp.DigitalNetB2(d,randomize=\"NUS\",replications=r).gen_samples(n),{\"color\":COLORS[3],\"marker\":MARKERS[3]}),\n    \"Halton NUS\": (lambda r,n,d: qp.Halton(d,randomize=\"NUS\",replications=r).gen_samples(n),{\"color\":COLORS[4],\"marker\":MARKERS[4]}),\n    \"Lattice Shift\": (lambda r,n,d: qp.Lattice(d,replications=r).gen_samples(n),{\"color\":COLORS[5],\"marker\":MARKERS[5]}),\n    r\"SciPy DN${}_{1}$ LMS DS\": (lambda r,n,d: np.stack([scipy.stats.qmc.Sobol(d=d,scramble=True,bits=63).random(n) for i in range(r)],axis=0),{\"color\":COLORS[6],\"marker\":MARKERS[6]}),\n    \"Halton DP\": (lambda r,n,d: qp.Halton(d,randomize=\"DP\",replications=r).gen_samples(n),{\"color\":COLORS[7],\"marker\":MARKERS[7]}),\n    r\"PyTorch DN${}_{1}$ LMS DS\": (lambda r,n,d: np.stack([torch.quasirandom.SobolEngine(d,scramble=True).draw(n) for i in range(r)],axis=0),{\"color\":COLORS[8],\"marker\":MARKERS[8]}),\n    \"SciPy Halton DP\": (lambda r,n,d: np.stack([scipy.stats.qmc.Halton(d=d,scramble=True).random(n) for i in range(r)],axis=0),{\"color\":COLORS[9],\"marker\":MARKERS[9]}),\n})\nrds_noho = np.array([\n    [1,1],\n    [1,100],\n    [100,1],\n    [10,10],\n])\nt_noho = time_block(pointsets_noho_fns,rds_noho) \n</pre> pointsets_noho_fns = OrderedDict({     r\"DN${}_{1}$ LMS DS\": (lambda r,n,d: qp.DigitalNetB2(d,randomize=\"LMS_DS\",replications=r).gen_samples(n),{\"color\":COLORS[0],\"marker\":MARKERS[0]}),     \"Halton LMS DP\": (lambda r,n,d: qp.Halton(d,randomize=\"LMS_DS\",replications=r).gen_samples(n),{\"color\":COLORS[1],\"marker\":MARKERS[1]}),     \"IID\": (lambda r,n,d: qp.IIDStdUniform(d,replications=r).gen_samples(n),{\"color\":COLORS[2],\"marker\":MARKERS[2]}),     r\"DN${}_{1}$ NUS\": (lambda r,n,d: qp.DigitalNetB2(d,randomize=\"NUS\",replications=r).gen_samples(n),{\"color\":COLORS[3],\"marker\":MARKERS[3]}),     \"Halton NUS\": (lambda r,n,d: qp.Halton(d,randomize=\"NUS\",replications=r).gen_samples(n),{\"color\":COLORS[4],\"marker\":MARKERS[4]}),     \"Lattice Shift\": (lambda r,n,d: qp.Lattice(d,replications=r).gen_samples(n),{\"color\":COLORS[5],\"marker\":MARKERS[5]}),     r\"SciPy DN${}_{1}$ LMS DS\": (lambda r,n,d: np.stack([scipy.stats.qmc.Sobol(d=d,scramble=True,bits=63).random(n) for i in range(r)],axis=0),{\"color\":COLORS[6],\"marker\":MARKERS[6]}),     \"Halton DP\": (lambda r,n,d: qp.Halton(d,randomize=\"DP\",replications=r).gen_samples(n),{\"color\":COLORS[7],\"marker\":MARKERS[7]}),     r\"PyTorch DN${}_{1}$ LMS DS\": (lambda r,n,d: np.stack([torch.quasirandom.SobolEngine(d,scramble=True).draw(n) for i in range(r)],axis=0),{\"color\":COLORS[8],\"marker\":MARKERS[8]}),     \"SciPy Halton DP\": (lambda r,n,d: np.stack([scipy.stats.qmc.Halton(d=d,scramble=True).random(n) for i in range(r)],axis=0),{\"color\":COLORS[9],\"marker\":MARKERS[9]}), }) rds_noho = np.array([     [1,1],     [1,100],     [100,1],     [10,10], ]) t_noho = time_block(pointsets_noho_fns,rds_noho)  <pre>        DN${}_{1}$ LMS DS (r=   1, d=   1): 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, \n        DN${}_{1}$ LMS DS (r=   1, d= 100): 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, \n        DN${}_{1}$ LMS DS (r= 100, d=   1): 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, \n        DN${}_{1}$ LMS DS (r=  10, d=  10): 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, \n\n            Halton LMS DP (r=   1, d=   1): 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, \n            Halton LMS DP (r=   1, d= 100): 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, \n            Halton LMS DP (r= 100, d=   1): 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, \n            Halton LMS DP (r=  10, d=  10): 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, \n\n                      IID (r=   1, d=   1): 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, \n                      IID (r=   1, d= 100): 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, \n                      IID (r= 100, d=   1): 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, \n                      IID (r=  10, d=  10): 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, \n\n           DN${}_{1}$ NUS (r=   1, d=   1): 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, \n           DN${}_{1}$ NUS (r=   1, d= 100): 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n           DN${}_{1}$ NUS (r= 100, d=   1): 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n           DN${}_{1}$ NUS (r=  10, d=  10): 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n\n               Halton NUS (r=   1, d=   1): 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, \n               Halton NUS (r=   1, d= 100): 0, 1, 2, 3, 4, \n               Halton NUS (r= 100, d=   1): 0, 1, 2, 3, 4, 5, \n               Halton NUS (r=  10, d=  10): 0, 1, 2, 3, 4, 5, \n\n            Lattice Shift (r=   1, d=   1): 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, \n            Lattice Shift (r=   1, d= 100): 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, \n            Lattice Shift (r= 100, d=   1): 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, \n            Lattice Shift (r=  10, d=  10): 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, \n\n  SciPy DN${}_{1}$ LMS DS (r=   1, d=   1): 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, \n  SciPy DN${}_{1}$ LMS DS (r=   1, d= 100): 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, \n  SciPy DN${}_{1}$ LMS DS (r= 100, d=   1): 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, \n  SciPy DN${}_{1}$ LMS DS (r=  10, d=  10): 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, \n\n                Halton DP (r=   1, d=   1): 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, \n                Halton DP (r=   1, d= 100): 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, \n                Halton DP (r= 100, d=   1): 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, \n                Halton DP (r=  10, d=  10): 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, \n\nPyTorch DN${}_{1}$ LMS DS (r=   1, d=   1): 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, \nPyTorch DN${}_{1}$ LMS DS (r=   1, d= 100): 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, \nPyTorch DN${}_{1}$ LMS DS (r= 100, d=   1): 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, \nPyTorch DN${}_{1}$ LMS DS (r=  10, d=  10): 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, \n\n          SciPy Halton DP (r=   1, d=   1): 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, \n          SciPy Halton DP (r=   1, d= 100): 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, \n          SciPy Halton DP (r= 100, d=   1): 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, \n          SciPy Halton DP (r=  10, d=  10): 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, \n\n</pre> In\u00a0[20]: Copied! <pre>pointsets_dnb2_lms_ds_nus_ho_fns = OrderedDict({\n    r\"DN${}_{1}$ LMS DS\": (lambda r,n,d: qp.DigitalNetB2(d,randomize=\"LMS_DS\",replications=r,alpha=1).gen_samples(n),{\"color\":COLORS[0],\"marker\":MARKERS[0]}),\n    r\"DN${}_{1}$ NUS\": (lambda r,n,d: qp.DigitalNetB2(d,randomize=\"NUS\",replications=r,alpha=1).gen_samples(n),{\"color\":COLORS[3],\"marker\":MARKERS[3]}),\n    r\"DN${}_{2}$ LMS DS\": (lambda r,n,d: qp.DigitalNetB2(d,randomize=\"LMS_DS\",replications=r,alpha=2).gen_samples(n),{\"color\":COLORS[0],\"marker\":MARKERS[1]}),\n    r\"DN${}_{2}$ NUS\": (lambda r,n,d: qp.DigitalNetB2(d,randomize=\"NUS\",replications=r,alpha=2).gen_samples(n),{\"color\":COLORS[3],\"marker\":MARKERS[4]}),\n    r\"DN${}_{3}$ LMS DS\": (lambda r,n,d: qp.DigitalNetB2(d,randomize=\"LMS_DS\",replications=r,alpha=3).gen_samples(n),{\"color\":COLORS[0],\"marker\":MARKERS[2]}),\n    r\"DN${}_{3}$ NUS\": (lambda r,n,d: qp.DigitalNetB2(d,randomize=\"NUS\",replications=r,alpha=3).gen_samples(n),{\"color\":COLORS[3],\"marker\":MARKERS[5]}),\n    r\"DN${}_{4}$ LMS DS\": (lambda r,n,d: qp.DigitalNetB2(d,randomize=\"LMS_DS\",replications=r,alpha=4).gen_samples(n),{\"color\":COLORS[0],\"marker\":MARKERS[7]}),\n    r\"DN${}_{4}$ NUS\": (lambda r,n,d: qp.DigitalNetB2(d,randomize=\"NUS\",replications=r,alpha=4).gen_samples(n),{\"color\":COLORS[3],\"marker\":MARKERS[6]}),\n})\nrds_dnb2_lms_ds_nus_ho = np.array([\n    [1,1],\n    [1,100],\n    [100,1],\n    [10,10],\n])\nt_dnb2_lms_ds_nus_ho = time_block(pointsets_dnb2_lms_ds_nus_ho_fns,rds_dnb2_lms_ds_nus_ho)\n</pre> pointsets_dnb2_lms_ds_nus_ho_fns = OrderedDict({     r\"DN${}_{1}$ LMS DS\": (lambda r,n,d: qp.DigitalNetB2(d,randomize=\"LMS_DS\",replications=r,alpha=1).gen_samples(n),{\"color\":COLORS[0],\"marker\":MARKERS[0]}),     r\"DN${}_{1}$ NUS\": (lambda r,n,d: qp.DigitalNetB2(d,randomize=\"NUS\",replications=r,alpha=1).gen_samples(n),{\"color\":COLORS[3],\"marker\":MARKERS[3]}),     r\"DN${}_{2}$ LMS DS\": (lambda r,n,d: qp.DigitalNetB2(d,randomize=\"LMS_DS\",replications=r,alpha=2).gen_samples(n),{\"color\":COLORS[0],\"marker\":MARKERS[1]}),     r\"DN${}_{2}$ NUS\": (lambda r,n,d: qp.DigitalNetB2(d,randomize=\"NUS\",replications=r,alpha=2).gen_samples(n),{\"color\":COLORS[3],\"marker\":MARKERS[4]}),     r\"DN${}_{3}$ LMS DS\": (lambda r,n,d: qp.DigitalNetB2(d,randomize=\"LMS_DS\",replications=r,alpha=3).gen_samples(n),{\"color\":COLORS[0],\"marker\":MARKERS[2]}),     r\"DN${}_{3}$ NUS\": (lambda r,n,d: qp.DigitalNetB2(d,randomize=\"NUS\",replications=r,alpha=3).gen_samples(n),{\"color\":COLORS[3],\"marker\":MARKERS[5]}),     r\"DN${}_{4}$ LMS DS\": (lambda r,n,d: qp.DigitalNetB2(d,randomize=\"LMS_DS\",replications=r,alpha=4).gen_samples(n),{\"color\":COLORS[0],\"marker\":MARKERS[7]}),     r\"DN${}_{4}$ NUS\": (lambda r,n,d: qp.DigitalNetB2(d,randomize=\"NUS\",replications=r,alpha=4).gen_samples(n),{\"color\":COLORS[3],\"marker\":MARKERS[6]}), }) rds_dnb2_lms_ds_nus_ho = np.array([     [1,1],     [1,100],     [100,1],     [10,10], ]) t_dnb2_lms_ds_nus_ho = time_block(pointsets_dnb2_lms_ds_nus_ho_fns,rds_dnb2_lms_ds_nus_ho) <pre>        DN${}_{1}$ LMS DS (r=   1, d=   1): 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, \n        DN${}_{1}$ LMS DS (r=   1, d= 100): 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, \n        DN${}_{1}$ LMS DS (r= 100, d=   1): 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, \n        DN${}_{1}$ LMS DS (r=  10, d=  10): 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, \n\n           DN${}_{1}$ NUS (r=   1, d=   1): 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, \n           DN${}_{1}$ NUS (r=   1, d= 100): 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n           DN${}_{1}$ NUS (r= 100, d=   1): 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n           DN${}_{1}$ NUS (r=  10, d=  10): 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n\n        DN${}_{2}$ LMS DS (r=   1, d=   1): 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, \n        DN${}_{2}$ LMS DS (r=   1, d= 100): 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, \n        DN${}_{2}$ LMS DS (r= 100, d=   1): 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, \n        DN${}_{2}$ LMS DS (r=  10, d=  10): 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, \n\n           DN${}_{2}$ NUS (r=   1, d=   1): 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, \n           DN${}_{2}$ NUS (r=   1, d= 100): 0, 1, 2, 3, 4, 5, 6, 7, 8, \n           DN${}_{2}$ NUS (r= 100, d=   1): 0, 1, 2, 3, 4, 5, 6, 7, 8, \n           DN${}_{2}$ NUS (r=  10, d=  10): 0, 1, 2, 3, 4, 5, 6, 7, 8, \n\n        DN${}_{3}$ LMS DS (r=   1, d=   1): 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, \n        DN${}_{3}$ LMS DS (r=   1, d= 100): 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, \n        DN${}_{3}$ LMS DS (r= 100, d=   1): 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, \n        DN${}_{3}$ LMS DS (r=  10, d=  10): 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, \n\n           DN${}_{3}$ NUS (r=   1, d=   1): 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, \n           DN${}_{3}$ NUS (r=   1, d= 100): 0, 1, 2, 3, 4, 5, 6, 7, \n           DN${}_{3}$ NUS (r= 100, d=   1): 0, 1, 2, 3, 4, 5, 6, 7, \n           DN${}_{3}$ NUS (r=  10, d=  10): 0, 1, 2, 3, 4, 5, 6, 7, \n\n        DN${}_{4}$ LMS DS (r=   1, d=   1): 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, \n        DN${}_{4}$ LMS DS (r=   1, d= 100): 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, \n        DN${}_{4}$ LMS DS (r= 100, d=   1): 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, \n        DN${}_{4}$ LMS DS (r=  10, d=  10): 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, \n\n           DN${}_{4}$ NUS (r=   1, d=   1): 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, \n           DN${}_{4}$ NUS (r=   1, d= 100): 0, 1, 2, 3, 4, 5, 6, 7, \n           DN${}_{4}$ NUS (r= 100, d=   1): 0, 1, 2, 3, 4, 5, 6, 7, \n           DN${}_{4}$ NUS (r=  10, d=  10): 0, 1, 2, 3, 4, 5, 6, 7, \n\n</pre> In\u00a0[21]: Copied! <pre>rds_ft = np.array([\n    [1,1],\n    [10,1],\n    [100,1],\n    [1000,1],\n])\nassert (rds_ft[:,1]==1).all()\n_rmax = 100\nx_fft  = np.random.rand(_rmax*2**(m_max))+1j*np.random.rand(_rmax*2**(m_max))\nx_fwht = np.random.rand(_rmax*2**(m_max))\nft_fns = OrderedDict({\n    \"FFT BR\": (lambda r,n,d: qp.fftbr(x_fft[:r*n].reshape((r,n))),{\"color\":COLORS[0],\"marker\":MARKERS[0]}),\n    \"SciPy FFT\": (lambda r,n,d: scipy.fft.fft(x_fft[:r*n].reshape((r,n))),{\"color\":COLORS[0],\"marker\":MARKERS[1]}),\n    \"IFFT BR\": (lambda r,n,d: qp.ifftbr(x_fft[:r*n].reshape((r,n))),{\"color\":COLORS[1],\"marker\":MARKERS[2]}),\n    \"SciPy IFFT\": (lambda r,n,d: scipy.fft.ifft(x_fft[:r*n].reshape((r,n))),{\"color\":COLORS[1],\"marker\":MARKERS[3]}),\n    \"FWHT\": (lambda r,n,d: qp.fwht(x_fwht[:r*n].reshape((r,n))),{\"color\":COLORS[2],\"marker\":MARKERS[4]}),\n    \"SymPy FWHT\": (lambda r,n,d: np.stack([sympy.fwht(x_fwht_i) for x_fwht_i in x_fwht[:r*n].reshape((r,n))],axis=0),{\"color\":COLORS[2],\"marker\":MARKERS[5]}),\n})\nt_ft = time_block(ft_fns,rds_ft)\n</pre> rds_ft = np.array([     [1,1],     [10,1],     [100,1],     [1000,1], ]) assert (rds_ft[:,1]==1).all() _rmax = 100 x_fft  = np.random.rand(_rmax*2**(m_max))+1j*np.random.rand(_rmax*2**(m_max)) x_fwht = np.random.rand(_rmax*2**(m_max)) ft_fns = OrderedDict({     \"FFT BR\": (lambda r,n,d: qp.fftbr(x_fft[:r*n].reshape((r,n))),{\"color\":COLORS[0],\"marker\":MARKERS[0]}),     \"SciPy FFT\": (lambda r,n,d: scipy.fft.fft(x_fft[:r*n].reshape((r,n))),{\"color\":COLORS[0],\"marker\":MARKERS[1]}),     \"IFFT BR\": (lambda r,n,d: qp.ifftbr(x_fft[:r*n].reshape((r,n))),{\"color\":COLORS[1],\"marker\":MARKERS[2]}),     \"SciPy IFFT\": (lambda r,n,d: scipy.fft.ifft(x_fft[:r*n].reshape((r,n))),{\"color\":COLORS[1],\"marker\":MARKERS[3]}),     \"FWHT\": (lambda r,n,d: qp.fwht(x_fwht[:r*n].reshape((r,n))),{\"color\":COLORS[2],\"marker\":MARKERS[4]}),     \"SymPy FWHT\": (lambda r,n,d: np.stack([sympy.fwht(x_fwht_i) for x_fwht_i in x_fwht[:r*n].reshape((r,n))],axis=0),{\"color\":COLORS[2],\"marker\":MARKERS[5]}), }) t_ft = time_block(ft_fns,rds_ft) <pre>                   FFT BR (r=   1, d=   1): 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, \n                   FFT BR (r=  10, d=   1): 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, \n                   FFT BR (r= 100, d=   1): 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, \n                   FFT BR (r=1000, d=   1): 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, \n\n                SciPy FFT (r=   1, d=   1): 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, \n                SciPy FFT (r=  10, d=   1): 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, \n                SciPy FFT (r= 100, d=   1): 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, \n                SciPy FFT (r=1000, d=   1): 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, \n\n                  IFFT BR (r=   1, d=   1): 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, \n                  IFFT BR (r=  10, d=   1): 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, \n                  IFFT BR (r= 100, d=   1): 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, \n                  IFFT BR (r=1000, d=   1): 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, \n\n               SciPy IFFT (r=   1, d=   1): 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, \n               SciPy IFFT (r=  10, d=   1): 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, \n               SciPy IFFT (r= 100, d=   1): 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, \n               SciPy IFFT (r=1000, d=   1): 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, \n\n                     FWHT (r=   1, d=   1): 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, \n                     FWHT (r=  10, d=   1): 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, \n                     FWHT (r= 100, d=   1): 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, \n                     FWHT (r=1000, d=   1): 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, \n\n               SymPy FWHT (r=   1, d=   1): 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, \n               SymPy FWHT (r=  10, d=   1): 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, \n               SymPy FWHT (r= 100, d=   1): 0, 1, 2, 3, 4, 5, 6, 7, 8, \n               SymPy FWHT (r=1000, d=   1): 0, 1, 2, 3, 4, 5, \n\n</pre> In\u00a0[45]: Copied! <pre>ncols = 4\nnrows = 3\nmvec = np.arange(0,m_max+1)\nnvec = 2**mvec\nfig = pyplot.figure(figsize=(MW2,MW2/ncols*nrows*1.2),constrained_layout=False,tight_layout=False)#,sharey=True,sharex=True)\nsubfigs = fig.subfigures(nrows=nrows,ncols=1)\nax = np.stack([subfigs[j].subplots(nrows=1,ncols=ncols,sharey=True,sharex=True) for j in range(nrows)],axis=0)\ncommonkwargs = {\"markersize\":2.5,\"linewidth\":.5}#,\"markerfacecolor\":'black',\"markeredgecolor\":'white'}\n# statfunc = lambda x: np.nanquantile(x[:,1:],q=.5,axis=1)\nstatfunc = lambda x: np.nanmean(x[:,1:],axis=1)\nfor name in list(t_noho.keys()):\n    for j in range(ncols):\n        ax[0,j].plot(nvec,statfunc(t_noho[name][j,mvec]),label=name,**pointsets_noho_fns[name][1],**commonkwargs)\nfor name in list(t_dnb2_lms_ds_nus_ho.keys()):\n    for j in range(ncols):\n        ax[1,j].plot(nvec,statfunc(t_dnb2_lms_ds_nus_ho[name][j,mvec]),label=name,**pointsets_dnb2_lms_ds_nus_ho_fns[name][1],**commonkwargs)\nfor name in list(t_ft.keys()):\n    for j in range(ncols):\n        ax[2,j].plot(nvec,statfunc(t_ft[name][j,mvec]),label=name,**ft_fns[name][1],**commonkwargs)\nsubfigs[0].legend(*ax[0,0].get_legend_handles_labels(),frameon=False,bbox_to_anchor=(.925,.14),ncol=4)#,fontsize=\"medium\")\nsubfigs[1].legend(*ax[1,0].get_legend_handles_labels(),frameon=False,bbox_to_anchor=(.82,.1),ncol=4)#,fontsize=\"medium\")\nsubfigs[2].legend(*ax[2,0].get_legend_handles_labels(),frameon=False,bbox_to_anchor=(.64,.125),ncol=3)#,fontsize=\"medium\")\nfor i in range(nrows):\n    for j in range(ncols):\n        ax[i,j].set_xscale('log',base=2)\n        ax[i,j].set_yscale('log',base=10)\n        #ax[i,j].yaxis.set_tick_params(labelleft=True)\n        ax[i,j].xaxis.set_tick_params(labelbottom=True)\n        ax[i,j].grid(True)\n        ax[i,j].set_xlim(nvec[0],nvec[-1])\n        _xmin,_xmax = ax[i,j].get_xlim()\n        _ymin,_ymax = ax[i,j].get_ylim()\n        ax[i,j].set_aspect((np.log2(_xmax)-np.log2(_xmin))/(np.log10(_ymax)-np.log10(_ymin)))\n    #ax[i,0].set_ylabel('time (sec)',fontsize=\"xx-large\")\nfor j in range(ncols):\n    ax[0,j].set_title(r\"$(R,d)=(%d,%d)$\"%(rds_noho[j,0],rds_noho[j,1]))\n    ax[1,j].set_title(r\"$(R,d)=(%d,%d)$\"%(rds_dnb2_lms_ds_nus_ho[j,0],rds_dnb2_lms_ds_nus_ho[j,1]))\n    ax[2,j].set_title(r\"$R=%d$\"%rds_ft[j,0])\nax[0,0].set_ylabel(\"popular pointsets\")\nax[1,0].set_ylabel(\"higher-order digital nets\")\nax[2,0].set_ylabel(\"fast transforms\");\nfig.text(s=r\"time (sec) vs number of points $n$\",x=.42,y=.98,fontsize=\"x-large\");\nfig.savefig(\"outputs/timing.png\",format=\"png\",dpi=1024,transparent=True,bbox_inches=\"tight\")\n</pre> ncols = 4 nrows = 3 mvec = np.arange(0,m_max+1) nvec = 2**mvec fig = pyplot.figure(figsize=(MW2,MW2/ncols*nrows*1.2),constrained_layout=False,tight_layout=False)#,sharey=True,sharex=True) subfigs = fig.subfigures(nrows=nrows,ncols=1) ax = np.stack([subfigs[j].subplots(nrows=1,ncols=ncols,sharey=True,sharex=True) for j in range(nrows)],axis=0) commonkwargs = {\"markersize\":2.5,\"linewidth\":.5}#,\"markerfacecolor\":'black',\"markeredgecolor\":'white'} # statfunc = lambda x: np.nanquantile(x[:,1:],q=.5,axis=1) statfunc = lambda x: np.nanmean(x[:,1:],axis=1) for name in list(t_noho.keys()):     for j in range(ncols):         ax[0,j].plot(nvec,statfunc(t_noho[name][j,mvec]),label=name,**pointsets_noho_fns[name][1],**commonkwargs) for name in list(t_dnb2_lms_ds_nus_ho.keys()):     for j in range(ncols):         ax[1,j].plot(nvec,statfunc(t_dnb2_lms_ds_nus_ho[name][j,mvec]),label=name,**pointsets_dnb2_lms_ds_nus_ho_fns[name][1],**commonkwargs) for name in list(t_ft.keys()):     for j in range(ncols):         ax[2,j].plot(nvec,statfunc(t_ft[name][j,mvec]),label=name,**ft_fns[name][1],**commonkwargs) subfigs[0].legend(*ax[0,0].get_legend_handles_labels(),frameon=False,bbox_to_anchor=(.925,.14),ncol=4)#,fontsize=\"medium\") subfigs[1].legend(*ax[1,0].get_legend_handles_labels(),frameon=False,bbox_to_anchor=(.82,.1),ncol=4)#,fontsize=\"medium\") subfigs[2].legend(*ax[2,0].get_legend_handles_labels(),frameon=False,bbox_to_anchor=(.64,.125),ncol=3)#,fontsize=\"medium\") for i in range(nrows):     for j in range(ncols):         ax[i,j].set_xscale('log',base=2)         ax[i,j].set_yscale('log',base=10)         #ax[i,j].yaxis.set_tick_params(labelleft=True)         ax[i,j].xaxis.set_tick_params(labelbottom=True)         ax[i,j].grid(True)         ax[i,j].set_xlim(nvec[0],nvec[-1])         _xmin,_xmax = ax[i,j].get_xlim()         _ymin,_ymax = ax[i,j].get_ylim()         ax[i,j].set_aspect((np.log2(_xmax)-np.log2(_xmin))/(np.log10(_ymax)-np.log10(_ymin)))     #ax[i,0].set_ylabel('time (sec)',fontsize=\"xx-large\") for j in range(ncols):     ax[0,j].set_title(r\"$(R,d)=(%d,%d)$\"%(rds_noho[j,0],rds_noho[j,1]))     ax[1,j].set_title(r\"$(R,d)=(%d,%d)$\"%(rds_dnb2_lms_ds_nus_ho[j,0],rds_dnb2_lms_ds_nus_ho[j,1]))     ax[2,j].set_title(r\"$R=%d$\"%rds_ft[j,0]) ax[0,0].set_ylabel(\"popular pointsets\") ax[1,0].set_ylabel(\"higher-order digital nets\") ax[2,0].set_ylabel(\"fast transforms\"); fig.text(s=r\"time (sec) vs number of points $n$\",x=.42,y=.98,fontsize=\"x-large\"); fig.savefig(\"outputs/timing.png\",format=\"png\",dpi=1024,transparent=True,bbox_inches=\"tight\") <pre>/var/folders/xk/w1s5c54x0zv90dgqk3vpmbsw004hmz/T/ipykernel_57279/3255602729.py:5: UserWarning: The Figure parameters 'tight_layout' and 'constrained_layout' cannot be used together. Please use 'layout' parameter\n  fig = pyplot.figure(figsize=(MW2,MW2/ncols*nrows*1.2),constrained_layout=False,tight_layout=False)#,sharey=True,sharex=True)\n/var/folders/xk/w1s5c54x0zv90dgqk3vpmbsw004hmz/T/ipykernel_57279/3255602729.py:10: RuntimeWarning: Mean of empty slice\n  statfunc = lambda x: np.nanmean(x[:,1:],axis=1)\n</pre> In\u00a0[24]: Copied! <pre># https://www.sfu.ca/~ssurjano/sulf.html\ndef sulfer_func(t):\n    Tr       = t[...,0]\n    fAc      = t[...,1]\n    fRs      = t[...,2]\n    beta_bar = t[...,3]\n    Psi_e    = t[...,4]\n    f_Psi_e  = t[...,5]\n    Q        = t[...,6]\n    Y        = t[...,7]\n    L        = t[...,8]\n    S0 = 1366;\n    A  = 5*10**14;\n    fact1 = (S0**2) * fAc * (Tr**2) * fRs**2 * beta_bar * Psi_e * f_Psi_e;\n    fact2 = 3*Q*Y*L / A;\n    DeltaF = -1/2 * fact1 * fact2;\n    return DeltaF\nsulfer_cf = qp.CustomFun(\n    qp.SciPyWrapper(\n        qp.IIDStdUniform(9),\n        [   scipy.stats.lognorm(scale=0.76, s=np.log(1.2)),\n            scipy.stats.lognorm(scale=0.39, s=np.log(1.1)),\n            scipy.stats.lognorm(scale=0.85, s=np.log(1.1)),\n            scipy.stats.lognorm(scale=0.3,  s=np.log(1.3)),\n            scipy.stats.lognorm(scale=5.0,  s=np.log(1.4)),\n            scipy.stats.lognorm(scale=1.7,  s=np.log(1.2)),\n            scipy.stats.lognorm(scale=71.0, s=np.log(1.15)),\n            scipy.stats.lognorm(scale=0.5,  s=np.log(1.5)),\n            scipy.stats.lognorm(scale=5.5,  s=np.log(1.5)),]),\n    sulfer_func)\n</pre> # https://www.sfu.ca/~ssurjano/sulf.html def sulfer_func(t):     Tr       = t[...,0]     fAc      = t[...,1]     fRs      = t[...,2]     beta_bar = t[...,3]     Psi_e    = t[...,4]     f_Psi_e  = t[...,5]     Q        = t[...,6]     Y        = t[...,7]     L        = t[...,8]     S0 = 1366;     A  = 5*10**14;     fact1 = (S0**2) * fAc * (Tr**2) * fRs**2 * beta_bar * Psi_e * f_Psi_e;     fact2 = 3*Q*Y*L / A;     DeltaF = -1/2 * fact1 * fact2;     return DeltaF sulfer_cf = qp.CustomFun(     qp.SciPyWrapper(         qp.IIDStdUniform(9),         [   scipy.stats.lognorm(scale=0.76, s=np.log(1.2)),             scipy.stats.lognorm(scale=0.39, s=np.log(1.1)),             scipy.stats.lognorm(scale=0.85, s=np.log(1.1)),             scipy.stats.lognorm(scale=0.3,  s=np.log(1.3)),             scipy.stats.lognorm(scale=5.0,  s=np.log(1.4)),             scipy.stats.lognorm(scale=1.7,  s=np.log(1.2)),             scipy.stats.lognorm(scale=71.0, s=np.log(1.15)),             scipy.stats.lognorm(scale=0.5,  s=np.log(1.5)),             scipy.stats.lognorm(scale=5.5,  s=np.log(1.5)),]),     sulfer_func) In\u00a0[25]: Copied! <pre># https://www.sfu.ca/~ssurjano/borehole.html \ndef borehole_func(t):\n    rw = t[...,0];\n    r  = t[...,1];\n    Tu = t[...,2];\n    Hu = t[...,3];\n    Tl = t[...,4];\n    Hl = t[...,5];\n    L  = t[...,6];\n    Kw = t[...,7];\n    frac1 = 2 * np.pi * Tu * (Hu-Hl);\n    frac2a = 2*L*Tu / (np.log(r/rw)*rw**2*Kw);\n    frac2b = Tu / Tl;\n    frac2 = np.log(r/rw) * (1+frac2a+frac2b);\n    y = frac1 / frac2;\n    return y \nborehole_cf = qp.CustomFun(\n    qp.SciPyWrapper(\n        qp.IIDStdUniform(8),\n        [   scipy.stats.norm(loc=0.10,scale=0.0161812),\n            scipy.stats.lognorm(scale=np.exp(7.71),s=1.0056),\n            scipy.stats.uniform(loc=63070,scale=115600-63070),\n            scipy.stats.uniform(loc=990,scale=1110-990),\n            scipy.stats.uniform(loc=63.1,scale=116-63.1),\n            scipy.stats.uniform(loc=700,scale=820-700),\n            scipy.stats.uniform(loc=1120,scale=1680-1120),\n            scipy.stats.uniform(loc=9855,scale=12045-9855),]),\n    borehole_func)\n</pre> # https://www.sfu.ca/~ssurjano/borehole.html  def borehole_func(t):     rw = t[...,0];     r  = t[...,1];     Tu = t[...,2];     Hu = t[...,3];     Tl = t[...,4];     Hl = t[...,5];     L  = t[...,6];     Kw = t[...,7];     frac1 = 2 * np.pi * Tu * (Hu-Hl);     frac2a = 2*L*Tu / (np.log(r/rw)*rw**2*Kw);     frac2b = Tu / Tl;     frac2 = np.log(r/rw) * (1+frac2a+frac2b);     y = frac1 / frac2;     return y  borehole_cf = qp.CustomFun(     qp.SciPyWrapper(         qp.IIDStdUniform(8),         [   scipy.stats.norm(loc=0.10,scale=0.0161812),             scipy.stats.lognorm(scale=np.exp(7.71),s=1.0056),             scipy.stats.uniform(loc=63070,scale=115600-63070),             scipy.stats.uniform(loc=990,scale=1110-990),             scipy.stats.uniform(loc=63.1,scale=116-63.1),             scipy.stats.uniform(loc=700,scale=820-700),             scipy.stats.uniform(loc=1120,scale=1680-1120),             scipy.stats.uniform(loc=9855,scale=12045-9855),]),     borehole_func) In\u00a0[26]: Copied! <pre># https://www.sfu.ca/~ssurjano/webetal96.html\nwebster_cf = qp.CustomFun(\n    qp.SciPyWrapper(\n        qp.IIDStdUniform(2),\n        [   scipy.stats.uniform(loc=1,scale=10-1),\n            scipy.stats.norm(loc=2,scale=1)]),\n    lambda x: x[:,0]**2+x[:,1]**3)\n</pre> # https://www.sfu.ca/~ssurjano/webetal96.html webster_cf = qp.CustomFun(     qp.SciPyWrapper(         qp.IIDStdUniform(2),         [   scipy.stats.uniform(loc=1,scale=10-1),             scipy.stats.norm(loc=2,scale=1)]),     lambda x: x[:,0]**2+x[:,1]**3) In\u00a0[27]: Copied! <pre>def oakley_ohagan15_func(t):\n    a1 = np.array([0.0118, 0.0456, 0.2297, 0.0393, 0.1177, 0.3865, 0.3897, 0.6061, 0.6159, 0.4005, 1.0741, 1.1474, 0.7880, 1.1242, 1.1982])\n    a2 = np.array([0.4341, 0.0887, 0.0512, 0.3233, 0.1489, 1.0360, 0.9892, 0.9672, 0.8977, 0.8083, 1.8426, 2.4712, 2.3946, 2.0045, 2.2621])\n    a3 = np.array([0.1044, 0.2057, 0.0774, 0.2730, 0.1253, 0.7526, 0.8570, 1.0331, 0.8388, 0.7970, 2.2145, 2.0382, 2.4004, 2.0541, 1.9845])\n    M = np.array([\n        [-0.022482886,  -0.18501666,  0.13418263,   0.36867264,   0.17172785,   0.13651143,  -0.44034404, -0.081422854,   0.71321025,  -0.44361072,   0.50383394, -0.024101458, -0.045939684,   0.21666181,  0.055887417],\n        [   0.25659630,  0.053792287,  0.25800381,   0.23795905,  -0.59125756, -0.081627077,  -0.28749073,   0.41581639,   0.49752241,  0.083893165,  -0.11056683,  0.033222351,  -0.13979497, -0.031020556,  -0.22318721],\n        [ -0.055999811,   0.19542252, 0.095529005,  -0.28626530,  -0.14441303,   0.22369356,   0.14527412,   0.28998481,   0.23105010,  -0.31929879,  -0.29039128,  -0.20956898,   0.43139047,  0.024429152,  0.044904409],\n        [   0.66448103,   0.43069872,  0.29924645,  -0.16202441,  -0.31479544,  -0.39026802,   0.17679822,  0.057952663,   0.17230342,   0.13466011,  -0.35275240,   0.25146896, -0.018810529,   0.36482392,  -0.32504618],\n        [  -0.12127800,   0.12463327,  0.10656519,  0.046562296,  -0.21678617,   0.19492172, -0.065521126,  0.024404669, -0.096828860,   0.19366196,   0.33354757,   0.31295994, -0.083615456,  -0.25342082,   0.37325717],\n        [  -0.28376230,  -0.32820154, -0.10496068,  -0.22073452,  -0.13708154,  -0.14426375,  -0.11503319,   0.22424151, -0.030395022,  -0.51505615,  0.017254978,  0.038957118,   0.36069184,   0.30902452,  0.050030193],\n        [ -0.077875893, 0.0037456560,  0.88685604,  -0.26590028, -0.079325357, -0.042734919,  -0.18653782,  -0.35604718,  -0.17497421,  0.088699956,   0.40025886, -0.055979693,   0.13724479,   0.21485613, -0.011265799],\n        [ -0.092294730,   0.59209563, 0.031338285, -0.033080861,  -0.24308858, -0.099798547,  0.034460195,  0.095119813,  -0.33801620, 0.0063860024,  -0.61207299,  0.081325416,   0.88683114,   0.14254905,   0.14776204],\n        [  -0.13189434,   0.52878496,  0.12652391,  0.045113625,   0.58373514,   0.37291503,   0.11395325,  -0.29479222,  -0.57014085,   0.46291592, -0.094050179,   0.13959097,  -0.38607402,  -0.44897060,  -0.14602419],\n        [  0.058107658,  -0.32289338, 0.093139162,  0.072427234,  -0.56919401,   0.52554237,   0.23656926, -0.011782016,  0.071820601,  0.078277291,  -0.13355752,   0.22722721,   0.14369455,  -0.45198935,  -0.55574794],\n        [   0.66145875,   0.34633299,  0.14098019,   0.51882591,  -0.28019898,  -0.16032260, -0.068413337,  -0.20428242,  0.069672173,   0.23112577, -0.044368579,  -0.16455425,   0.21620977, 0.0042702105, -0.087399014],\n        [   0.31599556, -0.027551859,  0.13434254,   0.13497371,  0.054005680,  -0.17374789,   0.17525393,  0.060258929,  -0.17914162,  -0.31056619,  -0.25358691,  0.025847535,  -0.43006001,  -0.62266361, -0.033996882],\n        [  -0.29038151,  0.034101270, 0.034903413,  -0.12121764,  0.026030714,  -0.33546274,  -0.41424111,  0.053248380,  -0.27099455, -0.026251302,   0.41024137,   0.26636349,   0.15582891,  -0.18666254,  0.019895831],\n        [  -0.24388652,  -0.44098852, 0.012618825,   0.24945112,  0.071101888,   0.24623792,   0.17484502, 0.0085286769,   0.25147070,  -0.14659862, -0.084625150,   0.36931333,  -0.29955293,   0.11044360,  -0.75690139],\n        [  0.041494323,  -0.25980564,  0.46402128,  -0.36112127,  -0.94980789,  -0.16504063, 0.0030943325,  0.052792942,   0.22523648,   0.38390366,   0.45562427,  -0.18631744, 0.0082333995,   0.16670803,   0.16045688]])\n    return (a1*t).sum(-1) + (a2*np.sin(t)).sum(-1) + (a3*np.cos(t)).sum(-1) + (t*np.einsum(\"ij,...j-&gt;...i\",M,t)).sum(-1)\noakley_ohagan15_cf = qp.CustomFun(\n        qp.Gaussian(qp.IIDStdUniform(15)),\n        oakley_ohagan15_func)\n</pre> def oakley_ohagan15_func(t):     a1 = np.array([0.0118, 0.0456, 0.2297, 0.0393, 0.1177, 0.3865, 0.3897, 0.6061, 0.6159, 0.4005, 1.0741, 1.1474, 0.7880, 1.1242, 1.1982])     a2 = np.array([0.4341, 0.0887, 0.0512, 0.3233, 0.1489, 1.0360, 0.9892, 0.9672, 0.8977, 0.8083, 1.8426, 2.4712, 2.3946, 2.0045, 2.2621])     a3 = np.array([0.1044, 0.2057, 0.0774, 0.2730, 0.1253, 0.7526, 0.8570, 1.0331, 0.8388, 0.7970, 2.2145, 2.0382, 2.4004, 2.0541, 1.9845])     M = np.array([         [-0.022482886,  -0.18501666,  0.13418263,   0.36867264,   0.17172785,   0.13651143,  -0.44034404, -0.081422854,   0.71321025,  -0.44361072,   0.50383394, -0.024101458, -0.045939684,   0.21666181,  0.055887417],         [   0.25659630,  0.053792287,  0.25800381,   0.23795905,  -0.59125756, -0.081627077,  -0.28749073,   0.41581639,   0.49752241,  0.083893165,  -0.11056683,  0.033222351,  -0.13979497, -0.031020556,  -0.22318721],         [ -0.055999811,   0.19542252, 0.095529005,  -0.28626530,  -0.14441303,   0.22369356,   0.14527412,   0.28998481,   0.23105010,  -0.31929879,  -0.29039128,  -0.20956898,   0.43139047,  0.024429152,  0.044904409],         [   0.66448103,   0.43069872,  0.29924645,  -0.16202441,  -0.31479544,  -0.39026802,   0.17679822,  0.057952663,   0.17230342,   0.13466011,  -0.35275240,   0.25146896, -0.018810529,   0.36482392,  -0.32504618],         [  -0.12127800,   0.12463327,  0.10656519,  0.046562296,  -0.21678617,   0.19492172, -0.065521126,  0.024404669, -0.096828860,   0.19366196,   0.33354757,   0.31295994, -0.083615456,  -0.25342082,   0.37325717],         [  -0.28376230,  -0.32820154, -0.10496068,  -0.22073452,  -0.13708154,  -0.14426375,  -0.11503319,   0.22424151, -0.030395022,  -0.51505615,  0.017254978,  0.038957118,   0.36069184,   0.30902452,  0.050030193],         [ -0.077875893, 0.0037456560,  0.88685604,  -0.26590028, -0.079325357, -0.042734919,  -0.18653782,  -0.35604718,  -0.17497421,  0.088699956,   0.40025886, -0.055979693,   0.13724479,   0.21485613, -0.011265799],         [ -0.092294730,   0.59209563, 0.031338285, -0.033080861,  -0.24308858, -0.099798547,  0.034460195,  0.095119813,  -0.33801620, 0.0063860024,  -0.61207299,  0.081325416,   0.88683114,   0.14254905,   0.14776204],         [  -0.13189434,   0.52878496,  0.12652391,  0.045113625,   0.58373514,   0.37291503,   0.11395325,  -0.29479222,  -0.57014085,   0.46291592, -0.094050179,   0.13959097,  -0.38607402,  -0.44897060,  -0.14602419],         [  0.058107658,  -0.32289338, 0.093139162,  0.072427234,  -0.56919401,   0.52554237,   0.23656926, -0.011782016,  0.071820601,  0.078277291,  -0.13355752,   0.22722721,   0.14369455,  -0.45198935,  -0.55574794],         [   0.66145875,   0.34633299,  0.14098019,   0.51882591,  -0.28019898,  -0.16032260, -0.068413337,  -0.20428242,  0.069672173,   0.23112577, -0.044368579,  -0.16455425,   0.21620977, 0.0042702105, -0.087399014],         [   0.31599556, -0.027551859,  0.13434254,   0.13497371,  0.054005680,  -0.17374789,   0.17525393,  0.060258929,  -0.17914162,  -0.31056619,  -0.25358691,  0.025847535,  -0.43006001,  -0.62266361, -0.033996882],         [  -0.29038151,  0.034101270, 0.034903413,  -0.12121764,  0.026030714,  -0.33546274,  -0.41424111,  0.053248380,  -0.27099455, -0.026251302,   0.41024137,   0.26636349,   0.15582891,  -0.18666254,  0.019895831],         [  -0.24388652,  -0.44098852, 0.012618825,   0.24945112,  0.071101888,   0.24623792,   0.17484502, 0.0085286769,   0.25147070,  -0.14659862, -0.084625150,   0.36931333,  -0.29955293,   0.11044360,  -0.75690139],         [  0.041494323,  -0.25980564,  0.46402128,  -0.36112127,  -0.94980789,  -0.16504063, 0.0030943325,  0.052792942,   0.22523648,   0.38390366,   0.45562427,  -0.18631744, 0.0082333995,   0.16670803,   0.16045688]])     return (a1*t).sum(-1) + (a2*np.sin(t)).sum(-1) + (a3*np.cos(t)).sum(-1) + (t*np.einsum(\"ij,...j-&gt;...i\",M,t)).sum(-1) oakley_ohagan15_cf = qp.CustomFun(         qp.Gaussian(qp.IIDStdUniform(15)),         oakley_ohagan15_func) In\u00a0[28]: Copied! <pre>def cbeam_func(t):\n    R = t[...,0]\n    E = t[...,1]\n    X = t[...,2]\n    Y = t[...,3]\n    L = 100;\n    D_0 = 2.2535;\n    w = 4;\n    t = 2;\n    Sterm1 = 600*Y / (w*(t**2));\n    Sterm2 = 600*X / ((w**2)*t);\n    S = Sterm1 + Sterm2;\n    Dfact1 = 4*(L**3) / (E*w*t);\n    Dfact2 = np.sqrt((Y/(t**2))**2 + (X/(w**2))**2);\n    D = Dfact1 * Dfact2\n    return D \ncbeam_cf = qp.CustomFun(\n    qp.Gaussian(qp.IIDStdUniform(4),mean=[40000,2.9e7,500,1000],covariance=[2000**2,1.45e6**2,100**2,100**2]),\n    cbeam_func)\n</pre> def cbeam_func(t):     R = t[...,0]     E = t[...,1]     X = t[...,2]     Y = t[...,3]     L = 100;     D_0 = 2.2535;     w = 4;     t = 2;     Sterm1 = 600*Y / (w*(t**2));     Sterm2 = 600*X / ((w**2)*t);     S = Sterm1 + Sterm2;     Dfact1 = 4*(L**3) / (E*w*t);     Dfact2 = np.sqrt((Y/(t**2))**2 + (X/(w**2))**2);     D = Dfact1 * Dfact2     return D  cbeam_cf = qp.CustomFun(     qp.Gaussian(qp.IIDStdUniform(4),mean=[40000,2.9e7,500,1000],covariance=[2000**2,1.45e6**2,100**2,100**2]),     cbeam_func) In\u00a0[29]: Copied! <pre>def G_func(t):\n    d = t.shape[-1]\n    a = (np.arange(1,d+1)-2)/2\n    return ((np.abs(4*t-2)+a)/(1+a)).prod(-1)\nG_cf = qp.CustomFun(\n    qp.Uniform(qp.IIDStdUniform(3)),\n    G_func)\n</pre> def G_func(t):     d = t.shape[-1]     a = (np.arange(1,d+1)-2)/2     return ((np.abs(4*t-2)+a)/(1+a)).prod(-1) G_cf = qp.CustomFun(     qp.Uniform(qp.IIDStdUniform(3)),     G_func) In\u00a0[30]: Copied! <pre>simple_func_1d = qp.CustomFun(qp.Uniform(qp.IIDStdUniform(1)),lambda x: x[...,0]*np.exp(x[...,0])-1.)\n</pre> simple_func_1d = qp.CustomFun(qp.Uniform(qp.IIDStdUniform(1)),lambda x: x[...,0]*np.exp(x[...,0])-1.) In\u00a0[31]: Copied! <pre>simple_func_2d = qp.CustomFun(qp.Uniform(qp.IIDStdUniform(2)),lambda x: x[...,1]*np.exp(x[...,0]*x[...,1])/(np.exp(1)-2)-1)\n</pre> simple_func_2d = qp.CustomFun(qp.Uniform(qp.IIDStdUniform(2)),lambda x: x[...,1]*np.exp(x[...,0]*x[...,1])/(np.exp(1)-2)-1) In\u00a0[32]: Copied! <pre>oakley_ohagan_2d = qp.CustomFun(\n        qp.Uniform(qp.IIDStdUniform(2),lower_bound=-0.01,upper_bound=0.01),\n        lambda x: 5+x[...,0]+x[...,1]+2*np.cos(x[...,0])+2*np.sin(x[...,1]))\n</pre> oakley_ohagan_2d = qp.CustomFun(         qp.Uniform(qp.IIDStdUniform(2),lower_bound=-0.01,upper_bound=0.01),         lambda x: 5+x[...,0]+x[...,1]+2*np.cos(x[...,0])+2*np.sin(x[...,1])) In\u00a0[33]: Copied! <pre>genz_oscillatory3_3d = qp.Genz(qp.IIDStdUniform(3),kind_func='oscillatory',kind_coeff=3)\ngenz_cornerpeak2_3d = qp.Genz(qp.IIDStdUniform(3),kind_func='corner-peak',kind_coeff=2)\n</pre> genz_oscillatory3_3d = qp.Genz(qp.IIDStdUniform(3),kind_func='oscillatory',kind_coeff=3) genz_cornerpeak2_3d = qp.Genz(qp.IIDStdUniform(3),kind_func='corner-peak',kind_coeff=2) In\u00a0[34]: Copied! <pre>ishigami = qp.Ishigami(qp.IIDStdUniform(3))\n</pre> ishigami = qp.Ishigami(qp.IIDStdUniform(3)) In\u00a0[35]: Copied! <pre>def convergence_block(integrands, pointsets_fns, r, m_max):\n    times = {} \n    muhathats = {} \n    rmses = {}\n    for name,integrand in integrands.items():\n        times[name] = {} \n        muhathats[name] = {} \n        rmses[name] = {} \n        for pname,(generator,pltkwargs) in pointsets_fns.items():\n            times[name][pname] = np.nan*np.empty(m_max+1,dtype=np.float64)\n            muhathats[name][pname] = np.nan*np.empty(m_max+1,dtype=np.float64)\n            rmses[name][pname] = np.nan*np.empty(m_max+1,dtype=np.float64)\n            print(\"%35s %-35s: \"%(name,pname),end=\"\",flush=True)\n            for m in range(0,m_max+1):\n                print(\"%d, \"%m,end='',flush=True)\n                t0 = timeit.default_timer()\n                x = generator(r,2**m,integrand.d)\n                times[name][pname][m] = timeit.default_timer()-t0\n                y = integrand.f(x)\n                muhats = y.mean(1)\n                muhathat = y.mean()\n                muhathats[name][pname][m] = muhathat\n                rmses[name][pname][m] = np.sqrt(np.mean((muhats-muhathat)**2/(r*(r-1))))\n            print()\n        print()\n    return times,muhathats,rmses\n</pre> def convergence_block(integrands, pointsets_fns, r, m_max):     times = {}      muhathats = {}      rmses = {}     for name,integrand in integrands.items():         times[name] = {}          muhathats[name] = {}          rmses[name] = {}          for pname,(generator,pltkwargs) in pointsets_fns.items():             times[name][pname] = np.nan*np.empty(m_max+1,dtype=np.float64)             muhathats[name][pname] = np.nan*np.empty(m_max+1,dtype=np.float64)             rmses[name][pname] = np.nan*np.empty(m_max+1,dtype=np.float64)             print(\"%35s %-35s: \"%(name,pname),end=\"\",flush=True)             for m in range(0,m_max+1):                 print(\"%d, \"%m,end='',flush=True)                 t0 = timeit.default_timer()                 x = generator(r,2**m,integrand.d)                 times[name][pname][m] = timeit.default_timer()-t0                 y = integrand.f(x)                 muhats = y.mean(1)                 muhathat = y.mean()                 muhathats[name][pname][m] = muhathat                 rmses[name][pname][m] = np.sqrt(np.mean((muhats-muhathat)**2/(r*(r-1))))             print()         print()     return times,muhathats,rmses In\u00a0[36]: Copied! <pre>funcs = OrderedDict({\n    r\"$f(x) = x e^x - 1$\": simple_func_1d,\n    r\"$f(x_1,x_2) = x_2 e^{x_1 x_2}/(e-2)-1$\": simple_func_2d,\n    r\"Oakley-O'Hagan, $d=2$\": oakley_ohagan_2d,\n    r\"G-Function, $d=%d$\"%G_cf.d: G_cf,\n    r\"Genz Oscillatory, $d=3$\": genz_oscillatory3_3d,\n    r\"Genz Corner-peak, $d=3$\": genz_cornerpeak2_3d,\n    #\"Ishigami\": ishigami, \n    # \"Sulfer\": sulfer_cf,\n    #\"Borehole\": borehole_cf,\n    # \"Webster\": webster_cf,\n    #r\"Oakley-O'Hagan with $d=15$\": oakley_ohagan15_cf,\n    # \"Cantilever Beam\": cbeam_cf,\n    # r\"Box Integral, $d=?$\": qp.BoxIntegral(qp.IIDStdUniform(4),s=-5),\n})\nseed = 7\npointsets = OrderedDict({\n    \"IID\": (lambda r,n,d: qp.IIDStdUniform(d,replications=r,seed=seed).gen_samples(n),{\"color\":COLORS[2],\"marker\":MARKERS[2]}),\n    \"Lattice Shift\": (lambda r,n,d: qp.Lattice(d,replications=r,seed=seed).gen_samples(n),{\"color\":COLORS[5],\"marker\":MARKERS[5]}),\n    r\"DN${}_{1}$ LMS DS\": (lambda r,n,d: qp.DigitalNetB2(d,randomize=\"LMS_DS\",replications=r,seed=seed).gen_samples(n),{\"color\":COLORS[0],\"marker\":MARKERS[0]}),\n    r\"DN${}_{2}$ LMS DS\": (lambda r,n,d: qp.DigitalNetB2(d,randomize=\"LMS_DS\",replications=r,seed=seed,alpha=2).gen_samples(n),{\"color\":COLORS[0],\"marker\":MARKERS[1]}),\n    r\"DN${}_{3}$ LMS DS\": (lambda r,n,d: qp.DigitalNetB2(d,randomize=\"LMS_DS\",replications=r,seed=seed,alpha=3).gen_samples(n),{\"color\":COLORS[0],\"marker\":MARKERS[2]}),\n})\nm_max = 17\nr = 500\ntimes,muhathats,rmses = convergence_block(funcs,pointsets,r=r,m_max=m_max)\n</pre> funcs = OrderedDict({     r\"$f(x) = x e^x - 1$\": simple_func_1d,     r\"$f(x_1,x_2) = x_2 e^{x_1 x_2}/(e-2)-1$\": simple_func_2d,     r\"Oakley-O'Hagan, $d=2$\": oakley_ohagan_2d,     r\"G-Function, $d=%d$\"%G_cf.d: G_cf,     r\"Genz Oscillatory, $d=3$\": genz_oscillatory3_3d,     r\"Genz Corner-peak, $d=3$\": genz_cornerpeak2_3d,     #\"Ishigami\": ishigami,      # \"Sulfer\": sulfer_cf,     #\"Borehole\": borehole_cf,     # \"Webster\": webster_cf,     #r\"Oakley-O'Hagan with $d=15$\": oakley_ohagan15_cf,     # \"Cantilever Beam\": cbeam_cf,     # r\"Box Integral, $d=?$\": qp.BoxIntegral(qp.IIDStdUniform(4),s=-5), }) seed = 7 pointsets = OrderedDict({     \"IID\": (lambda r,n,d: qp.IIDStdUniform(d,replications=r,seed=seed).gen_samples(n),{\"color\":COLORS[2],\"marker\":MARKERS[2]}),     \"Lattice Shift\": (lambda r,n,d: qp.Lattice(d,replications=r,seed=seed).gen_samples(n),{\"color\":COLORS[5],\"marker\":MARKERS[5]}),     r\"DN${}_{1}$ LMS DS\": (lambda r,n,d: qp.DigitalNetB2(d,randomize=\"LMS_DS\",replications=r,seed=seed).gen_samples(n),{\"color\":COLORS[0],\"marker\":MARKERS[0]}),     r\"DN${}_{2}$ LMS DS\": (lambda r,n,d: qp.DigitalNetB2(d,randomize=\"LMS_DS\",replications=r,seed=seed,alpha=2).gen_samples(n),{\"color\":COLORS[0],\"marker\":MARKERS[1]}),     r\"DN${}_{3}$ LMS DS\": (lambda r,n,d: qp.DigitalNetB2(d,randomize=\"LMS_DS\",replications=r,seed=seed,alpha=3).gen_samples(n),{\"color\":COLORS[0],\"marker\":MARKERS[2]}), }) m_max = 17 r = 500 times,muhathats,rmses = convergence_block(funcs,pointsets,r=r,m_max=m_max) <pre>                 $f(x) = x e^x - 1$ IID                                : 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, \n                 $f(x) = x e^x - 1$ Lattice Shift                      : 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, \n                 $f(x) = x e^x - 1$ DN${}_{1}$ LMS DS                  : 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, \n                 $f(x) = x e^x - 1$ DN${}_{2}$ LMS DS                  : 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, \n                 $f(x) = x e^x - 1$ DN${}_{3}$ LMS DS                  : 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, \n\n$f(x_1,x_2) = x_2 e^{x_1 x_2}/(e-2)-1$ IID                                : 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, \n$f(x_1,x_2) = x_2 e^{x_1 x_2}/(e-2)-1$ Lattice Shift                      : 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, \n$f(x_1,x_2) = x_2 e^{x_1 x_2}/(e-2)-1$ DN${}_{1}$ LMS DS                  : 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, \n$f(x_1,x_2) = x_2 e^{x_1 x_2}/(e-2)-1$ DN${}_{2}$ LMS DS                  : 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, \n$f(x_1,x_2) = x_2 e^{x_1 x_2}/(e-2)-1$ DN${}_{3}$ LMS DS                  : 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, \n\n              Oakley-O'Hagan, $d=2$ IID                                : 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, \n              Oakley-O'Hagan, $d=2$ Lattice Shift                      : 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, \n              Oakley-O'Hagan, $d=2$ DN${}_{1}$ LMS DS                  : 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, \n              Oakley-O'Hagan, $d=2$ DN${}_{2}$ LMS DS                  : 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, \n              Oakley-O'Hagan, $d=2$ DN${}_{3}$ LMS DS                  : 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, \n\n                  G-Function, $d=3$ IID                                : 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, \n                  G-Function, $d=3$ Lattice Shift                      : 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, \n                  G-Function, $d=3$ DN${}_{1}$ LMS DS                  : 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, \n                  G-Function, $d=3$ DN${}_{2}$ LMS DS                  : 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, \n                  G-Function, $d=3$ DN${}_{3}$ LMS DS                  : 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, \n\n            Genz Oscillatory, $d=3$ IID                                : 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, \n            Genz Oscillatory, $d=3$ Lattice Shift                      : 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, \n            Genz Oscillatory, $d=3$ DN${}_{1}$ LMS DS                  : 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, \n            Genz Oscillatory, $d=3$ DN${}_{2}$ LMS DS                  : 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, \n            Genz Oscillatory, $d=3$ DN${}_{3}$ LMS DS                  : 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, \n\n            Genz Corner-peak, $d=3$ IID                                : 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, \n            Genz Corner-peak, $d=3$ Lattice Shift                      : 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, \n            Genz Corner-peak, $d=3$ DN${}_{1}$ LMS DS                  : 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, \n            Genz Corner-peak, $d=3$ DN${}_{2}$ LMS DS                  : 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, \n            Genz Corner-peak, $d=3$ DN${}_{3}$ LMS DS                  : 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, \n\n</pre> In\u00a0[43]: Copied! <pre>nrows = 2\nncols = 3 \nfig,ax = pyplot.subplots(nrows=nrows,ncols=ncols,figsize=(MW2,MW2/ncols*nrows),sharey=False,sharex=True)\nax = np.atleast_1d(ax)\nmvec = np.arange(0,m_max+1)\nnvec = 2**mvec\ncommonkwargs = {\"markersize\":3,\"linewidth\":1}#,\"markerfacecolor\":'black',\"markeredgecolor\":'white'}\nfor i,name in enumerate(funcs.keys()):\n    i1,i2 = i//ncols,i%ncols\n    avgi = 0\n    # avgi = rmse_noho[:,mvec[0]].mean()\n    for j,(pname,(generator,pltkwargs)) in enumerate(pointsets.items()):\n        ax[i1,i2].plot(nvec,rmses[name][pname][mvec],label=pname,**pltkwargs,**commonkwargs)\n        avgi += rmses[name][pname][mvec[0]]\n    avgi /= len(pointsets)\n    for p,sp in zip([-1/2,-1.,-3/2,-5/2,-7/2],[\"-1/2\",\"-1\",\"-3/2\",\"-5/2\",\"-7/2\"]):\n        n0 = 2**mvec[0]\n        kappa = avgi/(n0**p)\n        nf = 2**mvec[-1]\n        lf = kappa*nf**p\n        ax[i1,i2].plot([nvec[0],nf],[kappa*n0**p,lf],marker=\"none\",color=\"black\",alpha=.5,**commonkwargs)\n        if i2==2:\n            ax[i1,i2].text(2*nf,lf,r\"$\\mathcal{O}(n^{%s})$\"%sp)\n    ax[i1,i2].set_yscale('log',base=10)\n    ax[i1,i2].set_title(name)\n    ax[i1,i2].grid(True) \n    ax[i1,i2].set_xlim(nvec[0],nvec[-1])\n    ax[i1,i2].set_xscale('log',base=2)\n    ax[i1,i2].xaxis.set_tick_params(labelleft=True)\nfig.legend(*ax[0,0].get_legend_handles_labels(),frameon=False,loc=\"lower center\",bbox_to_anchor=(.5,-.075),ncol=5)\nfig.suptitle(\"RMSE vs number of points $n$\",fontsize=\"large\");\nfig.savefig(\"outputs/convergence.png\",format=\"png\",dpi=1024,transparent=True,bbox_inches=\"tight\")\n</pre> nrows = 2 ncols = 3  fig,ax = pyplot.subplots(nrows=nrows,ncols=ncols,figsize=(MW2,MW2/ncols*nrows),sharey=False,sharex=True) ax = np.atleast_1d(ax) mvec = np.arange(0,m_max+1) nvec = 2**mvec commonkwargs = {\"markersize\":3,\"linewidth\":1}#,\"markerfacecolor\":'black',\"markeredgecolor\":'white'} for i,name in enumerate(funcs.keys()):     i1,i2 = i//ncols,i%ncols     avgi = 0     # avgi = rmse_noho[:,mvec[0]].mean()     for j,(pname,(generator,pltkwargs)) in enumerate(pointsets.items()):         ax[i1,i2].plot(nvec,rmses[name][pname][mvec],label=pname,**pltkwargs,**commonkwargs)         avgi += rmses[name][pname][mvec[0]]     avgi /= len(pointsets)     for p,sp in zip([-1/2,-1.,-3/2,-5/2,-7/2],[\"-1/2\",\"-1\",\"-3/2\",\"-5/2\",\"-7/2\"]):         n0 = 2**mvec[0]         kappa = avgi/(n0**p)         nf = 2**mvec[-1]         lf = kappa*nf**p         ax[i1,i2].plot([nvec[0],nf],[kappa*n0**p,lf],marker=\"none\",color=\"black\",alpha=.5,**commonkwargs)         if i2==2:             ax[i1,i2].text(2*nf,lf,r\"$\\mathcal{O}(n^{%s})$\"%sp)     ax[i1,i2].set_yscale('log',base=10)     ax[i1,i2].set_title(name)     ax[i1,i2].grid(True)      ax[i1,i2].set_xlim(nvec[0],nvec[-1])     ax[i1,i2].set_xscale('log',base=2)     ax[i1,i2].xaxis.set_tick_params(labelleft=True) fig.legend(*ax[0,0].get_legend_handles_labels(),frameon=False,loc=\"lower center\",bbox_to_anchor=(.5,-.075),ncol=5) fig.suptitle(\"RMSE vs number of points $n$\",fontsize=\"large\"); fig.savefig(\"outputs/convergence.png\",format=\"png\",dpi=1024,transparent=True,bbox_inches=\"tight\") In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"demos/talk_paper_demos/ACMTOMS_Sorokin_2025/acm_toms_sorokin_2025/#quasi-monte-carlo-generators-randomizers-and-fast-transforms","title":"Quasi-Monte Carlo Generators, Randomizers, and Fast Transforms\u00b6","text":""},{"location":"demos/talk_paper_demos/ACMTOMS_Sorokin_2025/acm_toms_sorokin_2025/#setup","title":"Setup\u00b6","text":""},{"location":"demos/talk_paper_demos/ACMTOMS_Sorokin_2025/acm_toms_sorokin_2025/#snippets","title":"Snippets\u00b6","text":""},{"location":"demos/talk_paper_demos/ACMTOMS_Sorokin_2025/acm_toms_sorokin_2025/#point-sets","title":"Point Sets\u00b6","text":""},{"location":"demos/talk_paper_demos/ACMTOMS_Sorokin_2025/acm_toms_sorokin_2025/#kernel-methods","title":"Kernel Methods\u00b6","text":""},{"location":"demos/talk_paper_demos/ACMTOMS_Sorokin_2025/acm_toms_sorokin_2025/#lattice-fftbr-ifftbr","title":"Lattice + FFTBR + IFFTBR\u00b6","text":""},{"location":"demos/talk_paper_demos/ACMTOMS_Sorokin_2025/acm_toms_sorokin_2025/#digital-net-fwht","title":"Digital Net + FWHT\u00b6","text":""},{"location":"demos/talk_paper_demos/ACMTOMS_Sorokin_2025/acm_toms_sorokin_2025/#integration","title":"Integration\u00b6","text":""},{"location":"demos/talk_paper_demos/ACMTOMS_Sorokin_2025/acm_toms_sorokin_2025/#pointsets","title":"Pointsets\u00b6","text":""},{"location":"demos/talk_paper_demos/ACMTOMS_Sorokin_2025/acm_toms_sorokin_2025/#generation-time","title":"Generation time\u00b6","text":""},{"location":"demos/talk_paper_demos/ACMTOMS_Sorokin_2025/acm_toms_sorokin_2025/#convergence","title":"Convergence\u00b6","text":""},{"location":"demos/talk_paper_demos/ACMTOMS_Sorokin_2025/acm_toms_sorokin_2025/#test-functions","title":"Test Functions\u00b6","text":""},{"location":"demos/talk_paper_demos/ACMTOMS_Sorokin_2025/acm_toms_sorokin_2025/#simulation","title":"Simulation\u00b6","text":""},{"location":"demos/talk_paper_demos/Argonne_Talk_2023_May/Argonne_2023_Talk_Figures/","title":"2023 Argonne Talk","text":"In\u00a0[1]: Copied! <pre>import matplotlib.pyplot as plt\nimport numpy as np\nimport scipy.integrate as Nintegrate\nimport qmcpy as qp\nfrom sympy import * #so that we can do symbolic integration\nimport time  #timing routines\nimport warnings  #to suppress warnings when needed\nimport pickle  #write output to a file and load it back in\nfrom copy import deepcopy\nfrom matplotlib.patches import Polygon\n\nnp.set_printoptions(threshold=10)\n\nplt.rc('font', size=16)  #set defaults so that the plots are readable\nplt.rc('font', serif = \"Computer Modern Roman\")\nplt.rc('text', usetex=True)\nplt.rc('text.latex', preamble=r'\\usepackage{amsmath,amssymb,bm}')\nplt.rc('axes', titlesize=16)\nplt.rc('axes', labelsize=16)\nplt.rc('xtick', labelsize=16)\nplt.rc('ytick', labelsize=16)\nplt.rc('legend', fontsize=16)\nplt.rc('figure', titlesize=16)\n\n#a helpful plotting method to show increasing numbers of points\ndef plot_successive_points(distrib,ld_name,first_n=64,n_cols=1,\n                           pt_clr=['tab:blue', 'tab:orange', 'k', 'tab:cyan', 'tab:purple', 'tab:orange'],\n                           xlim=[0,1],ylim=[0,1],ntitle=True,titleText='',\n                          coordlist=[[0,1]],nrep=1):\n  fig,ax = plt.subplots(nrows=1,ncols=n_cols,figsize=(5*n_cols,5.5))\n  if n_cols==1: ax = [ax]\n  last_n = first_n*(2**n_cols)\n  if isinstance(distrib, list):\n    nrep = len(distrib)\n  else:\n    nrep = 1\n  if nrep==1: points = distrib.gen_samples(n=last_n)\n  ncoord = len(coordlist)\n  for i in range(n_cols):\n    n = first_n\n    nstart = 0\n    if nrep &gt; 1:\n      points = distrib[i].gen_samples(n=last_n)\n      ax[i].scatter(points[nstart:n,0],points[nstart:n,1],color=pt_clr[0])\n      ax[i].set_xlim(xlim); ax[i].set_xticks(xlim); ax[i].set_xlabel('$x_{i,'+str(coordlist[0][0]+1)+'}$')\n      ax[i].set_ylim(ylim); ax[i].set_yticks(ylim); ax[i].set_ylabel('$x_{i,'+str(coordlist[0][1]+1)+'}$')\n    elif ncoord &gt;1:\n      ax[i].scatter(points[nstart:n,coordlist[i][0]],points[nstart:n,coordlist[i][1]],color=pt_clr[0])\n      ax[i].set_xlim(xlim); ax[i].set_xticks(xlim); ax[i].set_xlabel('$x_{i,'+str(coordlist[i][0]+1)+'}$')\n      ax[i].set_ylim(ylim); ax[i].set_yticks(ylim); ax[i].set_ylabel('$x_{i,'+str(coordlist[i][1]+1)+'}$')\n    else:\n      for j in range(i+1):\n        n = first_n*(2**j)\n        ax[i].scatter(points[nstart:n,0],points[nstart:n,1],color=pt_clr[j])\n        nstart = n\n        if ntitle == True:\n          ax[i].set_title('n = %d'%n)\n        else:\n          ax[i].set_title(titleText)\n        ax[i].set_xlabel('$x_{i,'+str(coordlist[0][0]+1)+'}$')\n        ax[i].set_ylabel('$x_{i,'+str(coordlist[0][1]+1)+'}$')\n    ax[i].set_xlim(xlim); ax[i].set_xticks(xlim)\n    ax[i].set_ylim(ylim); ax[i].set_yticks(ylim)\n    ax[i].set_aspect((xlim[1]-xlim[0])/(ylim[1]-ylim[0]))\n  if ld_name != \"\":\n    fig.suptitle('%s Points'%ld_name, y=0.87)\n  return fig\n</pre> import matplotlib.pyplot as plt import numpy as np import scipy.integrate as Nintegrate import qmcpy as qp from sympy import * #so that we can do symbolic integration import time  #timing routines import warnings  #to suppress warnings when needed import pickle  #write output to a file and load it back in from copy import deepcopy from matplotlib.patches import Polygon  np.set_printoptions(threshold=10)  plt.rc('font', size=16)  #set defaults so that the plots are readable plt.rc('font', serif = \"Computer Modern Roman\") plt.rc('text', usetex=True) plt.rc('text.latex', preamble=r'\\usepackage{amsmath,amssymb,bm}') plt.rc('axes', titlesize=16) plt.rc('axes', labelsize=16) plt.rc('xtick', labelsize=16) plt.rc('ytick', labelsize=16) plt.rc('legend', fontsize=16) plt.rc('figure', titlesize=16)  #a helpful plotting method to show increasing numbers of points def plot_successive_points(distrib,ld_name,first_n=64,n_cols=1,                            pt_clr=['tab:blue', 'tab:orange', 'k', 'tab:cyan', 'tab:purple', 'tab:orange'],                            xlim=[0,1],ylim=[0,1],ntitle=True,titleText='',                           coordlist=[[0,1]],nrep=1):   fig,ax = plt.subplots(nrows=1,ncols=n_cols,figsize=(5*n_cols,5.5))   if n_cols==1: ax = [ax]   last_n = first_n*(2**n_cols)   if isinstance(distrib, list):     nrep = len(distrib)   else:     nrep = 1   if nrep==1: points = distrib.gen_samples(n=last_n)   ncoord = len(coordlist)   for i in range(n_cols):     n = first_n     nstart = 0     if nrep &gt; 1:       points = distrib[i].gen_samples(n=last_n)       ax[i].scatter(points[nstart:n,0],points[nstart:n,1],color=pt_clr[0])       ax[i].set_xlim(xlim); ax[i].set_xticks(xlim); ax[i].set_xlabel('$x_{i,'+str(coordlist[0][0]+1)+'}$')       ax[i].set_ylim(ylim); ax[i].set_yticks(ylim); ax[i].set_ylabel('$x_{i,'+str(coordlist[0][1]+1)+'}$')     elif ncoord &gt;1:       ax[i].scatter(points[nstart:n,coordlist[i][0]],points[nstart:n,coordlist[i][1]],color=pt_clr[0])       ax[i].set_xlim(xlim); ax[i].set_xticks(xlim); ax[i].set_xlabel('$x_{i,'+str(coordlist[i][0]+1)+'}$')       ax[i].set_ylim(ylim); ax[i].set_yticks(ylim); ax[i].set_ylabel('$x_{i,'+str(coordlist[i][1]+1)+'}$')     else:       for j in range(i+1):         n = first_n*(2**j)         ax[i].scatter(points[nstart:n,0],points[nstart:n,1],color=pt_clr[j])         nstart = n         if ntitle == True:           ax[i].set_title('n = %d'%n)         else:           ax[i].set_title(titleText)         ax[i].set_xlabel('$x_{i,'+str(coordlist[0][0]+1)+'}$')         ax[i].set_ylabel('$x_{i,'+str(coordlist[0][1]+1)+'}$')     ax[i].set_xlim(xlim); ax[i].set_xticks(xlim)     ax[i].set_ylim(ylim); ax[i].set_yticks(ylim)     ax[i].set_aspect((xlim[1]-xlim[0])/(ylim[1]-ylim[0]))   if ld_name != \"\":     fig.suptitle('%s Points'%ld_name, y=0.87)   return fig In\u00a0[2]: Copied! <pre>figpath = '' #this path sends the figures to the directory that you want\n</pre> figpath = '' #this path sends the figures to the directory that you want In\u00a0[3]: Copied! <pre>x = Symbol('x')                       #do some symbolic computation \nfsim = 30*x*exp(-5*x)                 #our test function\nintf = N(integrate(fsim, (x, 0, 1)))  #and its integral\nprint(intf)\n</pre> x = Symbol('x')                       #do some symbolic computation  fsim = 30*x*exp(-5*x)                 #our test function intf = N(integrate(fsim, (x, 0, 1)))  #and its integral print(intf) <pre>1.15148678160658\n</pre> In\u00a0[4]: Copied! <pre>f = lambdify(x,fsim)                     #make it a numeric function\nxnodes = np.array([0, 0.2, 0.6, 0.8, 1]) #choose nodes to apply the trapezoidla rule\nfnodes = f(xnodes)                       #function values at those nodes\nxplot = np.linspace(0, 1, 1000)          #nodes for polotting our function\nfig,ax = plt.subplots()\nxfpatch = np.append(np.transpose(np.array([xnodes, fnodes])),[[1,0]],axis=0)  #the trapezoidal rule approximation\nax.add_patch(Polygon(xfpatch,facecolor='tab:cyan'))  #plot the trapezoidal rule\nplt.plot(xplot, f(xplot), color='tab:blue')          #and the function\nplt.scatter(xnodes, fnodes, color='tab:orange')     #and the nodes\nax.set_xlabel('$x$')\nax.set_ylabel('$f(x)$')\nxlabels = []\nfor i in range(xnodes.size): xlabels.append(f'$x_{i}$')    \nax.set_xticks(xnodes,labels = xlabels) \nplt.tight_layout() \nfig.savefig(figpath+'traprule.eps',format='eps',bbox_inches='tight')\n</pre> f = lambdify(x,fsim)                     #make it a numeric function xnodes = np.array([0, 0.2, 0.6, 0.8, 1]) #choose nodes to apply the trapezoidla rule fnodes = f(xnodes)                       #function values at those nodes xplot = np.linspace(0, 1, 1000)          #nodes for polotting our function fig,ax = plt.subplots() xfpatch = np.append(np.transpose(np.array([xnodes, fnodes])),[[1,0]],axis=0)  #the trapezoidal rule approximation ax.add_patch(Polygon(xfpatch,facecolor='tab:cyan'))  #plot the trapezoidal rule plt.plot(xplot, f(xplot), color='tab:blue')          #and the function plt.scatter(xnodes, fnodes, color='tab:orange')     #and the nodes ax.set_xlabel('$x$') ax.set_ylabel('$f(x)$') xlabels = [] for i in range(xnodes.size): xlabels.append(f'$x_{i}$')     ax.set_xticks(xnodes,labels = xlabels)  plt.tight_layout()  fig.savefig(figpath+'traprule.eps',format='eps',bbox_inches='tight') In\u00a0[5]: Copied! <pre>xdiff = np.diff(xnodes)\ntrap_rule = np.sum((fnodes[:-1]*xdiff+fnodes[1:]*xdiff)/2)\nerr_trap = intf - trap_rule \nprint(err_trap)\n</pre> xdiff = np.diff(xnodes) trap_rule = np.sum((fnodes[:-1]*xdiff+fnodes[1:]*xdiff)/2) err_trap = intf - trap_rule  print(err_trap) <pre>0.112324710648342\n</pre> In\u00a0[6]: Copied! <pre>discrepancy = np.max(xdiff**2)/8                          #quality of the nodes\nf2diffsim = diff(fsim,x,2)\nprint(f2diffsim)\nf2diff = lambdify(x,f2diffsim) \nfig,ax = plt.subplots()\nplt.plot(xplot, f2diff(xplot), color='tab:blue')\nax.set_xlabel('$x$')\nax.set_ylabel(\"$f''(x)$\");\nvariation = N(integrate(abs(f2diffsim), (x, 0, 1)))   #roughness of the integrand\nNvariation = Nintegrate.quad(lambda x: abs(f2diff(x)),0,1,epsabs = 1e-14)  #checking whether the symbolic integration is correct\nprint(abs(Nvariation[0]-variation) &lt;= 1e-10)          #do the symbolic and numerical integration methods agree\nconfound = err_trap/(discrepancy*variation)           #how unlucky the integrand is, should be between -1 and +1\nprint([confound,discrepancy, variation])              #the trapezoidal rule error is the product of these three numbers\nfig.savefig(figpath+'traprulef2.eps',format='eps',bbox_inches='tight')\n</pre> discrepancy = np.max(xdiff**2)/8                          #quality of the nodes f2diffsim = diff(fsim,x,2) print(f2diffsim) f2diff = lambdify(x,f2diffsim)  fig,ax = plt.subplots() plt.plot(xplot, f2diff(xplot), color='tab:blue') ax.set_xlabel('$x$') ax.set_ylabel(\"$f''(x)$\"); variation = N(integrate(abs(f2diffsim), (x, 0, 1)))   #roughness of the integrand Nvariation = Nintegrate.quad(lambda x: abs(f2diff(x)),0,1,epsabs = 1e-14)  #checking whether the symbolic integration is correct print(abs(Nvariation[0]-variation) &lt;= 1e-10)          #do the symbolic and numerical integration methods agree confound = err_trap/(discrepancy*variation)           #how unlucky the integrand is, should be between -1 and +1 print([confound,discrepancy, variation])              #the trapezoidal rule error is the product of these three numbers fig.savefig(figpath+'traprulef2.eps',format='eps',bbox_inches='tight') <pre>150*(5*x - 2)*exp(-5*x)\nTrue\n[0.150522653770521, 0.019999999999999997, 37.3115633543065]\n</pre> In\u00a0[7]: Copied! <pre>d = 15 #dimension\nn = 64 #number of points\nnrand = 4 #number of randomizations\ncoordlist=[[0,1],[1,2],[4,10],[13,14]]\nld = qp.Lattice(d) #define the generator\nxpts = ld.gen_samples(n) #generate points\nprint(xpts)\nfig = plot_successive_points(ld,'',first_n=4*n,n_cols=1,ntitle=False,titleText=r'Lattice $\\boldsymbol{X} \\sim \\mathcal{U}[0,1]^d$')\nplt.tight_layout() \nfig.savefig(figpath+'latticeptssingle.eps',format='eps',bbox_inches='tight')\nfig = plot_successive_points(ld,'Lattice',first_n=n,n_cols=4)\nfig.savefig(figpath+'latticeptsseq.eps',format='eps',bbox_inches='tight')\nldrand = ld.spawn(nrand) #randomize\nfig = plot_successive_points(ldrand,'Randomized Lattice',first_n=4*n,n_cols=nrand)\nfig.savefig(figpath+'latticeptsrand.eps',format='eps',bbox_inches='tight')\nfig = plot_successive_points(ld,'Projections of Lattice',first_n=4*n,n_cols=nrand,coordlist=coordlist)\nfig.savefig(figpath+'latticeptsproj.eps',format='eps',bbox_inches='tight')\n</pre> d = 15 #dimension n = 64 #number of points nrand = 4 #number of randomizations coordlist=[[0,1],[1,2],[4,10],[13,14]] ld = qp.Lattice(d) #define the generator xpts = ld.gen_samples(n) #generate points print(xpts) fig = plot_successive_points(ld,'',first_n=4*n,n_cols=1,ntitle=False,titleText=r'Lattice $\\boldsymbol{X} \\sim \\mathcal{U}[0,1]^d$') plt.tight_layout()  fig.savefig(figpath+'latticeptssingle.eps',format='eps',bbox_inches='tight') fig = plot_successive_points(ld,'Lattice',first_n=n,n_cols=4) fig.savefig(figpath+'latticeptsseq.eps',format='eps',bbox_inches='tight') ldrand = ld.spawn(nrand) #randomize fig = plot_successive_points(ldrand,'Randomized Lattice',first_n=4*n,n_cols=nrand) fig.savefig(figpath+'latticeptsrand.eps',format='eps',bbox_inches='tight') fig = plot_successive_points(ld,'Projections of Lattice',first_n=4*n,n_cols=nrand,coordlist=coordlist) fig.savefig(figpath+'latticeptsproj.eps',format='eps',bbox_inches='tight') <pre>[[0.15289077 0.1012989  0.19275833 ... 0.04742184 0.83174917 0.38357771]\n [0.65289077 0.6012989  0.69275833 ... 0.54742184 0.33174917 0.88357771]\n [0.40289077 0.8512989  0.94275833 ... 0.29742184 0.08174917 0.63357771]\n ...\n [0.88726577 0.1794239  0.89588333 ... 0.09429684 0.25362417 0.05545271]\n [0.63726577 0.4294239  0.14588333 ... 0.84429684 0.00362417 0.80545271]\n [0.13726577 0.9294239  0.64588333 ... 0.34429684 0.50362417 0.30545271]]\n</pre> In\u00a0[10]: Copied! <pre>ld = qp.Sobol(d,seed=11) #define the generator\nxpts_Sobol = ld.gen_samples(n) #generate points\nfig = plot_successive_points(ld,'',first_n=4*n,n_cols=1,ntitle=False,titleText=r\"Sobol' $\\boldsymbol{X} \\sim \\mathcal{U}[0,1]^d$\")\nplt.tight_layout() \nfig.savefig(figpath+'sobolptssingle.eps',format='eps',bbox_inches='tight')\nfig = plot_successive_points(ld,'Sobol\\'',first_n=n,n_cols=4)\nfig.savefig(figpath+'sobolptsseq.eps',format='eps',bbox_inches='tight')\nldrand = ld.spawn(nrand) #randomize\nfig = plot_successive_points(ldrand,'Randomized Sobol\\'',first_n=4*n,n_cols=nrand)\nfig.savefig(figpath+'sobolptsrand.eps',format='eps',bbox_inches='tight')\nfig = plot_successive_points(ld,'Projections of Sobol\\'',first_n=4*n,n_cols=nrand,coordlist=coordlist)\nfig.savefig(figpath+'sobolptsproj.eps',format='eps',bbox_inches='tight')\n</pre> ld = qp.Sobol(d,seed=11) #define the generator xpts_Sobol = ld.gen_samples(n) #generate points fig = plot_successive_points(ld,'',first_n=4*n,n_cols=1,ntitle=False,titleText=r\"Sobol' $\\boldsymbol{X} \\sim \\mathcal{U}[0,1]^d$\") plt.tight_layout()  fig.savefig(figpath+'sobolptssingle.eps',format='eps',bbox_inches='tight') fig = plot_successive_points(ld,'Sobol\\'',first_n=n,n_cols=4) fig.savefig(figpath+'sobolptsseq.eps',format='eps',bbox_inches='tight') ldrand = ld.spawn(nrand) #randomize fig = plot_successive_points(ldrand,'Randomized Sobol\\'',first_n=4*n,n_cols=nrand) fig.savefig(figpath+'sobolptsrand.eps',format='eps',bbox_inches='tight') fig = plot_successive_points(ld,'Projections of Sobol\\'',first_n=4*n,n_cols=nrand,coordlist=coordlist) fig.savefig(figpath+'sobolptsproj.eps',format='eps',bbox_inches='tight') In\u00a0[11]: Copied! <pre>ld = qp.Halton(d) #define the generator\nxpts_Halton = ld.gen_samples(n) #generate points\nfig = plot_successive_points(ld,'',first_n=4*n,n_cols=1,ntitle=False,titleText=r'Halton $\\boldsymbol{X} \\sim \\mathcal{U}[0,1]^d$')\nplt.tight_layout() \nfig.savefig(figpath+'haltonptssingle.eps',format='eps',bbox_inches='tight')\nfig = plot_successive_points(ld,'Halton',first_n=n,n_cols=4)\nfig.savefig(figpath+'haltonptsseq.eps',format='eps',bbox_inches='tight')\nldrand = ld.spawn(nrand) #randomize\nfig = plot_successive_points(ldrand,'Randomized Halton',first_n=4*n,n_cols=nrand)\nfig.savefig(figpath+'haltonptsrand.eps',format='eps',bbox_inches='tight')\nfig = plot_successive_points(ld,'Projections of Halton',first_n=4*n,n_cols=nrand,coordlist=coordlist)\nfig.savefig(figpath+'haltonptsproj.eps',format='eps',bbox_inches='tight')\n</pre> ld = qp.Halton(d) #define the generator xpts_Halton = ld.gen_samples(n) #generate points fig = plot_successive_points(ld,'',first_n=4*n,n_cols=1,ntitle=False,titleText=r'Halton $\\boldsymbol{X} \\sim \\mathcal{U}[0,1]^d$') plt.tight_layout()  fig.savefig(figpath+'haltonptssingle.eps',format='eps',bbox_inches='tight') fig = plot_successive_points(ld,'Halton',first_n=n,n_cols=4) fig.savefig(figpath+'haltonptsseq.eps',format='eps',bbox_inches='tight') ldrand = ld.spawn(nrand) #randomize fig = plot_successive_points(ldrand,'Randomized Halton',first_n=4*n,n_cols=nrand) fig.savefig(figpath+'haltonptsrand.eps',format='eps',bbox_inches='tight') fig = plot_successive_points(ld,'Projections of Halton',first_n=4*n,n_cols=nrand,coordlist=coordlist) fig.savefig(figpath+'haltonptsproj.eps',format='eps',bbox_inches='tight') In\u00a0[12]: Copied! <pre>iid = qp.IIDStdUniform(d) #define the generator\nxpts = ld.gen_samples(n) #generate points\nxpts\nfig = plot_successive_points(iid,'',first_n=4*n,n_cols=1,ntitle=False,titleText=r'IID $\\boldsymbol{X}\\sim\\mathcal{U}[0,1]^d$')\nplt.tight_layout() \nfig.savefig(figpath+'iidptssingle.eps',format='eps',bbox_inches='tight')\nfig = plot_successive_points(iid,'IID',first_n=n,n_cols=4)\nfig.savefig(figpath+'iidptsseq.eps',format='eps',bbox_inches='tight')\niidrand = iid.spawn(nrand) #randomize\nfig = plot_successive_points(iidrand,'IID',first_n=4*n,n_cols=nrand)\nfig.savefig(figpath+'iidptsrand.eps',format='eps',bbox_inches='tight')\nfig = plot_successive_points(iid,'Projections of IID',first_n=4*n,n_cols=nrand,coordlist=coordlist)\nfig.savefig(figpath+'iidptsproj.eps',format='eps',bbox_inches='tight')\n</pre> iid = qp.IIDStdUniform(d) #define the generator xpts = ld.gen_samples(n) #generate points xpts fig = plot_successive_points(iid,'',first_n=4*n,n_cols=1,ntitle=False,titleText=r'IID $\\boldsymbol{X}\\sim\\mathcal{U}[0,1]^d$') plt.tight_layout()  fig.savefig(figpath+'iidptssingle.eps',format='eps',bbox_inches='tight') fig = plot_successive_points(iid,'IID',first_n=n,n_cols=4) fig.savefig(figpath+'iidptsseq.eps',format='eps',bbox_inches='tight') iidrand = iid.spawn(nrand) #randomize fig = plot_successive_points(iidrand,'IID',first_n=4*n,n_cols=nrand) fig.savefig(figpath+'iidptsrand.eps',format='eps',bbox_inches='tight') fig = plot_successive_points(iid,'Projections of IID',first_n=4*n,n_cols=nrand,coordlist=coordlist) fig.savefig(figpath+'iidptsproj.eps',format='eps',bbox_inches='tight') In\u00a0[13]: Copied! <pre>with open(figpath+'iid_ld.pkl','rb') as myfile: tol_vec,n_tol,ii_iid,ld_time,ld_n,iid_time,iid_n,best_solution_i = pickle.load(myfile)\nprint(best_solution_i)\nfig,ax = plt.subplots(nrows=1,ncols=2,figsize=(13,5.5))\nax[0].scatter(tol_vec[0:n_tol],ld_time[0:n_tol],color='tab:blue'); \nax[0].plot(tol_vec[0:n_tol],[(ld_time[0]*tol_vec[0])/tol_vec[jj] for jj in range(n_tol)],color='tab:blue')\nax[0].scatter(tol_vec[ii_iid:n_tol],iid_time[ii_iid:n_tol],color='tab:orange'); \nax[0].plot(tol_vec[ii_iid:n_tol],[(iid_time[ii_iid]*(tol_vec[ii_iid]**2))/(tol_vec[jj]**2) for jj in range(ii_iid,n_tol)],color='tab:orange')\nax[0].set_ylim([0.001,1000]); ax[0].set_ylabel('Time (s)')\nax[1].scatter(tol_vec[0:n_tol],ld_n[0:n_tol],color='tab:blue'); \nax[1].plot(tol_vec[0:n_tol],[(ld_n[0]*tol_vec[0])/tol_vec[jj] for jj in range(n_tol)],color='tab:blue')\nax[1].scatter(tol_vec[ii_iid:n_tol],iid_n[ii_iid:n_tol],color='tab:orange'); \nax[1].plot(tol_vec[ii_iid:n_tol],[(iid_n[ii_iid]*(tol_vec[ii_iid]**2))/(tol_vec[jj]**2) for jj in range(ii_iid,n_tol)],color='tab:orange')\nax[1].set_ylim([1e2,1e8]); ax[1].set_ylabel('n')\nfor ii in range(2):\n  ax[ii].set_xlim([0.007,100]); ax[ii].set_xlabel('Tolerance, '+r'$\\varepsilon$')\n  ax[ii].set_xscale('log'); ax[ii].set_yscale('log')\n  ax[ii].legend(['Lattice',r'$\\mathcal{O}(\\varepsilon^{-1})$','IID',r'$\\mathcal{O}(\\varepsilon^{-2})$'],frameon=False)\n  ax[ii].set_aspect(0.65)\nax[0].set_yticks([1, 60, 3600], labels = ['1 sec', '1 min', '1 hr'])    \nfig.savefig(figpath+'iidldbeam.eps',format='eps',bbox_inches='tight')\n</pre> with open(figpath+'iid_ld.pkl','rb') as myfile: tol_vec,n_tol,ii_iid,ld_time,ld_n,iid_time,iid_n,best_solution_i = pickle.load(myfile) print(best_solution_i) fig,ax = plt.subplots(nrows=1,ncols=2,figsize=(13,5.5)) ax[0].scatter(tol_vec[0:n_tol],ld_time[0:n_tol],color='tab:blue');  ax[0].plot(tol_vec[0:n_tol],[(ld_time[0]*tol_vec[0])/tol_vec[jj] for jj in range(n_tol)],color='tab:blue') ax[0].scatter(tol_vec[ii_iid:n_tol],iid_time[ii_iid:n_tol],color='tab:orange');  ax[0].plot(tol_vec[ii_iid:n_tol],[(iid_time[ii_iid]*(tol_vec[ii_iid]**2))/(tol_vec[jj]**2) for jj in range(ii_iid,n_tol)],color='tab:orange') ax[0].set_ylim([0.001,1000]); ax[0].set_ylabel('Time (s)') ax[1].scatter(tol_vec[0:n_tol],ld_n[0:n_tol],color='tab:blue');  ax[1].plot(tol_vec[0:n_tol],[(ld_n[0]*tol_vec[0])/tol_vec[jj] for jj in range(n_tol)],color='tab:blue') ax[1].scatter(tol_vec[ii_iid:n_tol],iid_n[ii_iid:n_tol],color='tab:orange');  ax[1].plot(tol_vec[ii_iid:n_tol],[(iid_n[ii_iid]*(tol_vec[ii_iid]**2))/(tol_vec[jj]**2) for jj in range(ii_iid,n_tol)],color='tab:orange') ax[1].set_ylim([1e2,1e8]); ax[1].set_ylabel('n') for ii in range(2):   ax[ii].set_xlim([0.007,100]); ax[ii].set_xlabel('Tolerance, '+r'$\\varepsilon$')   ax[ii].set_xscale('log'); ax[ii].set_yscale('log')   ax[ii].legend(['Lattice',r'$\\mathcal{O}(\\varepsilon^{-1})$','IID',r'$\\mathcal{O}(\\varepsilon^{-2})$'],frameon=False)   ax[ii].set_aspect(0.65) ax[0].set_yticks([1, 60, 3600], labels = ['1 sec', '1 min', '1 hr'])     fig.savefig(figpath+'iidldbeam.eps',format='eps',bbox_inches='tight') <pre>[1037.12106673]\n</pre> In\u00a0[14]: Copied! <pre>with open(figpath+'ld_parallel.pkl','rb') as myfile: tol_vec,n_tol,ld_time,ld_n,ld_p_time,ld_p_n,best_solution = pickle.load(myfile)\nprint(best_solution_i)\nfig,ax = plt.subplots(nrows=1,ncols=2,figsize=(11,4))\nax[0].scatter(tol_vec[0:n_tol],ld_time[0:n_tol],color='tab:blue'); \nax[0].plot(tol_vec[0:n_tol],[(ld_time[0]*tol_vec[0])/tol_vec[jj] for jj in range(n_tol)],color='tab:blue')\nax[0].scatter(tol_vec[0:n_tol],ld_p_time[0:n_tol],color='tab:orange',marker = '+',s=100,linewidths=2); \n#ax[0].plot(tol_vec[0:n_tol],[(ld_p_time[0]*tol_vec[0])/tol_vec[jj] for jj in range(n_tol)],color='tab:orange')\nax[0].set_ylim([0.05,500]); ax[0].set_ylabel('Time')\nax[1].scatter(tol_vec[0:n_tol],ld_n[0:n_tol],color='tab:blue'); \nax[1].plot(tol_vec[0:n_tol],[(ld_n[0]*tol_vec[0])/tol_vec[jj] for jj in range(n_tol)],color='tab:blue')\nax[1].scatter(tol_vec[0:n_tol],ld_p_n[0:n_tol],color='tab:orange',marker = '+',s=100,linewidths=2); \n#ax[1].plot(tol_vec[0:n_tol],[(ld_p_n[0]*tol_vec[0])/tol_vec[jj] for jj in range(n_tol)],color='tab:orange')\nax[1].set_ylim([10,1e5]); ax[1].set_ylabel('n')\nfor ii in range(2):\n  ax[ii].set_xlim([0.007,4]); ax[ii].set_xlabel('Tolerance, '+r'$\\varepsilon$')\n  ax[ii].set_xscale('log'); ax[ii].set_yscale('log')\n  ax[ii].legend(['Lattice',r'$\\mathcal{O}(\\varepsilon^{-1})$','Lattice Parallel',r'$\\mathcal{O}(\\varepsilon^{-1})$'],frameon=False)\n  ax[ii].set_aspect(0.6)\nax[0].set_yticks([1, 60], labels = ['1 sec', '1 min'])\nfig.savefig(figpath+'ldparallelbeam.eps',format='eps',bbox_inches='tight')\n</pre> with open(figpath+'ld_parallel.pkl','rb') as myfile: tol_vec,n_tol,ld_time,ld_n,ld_p_time,ld_p_n,best_solution = pickle.load(myfile) print(best_solution_i) fig,ax = plt.subplots(nrows=1,ncols=2,figsize=(11,4)) ax[0].scatter(tol_vec[0:n_tol],ld_time[0:n_tol],color='tab:blue');  ax[0].plot(tol_vec[0:n_tol],[(ld_time[0]*tol_vec[0])/tol_vec[jj] for jj in range(n_tol)],color='tab:blue') ax[0].scatter(tol_vec[0:n_tol],ld_p_time[0:n_tol],color='tab:orange',marker = '+',s=100,linewidths=2);  #ax[0].plot(tol_vec[0:n_tol],[(ld_p_time[0]*tol_vec[0])/tol_vec[jj] for jj in range(n_tol)],color='tab:orange') ax[0].set_ylim([0.05,500]); ax[0].set_ylabel('Time') ax[1].scatter(tol_vec[0:n_tol],ld_n[0:n_tol],color='tab:blue');  ax[1].plot(tol_vec[0:n_tol],[(ld_n[0]*tol_vec[0])/tol_vec[jj] for jj in range(n_tol)],color='tab:blue') ax[1].scatter(tol_vec[0:n_tol],ld_p_n[0:n_tol],color='tab:orange',marker = '+',s=100,linewidths=2);  #ax[1].plot(tol_vec[0:n_tol],[(ld_p_n[0]*tol_vec[0])/tol_vec[jj] for jj in range(n_tol)],color='tab:orange') ax[1].set_ylim([10,1e5]); ax[1].set_ylabel('n') for ii in range(2):   ax[ii].set_xlim([0.007,4]); ax[ii].set_xlabel('Tolerance, '+r'$\\varepsilon$')   ax[ii].set_xscale('log'); ax[ii].set_yscale('log')   ax[ii].legend(['Lattice',r'$\\mathcal{O}(\\varepsilon^{-1})$','Lattice Parallel',r'$\\mathcal{O}(\\varepsilon^{-1})$'],frameon=False)   ax[ii].set_aspect(0.6) ax[0].set_yticks([1, 60], labels = ['1 sec', '1 min']) fig.savefig(figpath+'ldparallelbeam.eps',format='eps',bbox_inches='tight') <pre>[1037.12106673]\n</pre> In\u00a0[15]: Copied! <pre>fig,ax = plt.subplots(figsize=(6,3))\nax.plot(best_solution,'-')\nax.set_xlim([0,len(best_solution)-1]); ax.set_xlabel('Position')\nax.set_ylim([1050,-50]);  ax.set_ylabel('Mean Deflection');\nax.set_aspect(0.02)\nfig.suptitle('Cantilevered Beam')\nfig.savefig(figpath+'cantileveredbeamwords.eps',format='eps',bbox_inches='tight')\n</pre> fig,ax = plt.subplots(figsize=(6,3)) ax.plot(best_solution,'-') ax.set_xlim([0,len(best_solution)-1]); ax.set_xlabel('Position') ax.set_ylim([1050,-50]);  ax.set_ylabel('Mean Deflection'); ax.set_aspect(0.02) fig.suptitle('Cantilevered Beam') fig.savefig(figpath+'cantileveredbeamwords.eps',format='eps',bbox_inches='tight') <p>Below is long-running code, that we rarely wish to run</p> In\u00a0[16]: Copied! <pre>import pandas as pd\nfrom sklearn.model_selection import train_test_split\ndf = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/haberman/haberman.data',header=None)\ndf.columns = ['Age','1900 Year','Axillary Nodes','Survival Status']\ndf.loc[df['Survival Status']==2,'Survival Status'] = 0\nx,y = df[['Age','1900 Year','Axillary Nodes']],df['Survival Status']\nxt,xv,yt,yv = train_test_split(x,y,test_size=.33,random_state=7)\n</pre> import pandas as pd from sklearn.model_selection import train_test_split df = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/haberman/haberman.data',header=None) df.columns = ['Age','1900 Year','Axillary Nodes','Survival Status'] df.loc[df['Survival Status']==2,'Survival Status'] = 0 x,y = df[['Age','1900 Year','Axillary Nodes']],df['Survival Status'] xt,xv,yt,yv = train_test_split(x,y,test_size=.33,random_state=7) In\u00a0[17]: Copied! <pre>print(df.head(),'\\n')\nprint(df[['Age','1900 Year','Axillary Nodes']].describe(),'\\n')\nprint(df['Survival Status'].astype(str).describe())\nprint('\\ntrain samples: %d test samples: %d\\n'%(len(xt),len(xv)))\nprint('train positives %d   train negatives: %d'%(np.sum(yt==1),np.sum(yt==0)))\nprint(' test positives %d    test negatives: %d'%(np.sum(yv==1),np.sum(yv==0)))\nxt.head()\n</pre> print(df.head(),'\\n') print(df[['Age','1900 Year','Axillary Nodes']].describe(),'\\n') print(df['Survival Status'].astype(str).describe()) print('\\ntrain samples: %d test samples: %d\\n'%(len(xt),len(xv))) print('train positives %d   train negatives: %d'%(np.sum(yt==1),np.sum(yt==0))) print(' test positives %d    test negatives: %d'%(np.sum(yv==1),np.sum(yv==0))) xt.head() <pre>   Age  1900 Year  Axillary Nodes  Survival Status\n0   30         64               1                1\n1   30         62               3                1\n2   30         65               0                1\n3   31         59               2                1\n4   31         65               4                1 \n\n              Age   1900 Year  Axillary Nodes\ncount  306.000000  306.000000      306.000000\nmean    52.457516   62.852941        4.026144\nstd     10.803452    3.249405        7.189654\nmin     30.000000   58.000000        0.000000\n25%     44.000000   60.000000        0.000000\n50%     52.000000   63.000000        1.000000\n75%     60.750000   65.750000        4.000000\nmax     83.000000   69.000000       52.000000 \n\ncount     306\nunique      2\ntop         1\nfreq      225\nName: Survival Status, dtype: object\n\ntrain samples: 205 test samples: 101\n\ntrain positives 151   train negatives: 54\n test positives 74    test negatives: 27\n</pre> Out[17]: Age 1900 Year Axillary Nodes 46 41 58 0 199 57 64 1 115 49 64 10 128 50 61 0 249 63 63 0 In\u00a0[18]: Copied! <pre>blr = qp.BayesianLRCoeffs(\n    sampler = qp.DigitalNetB2(4,seed=1),\n    feature_array = xt, # np.ndarray of shape (n,d-1)\n    response_vector = yt, # np.ndarray of shape (n,)\n    prior_mean = 0, # normal prior mean = (0,0,...,0)\n    prior_covariance = 5) # normal prior covariance = I\nqmc_sc = qp.CubQMCNetG(blr,\n    abs_tol = .1,\n    rel_tol = .5,\n    error_fun = \"BOTH\",\n    n_limit=2**18)\nblr_coefs,blr_data = qmc_sc.integrate()\nprint(blr_data)\n# Data (Data)\n#     solution        [ 0.262 -0.043 -0.226 -1.203]\n#     comb_bound_low  [ 0.185 -0.067 -0.306 -1.656]\n#     comb_bound_high [ 0.347 -0.035 -0.163 -0.75 ]\n#     comb_bound_diff [0.162 0.031 0.143 0.906]\n#     comb_flags      [ True  True  True False]\n#     n_total         2^(18)\n#     n               [[  1024   1024   1024 262144]\n#                      [  1024   1024   1024 262144]]\n#     time_integrate  3.120\n</pre> blr = qp.BayesianLRCoeffs(     sampler = qp.DigitalNetB2(4,seed=1),     feature_array = xt, # np.ndarray of shape (n,d-1)     response_vector = yt, # np.ndarray of shape (n,)     prior_mean = 0, # normal prior mean = (0,0,...,0)     prior_covariance = 5) # normal prior covariance = I qmc_sc = qp.CubQMCNetG(blr,     abs_tol = .1,     rel_tol = .5,     error_fun = \"BOTH\",     n_limit=2**18) blr_coefs,blr_data = qmc_sc.integrate() print(blr_data) # Data (Data) #     solution        [ 0.262 -0.043 -0.226 -1.203] #     comb_bound_low  [ 0.185 -0.067 -0.306 -1.656] #     comb_bound_high [ 0.347 -0.035 -0.163 -0.75 ] #     comb_bound_diff [0.162 0.031 0.143 0.906] #     comb_flags      [ True  True  True False] #     n_total         2^(18) #     n               [[  1024   1024   1024 262144] #                      [  1024   1024   1024 262144]] #     time_integrate  3.120 <pre>Data (Data)\n    solution        [ 0.262 -0.043 -0.226 -1.203]\n    comb_bound_low  [ 0.185 -0.067 -0.306 -1.656]\n    comb_bound_high [ 0.347 -0.035 -0.163 -0.75 ]\n    comb_bound_diff [0.162 0.031 0.143 0.906]\n    comb_flags      [ True  True  True False]\n    n_total         2^(18)\n    n               [[  1024   1024   1024 262144]\n                     [  1024   1024   1024 262144]]\n    time_integrate  4.105\nCubQMCNetG (AbstractStoppingCriterion)\n    abs_tol         0.100\n    rel_tol         2^(-1)\n    n_init          2^(10)\n    n_limit         2^(18)\nBayesianLRCoeffs (AbstractIntegrand)\nGaussian (AbstractTrueMeasure)\n    mean            0\n    covariance      5\n    decomp_type     PCA\nDigitalNetB2 (AbstractLDDiscreteDistribution)\n    d               2^(2)\n    replications    1\n    randomize       LMS_DS\n    gen_mats_source joe_kuo.6.21201.txt\n    order           NATURAL\n    t               63\n    alpha           1\n    n_limit         2^(32)\n    entropy         1\n</pre> <pre>/Users/alegresor/Desktop/QMCSoftware/qmcpy/stopping_criterion/abstract_cub_qmc_ld_g.py:215: MaxSamplesWarning: \n                Already generated 262144 samples.\n                Trying to generate 262144 new samples would exceeds n_limit = 262144.\n                No more samples will be generated.\n                Note that error tolerances may not be satisfied. \n  warnings.warn(warning_s, MaxSamplesWarning)\n</pre> In\u00a0[19]: Copied! <pre>from sklearn.linear_model import LogisticRegression\ndef metrics(y,yhat):\n    y,yhat = np.array(y),np.array(yhat)\n    tp = np.sum((y==1)*(yhat==1))\n    tn = np.sum((y==0)*(yhat==0))\n    fp = np.sum((y==0)*(yhat==1))\n    fn = np.sum((y==1)*(yhat==0))\n    accuracy = (tp+tn)/(len(y))\n    precision = tp/(tp+fp)\n    recall = tp/(tp+fn)\n    return [accuracy,precision,recall]\n\nresults = pd.DataFrame({name:[] for name in ['method','Age','1900 Year','Axillary Nodes','Intercept','Accuracy','Precision','Recall']})\nfor i,l1_ratio in enumerate([0,.5,1]):\n    lr = LogisticRegression(random_state=7,penalty=\"elasticnet\",solver='saga',l1_ratio=l1_ratio).fit(xt,yt)\n    results.loc[i] = [r'Elastic-Net \\lambda=%.1f'%l1_ratio]+lr.coef_.squeeze().tolist()+[lr.intercept_.item()]+metrics(yv,lr.predict(xv))\n\nblr_predict = lambda x: 1/(1+np.exp(-np.array(x)@blr_coefs[:-1]-blr_coefs[-1]))&gt;=.5\nblr_train_accuracy = np.mean(blr_predict(xt)==yt)\nblr_test_accuracy = np.mean(blr_predict(xv)==yv)\nresults.loc[len(results)] = ['Bayesian']+blr_coefs.squeeze().tolist()+metrics(yv,blr_predict(xv))\n\nimport warnings\nwarnings.simplefilter('ignore',FutureWarning)\nresults.set_index('method',inplace=True)\nprint(results.head())\n#root: results.to_latex(root+'lr_table.tex',formatters={'%s'%tt:lambda v:'%.1f'%(100*v) for tt in ['accuracy','precision','recall']},float_format=\"%.2e\")\n</pre> from sklearn.linear_model import LogisticRegression def metrics(y,yhat):     y,yhat = np.array(y),np.array(yhat)     tp = np.sum((y==1)*(yhat==1))     tn = np.sum((y==0)*(yhat==0))     fp = np.sum((y==0)*(yhat==1))     fn = np.sum((y==1)*(yhat==0))     accuracy = (tp+tn)/(len(y))     precision = tp/(tp+fp)     recall = tp/(tp+fn)     return [accuracy,precision,recall]  results = pd.DataFrame({name:[] for name in ['method','Age','1900 Year','Axillary Nodes','Intercept','Accuracy','Precision','Recall']}) for i,l1_ratio in enumerate([0,.5,1]):     lr = LogisticRegression(random_state=7,penalty=\"elasticnet\",solver='saga',l1_ratio=l1_ratio).fit(xt,yt)     results.loc[i] = [r'Elastic-Net \\lambda=%.1f'%l1_ratio]+lr.coef_.squeeze().tolist()+[lr.intercept_.item()]+metrics(yv,lr.predict(xv))  blr_predict = lambda x: 1/(1+np.exp(-np.array(x)@blr_coefs[:-1]-blr_coefs[-1]))&gt;=.5 blr_train_accuracy = np.mean(blr_predict(xt)==yt) blr_test_accuracy = np.mean(blr_predict(xv)==yv) results.loc[len(results)] = ['Bayesian']+blr_coefs.squeeze().tolist()+metrics(yv,blr_predict(xv))  import warnings warnings.simplefilter('ignore',FutureWarning) results.set_index('method',inplace=True) print(results.head()) #root: results.to_latex(root+'lr_table.tex',formatters={'%s'%tt:lambda v:'%.1f'%(100*v) for tt in ['accuracy','precision','recall']},float_format=\"%.2e\") <pre>                              Age  1900 Year  Axillary Nodes  Intercept  \\\nmethod                                                                    \nElastic-Net \\lambda=0.0 -0.012279   0.034401       -0.115153   0.001990   \nElastic-Net \\lambda=0.5 -0.012041   0.034170       -0.114770   0.002025   \nElastic-Net \\lambda=1.0 -0.011803   0.033940       -0.114387   0.002061   \nBayesian                 0.262086  -0.043253       -0.225631  -1.202777   \n\n                         Accuracy  Precision    Recall  \nmethod                                                  \nElastic-Net \\lambda=0.0  0.742574   0.766667  0.932432  \nElastic-Net \\lambda=0.5  0.742574   0.766667  0.932432  \nElastic-Net \\lambda=1.0  0.742574   0.766667  0.932432  \nBayesian                 0.732673   0.737374  0.986486  \n</pre> In\u00a0[53]: Copied! <pre>option_parameters = {\n            'option': \"ASIAN\",\n            'volatility' : .2,\n            'start_price' : 100,\n            'strike_price' : 100,\n            'interest_rate' : 0.05}\nm_steps = 7\nnsteps = np.zeros(m_steps,dtype=int)\npca_price = np.zeros(m_steps)\nchol_price = np.zeros(m_steps)\npca_time = np.zeros(m_steps)\nchol_time = np.zeros(m_steps)\nabstol=1e-4\nfor level in range(m_steps):\n    nsteps[level] = 2**level\n    aco = qp.FinancialOption(qp.Sobol(nsteps[level]), **option_parameters, decomp_type = \"PCA\")\n    approx_solution, data = qp.CubQMCSobolG(aco, abs_tol=abstol).integrate()\n    pca_price[level] = approx_solution\n    pca_time[level] = data.time_integrate\n    print(\"Asian Option true value (%d time steps): %.5f (to within %.0e) in %.2f seconds using PCA\"%(nsteps[level], approx_solution, abstol, data.time_integrate))\n    aco = qp.FinancialOption(qp.Sobol(nsteps[level]), **option_parameters, decomp_type = \"Cholesky\")\n    approx_solution, data = qp.CubQMCSobolG(aco, abs_tol=abstol).integrate()\n    chol_price[level] = approx_solution\n    chol_time[level] = data.time_integrate\n    print(\"Asian Option true value (%d time steps): %.5f (to within %.0e) in %.2f seconds using Cholesky\"%(nsteps[level], approx_solution, abstol, data.time_integrate))\n</pre> option_parameters = {             'option': \"ASIAN\",             'volatility' : .2,             'start_price' : 100,             'strike_price' : 100,             'interest_rate' : 0.05} m_steps = 7 nsteps = np.zeros(m_steps,dtype=int) pca_price = np.zeros(m_steps) chol_price = np.zeros(m_steps) pca_time = np.zeros(m_steps) chol_time = np.zeros(m_steps) abstol=1e-4 for level in range(m_steps):     nsteps[level] = 2**level     aco = qp.FinancialOption(qp.Sobol(nsteps[level]), **option_parameters, decomp_type = \"PCA\")     approx_solution, data = qp.CubQMCSobolG(aco, abs_tol=abstol).integrate()     pca_price[level] = approx_solution     pca_time[level] = data.time_integrate     print(\"Asian Option true value (%d time steps): %.5f (to within %.0e) in %.2f seconds using PCA\"%(nsteps[level], approx_solution, abstol, data.time_integrate))     aco = qp.FinancialOption(qp.Sobol(nsteps[level]), **option_parameters, decomp_type = \"Cholesky\")     approx_solution, data = qp.CubQMCSobolG(aco, abs_tol=abstol).integrate()     chol_price[level] = approx_solution     chol_time[level] = data.time_integrate     print(\"Asian Option true value (%d time steps): %.5f (to within %.0e) in %.2f seconds using Cholesky\"%(nsteps[level], approx_solution, abstol, data.time_integrate)) <pre>Asian Option true value (1 time steps): 5.22530 (to within 1e-04) in 0.09 seconds using PCA\nAsian Option true value (1 time steps): 5.22529 (to within 1e-04) in 0.08 seconds using Cholesky\nAsian Option true value (2 time steps): 5.63592 (to within 1e-04) in 0.11 seconds using PCA\nAsian Option true value (2 time steps): 5.63593 (to within 1e-04) in 0.24 seconds using Cholesky\nAsian Option true value (4 time steps): 5.73171 (to within 1e-04) in 0.44 seconds using PCA\nAsian Option true value (4 time steps): 5.73169 (to within 1e-04) in 0.87 seconds using Cholesky\nAsian Option true value (8 time steps): 5.75525 (to within 1e-04) in 0.64 seconds using PCA\nAsian Option true value (8 time steps): 5.75517 (to within 1e-04) in 6.24 seconds using Cholesky\nAsian Option true value (16 time steps): 5.76114 (to within 1e-04) in 1.28 seconds using PCA\nAsian Option true value (16 time steps): 5.76139 (to within 1e-04) in 25.96 seconds using Cholesky\nAsian Option true value (32 time steps): 5.76260 (to within 1e-04) in 2.81 seconds using PCA\nAsian Option true value (32 time steps): 5.76245 (to within 1e-04) in 147.31 seconds using Cholesky\nAsian Option true value (64 time steps): 5.76296 (to within 1e-04) in 4.69 seconds using PCA\nAsian Option true value (64 time steps): 5.76281 (to within 1e-04) in 442.84 seconds using Cholesky\n</pre> In\u00a0[54]: Copied! <pre>fig,ax = plt.subplots(nrows=1,ncols=2,figsize=(13,4.5))\nax[0].scatter(nsteps,pca_price,color='tab:blue'); \nax[0].scatter(nsteps,chol_price,color='tab:orange'); \nax[0].set_xscale('log')\nax[1].scatter(nsteps,pca_time,color='tab:blue'); \nax[1].scatter(nsteps,chol_time,color='tab:orange'); \nax[1].set_xscale('log')\nax[1].set_yscale('log')\nax[0].set_xlabel(r'\\# time steps, $d$');\nax[1].set_xlabel(r'\\# time steps, $d$');\nax[0].set_ylabel(r'Option Price');\nax[1].set_ylabel(r'Computation Time (sec)');\nax[0].legend(['Eigenvalue','Cholesky'],frameon=False)\nax[1].legend(['Eigenvalue','Cholesky'],frameon=False)\nfig.savefig(figpath+'optionpricing.eps',format='eps',bbox_inches='tight')\n</pre> fig,ax = plt.subplots(nrows=1,ncols=2,figsize=(13,4.5)) ax[0].scatter(nsteps,pca_price,color='tab:blue');  ax[0].scatter(nsteps,chol_price,color='tab:orange');  ax[0].set_xscale('log') ax[1].scatter(nsteps,pca_time,color='tab:blue');  ax[1].scatter(nsteps,chol_time,color='tab:orange');  ax[1].set_xscale('log') ax[1].set_yscale('log') ax[0].set_xlabel(r'\\# time steps, $d$'); ax[1].set_xlabel(r'\\# time steps, $d$'); ax[0].set_ylabel(r'Option Price'); ax[1].set_ylabel(r'Computation Time (sec)'); ax[0].legend(['Eigenvalue','Cholesky'],frameon=False) ax[1].legend(['Eigenvalue','Cholesky'],frameon=False) fig.savefig(figpath+'optionpricing.eps',format='eps',bbox_inches='tight') In\u00a0[55]: Copied! <pre>qp.util.stop_notebook()\n</pre> qp.util.stop_notebook() In\u00a0[3]: Copied! <pre>import umbridge #this is the connector\n!docker run --name muqbp -d -it -p 4243:4243 linusseelinger/benchmark-muq-beam-propagation:latest #get beam example\nd = 3 #dimension of the randomness\nlb = 1 #lower bound on randomness\nub = 1.2 #upper bound on randomness\numbridge_config = {\"d\": d}\nmodel = umbridge.HTTPModel('http://localhost:4243','forward') #this is the original model\noutindex = -1 #choose last element of the vector of beam deflections\nmodeli = deepcopy(model) #and construct a model for just that deflection\nmodeli.get_output_sizes = lambda *args : [1]\nmodeli.get_output_sizes()\nmodeli.__call__ = lambda *args,**kwargs: [[model.__call__(*args,**kwargs)[0][outindex]]]\n</pre> import umbridge #this is the connector !docker run --name muqbp -d -it -p 4243:4243 linusseelinger/benchmark-muq-beam-propagation:latest #get beam example d = 3 #dimension of the randomness lb = 1 #lower bound on randomness ub = 1.2 #upper bound on randomness umbridge_config = {\"d\": d} model = umbridge.HTTPModel('http://localhost:4243','forward') #this is the original model outindex = -1 #choose last element of the vector of beam deflections modeli = deepcopy(model) #and construct a model for just that deflection modeli.get_output_sizes = lambda *args : [1] modeli.get_output_sizes() modeli.__call__ = lambda *args,**kwargs: [[model.__call__(*args,**kwargs)[0][outindex]]] <pre>docker: Error response from daemon: Conflict. The container name \"/muqbp\" is already in use by container \"d30a451e4f61fd8d33dd7bb63d9338183a4015fe9dbbb755727cf7f3596fecee\". You have to remove (or rename) that container to be able to reuse that name.\nSee 'docker run --help'.\n</pre> In\u00a0[\u00a0]: Copied! <pre>ld = qp.Uniform(qp.Lattice(d,seed=7),lower_bound=lb,upper_bound=ub) #lattice points for this problem\nld_integ = qp.UMBridgeWrapper(ld,modeli,umbridge_config,parallel=False) #integrand\niid = qp.Uniform(qp.IIDStdUniform(d),lower_bound=lb,upper_bound=ub) #iid points for this problem\niid_integ = qp.UMBridgeWrapper(iid,modeli,umbridge_config,parallel=False) #integrand\ntol = 0.01  #smallest tolerance\n\nn_tol = 14  #number of different tolerances\nii_iid = 9  #make this larger to reduce the time required by not running all cases for IID\ntol_vec = [tol*(2**ii) for ii in range(n_tol)]  #initialize vector of tolerances\nld_time = [0]*n_tol; ld_n = [0]*n_tol  #low discrepancy time and number of function values\niid_time = [0]*n_tol; iid_n = [0]*n_tol  #IID time and number of function values\nprint(f'\\nCantilever Beam\\n')\nprint('iteration ', end = '')\nfor ii in range(n_tol):\n  solution, data = qp.CubQMCLatticeG(ld_integ, abs_tol = tol_vec[ii]).integrate()\n  if ii == 0:\n    best_solution_i = solution\n  ld_time[ii] = data.time_integrate\n  ld_n[ii] = data.n_total\n  if ii &gt;= ii_iid:\n    solution, data = qp.CubMCG(iid_integ, abs_tol = tol_vec[ii]).integrate()\n    iid_time[ii] = data.time_integrate\n    iid_n[ii] = data.n_total\n  print(ii, end = ' ')\nwith open(figpath+'iid_ld.pkl','wb') as myfile:pickle.dump([tol_vec,n_tol,ii_iid,ld_time,ld_n,iid_time,iid_n,best_solution_i],myfile)\n</pre> ld = qp.Uniform(qp.Lattice(d,seed=7),lower_bound=lb,upper_bound=ub) #lattice points for this problem ld_integ = qp.UMBridgeWrapper(ld,modeli,umbridge_config,parallel=False) #integrand iid = qp.Uniform(qp.IIDStdUniform(d),lower_bound=lb,upper_bound=ub) #iid points for this problem iid_integ = qp.UMBridgeWrapper(iid,modeli,umbridge_config,parallel=False) #integrand tol = 0.01  #smallest tolerance  n_tol = 14  #number of different tolerances ii_iid = 9  #make this larger to reduce the time required by not running all cases for IID tol_vec = [tol*(2**ii) for ii in range(n_tol)]  #initialize vector of tolerances ld_time = [0]*n_tol; ld_n = [0]*n_tol  #low discrepancy time and number of function values iid_time = [0]*n_tol; iid_n = [0]*n_tol  #IID time and number of function values print(f'\\nCantilever Beam\\n') print('iteration ', end = '') for ii in range(n_tol):   solution, data = qp.CubQMCLatticeG(ld_integ, abs_tol = tol_vec[ii]).integrate()   if ii == 0:     best_solution_i = solution   ld_time[ii] = data.time_integrate   ld_n[ii] = data.n_total   if ii &gt;= ii_iid:     solution, data = qp.CubMCG(iid_integ, abs_tol = tol_vec[ii]).integrate()     iid_time[ii] = data.time_integrate     iid_n[ii] = data.n_total   print(ii, end = ' ') with open(figpath+'iid_ld.pkl','wb') as myfile:pickle.dump([tol_vec,n_tol,ii_iid,ld_time,ld_n,iid_time,iid_n,best_solution_i],myfile) In\u00a0[\u00a0]: Copied! <pre>ld_integ = qp.UMBridgeWrapper(ld,model,umbridge_config,parallel=False) #integrand\nld_integ_p = qp.UMBridgeWrapper(ld,model,umbridge_config,parallel=8) #integrand with parallel processing\n\ntol = 0.01\nn_tol = 9  #number of different tolerances\ntol_vec = [tol*(2**ii) for ii in range(n_tol)]  #initialize vector of tolerances\nld_time = [0]*n_tol; ld_n = [0]*n_tol  #low discrepancy time and number of function values\nld_p_time = [0]*n_tol; ld_p_n = [0]*n_tol  #low discrepancy time and number of function values with parallel\nprint(f'\\nCantilever Beam\\n')\nprint('iteration ', end = '')\nfor ii in range(n_tol):\n  solution, data = qp.CubQMCLatticeG(ld_integ, abs_tol = tol_vec[ii]).integrate()\n  if ii == 0:\n    best_solution = solution\n  ld_time[ii] = data.time_integrate\n  ld_n[ii] = data.n_total\n  solution, data = qp.CubQMCLatticeG(ld_integ_p, abs_tol = tol_vec[ii]).integrate()\n  ld_p_time[ii] = data.time_integrate\n  ld_p_n[ii] = data.n_total\n  print(ii, end = ' ') \nwith open(figpath+'ld_parallel.pkl','wb') as myfile:pickle.dump([tol_vec,n_tol,ld_time,ld_n,ld_p_time,ld_p_n,best_solution],myfile)\n</pre> ld_integ = qp.UMBridgeWrapper(ld,model,umbridge_config,parallel=False) #integrand ld_integ_p = qp.UMBridgeWrapper(ld,model,umbridge_config,parallel=8) #integrand with parallel processing  tol = 0.01 n_tol = 9  #number of different tolerances tol_vec = [tol*(2**ii) for ii in range(n_tol)]  #initialize vector of tolerances ld_time = [0]*n_tol; ld_n = [0]*n_tol  #low discrepancy time and number of function values ld_p_time = [0]*n_tol; ld_p_n = [0]*n_tol  #low discrepancy time and number of function values with parallel print(f'\\nCantilever Beam\\n') print('iteration ', end = '') for ii in range(n_tol):   solution, data = qp.CubQMCLatticeG(ld_integ, abs_tol = tol_vec[ii]).integrate()   if ii == 0:     best_solution = solution   ld_time[ii] = data.time_integrate   ld_n[ii] = data.n_total   solution, data = qp.CubQMCLatticeG(ld_integ_p, abs_tol = tol_vec[ii]).integrate()   ld_p_time[ii] = data.time_integrate   ld_p_n[ii] = data.n_total   print(ii, end = ' ')  with open(figpath+'ld_parallel.pkl','wb') as myfile:pickle.dump([tol_vec,n_tol,ld_time,ld_n,ld_p_time,ld_p_n,best_solution],myfile) In\u00a0[\u00a0]: Copied! <pre>!docker rm -f muqbp #shut down docker image\n</pre> !docker rm -f muqbp #shut down docker image In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"demos/talk_paper_demos/Argonne_Talk_2023_May/Argonne_2023_Talk_Figures/#argonne-lab-talk","title":"Argonne Lab Talk\u00b6","text":"<p>Computations and figures for a seminar at Argonne Lab</p> <p>presented on Thursday, May 18, 2023, slides here</p> <p>To run this notebook you need to pip install</p> <pre><code>\u2022 qmcpy\n\u2022 sympy\n\u2022 docker</code></pre>"},{"location":"demos/talk_paper_demos/Argonne_Talk_2023_May/Argonne_2023_Talk_Figures/#import-the-necessary-packages-and-set-up-plotting-routines","title":"Import the necessary packages and set up plotting routines\u00b6","text":""},{"location":"demos/talk_paper_demos/Argonne_Talk_2023_May/Argonne_2023_Talk_Figures/#set-the-path-to-save-the-figures-here","title":"Set the path to save the figures here\u00b6","text":""},{"location":"demos/talk_paper_demos/Argonne_Talk_2023_May/Argonne_2023_Talk_Figures/#trapezoidal-rule-and-its-error","title":"Trapezoidal rule and its error\u00b6","text":""},{"location":"demos/talk_paper_demos/Argonne_Talk_2023_May/Argonne_2023_Talk_Figures/#define-test-function","title":"Define test function\u00b6","text":""},{"location":"demos/talk_paper_demos/Argonne_Talk_2023_May/Argonne_2023_Talk_Figures/#plot-test-function-and-trapezoidal-rule","title":"Plot test function and trapezoidal rule\u00b6","text":""},{"location":"demos/talk_paper_demos/Argonne_Talk_2023_May/Argonne_2023_Talk_Figures/#compute-trapezoidal-rule-and-its-error","title":"Compute trapezoidal rule and its error\u00b6","text":""},{"location":"demos/talk_paper_demos/Argonne_Talk_2023_May/Argonne_2023_Talk_Figures/#compute-elements-of-the-trio-identity","title":"Compute elements of the trio identity\u00b6","text":""},{"location":"demos/talk_paper_demos/Argonne_Talk_2023_May/Argonne_2023_Talk_Figures/#plots-of-iid-and-low-discrepancy-ld-points","title":"Plots of IID and Low Discrepancy (LD) Points\u00b6","text":"<p>These plots show how LD points fill space better than IID points</p>"},{"location":"demos/talk_paper_demos/Argonne_Talk_2023_May/Argonne_2023_Talk_Figures/#lattice-points-first","title":"Lattice points first\u00b6","text":""},{"location":"demos/talk_paper_demos/Argonne_Talk_2023_May/Argonne_2023_Talk_Figures/#next-sobol-points","title":"Next Sobol' points\u00b6","text":""},{"location":"demos/talk_paper_demos/Argonne_Talk_2023_May/Argonne_2023_Talk_Figures/#also-halton-points","title":"Also Halton points\u00b6","text":""},{"location":"demos/talk_paper_demos/Argonne_Talk_2023_May/Argonne_2023_Talk_Figures/#compare-to-iid","title":"Compare to IID\u00b6","text":"<p>Note that there are more gaps and clusters</p>"},{"location":"demos/talk_paper_demos/Argonne_Talk_2023_May/Argonne_2023_Talk_Figures/#beam-example-figures","title":"Beam Example Figures\u00b6","text":"<p>Using computations done below</p>"},{"location":"demos/talk_paper_demos/Argonne_Talk_2023_May/Argonne_2023_Talk_Figures/#plot-the-time-and-sample-size-required-to-solve-for-the-deflection-of-the-end-point-using-iid-and-low-discrepancy","title":"Plot the time and sample size required to solve for the deflection of the end point using IID and low discrepancy\u00b6","text":""},{"location":"demos/talk_paper_demos/Argonne_Talk_2023_May/Argonne_2023_Talk_Figures/#plot-the-time-and-sample-size-required-to-solve-for-the-deflection-of-the-whole-beam-using-low-discrepancy-with-and-without-parallel","title":"Plot the time and sample size required to solve for the deflection of the whole beam using low discrepancy with and without parallel\u00b6","text":""},{"location":"demos/talk_paper_demos/Argonne_Talk_2023_May/Argonne_2023_Talk_Figures/#plot-of-beam-solution","title":"Plot of beam solution\u00b6","text":""},{"location":"demos/talk_paper_demos/Argonne_Talk_2023_May/Argonne_2023_Talk_Figures/#bayesian-logistic-regression","title":"Bayesian Logistic Regression\u00b6","text":"<p>This is an example where the solution is ratio of two integrals</p>"},{"location":"demos/talk_paper_demos/Argonne_Talk_2023_May/Argonne_2023_Talk_Figures/#asian-option","title":"Asian Option\u00b6","text":""},{"location":"demos/talk_paper_demos/Argonne_Talk_2023_May/Argonne_2023_Talk_Figures/#comparing-pca-and-cholesky-decompositions","title":"Comparing PCA and Cholesky Decompositions\u00b6","text":"<p>Note that the Cholesky decomposition might not even meet the error tolerance</p>"},{"location":"demos/talk_paper_demos/Argonne_Talk_2023_May/Argonne_2023_Talk_Figures/#beam-example-computations","title":"Beam Example Computations\u00b6","text":""},{"location":"demos/talk_paper_demos/Argonne_Talk_2023_May/Argonne_2023_Talk_Figures/#set-up-the-problem-using-a-docker-container-to-solve-the-ode","title":"Set up the problem using a docker container to solve the ODE\u00b6","text":"<p>To run this, you need to be running the docker application, https://www.docker.com/products/docker-desktop/</p>"},{"location":"demos/talk_paper_demos/Argonne_Talk_2023_May/Argonne_2023_Talk_Figures/#first-we-compute-the-time-required-to-solve-for-the-deflection-of-the-end-point-using-iid-and-low-discrepancy","title":"First we compute the time required to solve for the deflection of the end point using IID and low discrepancy\u00b6","text":""},{"location":"demos/talk_paper_demos/Argonne_Talk_2023_May/Argonne_2023_Talk_Figures/#next-we-compute-the-time-required-to-solve-for-the-deflection-of-the-whole-beam-using-low-discrepancy-with-and-without-parallel","title":"Next, we compute the time required to solve for the deflection of the whole beam using low discrepancy with and without parallel\u00b6","text":""},{"location":"demos/talk_paper_demos/Argonne_Talk_2023_May/Argonne_2023_Talk_Figures/#shut-down-docker","title":"Shut down docker\u00b6","text":""},{"location":"demos/talk_paper_demos/JOSS2025/joss2025/","title":"2025 JOSS Paper","text":"In\u00a0[3]: Copied! <pre>import qmcpy as qp \nimport numpy as np\nimport os\n</pre> import qmcpy as qp  import numpy as np import os In\u00a0[4]: Copied! <pre>import matplotlib \nmatplotlib.rcParams['figure.dpi'] = 256\nfrom matplotlib import pyplot \nimport tueplots \nimport seaborn as sns\nfrom tueplots import bundles,figsizes\npyplot.rcParams.update(bundles.probnum2025())\nCOLORS = ['xkcd:purple', 'xkcd:green', 'xkcd:blue', 'xkcd:orange']\nMARKERS = [\"*\",\"^\",\"o\",\"s\"]\nMARKERSIZE = 5\nOUTDIR = \"JOSS2025.outputs\"\nFIGWIDTH = 500/72\nassert os.path.isdir(OUTDIR)\n</pre> import matplotlib  matplotlib.rcParams['figure.dpi'] = 256 from matplotlib import pyplot  import tueplots  import seaborn as sns from tueplots import bundles,figsizes pyplot.rcParams.update(bundles.probnum2025()) COLORS = ['xkcd:purple', 'xkcd:green', 'xkcd:blue', 'xkcd:orange'] MARKERS = [\"*\",\"^\",\"o\",\"s\"] MARKERSIZE = 5 OUTDIR = \"JOSS2025.outputs\" FIGWIDTH = 500/72 assert os.path.isdir(OUTDIR) In\u00a0[5]: Copied! <pre>import qmcpy as qp \nqp.__version__\n</pre> import qmcpy as qp  qp.__version__ Out[5]: <pre>'2.0'</pre> In\u00a0[6]: Copied! <pre>dnb2 = qp.DigitalNetB2(dimension = 2, randomize = \"LMS_DS\")\nx = dnb2(2**7)\nprint(x.shape)\n</pre> dnb2 = qp.DigitalNetB2(dimension = 2, randomize = \"LMS_DS\") x = dnb2(2**7) print(x.shape)  <pre>(128, 2)\n</pre> In\u00a0[7]: Copied! <pre>dnb2 = qp.DigitalNetB2(dimension = 2, replications=5, randomize = \"LMS_DS\")\nx = dnb2(2**7)\nprint(x.shape)\n</pre> dnb2 = qp.DigitalNetB2(dimension = 2, replications=5, randomize = \"LMS_DS\") x = dnb2(2**7) print(x.shape) <pre>(5, 128, 2)\n</pre> In\u00a0[8]: Copied! <pre>dnb2 = qp.DigitalNetB2( # special case of a digital net\n    dimension = 12, # monthly monitoring \n    seed = 7, # for reproducibility\n)\nasian_option = qp.FinancialOption(\n    sampler = dnb2 ,\n    option = \"ASIAN\",\n    call_put = \"CALL\",\n    asian_mean = \"GEOMETRIC\",\n    asian_mean_quadrature_rule = \"RIGHT\", \n    volatility = 0.5, \n    start_price = 30., \n    strike_price = 35., \n    interest_rate = 0.01, # 1% interest rate\n    t_final = 1, # 1 year \n)\nqmc_algorithm = qp.CubQMCNetG(\n    integrand = asian_option,\n    abs_tol = 1e-3,\n    rel_tol = 0,\n    n_init = 2**8,\n)\napprox_value,data = qmc_algorithm.integrate()\nprint(approx_value)\nprint(data)\n</pre> dnb2 = qp.DigitalNetB2( # special case of a digital net     dimension = 12, # monthly monitoring      seed = 7, # for reproducibility ) asian_option = qp.FinancialOption(     sampler = dnb2 ,     option = \"ASIAN\",     call_put = \"CALL\",     asian_mean = \"GEOMETRIC\",     asian_mean_quadrature_rule = \"RIGHT\",      volatility = 0.5,      start_price = 30.,      strike_price = 35.,      interest_rate = 0.01, # 1% interest rate     t_final = 1, # 1 year  ) qmc_algorithm = qp.CubQMCNetG(     integrand = asian_option,     abs_tol = 1e-3,     rel_tol = 0,     n_init = 2**8, ) approx_value,data = qmc_algorithm.integrate() print(approx_value) print(data) <pre>1.7665677027586677\nData (Data)\n    solution        1.767\n    comb_bound_low  1.766\n    comb_bound_high 1.767\n    comb_bound_diff 0.002\n    comb_flags      1\n    n_total         2^(15)\n    n               2^(15)\n    time_integrate  0.031\nCubQMCNetG (AbstractStoppingCriterion)\n    abs_tol         0.001\n    rel_tol         0\n    n_init          2^(8)\n    n_limit         2^(35)\nFinancialOption (AbstractIntegrand)\n    option          ASIAN\n    call_put        CALL\n    volatility      2^(-1)\n    start_price     30\n    strike_price    35\n    interest_rate   0.010\n    t_final         1\n    asian_mean      GEOMETRIC\nBrownianMotion (AbstractTrueMeasure)\n    time_vec        [0.083 0.167 0.25  ... 0.833 0.917 1.   ]\n    drift           0\n    mean            [0. 0. 0. ... 0. 0. 0.]\n    covariance      [[0.083 0.083 0.083 ... 0.083 0.083 0.083]\n                     [0.083 0.167 0.167 ... 0.167 0.167 0.167]\n                     [0.083 0.167 0.25  ... 0.25  0.25  0.25 ]\n                     ...\n                     [0.083 0.167 0.25  ... 0.833 0.833 0.833]\n                     [0.083 0.167 0.25  ... 0.833 0.917 0.917]\n                     [0.083 0.167 0.25  ... 0.833 0.917 1.   ]]\n    decomp_type     PCA\nDigitalNetB2 (AbstractLDDiscreteDistribution)\n    d               12\n    replications    1\n    randomize       LMS DS\n    gen_mats_source joe_kuo.6.21201.txt\n    order           RADICAL INVERSE\n    t               63\n    alpha           1\n    n_limit         2^(32)\n    entropy         7\n</pre> In\u00a0[9]: Copied! <pre>import numpy as np\nimport scipy.stats\ndef borehole(t): # t.shape == (..., n, d) for n samples in d dimensions\n    \"\"\" https://www.sfu.ca/~ssurjano/borehole.html \"\"\"\n    rw, r, Tu, Hu, Tl, Hl, L, Kw = np.moveaxis(t, -1, 0)\n    numer = 2 * np.pi * Tu * (Hu-Hl) / np.log(r / rw)\n    denom =  1 + 2 * L * Tu / (np.log(r / rw) * rw**2 * Kw) + Tu / Tl\n    y = numer / denom\n    return y\nindep_distribs = [\n    scipy.stats.norm(loc=0.10, scale=0.0161812),\n    scipy.stats.lognorm(scale=np.exp(7.71), s=1.0056),\n    scipy.stats.uniform(loc=63070, scale=115600-63070),\n    scipy.stats.uniform(loc=990, scale=1110-990),\n    scipy.stats.uniform(loc=63.1, scale=116-63.1),\n    scipy.stats.uniform(loc=700, scale=820-700),\n    scipy.stats.uniform(loc=1120, scale=1680-1120),\n    scipy.stats.uniform(loc=9855, scale=12045-9855),\n]\ndiscrete_distrib = qp.Lattice(dimension = 8, seed = 11)\ntrue_measure = qp.SciPyWrapper(discrete_distrib, indep_distribs)\nintegrand = qp.CustomFun(true_measure, borehole)\nqmc_algorithm = qp.CubQMCBayesLatticeG(\n    integrand = integrand,\n    ptransform = \"BAKER\", # periodization transform \n    abs_tol = 1e-3,\n    rel_tol = 1e-5,\n    error_fun = \"BOTH\" # abs and rel tol satisfied, default is \"EITHER\",\n)\napprox_value,data = qmc_algorithm.integrate()\nprint(approx_value)\nprint(data)\n</pre> import numpy as np import scipy.stats def borehole(t): # t.shape == (..., n, d) for n samples in d dimensions     \"\"\" https://www.sfu.ca/~ssurjano/borehole.html \"\"\"     rw, r, Tu, Hu, Tl, Hl, L, Kw = np.moveaxis(t, -1, 0)     numer = 2 * np.pi * Tu * (Hu-Hl) / np.log(r / rw)     denom =  1 + 2 * L * Tu / (np.log(r / rw) * rw**2 * Kw) + Tu / Tl     y = numer / denom     return y indep_distribs = [     scipy.stats.norm(loc=0.10, scale=0.0161812),     scipy.stats.lognorm(scale=np.exp(7.71), s=1.0056),     scipy.stats.uniform(loc=63070, scale=115600-63070),     scipy.stats.uniform(loc=990, scale=1110-990),     scipy.stats.uniform(loc=63.1, scale=116-63.1),     scipy.stats.uniform(loc=700, scale=820-700),     scipy.stats.uniform(loc=1120, scale=1680-1120),     scipy.stats.uniform(loc=9855, scale=12045-9855), ] discrete_distrib = qp.Lattice(dimension = 8, seed = 11) true_measure = qp.SciPyWrapper(discrete_distrib, indep_distribs) integrand = qp.CustomFun(true_measure, borehole) qmc_algorithm = qp.CubQMCBayesLatticeG(     integrand = integrand,     ptransform = \"BAKER\", # periodization transform      abs_tol = 1e-3,     rel_tol = 1e-5,     error_fun = \"BOTH\" # abs and rel tol satisfied, default is \"EITHER\", ) approx_value,data = qmc_algorithm.integrate() print(approx_value) print(data) <pre>73.73882925995098\nData (Data)\n    solution        73.739\n    comb_bound_low  73.739\n    comb_bound_high 73.739\n    comb_bound_diff 2.76e-04\n    comb_flags      1\n    n_total         2^(16)\n    n               2^(16)\n    time_integrate  0.326\nCubQMCBayesLatticeG (AbstractStoppingCriterion)\n    abs_tol         0.001\n    rel_tol         1.00e-05\n    n_init          2^(8)\n    n_limit         2^(22)\n    order           2^(1)\nCustomFun (AbstractIntegrand)\nSciPyWrapper (AbstractTrueMeasure)\nLattice (AbstractLDDiscreteDistribution)\n    d               2^(3)\n    replications    1\n    randomize       SHIFT\n    gen_vec_source  kuo.lattice-33002-1024-1048576.9125.txt\n    order           RADICAL INVERSE\n    n_limit         2^(20)\n    entropy         11\n</pre> In\u00a0[10]: Copied! <pre>ns = np.array([0,2**5,2**6,2**7])\nassert len(COLORS)&gt;=(len(ns)-1) and len(MARKERS)&gt;=(len(ns)-1)\nnmax = ns.max() \ndata = [\n    (\"IID\",qp.IIDStdUniform(2,seed=7)(nmax)),\n    (\"LD Lattice\",qp.Lattice(2,seed=7)(nmax)),\n    (\"LD Digital Net\",qp.DigitalNetB2(2,seed=7,randomize=\"NUS\")(nmax)),\n    (\"LD Halton\",qp.Halton(2,seed=7,randomize=\"LMS_DS\")(nmax)),\n]\nnrows = 1\nncols = len(data)\nfig,ax = pyplot.subplots(nrows=nrows,ncols=ncols,sharey=True,sharex=True,figsize=(FIGWIDTH,FIGWIDTH/ncols))\nfor i,(name,x) in enumerate(data):\n    for j in range(len(ns)-1):\n        nmin = ns[j]\n        nmax = ns[j+1]\n        ax[i].scatter(x[nmin:nmax,0],x[nmin:nmax,1],marker=MARKERS[j],color=COLORS[j],s=MARKERSIZE)\n    ax[i].set_xlim([0,1])\n    ax[i].set_ylim([0,1])\n    ax[i].set_xticks([0,1/4,1/2,3/4,1]); ax[i].set_xticklabels([r\"$0$\",r\"$1/4$\",r\"$1/2$\",r\"$3/4$\",r\"$1$\"])\n    ax[i].set_yticks([0,1/4,1/2,3/4,1]); ax[i].set_yticklabels([r\"$0$\",r\"$1/4$\",r\"$1/2$\",r\"$3/4$\",r\"$1$\"])\n    ax[i].grid()\n    ax[i].set_aspect(1) \n    ax[i].set_title(name)\nfig.savefig(OUTDIR+\"/points.pdf\")\nfig.savefig(OUTDIR+\"/points.svg\",transparent=True)\n</pre> ns = np.array([0,2**5,2**6,2**7]) assert len(COLORS)&gt;=(len(ns)-1) and len(MARKERS)&gt;=(len(ns)-1) nmax = ns.max()  data = [     (\"IID\",qp.IIDStdUniform(2,seed=7)(nmax)),     (\"LD Lattice\",qp.Lattice(2,seed=7)(nmax)),     (\"LD Digital Net\",qp.DigitalNetB2(2,seed=7,randomize=\"NUS\")(nmax)),     (\"LD Halton\",qp.Halton(2,seed=7,randomize=\"LMS_DS\")(nmax)), ] nrows = 1 ncols = len(data) fig,ax = pyplot.subplots(nrows=nrows,ncols=ncols,sharey=True,sharex=True,figsize=(FIGWIDTH,FIGWIDTH/ncols)) for i,(name,x) in enumerate(data):     for j in range(len(ns)-1):         nmin = ns[j]         nmax = ns[j+1]         ax[i].scatter(x[nmin:nmax,0],x[nmin:nmax,1],marker=MARKERS[j],color=COLORS[j],s=MARKERSIZE)     ax[i].set_xlim([0,1])     ax[i].set_ylim([0,1])     ax[i].set_xticks([0,1/4,1/2,3/4,1]); ax[i].set_xticklabels([r\"$0$\",r\"$1/4$\",r\"$1/2$\",r\"$3/4$\",r\"$1$\"])     ax[i].set_yticks([0,1/4,1/2,3/4,1]); ax[i].set_yticklabels([r\"$0$\",r\"$1/4$\",r\"$1/2$\",r\"$3/4$\",r\"$1$\"])     ax[i].grid()     ax[i].set_aspect(1)      ax[i].set_title(name) fig.savefig(OUTDIR+\"/points.pdf\") fig.savefig(OUTDIR+\"/points.svg\",transparent=True) In\u00a0[11]: Copied! <pre>tag = \"FULL\"\nforce = False\ndatapath = \"%s/%s.npy\"%(OUTDIR,tag)\nif (not os.path.isfile(datapath)) or force:\n    d = 12\n    n_init = 2**8\n    eps = 10**np.linspace(np.log10(5e-1),np.log10(1e-4),10)\n    trials = 100\n    with np.printoptions(formatter={\"float\":lambda x: \"%.1e\"%x}): print(\"eps = %s\"%str(eps))\n    problem = lambda discrete_distrib: \\\n        qp.FinancialOption(discrete_distrib,\n            option = \"ASIAN\",\n            call_put = \"CALL\",\n            asian_mean = \"GEOMETRIC\",\n            asian_mean_quadrature_rule = \"RIGHT\", \n            volatility = 0.5, \n            start_price = 30., \n            strike_price = 35., \n            interest_rate = 0.01,\n            t_final = 1)\n    exact_value = problem(qp.IIDStdUniform(d)).get_exact_value()\n    print(\"exact value = %.3f\\n\"%exact_value)\n    algorithms = {\n        \"IID MC\": [1e-2, lambda atol: qp.CubMCG(problem(qp.IIDStdUniform(d)),abs_tol=atol,rel_tol=0,n_init=n_init)], \n        \"QMC Digital Net'\": [0, lambda atol: qp.CubQMCNetG(problem(qp.DigitalNetB2(d)),abs_tol=atol,rel_tol=0,n_init=n_init)],\n        \"QMC Lattice\": [0, lambda atol: qp.CubQMCLatticeG(problem(qp.Lattice(d)),abs_tol=atol,rel_tol=0,n_init=n_init)],\n    }\n    keys = list(algorithms.keys())\n    n_totals = {}\n    times = {}\n    approxs = {}\n    for key,(eps_threshold,sc_constsruct) in algorithms.items():\n        print(key)\n        n_totals[key] = np.nan*np.ones((len(eps),trials))\n        times[key] = np.nan*np.ones((len(eps),trials))\n        approxs[key] = np.nan*np.ones((len(eps),trials))\n        for j in range(len(eps)):\n            if eps[j]&lt;eps_threshold: break\n            print(\"\\teps[j] = %.1e\"%eps[j])\n            for t in range(trials):\n                sc = sc_constsruct(eps[j])\n                sol,data = sc.integrate()\n                approxs[key][j,t] = sol\n                n_totals[key][j,t] = data.n_total \n                times[key][j,t] = data.time_integrate\n    _data = {\n        \"eps\": eps,\n        \"n_totals\": n_totals,\n        \"times\": times, \n        \"approxs\": approxs,\n        \"keys\": keys, \n        \"exact_value\": exact_value,\n    }\n    np.save(datapath,_data)\nelse:\n    _data = np.load(datapath,allow_pickle=True).item()\n    eps = _data[\"eps\"]\n    n_totals = _data[\"n_totals\"]\n    times = _data[\"times\"]\n    approxs = _data[\"approxs\"]\n    keys = _data[\"keys\"]\n    exact_value = _data[\"exact_value\"]\n</pre> tag = \"FULL\" force = False datapath = \"%s/%s.npy\"%(OUTDIR,tag) if (not os.path.isfile(datapath)) or force:     d = 12     n_init = 2**8     eps = 10**np.linspace(np.log10(5e-1),np.log10(1e-4),10)     trials = 100     with np.printoptions(formatter={\"float\":lambda x: \"%.1e\"%x}): print(\"eps = %s\"%str(eps))     problem = lambda discrete_distrib: \\         qp.FinancialOption(discrete_distrib,             option = \"ASIAN\",             call_put = \"CALL\",             asian_mean = \"GEOMETRIC\",             asian_mean_quadrature_rule = \"RIGHT\",              volatility = 0.5,              start_price = 30.,              strike_price = 35.,              interest_rate = 0.01,             t_final = 1)     exact_value = problem(qp.IIDStdUniform(d)).get_exact_value()     print(\"exact value = %.3f\\n\"%exact_value)     algorithms = {         \"IID MC\": [1e-2, lambda atol: qp.CubMCG(problem(qp.IIDStdUniform(d)),abs_tol=atol,rel_tol=0,n_init=n_init)],          \"QMC Digital Net'\": [0, lambda atol: qp.CubQMCNetG(problem(qp.DigitalNetB2(d)),abs_tol=atol,rel_tol=0,n_init=n_init)],         \"QMC Lattice\": [0, lambda atol: qp.CubQMCLatticeG(problem(qp.Lattice(d)),abs_tol=atol,rel_tol=0,n_init=n_init)],     }     keys = list(algorithms.keys())     n_totals = {}     times = {}     approxs = {}     for key,(eps_threshold,sc_constsruct) in algorithms.items():         print(key)         n_totals[key] = np.nan*np.ones((len(eps),trials))         times[key] = np.nan*np.ones((len(eps),trials))         approxs[key] = np.nan*np.ones((len(eps),trials))         for j in range(len(eps)):             if eps[j] <pre>eps = [5.0e-01 1.9e-01 7.5e-02 2.9e-02 1.1e-02 4.4e-03 1.7e-03 6.6e-04 2.6e-04\n 1.0e-04]\nexact value = 1.767\n\nIID MC\n\teps[j] = 5.0e-01\n\teps[j] = 1.9e-01\n\teps[j] = 7.5e-02\n\teps[j] = 2.9e-02\n\teps[j] = 1.1e-02\nQMC Digital Net'\n\teps[j] = 5.0e-01\n\teps[j] = 1.9e-01\n\teps[j] = 7.5e-02\n\teps[j] = 2.9e-02\n\teps[j] = 1.1e-02\n\teps[j] = 4.4e-03\n\teps[j] = 1.7e-03\n\teps[j] = 6.6e-04\n\teps[j] = 2.6e-04\n\teps[j] = 1.0e-04\nQMC Lattice\n\teps[j] = 5.0e-01\n\teps[j] = 1.9e-01\n\teps[j] = 7.5e-02\n\teps[j] = 2.9e-02\n\teps[j] = 1.1e-02\n\teps[j] = 4.4e-03\n\teps[j] = 1.7e-03\n\teps[j] = 6.6e-04\n\teps[j] = 2.6e-04\n\teps[j] = 1.0e-04\n</pre> In\u00a0[12]: Copied! <pre>nrows = 1\nncols = 3\nalpha = .25\nalpha2 = .75\nqlow = .1\nqhigh = .9\njstar = 4\nfig,ax = pyplot.subplots(nrows=nrows,ncols=ncols,figsize=(7/8*FIGWIDTH,7/8*FIGWIDTH/ncols),sharey=False,sharex=False)\nfor i,key in enumerate(keys):\n    n_total = n_totals[key] \n    time = times[key]\n    approx = approxs[key]\n    color = COLORS[i]\n    ax[0].plot(eps,np.quantile(n_total,.5,axis=1),label=key,color=color,marker=MARKERS[i],alpha=alpha2)\n    ax[0].fill_between(eps,np.quantile(n_total,qlow,axis=1),np.quantile(n_total,qhigh,axis=1),color=color,alpha=alpha)\n    ax[1].plot(eps,np.quantile(time[:,:],.5,axis=1),color=color,marker=MARKERS[i],alpha=alpha2)\n    ax[1].fill_between(eps,np.quantile(time,qlow,axis=1),np.quantile(time,qhigh,axis=1),color=color,alpha=alpha)\nerrors = np.stack([np.abs(exact_value-approxs[key][jstar]) for key in keys],axis=1)\nsns.violinplot(data=errors,ax=ax[2],log_scale=True,orient=\"h\",palette=COLORS[:len(keys)],inner=\"box\")\nfor i in range(2):\n    ax[i].set_xscale(\"log\",base=10) \n    ax[i].set_yscale(\"log\",base=10)\n    ax[i].set_xlabel(\"error tolerance\")\n    ax[i].xaxis.set_inverted(True)\nfor i in range(3):\n    for spine in [\"top\",\"right\"]: ax[i].spines[spine].set_visible(False)\nax[2].axvline(eps[jstar],color=\"xkcd:gray\",linestyle=(0,(1,1)),label=\"absolute error tolerance = %.2e\"%eps[jstar])\nax[0].set_ylabel(\"samples\")\nax[1].set_ylabel(\"time\")\nax[2].set_xlabel(\"true error\")\nax[2].get_yaxis().set_visible(False)\nax[2].spines[\"left\"].set_visible(False)\nfig.legend(frameon=False,loc=\"upper center\",ncols=4,bbox_to_anchor=(.5,1.1))\nfig.savefig(OUTDIR+\"/stopping_crit.pdf\")\nfig.savefig(OUTDIR+\"/stopping_crit.svg\",transparent=True)\n</pre> nrows = 1 ncols = 3 alpha = .25 alpha2 = .75 qlow = .1 qhigh = .9 jstar = 4 fig,ax = pyplot.subplots(nrows=nrows,ncols=ncols,figsize=(7/8*FIGWIDTH,7/8*FIGWIDTH/ncols),sharey=False,sharex=False) for i,key in enumerate(keys):     n_total = n_totals[key]      time = times[key]     approx = approxs[key]     color = COLORS[i]     ax[0].plot(eps,np.quantile(n_total,.5,axis=1),label=key,color=color,marker=MARKERS[i],alpha=alpha2)     ax[0].fill_between(eps,np.quantile(n_total,qlow,axis=1),np.quantile(n_total,qhigh,axis=1),color=color,alpha=alpha)     ax[1].plot(eps,np.quantile(time[:,:],.5,axis=1),color=color,marker=MARKERS[i],alpha=alpha2)     ax[1].fill_between(eps,np.quantile(time,qlow,axis=1),np.quantile(time,qhigh,axis=1),color=color,alpha=alpha) errors = np.stack([np.abs(exact_value-approxs[key][jstar]) for key in keys],axis=1) sns.violinplot(data=errors,ax=ax[2],log_scale=True,orient=\"h\",palette=COLORS[:len(keys)],inner=\"box\") for i in range(2):     ax[i].set_xscale(\"log\",base=10)      ax[i].set_yscale(\"log\",base=10)     ax[i].set_xlabel(\"error tolerance\")     ax[i].xaxis.set_inverted(True) for i in range(3):     for spine in [\"top\",\"right\"]: ax[i].spines[spine].set_visible(False) ax[2].axvline(eps[jstar],color=\"xkcd:gray\",linestyle=(0,(1,1)),label=\"absolute error tolerance = %.2e\"%eps[jstar]) ax[0].set_ylabel(\"samples\") ax[1].set_ylabel(\"time\") ax[2].set_xlabel(\"true error\") ax[2].get_yaxis().set_visible(False) ax[2].spines[\"left\"].set_visible(False) fig.legend(frameon=False,loc=\"upper center\",ncols=4,bbox_to_anchor=(.5,1.1)) fig.savefig(OUTDIR+\"/stopping_crit.pdf\") fig.savefig(OUTDIR+\"/stopping_crit.svg\",transparent=True) In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"demos/talk_paper_demos/JOSS2025/joss2025/#joss-2025","title":"JOSS 2025\u00b6","text":""},{"location":"demos/talk_paper_demos/JOSS2025/joss2025/#setup","title":"Setup\u00b6","text":""},{"location":"demos/talk_paper_demos/JOSS2025/joss2025/#listings","title":"Listings\u00b6","text":""},{"location":"demos/talk_paper_demos/JOSS2025/joss2025/#points","title":"Points\u00b6","text":""},{"location":"demos/talk_paper_demos/JOSS2025/joss2025/#convergence","title":"Convergence\u00b6","text":""},{"location":"demos/talk_paper_demos/MCQMC2022_Article_Figures/MCQMC2022_Article_Figures/","title":"2022 MCQMC Paper 1","text":"In\u00a0[1]: Copied! <pre>import matplotlib.pyplot as plt\nimport numpy as np\nimport qmcpy as qp\nimport time  #timing routines\nimport warnings  #to suppress warnings when needed\nimport pickle  #write output to a file and load it back in\nfrom copy import deepcopy\n\nplt.rc('font', size=16)  #set defaults so that the plots are readable\nplt.rc('axes', titlesize=16)\nplt.rc('axes', labelsize=16)\nplt.rc('xtick', labelsize=16)\nplt.rc('ytick', labelsize=16)\nplt.rc('legend', fontsize=16)\nplt.rc('figure', titlesize=16)\n\n#a helpful plotting method to show increasing numbers of points\ndef plot_successive_points(distrib,ld_name,first_n=64,n_cols=1,\n                           pt_clr=['tab:blue', 'tab:green', 'k', 'tab:cyan', 'tab:purple', 'tab:orange'],\n                           xlim=[0,1],ylim=[0,1]):\n  fig,ax = plt.subplots(nrows=1,ncols=n_cols,figsize=(5*n_cols,5.5))\n  if n_cols==1: ax = [ax]\n  last_n = first_n*(2**n_cols)\n  points = distrib.gen_samples(n=last_n)\n  for i in range(n_cols):\n    n = first_n\n    nstart = 0\n    for j in range(i+1):\n      n = first_n*(2**j)\n      ax[i].scatter(points[nstart:n,0],points[nstart:n,1],color=pt_clr[j])\n      nstart = n\n    ax[i].set_title('n = %d'%n)\n    ax[i].set_xlim(xlim); ax[i].set_xticks(xlim); ax[i].set_xlabel('$x_{i,1}$')\n    ax[i].set_ylim(ylim); ax[i].set_yticks(ylim); ax[i].set_ylabel('$x_{i,2}$')\n    ax[i].set_aspect((xlim[1]-xlim[0])/(ylim[1]-ylim[0]))\n  fig.suptitle('%s Points'%ld_name, y=0.9)\n  return fig\n\nprint('QMCPy Version',qp.__version__)\n</pre> import matplotlib.pyplot as plt import numpy as np import qmcpy as qp import time  #timing routines import warnings  #to suppress warnings when needed import pickle  #write output to a file and load it back in from copy import deepcopy  plt.rc('font', size=16)  #set defaults so that the plots are readable plt.rc('axes', titlesize=16) plt.rc('axes', labelsize=16) plt.rc('xtick', labelsize=16) plt.rc('ytick', labelsize=16) plt.rc('legend', fontsize=16) plt.rc('figure', titlesize=16)  #a helpful plotting method to show increasing numbers of points def plot_successive_points(distrib,ld_name,first_n=64,n_cols=1,                            pt_clr=['tab:blue', 'tab:green', 'k', 'tab:cyan', 'tab:purple', 'tab:orange'],                            xlim=[0,1],ylim=[0,1]):   fig,ax = plt.subplots(nrows=1,ncols=n_cols,figsize=(5*n_cols,5.5))   if n_cols==1: ax = [ax]   last_n = first_n*(2**n_cols)   points = distrib.gen_samples(n=last_n)   for i in range(n_cols):     n = first_n     nstart = 0     for j in range(i+1):       n = first_n*(2**j)       ax[i].scatter(points[nstart:n,0],points[nstart:n,1],color=pt_clr[j])       nstart = n     ax[i].set_title('n = %d'%n)     ax[i].set_xlim(xlim); ax[i].set_xticks(xlim); ax[i].set_xlabel('$x_{i,1}$')     ax[i].set_ylim(ylim); ax[i].set_yticks(ylim); ax[i].set_ylabel('$x_{i,2}$')     ax[i].set_aspect((xlim[1]-xlim[0])/(ylim[1]-ylim[0]))   fig.suptitle('%s Points'%ld_name, y=0.9)   return fig  print('QMCPy Version',qp.__version__) <pre>QMCPy Version 1.6.3c\n</pre> In\u00a0[2]: Copied! <pre>figpath = '' #this path sends the figures to the desired directory\n</pre> figpath = '' #this path sends the figures to the desired directory In\u00a0[3]: Copied! <pre>d = 5 #dimension\nn = 32 #number of points\ncols = 3 #number of columns\nld = qp.Lattice(d) #define the generator\nfig = plot_successive_points(ld,'Lattice',first_n=n,n_cols=cols)\nfig.savefig(figpath+'latticepts.eps',format='eps',bbox_inches='tight')\n</pre> d = 5 #dimension n = 32 #number of points cols = 3 #number of columns ld = qp.Lattice(d) #define the generator fig = plot_successive_points(ld,'Lattice',first_n=n,n_cols=cols) fig.savefig(figpath+'latticepts.eps',format='eps',bbox_inches='tight') In\u00a0[4]: Copied! <pre>with open(figpath+'ld_parallel.pkl','rb') as myfile: tol_vec,n_tol,ld_time,ld_n,ld_p_time,ld_p_n,best_solution = pickle.load(myfile)\nprint(best_solution)\nfig,ax = plt.subplots(nrows=1,ncols=2,figsize=(11,4))\nax[0].scatter(tol_vec[0:n_tol],ld_time[0:n_tol],color='tab:blue'); \nax[0].plot(tol_vec[0:n_tol],[(ld_time[0]*tol_vec[0])/tol_vec[jj] for jj in range(n_tol)],color='tab:blue')\nax[0].scatter(tol_vec[0:n_tol],ld_p_time[0:n_tol],color='tab:orange',marker = '+',s=100,linewidths=2); \n#ax[0].plot(tol_vec[0:n_tol],[(ld_p_time[0]*tol_vec[0])/tol_vec[jj] for jj in range(n_tol)],color='tab:orange')\nax[0].set_ylim([0.05,500]); ax[0].set_ylabel('Time')\nax[1].scatter(tol_vec[0:n_tol],ld_n[0:n_tol],color='tab:blue'); \nax[1].plot(tol_vec[0:n_tol],[(ld_n[0]*tol_vec[0])/tol_vec[jj] for jj in range(n_tol)],color='tab:blue')\nax[1].scatter(tol_vec[0:n_tol],ld_p_n[0:n_tol],color='tab:orange',marker = '+',s=100,linewidths=2); \n#ax[1].plot(tol_vec[0:n_tol],[(ld_p_n[0]*tol_vec[0])/tol_vec[jj] for jj in range(n_tol)],color='tab:orange')\nax[1].set_ylim([10,1e5]); ax[1].set_ylabel('n')\nfor ii in range(2):\n  ax[ii].set_xlim([0.007,4]); ax[ii].set_xlabel('Tolerance, '+r'$\\varepsilon$')\n  ax[ii].set_xscale('log'); ax[ii].set_yscale('log')\n  ax[ii].legend(['Lattice',r'$\\mathcal{O}(\\varepsilon^{-1})$','Lattice Parallel',r'$\\mathcal{O}(\\varepsilon^{-1})$'],frameon=False)\n  ax[ii].set_aspect(0.6)\nax[0].set_yticks([1, 60], labels = ['1 sec', '1 min'])\nfig.savefig(figpath+'ldparallelbeam.eps',format='eps',bbox_inches='tight')\n</pre> with open(figpath+'ld_parallel.pkl','rb') as myfile: tol_vec,n_tol,ld_time,ld_n,ld_p_time,ld_p_n,best_solution = pickle.load(myfile) print(best_solution) fig,ax = plt.subplots(nrows=1,ncols=2,figsize=(11,4)) ax[0].scatter(tol_vec[0:n_tol],ld_time[0:n_tol],color='tab:blue');  ax[0].plot(tol_vec[0:n_tol],[(ld_time[0]*tol_vec[0])/tol_vec[jj] for jj in range(n_tol)],color='tab:blue') ax[0].scatter(tol_vec[0:n_tol],ld_p_time[0:n_tol],color='tab:orange',marker = '+',s=100,linewidths=2);  #ax[0].plot(tol_vec[0:n_tol],[(ld_p_time[0]*tol_vec[0])/tol_vec[jj] for jj in range(n_tol)],color='tab:orange') ax[0].set_ylim([0.05,500]); ax[0].set_ylabel('Time') ax[1].scatter(tol_vec[0:n_tol],ld_n[0:n_tol],color='tab:blue');  ax[1].plot(tol_vec[0:n_tol],[(ld_n[0]*tol_vec[0])/tol_vec[jj] for jj in range(n_tol)],color='tab:blue') ax[1].scatter(tol_vec[0:n_tol],ld_p_n[0:n_tol],color='tab:orange',marker = '+',s=100,linewidths=2);  #ax[1].plot(tol_vec[0:n_tol],[(ld_p_n[0]*tol_vec[0])/tol_vec[jj] for jj in range(n_tol)],color='tab:orange') ax[1].set_ylim([10,1e5]); ax[1].set_ylabel('n') for ii in range(2):   ax[ii].set_xlim([0.007,4]); ax[ii].set_xlabel('Tolerance, '+r'$\\varepsilon$')   ax[ii].set_xscale('log'); ax[ii].set_yscale('log')   ax[ii].legend(['Lattice',r'$\\mathcal{O}(\\varepsilon^{-1})$','Lattice Parallel',r'$\\mathcal{O}(\\varepsilon^{-1})$'],frameon=False)   ax[ii].set_aspect(0.6) ax[0].set_yticks([1, 60], labels = ['1 sec', '1 min']) fig.savefig(figpath+'ldparallelbeam.eps',format='eps',bbox_inches='tight') <pre>[   0.            4.09650336   15.63089718   33.92196588   58.36259037\n   88.42166624  123.63753779  163.62564457  208.07398743  256.74371736\n  309.46852588  362.17337763  410.80019812  455.47543786  496.40155276\n  533.85794874  568.20201672  599.87051305  629.38039738  657.32920342\n  684.39496877  712.15829288  742.93687638  776.09146089  811.06031655\n  847.35856113  884.57752606  922.38415394  960.52038491  998.8025606\n 1037.12106673]\n</pre> In\u00a0[5]: Copied! <pre>fig,ax = plt.subplots(figsize=(6,3))\nax.plot(best_solution,'-')\nax.set_xlim([0,len(best_solution)-1]); ax.set_xlabel('Position')\nax.set_ylim([1050,-50]);  ax.set_ylabel('Deflection');\nax.set_aspect(0.02)\nfig.suptitle('Cantilevered Beam')\nfig.savefig(figpath+'cantileveredbeamwords.eps',format='eps',bbox_inches='tight')\n</pre> fig,ax = plt.subplots(figsize=(6,3)) ax.plot(best_solution,'-') ax.set_xlim([0,len(best_solution)-1]); ax.set_xlabel('Position') ax.set_ylim([1050,-50]);  ax.set_ylabel('Deflection'); ax.set_aspect(0.02) fig.suptitle('Cantilevered Beam') fig.savefig(figpath+'cantileveredbeamwords.eps',format='eps',bbox_inches='tight') In\u00a0[6]: Copied! <pre>qp.util.stop_notebook()\n</pre> qp.util.stop_notebook() <p>Below is long-running code, that we rarely wish to run</p> In\u00a0[7]: Copied! <pre>import umbridge #this is the connector\n!docker run --name muqbp -d -it -p 4243:4243 linusseelinger/benchmark-muq-beam-propagation:latest #get beam example\nd = 3 #dimension of the randomness\nlb = 1 #lower bound on randomness\nub = 1.2 #upper bound on randomness\numbridge_config = {\"d\": d}\nmodel = umbridge.HTTPModel('http://localhost:4243','forward') #this is the original model\noutindex = -1 #choose last element of the vector of beam deflections\nmodeli = deepcopy(model) #and construct a model for just that deflection\nmodeli.get_output_sizes = lambda *args : [1]\nmodeli.get_output_sizes()\nmodeli.__call__ = lambda *args,**kwargs: [[model.__call__(*args,**kwargs)[0][outindex]]]\n</pre> import umbridge #this is the connector !docker run --name muqbp -d -it -p 4243:4243 linusseelinger/benchmark-muq-beam-propagation:latest #get beam example d = 3 #dimension of the randomness lb = 1 #lower bound on randomness ub = 1.2 #upper bound on randomness umbridge_config = {\"d\": d} model = umbridge.HTTPModel('http://localhost:4243','forward') #this is the original model outindex = -1 #choose last element of the vector of beam deflections modeli = deepcopy(model) #and construct a model for just that deflection modeli.get_output_sizes = lambda *args : [1] modeli.get_output_sizes() modeli.__call__ = lambda *args,**kwargs: [[model.__call__(*args,**kwargs)[0][outindex]]] <pre>docker: Error response from daemon: Conflict. The container name \"/muqbp\" is already in use by container \"d30a451e4f61fd8d33dd7bb63d9338183a4015fe9dbbb755727cf7f3596fecee\". You have to remove (or rename) that container to be able to reuse that name.\nSee 'docker run --help'.\n</pre> In\u00a0[\u00a0]: Copied! <pre>ld = qp.Uniform(qp.Lattice(d,seed=7),lower_bound=lb,upper_bound=ub) #lattice points for this problem\nld_integ = qp.UMBridgeWrapper(ld,modeli,umbridge_config,parallel=False) #integrand\niid = qp.Uniform(qp.IIDStdUniform(d),lower_bound=lb,upper_bound=ub) #iid points for this problem\niid_integ = qp.UMBridgeWrapper(iid,modeli,umbridge_config,parallel=False) #integrand\ntol = 0.01  #smallest tolerance\n\nn_tol = 14  #number of different tolerances\nii_iid = 9  #make this larger to reduce the time required by not running all cases for IID\ntol_vec = [tol*(2**ii) for ii in range(n_tol)]  #initialize vector of tolerances\nld_time = [0]*n_tol; ld_n = [0]*n_tol  #low discrepancy time and number of function values\niid_time = [0]*n_tol; iid_n = [0]*n_tol  #IID time and number of function values\nprint(f'\\nCantilever Beam\\n')\nprint('iteration ', end = '')\nfor ii in range(n_tol):\n  solution, data = qp.CubQMCLatticeG(ld_integ, abs_tol = tol_vec[ii]).integrate()\n  if ii == 0:\n    best_solution_i = solution\n  ld_time[ii] = data.time_integrate\n  ld_n[ii] = data.n_total\n  if ii &gt;= ii_iid:\n    solution, data = qp.CubMCG(iid_integ, abs_tol = tol_vec[ii]).integrate()\n    iid_time[ii] = data.time_integrate\n    iid_n[ii] = data.n_total\n  print(ii, end = ' ')\nwith open(figpath+'iid_ld.pkl','wb') as myfile:pickle.dump([tol_vec,n_tol,ii_iid,ld_time,ld_n,iid_time,iid_n,best_solution_i],myfile)\n</pre> ld = qp.Uniform(qp.Lattice(d,seed=7),lower_bound=lb,upper_bound=ub) #lattice points for this problem ld_integ = qp.UMBridgeWrapper(ld,modeli,umbridge_config,parallel=False) #integrand iid = qp.Uniform(qp.IIDStdUniform(d),lower_bound=lb,upper_bound=ub) #iid points for this problem iid_integ = qp.UMBridgeWrapper(iid,modeli,umbridge_config,parallel=False) #integrand tol = 0.01  #smallest tolerance  n_tol = 14  #number of different tolerances ii_iid = 9  #make this larger to reduce the time required by not running all cases for IID tol_vec = [tol*(2**ii) for ii in range(n_tol)]  #initialize vector of tolerances ld_time = [0]*n_tol; ld_n = [0]*n_tol  #low discrepancy time and number of function values iid_time = [0]*n_tol; iid_n = [0]*n_tol  #IID time and number of function values print(f'\\nCantilever Beam\\n') print('iteration ', end = '') for ii in range(n_tol):   solution, data = qp.CubQMCLatticeG(ld_integ, abs_tol = tol_vec[ii]).integrate()   if ii == 0:     best_solution_i = solution   ld_time[ii] = data.time_integrate   ld_n[ii] = data.n_total   if ii &gt;= ii_iid:     solution, data = qp.CubMCG(iid_integ, abs_tol = tol_vec[ii]).integrate()     iid_time[ii] = data.time_integrate     iid_n[ii] = data.n_total   print(ii, end = ' ') with open(figpath+'iid_ld.pkl','wb') as myfile:pickle.dump([tol_vec,n_tol,ii_iid,ld_time,ld_n,iid_time,iid_n,best_solution_i],myfile) In\u00a0[\u00a0]: Copied! <pre>ld_integ = qp.UMBridgeWrapper(ld,model,umbridge_config,parallel=False) #integrand\nld_integ_p = qp.UMBridgeWrapper(ld,model,umbridge_config,parallel=8) #integrand with parallel processing\n\ntol = 0.01\nn_tol = 9  #number of different tolerances\ntol_vec = [tol*(2**ii) for ii in range(n_tol)]  #initialize vector of tolerances\nld_time = [0]*n_tol; ld_n = [0]*n_tol  #low discrepancy time and number of function values\nld_p_time = [0]*n_tol; ld_p_n = [0]*n_tol  #low discrepancy time and number of function values with parallel\nprint(f'\\nCantilever Beam\\n')\nprint('iteration ', end = '')\nfor ii in range(n_tol):\n  solution, data = qp.CubQMCLatticeG(ld_integ, abs_tol = tol_vec[ii]).integrate()\n  if ii == 0:\n    best_solution = solution\n  ld_time[ii] = data.time_integrate\n  ld_n[ii] = data.n_total\n  solution, data = qp.CubQMCLatticeG(ld_integ_p, abs_tol = tol_vec[ii]).integrate()\n  ld_p_time[ii] = data.time_integrate\n  ld_p_n[ii] = data.n_total\n  print(ii, end = ' ') \nwith open(figpath+'ld_parallel.pkl','wb') as myfile:pickle.dump([tol_vec,n_tol,ld_time,ld_n,ld_p_time,ld_p_n,best_solution],myfile)\n</pre> ld_integ = qp.UMBridgeWrapper(ld,model,umbridge_config,parallel=False) #integrand ld_integ_p = qp.UMBridgeWrapper(ld,model,umbridge_config,parallel=8) #integrand with parallel processing  tol = 0.01 n_tol = 9  #number of different tolerances tol_vec = [tol*(2**ii) for ii in range(n_tol)]  #initialize vector of tolerances ld_time = [0]*n_tol; ld_n = [0]*n_tol  #low discrepancy time and number of function values ld_p_time = [0]*n_tol; ld_p_n = [0]*n_tol  #low discrepancy time and number of function values with parallel print(f'\\nCantilever Beam\\n') print('iteration ', end = '') for ii in range(n_tol):   solution, data = qp.CubQMCLatticeG(ld_integ, abs_tol = tol_vec[ii]).integrate()   if ii == 0:     best_solution = solution   ld_time[ii] = data.time_integrate   ld_n[ii] = data.n_total   solution, data = qp.CubQMCLatticeG(ld_integ_p, abs_tol = tol_vec[ii]).integrate()   ld_p_time[ii] = data.time_integrate   ld_p_n[ii] = data.n_total   print(ii, end = ' ')  with open(figpath+'ld_parallel.pkl','wb') as myfile:pickle.dump([tol_vec,n_tol,ld_time,ld_n,ld_p_time,ld_p_n,best_solution],myfile) In\u00a0[\u00a0]: Copied! <pre>!docker rm -f muqbp #shut down docker image\n</pre> !docker rm -f muqbp #shut down docker image"},{"location":"demos/talk_paper_demos/MCQMC2022_Article_Figures/MCQMC2022_Article_Figures/#challenges-in-developing-great-qmc-software","title":"Challenges in Developing Great QMC Software\u00b6","text":"<p>Computations and Figures for the MCQMC 2022 Article: Challenges in Developing Great Quasi-Monte Carlo Software</p>"},{"location":"demos/talk_paper_demos/MCQMC2022_Article_Figures/MCQMC2022_Article_Figures/#import-the-necessary-packages-and-set-up-plotting-routines","title":"Import the necessary packages and set up plotting routines\u00b6","text":""},{"location":"demos/talk_paper_demos/MCQMC2022_Article_Figures/MCQMC2022_Article_Figures/#make-sure-that-you-have-the-relevant-path-to-store-the-figures","title":"Make sure that you have the relevant path to store the figures\u00b6","text":""},{"location":"demos/talk_paper_demos/MCQMC2022_Article_Figures/MCQMC2022_Article_Figures/#here-are-some-plots-of-low-discrepancy-ld-lattice-points","title":"Here are some plots of Low Discrepancy (LD) Lattice Points\u00b6","text":""},{"location":"demos/talk_paper_demos/MCQMC2022_Article_Figures/MCQMC2022_Article_Figures/#beam-example-plots","title":"Beam Example Plots\u00b6","text":""},{"location":"demos/talk_paper_demos/MCQMC2022_Article_Figures/MCQMC2022_Article_Figures/#plot-the-time-and-sample-size-required-to-solve-for-the-deflection-of-the-whole-beam-using-low-discrepancy-with-and-without-parallel","title":"Plot the time and sample size required to solve for the deflection of the whole beam using low discrepancy with and without parallel\u00b6","text":""},{"location":"demos/talk_paper_demos/MCQMC2022_Article_Figures/MCQMC2022_Article_Figures/#plot-of-beam-solution","title":"Plot of beam solution\u00b6","text":""},{"location":"demos/talk_paper_demos/MCQMC2022_Article_Figures/MCQMC2022_Article_Figures/#beam-example-computations","title":"Beam Example Computations\u00b6","text":"<p>To run this, you need to be running the <code>docker</code> application, https://www.docker.com/products/docker-desktop/</p>"},{"location":"demos/talk_paper_demos/MCQMC2022_Article_Figures/MCQMC2022_Article_Figures/#set-up-the-problem-using-a-docker-container-to-solve-the-ode","title":"Set up the problem using a docker container to solve the ODE\u00b6","text":""},{"location":"demos/talk_paper_demos/MCQMC2022_Article_Figures/MCQMC2022_Article_Figures/#first-we-compute-the-time-required-to-solve-for-the-deflection-of-the-end-point-using-iid-and-low-discrepancy","title":"First we compute the time required to solve for the deflection of the end point using IID and low discrepancy\u00b6","text":""},{"location":"demos/talk_paper_demos/MCQMC2022_Article_Figures/MCQMC2022_Article_Figures/#next-we-compute-the-time-required-to-solve-for-the-deflection-of-the-whole-beam-using-low-discrepancy-with-and-without-parallel","title":"Next, we compute the time required to solve for the deflection of the whole beam using low discrepancy with and without parallel\u00b6","text":""},{"location":"demos/talk_paper_demos/MCQMC_Tutorial_2020/MCQMC_2020_QMC_Software_Tutorial/","title":"2020 MCQMC Software Tutorial","text":"In\u00a0[1]: Copied! <pre>import qmcpy  #we import the environment at the start to use it\nimport numpy as np  #basic numerical routines in Python\nimport time  #timing routines\nimport warnings  #to suppress warnings when needed\nimport torch  #only needed for PyTorch Sobol' backend\nfrom torch.quasirandom import SobolEngine\nfrom matplotlib import pyplot;  #plotting\n\npyplot.rc('font', size=16)  #set defaults so that the plots are readable\npyplot.rc('axes', titlesize=16)\npyplot.rc('axes', labelsize=16)\npyplot.rc('xtick', labelsize=16)\npyplot.rc('ytick', labelsize=16)\npyplot.rc('legend', fontsize=16)\npyplot.rc('figure', titlesize=16)\n\n#a helpful plotting method to show increasing numbers of points\ndef plot_successive_points(distrib,ld_name,first_n=64,n_cols=1,pt_clr='bgkcmy',\n                           xlim=[0,1],ylim=[0,1],coord1 = 0,coord2 = 1):\n  fig,ax = pyplot.subplots(nrows=1,ncols=n_cols,figsize=(5*n_cols,5.5))\n  if n_cols==1: ax = [ax]\n  last_n = first_n*(2**n_cols)\n  points = distrib.gen_samples(n=last_n)\n  for i in range(n_cols):\n    n = first_n\n    nstart = 0\n    for j in range(i+1):\n      n = first_n*(2**j)\n      ax[i].scatter(points[nstart:n,coord1],points[nstart:n,coord2],color=pt_clr[j])\n      nstart = n\n    ax[i].set_title('n = %d'%n)\n    ax[i].set_xlim(xlim); ax[i].set_xticks(xlim); ax[i].set_xlabel('$x_{i,%d}$'%(coord1+1))\n    ax[i].set_ylim(ylim); ax[i].set_yticks(ylim); ax[i].set_ylabel('$x_{i,%d}$'%(coord2+1))\n    ax[i].set_aspect((xlim[1]-xlim[0])/(ylim[1]-ylim[0]))\n  fig.suptitle('%s Points'%ld_name)\nprint('QMCPy Version',qmcpy.__version__)\n</pre> import qmcpy  #we import the environment at the start to use it import numpy as np  #basic numerical routines in Python import time  #timing routines import warnings  #to suppress warnings when needed import torch  #only needed for PyTorch Sobol' backend from torch.quasirandom import SobolEngine from matplotlib import pyplot;  #plotting  pyplot.rc('font', size=16)  #set defaults so that the plots are readable pyplot.rc('axes', titlesize=16) pyplot.rc('axes', labelsize=16) pyplot.rc('xtick', labelsize=16) pyplot.rc('ytick', labelsize=16) pyplot.rc('legend', fontsize=16) pyplot.rc('figure', titlesize=16)  #a helpful plotting method to show increasing numbers of points def plot_successive_points(distrib,ld_name,first_n=64,n_cols=1,pt_clr='bgkcmy',                            xlim=[0,1],ylim=[0,1],coord1 = 0,coord2 = 1):   fig,ax = pyplot.subplots(nrows=1,ncols=n_cols,figsize=(5*n_cols,5.5))   if n_cols==1: ax = [ax]   last_n = first_n*(2**n_cols)   points = distrib.gen_samples(n=last_n)   for i in range(n_cols):     n = first_n     nstart = 0     for j in range(i+1):       n = first_n*(2**j)       ax[i].scatter(points[nstart:n,coord1],points[nstart:n,coord2],color=pt_clr[j])       nstart = n     ax[i].set_title('n = %d'%n)     ax[i].set_xlim(xlim); ax[i].set_xticks(xlim); ax[i].set_xlabel('$x_{i,%d}$'%(coord1+1))     ax[i].set_ylim(ylim); ax[i].set_yticks(ylim); ax[i].set_ylabel('$x_{i,%d}$'%(coord2+1))     ax[i].set_aspect((xlim[1]-xlim[0])/(ylim[1]-ylim[0]))   fig.suptitle('%s Points'%ld_name) print('QMCPy Version',qmcpy.__version__) <pre>QMCPy Version 1.6.3.2a\n</pre> In\u00a0[2]: Copied! <pre>lattice = qmcpy.Lattice(dimension=2)  #define a discrete LD distribution based on an integration lattice\nprint(lattice)  #print the properties of the lattice object\nn = 16  #number of points to generate\npoints = lattice.gen_samples(n)  #construct some points\nprint(f'\\nLD Lattice Points with shape {points.shape}\\n'+str(points))  #these points have 15 significant digit precision but only three digits are shown\nplot_successive_points(lattice,'Lattice',n)\n</pre> lattice = qmcpy.Lattice(dimension=2)  #define a discrete LD distribution based on an integration lattice print(lattice)  #print the properties of the lattice object n = 16  #number of points to generate points = lattice.gen_samples(n)  #construct some points print(f'\\nLD Lattice Points with shape {points.shape}\\n'+str(points))  #these points have 15 significant digit precision but only three digits are shown plot_successive_points(lattice,'Lattice',n) <pre>Lattice (AbstractLDDiscreteDistribution)\n    d               2^(1)\n    replications    1\n    randomize       SHIFT\n    gen_vec_source  kuo.lattice-33002-1024-1048576.9125.txt\n    order           RADICAL INVERSE\n    n_limit         2^(20)\n    entropy         264030601762852985433978628854999910438\n\nLD Lattice Points with shape (16, 2)\n[[0.14448802 0.23129441]\n [0.64448802 0.73129441]\n [0.39448802 0.98129441]\n [0.89448802 0.48129441]\n [0.26948802 0.60629441]\n [0.76948802 0.10629441]\n [0.51948802 0.35629441]\n [0.01948802 0.85629441]\n [0.20698802 0.91879441]\n [0.70698802 0.41879441]\n [0.45698802 0.66879441]\n [0.95698802 0.16879441]\n [0.33198802 0.29379441]\n [0.83198802 0.79379441]\n [0.58198802 0.04379441]\n [0.08198802 0.54379441]]\n</pre> <p>Rerunning the commands above yields a different sequence of points because these points are randomly shifted modulo 1.</p> <p>We may also construct a subsequence of points in the middle of the sequence. Note that the points below match those above.</p> In\u00a0[3]: Copied! <pre>more_points = lattice.gen_samples(n_min=4,n_max=n)  #get more points\nprint('LD Lattice Points with shape',more_points.shape,'\\n'+str(more_points))\n</pre> more_points = lattice.gen_samples(n_min=4,n_max=n)  #get more points print('LD Lattice Points with shape',more_points.shape,'\\n'+str(more_points)) <pre>LD Lattice Points with shape (12, 2) \n[[0.26948802 0.60629441]\n [0.76948802 0.10629441]\n [0.51948802 0.35629441]\n [0.01948802 0.85629441]\n [0.20698802 0.91879441]\n [0.70698802 0.41879441]\n [0.45698802 0.66879441]\n [0.95698802 0.16879441]\n [0.33198802 0.29379441]\n [0.83198802 0.79379441]\n [0.58198802 0.04379441]\n [0.08198802 0.54379441]]\n</pre> <p>Each $d$-dimensional point is one row in the array.</p> <p>As we increase the number of points, they fill $[0,1]^d$ evenly.  The next points are placed in between the existing points.  Here we illustrate with $d=2$.</p> In\u00a0[4]: Copied! <pre>plot_successive_points(lattice,'Lattice',n_cols=5)\n</pre> plot_successive_points(lattice,'Lattice',n_cols=5) In\u00a0[5]: Copied! <pre>iid = qmcpy.IIDStdUniform(2)  #standard uniform IID random vector generator from NumPy\nprint(iid)  #print the properties of iid\nplot_successive_points(iid,'IID Uniform',n_cols=5)\n</pre> iid = qmcpy.IIDStdUniform(2)  #standard uniform IID random vector generator from NumPy print(iid)  #print the properties of iid plot_successive_points(iid,'IID Uniform',n_cols=5) <pre>IIDStdUniform (AbstractIIDDiscreteDistribution)\n    d               2^(1)\n    replications    1\n    entropy         151929946340103232908357805579611369175\n</pre> In\u00a0[6]: Copied! <pre>d = 16  #dimension\nn = 2  #number of points\nlattice = qmcpy.Lattice(d)  #define a discrete LD distribution based on an integration lattice\nlattice_pts = lattice.gen_samples(n)  #the first parameter in the .gen_samples method is the number of points\niid = qmcpy.IIDStdUniform(d)\niid_pts = iid.gen_samples(n)\nfig,ax = pyplot.subplots(nrows=1,ncols=2,figsize=(10,5.5))\nax[0].scatter(lattice_pts[0,0:d],lattice_pts[1,0:d],color='b')\nax[0].set_title('Transposed Lattice Points')\nax[1].scatter(iid_pts[0,0:d],iid_pts[1,0:d],color='b')\nax[1].set_title('Transposed IID Points')\nfor ii in range(2):\n  ax[ii].set_xlim([0,1]); ax[ii].set_xticks([0,1]); ax[ii].set_xlabel('$x_{1,j}$')\n  ax[ii].set_ylim([0,1]); ax[ii].set_yticks([0,1]); ax[ii].set_ylabel('$x_{2,j}$')\n  ax[ii].set_aspect(1)\n</pre> d = 16  #dimension n = 2  #number of points lattice = qmcpy.Lattice(d)  #define a discrete LD distribution based on an integration lattice lattice_pts = lattice.gen_samples(n)  #the first parameter in the .gen_samples method is the number of points iid = qmcpy.IIDStdUniform(d) iid_pts = iid.gen_samples(n) fig,ax = pyplot.subplots(nrows=1,ncols=2,figsize=(10,5.5)) ax[0].scatter(lattice_pts[0,0:d],lattice_pts[1,0:d],color='b') ax[0].set_title('Transposed Lattice Points') ax[1].scatter(iid_pts[0,0:d],iid_pts[1,0:d],color='b') ax[1].set_title('Transposed IID Points') for ii in range(2):   ax[ii].set_xlim([0,1]); ax[ii].set_xticks([0,1]); ax[ii].set_xlabel('$x_{1,j}$')   ax[ii].set_ylim([0,1]); ax[ii].set_yticks([0,1]); ax[ii].set_ylabel('$x_{2,j}$')   ax[ii].set_aspect(1) In\u00a0[7]: Copied! <pre>sobol = qmcpy.Sobol(2)  #Sobol LD generator\nprint(sobol) #note below that the default is qrng from Hofert and Lemieux\npoints = sobol.gen_samples(16)\nprint(f'\\nLD Sobol\\' Points with shape {points.shape}\\n'+str(points))\nplot_successive_points(sobol,'Sobol\\'',n_cols=5)\n</pre> sobol = qmcpy.Sobol(2)  #Sobol LD generator print(sobol) #note below that the default is qrng from Hofert and Lemieux points = sobol.gen_samples(16) print(f'\\nLD Sobol\\' Points with shape {points.shape}\\n'+str(points)) plot_successive_points(sobol,'Sobol\\'',n_cols=5) <pre>DigitalNetB2 (AbstractLDDiscreteDistribution)\n    d               2^(1)\n    replications    1\n    randomize       LMS DS\n    gen_mats_source joe_kuo.6.21201.txt\n    order           RADICAL INVERSE\n    t               63\n    alpha           1\n    n_limit         2^(32)\n    entropy         73208827127383995610457310988530317873\n\nLD Sobol' Points with shape (16, 2)\n[[0.9431598  0.47365992]\n [0.22125498 0.72515188]\n [0.66129464 0.90348679]\n [0.37738574 0.15491697]\n [0.79964553 0.60309908]\n [0.02104669 0.35362196]\n [0.58028993 0.0173281 ]\n [0.36467173 0.76778926]\n [0.88650748 0.81477058]\n [0.16853181 0.06430855]\n [0.72964997 0.30712722]\n [0.44965495 0.55660491]\n [0.87190807 0.20193054]\n [0.0894106  0.95049973]\n [0.52754482 0.67862835]\n [0.30804326 0.4271372 ]]\n</pre> In\u00a0[8]: Copied! <pre>halton = qmcpy.Halton(2)\nprint(halton)\npoints = halton.gen_samples(20)\nprint(f'\\nLD Halton Points with shape {points.shape}\\n'+str(points))\nplot_successive_points(halton,'Halton',n_cols=5,first_n=60)\n</pre> halton = qmcpy.Halton(2) print(halton) points = halton.gen_samples(20) print(f'\\nLD Halton Points with shape {points.shape}\\n'+str(points)) plot_successive_points(halton,'Halton',n_cols=5,first_n=60) <pre>Halton (AbstractLDDiscreteDistribution)\n    d               2^(1)\n    replications    1\n    randomize       LMS DP\n    t               63\n    n_limit         2^(32)\n    entropy         240669245053839331683025566616811162761\n\nLD Halton Points with shape (20, 2)\n[[0.44785589 0.67105156]\n [0.81034548 0.6477606 ]\n [0.01802002 0.17997437]\n [0.72376384 0.96444929]\n [0.31082674 0.49671198]\n [0.94638376 0.02481055]\n [0.22522047 0.82620954]\n [0.51752374 0.3543589 ]\n [0.40526643 0.33101523]\n [0.85320912 0.7104405 ]\n [0.06844815 0.57598763]\n [0.67312161 0.21524872]\n [0.35369039 0.89273277]\n [0.90379429 0.53194055]\n [0.17457824 0.06420577]\n [0.56795187 0.86102595]\n [0.49980629 0.39323972]\n [0.75841033 0.25883637]\n [0.03822453 0.74815281]\n [0.70357459 0.60958533]]\n</pre> In\u00a0[9]: Copied! <pre>ld = qmcpy.Lattice(64)  #define a discrete LD distribution\nprint(ld)  #print the properties of the lattice object\nstart_time = time.time()  #time now\npoints = ld.gen_samples(2**20)  #construct some points\nend_time = time.time()  #time after points are constructed\nprint(f'\\nLD Points with shape {points.shape}\\n'+str(points))\nprint(f'\\nTime to construct points is %.1e seconds'%(end_time - start_time))\n</pre> ld = qmcpy.Lattice(64)  #define a discrete LD distribution print(ld)  #print the properties of the lattice object start_time = time.time()  #time now points = ld.gen_samples(2**20)  #construct some points end_time = time.time()  #time after points are constructed print(f'\\nLD Points with shape {points.shape}\\n'+str(points)) print(f'\\nTime to construct points is %.1e seconds'%(end_time - start_time)) <pre>Lattice (AbstractLDDiscreteDistribution)\n    d               2^(6)\n    replications    1\n    randomize       SHIFT\n    gen_vec_source  kuo.lattice-33002-1024-1048576.9125.txt\n    order           RADICAL INVERSE\n    n_limit         2^(20)\n    entropy         323672018533134169508871551556781883883\n\nLD Points with shape (1048576, 64)\n[[4.84051442e-01 1.74980418e-01 6.35194475e-01 ... 7.14805235e-01\n  1.59923415e-01 7.06555592e-02]\n [9.84051442e-01 6.74980418e-01 1.35194475e-01 ... 2.14805235e-01\n  6.59923415e-01 5.70655559e-01]\n [7.34051442e-01 9.24980418e-01 3.85194475e-01 ... 9.64805235e-01\n  9.09923415e-01 8.20655559e-01]\n ...\n [2.34050488e-01 2.50775592e-01 6.81364710e-01 ... 9.84270681e-01\n  1.84181733e-02 2.36378406e-01]\n [9.84050488e-01 5.00775592e-01 9.31364710e-01 ... 7.34270681e-01\n  2.68418173e-01 4.86378406e-01]\n [4.84050488e-01 7.75592029e-04 4.31364710e-01 ... 2.34270681e-01\n  7.68418173e-01 9.86378406e-01]]\n\nTime to construct points is 1.7e-01 seconds\n</pre> In\u00a0[10]: Copied! <pre>ld = qmcpy.Sobol(2)  #Sobol' points\nanother_unif_ld = qmcpy.Uniform(ld, lower_bound=[-2,2], upper_bound=[2,4])  #define the desired probability distribution with sobol as input\npoints = another_unif_ld.gen_samples(2**8)\nprint(another_unif_ld)\nprint(f'\\nUniform LD points with shape {points.shape}\\n'+str(points))  #these points have 15 significant digit precision but only three digits are shown\nplot_successive_points(another_unif_ld,'Uniform LD',first_n=2**8,xlim=[-2,2],ylim=[2,4])\n</pre> ld = qmcpy.Sobol(2)  #Sobol' points another_unif_ld = qmcpy.Uniform(ld, lower_bound=[-2,2], upper_bound=[2,4])  #define the desired probability distribution with sobol as input points = another_unif_ld.gen_samples(2**8) print(another_unif_ld) print(f'\\nUniform LD points with shape {points.shape}\\n'+str(points))  #these points have 15 significant digit precision but only three digits are shown plot_successive_points(another_unif_ld,'Uniform LD',first_n=2**8,xlim=[-2,2],ylim=[2,4]) <pre>Uniform (AbstractTrueMeasure)\n    lower_bound     [-2  2]\n    upper_bound     [2 4]\n\nUniform LD points with shape (256, 2)\n[[ 1.63211619e+00  3.73527085e+00]\n [-1.96458235e+00  2.76718358e+00]\n [ 9.09260722e-01  2.10405185e+00]\n [-7.49905356e-01  3.38671429e+00]\n [ 1.09655014e+00  2.74751185e+00]\n [-1.31071479e+00  3.77735132e+00]\n [ 3.18856288e-01  3.10066974e+00]\n [-2.58190328e-02  2.38125511e+00]\n [ 1.85902575e+00  2.35110377e+00]\n [-1.55013461e+00  3.13179771e+00]\n [ 5.74189117e-01  3.99793234e+00]\n [-7.72499888e-01  2.52790732e+00]\n [ 1.38496040e+00  3.35363327e+00]\n [-1.20972064e+00  2.13615638e+00]\n [ 1.70346353e-01  2.98483491e+00]\n [-4.86928113e-01  3.51664289e+00]\n [ 1.59319338e+00  2.83569329e+00]\n [-1.75352515e+00  3.67851923e+00]\n [ 8.70309343e-01  3.45102593e+00]\n [-5.38819580e-01  2.04362477e+00]\n [ 1.18259209e+00  3.83803777e+00]\n [-1.47465285e+00  2.68294084e+00]\n [ 4.04926810e-01  2.43774730e+00]\n [-1.89785667e-01  3.03241946e+00]\n [ 1.92921095e+00  3.18830328e+00]\n [-1.72992369e+00  2.28284011e+00]\n [ 6.44406709e-01  2.58858045e+00]\n [-9.52321360e-01  3.93337465e+00]\n [ 1.33015606e+00  2.20048515e+00]\n [-1.01455070e+00  3.29318905e+00]\n [ 1.15509619e-01  3.58513540e+00]\n [-2.91725784e-01  2.92810048e+00]\n [ 1.69674328e+00  2.02283906e+00]\n [-1.89601564e+00  3.49030058e+00]\n [ 9.81610664e-01  3.63771572e+00]\n [-6.73680637e-01  2.85592312e+00]\n [ 1.04382486e+00  3.02634606e+00]\n [-1.35944513e+00  2.49563675e+00]\n [ 2.58530228e-01  2.62559582e+00]\n [-8.23294418e-02  3.84563622e+00]\n [ 1.78281433e+00  3.87505519e+00]\n [-1.62246743e+00  2.59520034e+00]\n [ 5.05639523e-01  2.27578809e+00]\n [-8.37113668e-01  3.24521810e+00]\n [ 1.44145742e+00  2.88827178e+00]\n [-1.14941179e+00  3.60634367e+00]\n [ 2.19059476e-01  3.27338160e+00]\n [-4.34216234e-01  2.24073455e+00]\n [ 1.51690842e+00  3.42208176e+00]\n [-1.82581517e+00  2.07936273e+00]\n [ 8.01804452e-01  2.79132255e+00]\n [-6.03508819e-01  3.69837261e+00]\n [ 1.23904058e+00  2.43523923e+00]\n [-1.41426473e+00  3.09068726e+00]\n [ 4.53717300e-01  3.78885312e+00]\n [-1.37120398e-01  2.69407406e+00]\n [ 1.99388680e+00  2.53843444e+00]\n [-1.66143601e+00  3.94351623e+00]\n [ 7.16679522e-01  3.18480345e+00]\n [-8.76049791e-01  2.34014643e+00]\n [ 1.27750410e+00  3.54175642e+00]\n [-1.06332406e+00  2.94891535e+00]\n [ 5.51386166e-02  2.17250235e+00]\n [-3.48160972e-01  3.32991865e+00]\n [ 1.68443108e+00  2.66428490e+00]\n [-1.96890853e+00  3.82156387e+00]\n [ 8.98955486e-01  3.04917923e+00]\n [-6.91855546e-01  2.45623134e+00]\n [ 1.08720775e+00  3.66175516e+00]\n [-1.25561124e+00  2.81720496e+00]\n [ 3.72134031e-01  2.06227642e+00]\n [-3.30914651e-02  3.46749554e+00]\n [ 1.83223923e+00  3.31185587e+00]\n [-1.50856227e+00  2.21693948e+00]\n [ 6.09782398e-01  2.91132119e+00]\n [-7.93551486e-01  3.56666242e+00]\n [ 1.42346943e+00  2.29961511e+00]\n [-1.23176561e+00  3.20677197e+00]\n [ 1.46475575e-01  3.91470354e+00]\n [-4.46349142e-01  2.57212184e+00]\n [ 1.59862091e+00  3.76507331e+00]\n [-1.80474448e+00  2.73253306e+00]\n [ 8.13112855e-01  2.39554271e+00]\n [-5.27659039e-01  3.11375194e+00]\n [ 1.12638706e+00  2.75289168e+00]\n [-1.46640614e+00  3.72218437e+00]\n [ 4.11345801e-01  3.39898800e+00]\n [-2.43918834e-01  2.11902634e+00]\n [ 1.88678700e+00  2.14844526e+00]\n [-1.70399443e+00  3.36859247e+00]\n [ 6.64358817e-01  3.50233576e+00]\n [-9.89012298e-01  2.97176378e+00]\n [ 1.35305252e+00  3.14610054e+00]\n [-1.05220258e+00  2.36417062e+00]\n [ 7.60300175e-02  2.51561415e+00]\n [-2.66757468e-01  3.98296885e+00]\n [ 1.73720513e+00  3.07783324e+00]\n [-1.92001307e+00  2.42066099e+00]\n [ 9.59692714e-01  2.70882325e+00]\n [-6.35054132e-01  3.80142033e+00]\n [ 1.02289814e+00  2.06461688e+00]\n [-1.32373269e+00  3.40951789e+00]\n [ 2.99983303e-01  3.71122997e+00]\n [-1.09240950e-01  2.80590412e+00]\n [ 1.77542454e+00  2.96178788e+00]\n [-1.56931658e+00  3.55632271e+00]\n [ 5.60869853e-01  3.31515758e+00]\n [-8.46338810e-01  2.15995383e+00]\n [ 1.49963234e+00  3.95828065e+00]\n [-1.15959764e+00  2.55098630e+00]\n [ 2.14614269e-01  2.32727723e+00]\n [-3.82026098e-01  3.17024050e+00]\n [ 1.54175770e+00  2.47744239e+00]\n [-1.86541953e+00  3.00935717e+00]\n [ 7.64277676e-01  3.86181997e+00]\n [-5.80492975e-01  2.64448040e+00]\n [ 1.20247642e+00  3.47412016e+00]\n [-1.39419538e+00  2.00395782e+00]\n [ 4.79529198e-01  2.87412082e+00]\n [-1.79671249e-01  3.65470795e+00]\n [ 1.93963436e+00  3.62455655e+00]\n [-1.65514201e+00  2.90524872e+00]\n [ 7.25051103e-01  2.22453890e+00]\n [-9.32135663e-01  3.25451570e+00]\n [ 1.28879166e+00  2.61139932e+00]\n [-1.12040307e+00  3.89392442e+00]\n [ 3.80216143e-03  3.22700857e+00]\n [-3.42860104e-01  2.25881449e+00]\n [ 1.64846946e+00  2.25745798e+00]\n [-1.94236157e+00  3.22540792e+00]\n [ 9.33914805e-01  3.90407310e+00]\n [-7.19383823e-01  2.62179213e+00]\n [ 1.12267729e+00  3.25998869e+00]\n [-1.28264265e+00  2.22976775e+00]\n [ 3.37659195e-01  2.89097689e+00]\n [-5.07107836e-03  3.61052886e+00]\n [ 1.86025008e+00  3.64141262e+00]\n [-1.54305808e+00  2.86106964e+00]\n [ 5.82737638e-01  2.01040743e+00]\n [-7.58099114e-01  3.48032563e+00]\n [ 1.39594306e+00  2.65365246e+00]\n [-1.19677768e+00  3.87123616e+00]\n [ 1.73028253e-01  3.00702416e+00]\n [-4.82285964e-01  2.47486523e+00]\n [ 1.56267932e+00  3.15718932e+00]\n [-1.77818702e+00  2.31398191e+00]\n [ 8.48096027e-01  2.55719177e+00]\n [-5.55180645e-01  3.96473026e+00]\n [ 1.16183658e+00  2.16937002e+00]\n [-1.49344805e+00  3.32432963e+00]\n [ 3.76847111e-01  3.55374555e+00]\n [-2.15905118e-01  2.95945487e+00]\n [ 1.91480262e+00  2.80430347e+00]\n [-1.73846451e+00  3.70987346e+00]\n [ 6.37322628e-01  3.41991071e+00]\n [-9.53537988e-01  2.07476556e+00]\n [ 1.32552137e+00  3.80664918e+00]\n [-1.01724039e+00  2.71429624e+00]\n [ 1.02574124e-01  2.40663330e+00]\n [-3.02716229e-01  3.06356141e+00]\n [ 1.70919046e+00  3.96891344e+00]\n [-1.88551344e+00  2.50131460e+00]\n [ 9.86733664e-01  2.36943290e+00]\n [-6.70502686e-01  3.15160697e+00]\n [ 1.05042069e+00  2.98213078e+00]\n [-1.35871681e+00  3.51245863e+00]\n [ 2.73426806e-01  3.36702716e+00]\n [-7.33003155e-02  2.14712409e+00]\n [ 1.81138234e+00  2.11648441e+00]\n [-1.59585973e+00  3.39669021e+00]\n [ 5.25906716e-01  3.73157487e+00]\n [-8.18806722e-01  2.76203804e+00]\n [ 1.46415899e+00  3.11999072e+00]\n [-1.13256241e+00  2.40202564e+00]\n [ 2.49085294e-01  2.71945429e+00]\n [-4.10042667e-01  3.75175039e+00]\n [ 1.51373826e+00  2.56982405e+00]\n [-1.83094564e+00  3.91216161e+00]\n [ 7.91310047e-01  3.21591834e+00]\n [-6.15963474e-01  2.30900561e+00]\n [ 1.23000375e+00  3.57314535e+00]\n [-1.42915375e+00  2.91755998e+00]\n [ 4.52981281e-01  2.20361656e+00]\n [-1.43708670e-01  3.29877709e+00]\n [ 1.97557215e+00  3.45319600e+00]\n [-1.68169565e+00  2.04822102e+00]\n [ 6.90064120e-01  2.82271138e+00]\n [-9.04610239e-01  3.66701745e+00]\n [ 1.25333832e+00  2.46635421e+00]\n [-1.09335735e+00  3.05954623e+00]\n [ 3.82970322e-02  3.82024270e+00]\n [-3.70870007e-01  2.66271959e+00]\n [ 1.65976941e+00  3.34321421e+00]\n [-1.99942245e+00  2.18555377e+00]\n [ 8.82594568e-01  2.94246598e+00]\n [-7.14068659e-01  3.53555119e+00]\n [ 1.06841247e+00  2.33097414e+00]\n [-1.27636677e+00  3.17538702e+00]\n [ 3.46014554e-01  3.94584901e+00]\n [-6.11712500e-02  2.54101136e+00]\n [ 1.82369833e+00  2.69543034e+00]\n [-1.52297062e+00  3.79045353e+00]\n [ 6.08565743e-01  3.08053835e+00]\n [-8.00635651e-01  2.42484617e+00]\n [ 1.42077994e+00  3.69289986e+00]\n [-1.23640015e+00  2.78609394e+00]\n [ 1.35485274e-01  2.09363479e+00]\n [-4.59284431e-01  3.43610969e+00]\n [ 1.62084184e+00  2.24233496e+00]\n [-1.78839100e+00  3.27473788e+00]\n [ 8.43634601e-01  3.59595062e+00]\n [-5.03004805e-01  2.87812287e+00]\n [ 1.15445917e+00  3.23998949e+00]\n [-1.44027908e+00  2.27031534e+00]\n [ 4.32093663e-01  2.60922826e+00]\n [-2.25115961e-01  3.88932725e+00]\n [ 1.89386349e+00  3.85868764e+00]\n [-1.70277019e+00  2.63889138e+00]\n [ 6.78759497e-01  2.48943152e+00]\n [-9.80463810e-01  3.01989669e+00]\n [ 1.36599563e+00  2.84650669e+00]\n [-1.04121972e+00  3.62854343e+00]\n [ 8.06723779e-02  3.49287749e+00]\n [-2.64075414e-01  2.02517184e+00]\n [ 1.73207452e+00  2.93064218e+00]\n [-1.92318343e+00  3.58743295e+00]\n [ 9.47237855e-01  3.28379830e+00]\n [-6.45548684e-01  2.19133855e+00]\n [ 1.00800914e+00  3.92713610e+00]\n [-1.33276944e+00  2.58209776e+00]\n [ 2.93395118e-01  2.29591913e+00]\n [-1.09976944e-01  3.20162644e+00]\n [ 1.75516493e+00  3.04647510e+00]\n [-1.58763115e+00  2.45204708e+00]\n [ 5.32309489e-01  2.67767879e+00]\n [-8.72954185e-01  3.83253158e+00]\n [ 1.46959891e+00  2.03325752e+00]\n [-1.18376362e+00  3.44090282e+00]\n [ 1.91905028e-01  3.68008429e+00]\n [-3.98867827e-01  2.83701422e+00]\n [ 1.55225969e+00  3.53094266e+00]\n [-1.85297249e+00  2.99889055e+00]\n [ 7.67455475e-01  2.13065019e+00]\n [-5.75370188e-01  3.34837122e+00]\n [ 1.20320483e+00  2.51778422e+00]\n [-1.38759952e+00  3.98756510e+00]\n [ 4.88558359e-01  3.13311863e+00]\n [-1.64774579e-01  2.35266884e+00]\n [ 1.96624215e+00  2.38355266e+00]\n [-1.62657397e+00  3.10321143e+00]\n [ 7.43358081e-01  3.76820472e+00]\n [-9.11868376e-01  2.73812111e+00]\n [ 1.30564082e+00  3.38023160e+00]\n [-1.09770165e+00  2.09781330e+00]\n [ 2.79755744e-02  2.78050674e+00]\n [-3.12834497e-01  3.74834986e+00]]\n</pre> In\u00a0[11]: Copied! <pre>ld = qmcpy.Lattice(2)\ngaussian_ld = qmcpy.Gaussian(ld, mean=[3,2], covariance=[[9,5], [5,4]])  #specify the desired mean and covariance of your multivariate Gaussian distribution\npoints = gaussian_ld.gen_samples(2**8)\nprint(gaussian_ld)\nprint(f'\\nGaussian LD Points with shape {points.shape}\\n'+str(points))  #these points have 15 significant digit precision but only three digits are shown\nplot_successive_points(gaussian_ld,'Gaussian LD',first_n=2**8,xlim=[-6,12],ylim=[-2,6])\n</pre> ld = qmcpy.Lattice(2) gaussian_ld = qmcpy.Gaussian(ld, mean=[3,2], covariance=[[9,5], [5,4]])  #specify the desired mean and covariance of your multivariate Gaussian distribution points = gaussian_ld.gen_samples(2**8) print(gaussian_ld) print(f'\\nGaussian LD Points with shape {points.shape}\\n'+str(points))  #these points have 15 significant digit precision but only three digits are shown plot_successive_points(gaussian_ld,'Gaussian LD',first_n=2**8,xlim=[-6,12],ylim=[-2,6]) <pre>Gaussian (AbstractTrueMeasure)\n    mean            [3 2]\n    covariance      [[9 5]\n                     [5 4]]\n    decomp_type     PCA\n\nGaussian LD Points with shape (256, 2)\n[[ 9.80784089e-01 -6.70135637e-01]\n [ 4.29201337e+00  3.08939243e+00]\n [ 2.70351262e+00  1.39177419e+00]\n [ 6.94591370e+00  5.61220091e+00]\n [ 1.22732660e+00  1.58259254e+00]\n [ 5.96795261e+00  2.99596433e+00]\n [ 2.44027019e+00  3.89861289e+00]\n [-1.57840943e+00 -8.93333455e-01]\n [ 1.08644957e+00  5.76156485e-01]\n [ 4.28113054e+00  4.32931865e+00]\n [ 2.76553638e+00  2.33245354e+00]\n [ 1.16726000e+01  6.26695439e+00]\n [ 2.91426798e+00 -1.81254438e-02]\n [ 6.31198394e+00  4.15908938e+00]\n [ 4.19815770e+00  2.11946301e+00]\n [-8.47601153e-01  5.25690347e-01]\n [ 3.23117324e-01  1.13251342e+00]\n [ 5.01299750e+00  2.51783050e+00]\n [ 3.94190142e+00 -8.97084397e-02]\n [ 8.38405732e+00  5.35168099e+00]\n [ 1.93590871e+00  1.01037649e+00]\n [ 5.35428481e+00  4.79291114e+00]\n [ 3.50884070e+00  2.69727756e+00]\n [-3.92563166e-01 -1.34090316e+00]\n [ 1.99818505e+00 -2.64924500e-01]\n [ 5.17839575e+00  3.54721126e+00]\n [ 3.44389373e+00  1.75332272e+00]\n [-3.09434454e+00 -7.35422900e-01]\n [ 2.01945669e+00  1.96944279e+00]\n [ 7.27802715e+00  3.68449383e+00]\n [ 3.36065324e+00  4.02606972e+00]\n [ 4.54772993e-02  2.20186863e-02]\n [-2.28034432e-01  1.66370309e+00]\n [ 4.64142815e+00  2.81801881e+00]\n [ 3.08866057e+00  1.03037222e+00]\n [ 7.63842422e+00  5.39260651e+00]\n [ 1.59034291e+00  1.28519127e+00]\n [ 6.56705459e+00  2.41716598e+00]\n [ 3.12967941e+00  3.04594991e+00]\n [-9.96559899e-01 -1.04047120e+00]\n [ 1.47195482e+00  2.74291161e-01]\n [ 4.78855644e+00  3.83706943e+00]\n [ 3.10372116e+00  2.04433326e+00]\n [-4.83428748e+00 -8.83988370e-02]\n [ 1.59676521e+00  2.38507554e+00]\n [ 6.76249366e+00  3.94719157e+00]\n [ 4.63717928e+00  1.68283168e+00]\n [-3.75332096e-01  2.50487752e-01]\n [ 7.19791850e-01  8.36442410e-01]\n [ 5.50904053e+00  2.02427292e+00]\n [ 2.37115736e+00  2.71142598e+00]\n [ 9.38353714e+00  5.47267375e+00]\n [ 2.31380818e+00  6.78318397e-01]\n [ 5.86980213e+00  4.39978668e+00]\n [ 3.84776228e+00  2.41617606e+00]\n [-1.57086820e+00  1.16086154e+00]\n [ 7.57617121e-01  2.04616706e+00]\n [ 5.55504055e+00  3.29274496e+00]\n [ 3.84864655e+00  1.35854687e+00]\n [-2.23685566e+00 -8.06858572e-01]\n [ 2.36515462e+00  1.67575215e+00]\n [ 8.12666627e+00  2.95547973e+00]\n [ 3.91586794e+00  3.41017200e+00]\n [ 4.67532378e-01 -2.38064730e-01]\n [ 1.50066305e+00 -1.32000733e+00]\n [ 4.46684609e+00  2.95268125e+00]\n [ 2.88485110e+00  1.22944002e+00]\n [ 7.29676633e+00  5.47080847e+00]\n [ 1.41414900e+00  1.42604782e+00]\n [ 6.21779887e+00  2.78437733e+00]\n [ 2.88390396e+00  3.31198873e+00]\n [-1.28308860e+00 -9.56198304e-01]\n [ 1.27325597e+00  4.36135487e-01]\n [ 4.56530836e+00  4.03249049e+00]\n [ 2.93710416e+00  2.18437228e+00]\n [-3.91402699e+00 -3.85694469e+00]\n [ 1.25186889e+00  2.80783663e+00]\n [ 6.53317939e+00  4.05377234e+00]\n [ 4.39651028e+00  1.93491215e+00]\n [-6.00491974e-01  3.75421729e-01]\n [ 5.28836279e-01  9.74250689e-01]\n [ 5.23007762e+00  2.32004358e+00]\n [ 2.09144320e+00  3.03428837e+00]\n [ 8.83218247e+00  5.38570864e+00]\n [ 2.11653447e+00  8.58389504e-01]\n [ 5.63166058e+00  4.56081781e+00]\n [ 3.67960417e+00  2.55428135e+00]\n [ 2.44456679e-02 -1.70939455e+00]\n [ 2.40267088e-01  2.73233648e+00]\n [ 5.36542734e+00  3.42017714e+00]\n [ 3.63105061e+00  1.58044418e+00]\n [-2.62790196e+00 -7.73512510e-01]\n [ 2.19607272e+00  1.81680154e+00]\n [ 7.60289295e+00  3.47033475e+00]\n [ 3.68784258e+00  3.63726842e+00]\n [ 2.53096150e-01 -9.91643665e-02]\n [ 8.71882533e-02  1.33595653e+00]\n [ 4.82112334e+00  2.67683927e+00]\n [ 3.34744682e+00  7.42028224e-01]\n [ 7.99446374e+00  5.35498027e+00]\n [ 1.76250063e+00  1.14945454e+00]\n [ 4.91904817e+00  5.28729484e+00]\n [ 3.32911627e+00  2.85538786e+00]\n [-7.06939793e-01 -1.15867333e+00]\n [ 1.69770894e+00  6.62606218e-02]\n [ 4.98849897e+00  3.68238565e+00]\n [ 3.27079641e+00  1.90363422e+00]\n [-3.70119871e+00 -6.58423391e-01]\n [ 1.82629835e+00  2.14819152e+00]\n [ 7.00694737e+00  3.82962630e+00]\n [ 5.03330779e+00  1.18022534e+00]\n [-1.62133607e-01  1.35816015e-01]\n [ 9.03807980e-01  7.06671980e-01]\n [ 6.48347952e+00  6.05492508e-01]\n [ 2.58162056e+00  2.50049634e+00]\n [ 1.01484810e+01  5.66443364e+00]\n [ 2.54910402e+00  4.35658204e-01]\n [ 6.09307251e+00  4.27122471e+00]\n [ 4.01857154e+00  2.27451894e+00]\n [-1.13898675e+00  7.36126324e-01]\n [ 1.01838301e+00  1.77331681e+00]\n [ 5.75302301e+00  3.15585439e+00]\n [ 4.15171717e+00  9.98632717e-01]\n [-1.89251109e+00 -8.45125326e-01]\n [ 2.53253640e+00  1.53691311e+00]\n [ 6.52451339e+00  5.90564465e+00]\n [ 4.11120689e+00  3.23730652e+00]\n [ 7.00349184e-01 -4.12794172e-01]\n [ 3.12903043e-01  5.06784577e-01]\n [ 5.23701863e+00  1.63323025e+00]\n [ 2.10370540e+00  2.42787169e+00]\n [ 7.81321429e+00  4.41672992e+00]\n [ 2.01401963e+00  3.82761606e-01]\n [ 5.39515676e+00  4.01847859e+00]\n [ 3.55624162e+00  2.15847632e+00]\n [-2.26869882e+00  4.36029399e-01]\n [ 4.45694705e-01  1.69437255e+00]\n [ 5.16895788e+00  2.97393410e+00]\n [ 3.59252940e+00  1.05911050e+00]\n [-6.23113932e+00 -3.35709699e+00]\n [ 2.06985359e+00  1.41634830e+00]\n [ 5.04691692e+00  6.33090977e+00]\n [ 3.61755087e+00  3.12698675e+00]\n [-4.38881857e-02 -6.48615372e-01]\n [ 1.10896483e+00 -5.12705353e-02]\n [ 4.43661688e+00  3.52690279e+00]\n [ 2.81934141e+00  1.79160221e+00]\n [ 7.64277320e+00  6.91022546e+00]\n [ 1.31890045e+00  2.07900114e+00]\n [ 6.20664465e+00  3.52121851e+00]\n [ 4.38023257e+00  1.35390774e+00]\n [-9.94371673e-01 -2.11710203e-01]\n [ 1.26109624e+00  1.00440968e+00]\n [ 6.26466286e+00  1.87693426e+00]\n [ 2.85587338e+00  2.76970758e+00]\n [-2.16989369e+00 -1.85498381e+00]\n [ 2.81967871e+00  7.41299630e-01]\n [ 6.72004106e+00  4.74064806e+00]\n [ 4.32142525e+00  2.54165766e+00]\n [-6.25766108e-01  1.21636543e+00]\n [ 6.98113320e-01  2.56831018e-01]\n [ 3.97847564e+00  3.96525834e+00]\n [ 2.48328928e+00  2.07511098e+00]\n [ 8.53730344e+00  4.19817722e+00]\n [ 2.81632848e+00 -6.26914449e-01]\n [ 5.79718770e+00  3.76389093e+00]\n [ 3.90988176e+00  1.84955683e+00]\n [-1.53008891e+00  1.27647173e-03]\n [ 8.94860040e-01  1.28696485e+00]\n [ 5.57540068e+00  2.64795152e+00]\n [ 2.29902873e+00  3.41115686e+00]\n [-3.22882722e+00 -1.99053931e+00]\n [ 2.41449031e+00  1.12931673e+00]\n [ 6.18606224e+00  5.01705735e+00]\n [ 3.97887355e+00  2.81703294e+00]\n [ 5.49044003e-01 -1.12896364e+00]\n [ 1.72428793e+00 -7.04494923e-01]\n [ 4.80648617e+00  3.23968068e+00]\n [ 3.16414175e+00  1.49316535e+00]\n [ 9.04343526e+00  6.65396255e+00]\n [ 1.72445071e+00  1.70107574e+00]\n [ 6.68233403e+00  3.20032331e+00]\n [ 3.12924643e+00  3.64602194e+00]\n [-5.17875678e-01 -4.04146572e-01]\n [ 1.61627185e+00  7.31596185e-01]\n [ 4.93439318e+00  4.38816869e+00]\n [ 3.22120292e+00  2.43896543e+00]\n [-1.29967617e+00 -2.05532926e+00]\n [ 1.38198489e+00  3.33261337e+00]\n [ 7.23904439e+00  4.56714696e+00]\n [ 4.69376994e+00  2.22292626e+00]\n [-9.71168169e-02  7.77271632e-01]\n [ 5.05714388e-01  3.83725512e-01]\n [ 3.55065517e+00  4.50880565e+00]\n [ 2.30390827e+00  2.23482021e+00]\n [ 8.14586073e+00  4.32739486e+00]\n [ 2.27527539e+00  1.05013998e-01]\n [ 5.59813573e+00  3.88528002e+00]\n [ 3.72748544e+00  2.01280341e+00]\n [-1.85110408e+00  1.56525580e-01]\n [ 6.89612337e-01  1.46063091e+00]\n [ 5.36201580e+00  2.82592661e+00]\n [ 3.96851925e+00  5.80277017e-01]\n [-4.05584301e+00 -2.25960929e+00]\n [ 2.23951448e+00  1.27754109e+00]\n [ 5.85828948e+00  5.27641679e+00]\n [ 3.80395141e+00  2.96223566e+00]\n [ 2.18955674e-01 -8.29581008e-01]\n [ 1.35717483e+00 -2.80033648e-01]\n [ 4.62550037e+00  3.37579336e+00]\n [ 2.98772871e+00  1.64894810e+00]\n [ 8.34861301e+00  6.62139654e+00]\n [ 1.53567911e+00  1.86793380e+00]\n [ 6.43010523e+00  3.38005596e+00]\n [ 5.07249565e+00  3.67437441e-01]\n [-7.52399767e-01 -3.05940995e-01]\n [ 1.43733165e+00  8.71143648e-01]\n [ 4.59764615e+00  4.75166677e+00]\n [ 3.04633166e+00  2.59159720e+00]\n [-1.73719364e+00 -1.90314909e+00]\n [ 3.11675256e+00  3.93446854e-01]\n [ 6.97647726e+00  4.64733947e+00]\n [ 4.50007427e+00  2.39381590e+00]\n [-3.33246279e-01  9.54386555e-01]\n [ 8.96106436e-01  1.16728837e-01]\n [ 4.22937758e+00  3.70978664e+00]\n [ 2.65294752e+00  1.93083775e+00]\n [ 9.06609324e+00  3.92140543e+00]\n [ 1.02842147e+00  2.40812980e+00]\n [ 5.99815714e+00  3.64536435e+00]\n [ 4.11596980e+00  1.64850390e+00]\n [-1.25047317e+00 -1.13258988e-01]\n [ 1.08225390e+00  1.13985760e+00]\n [ 5.83394485e+00  2.40004612e+00]\n [ 2.63087555e+00  3.00391129e+00]\n [-2.64764975e+00 -1.88023932e+00]\n [ 2.60263516e+00  9.59032325e-01]\n [ 6.46121587e+00  4.85729863e+00]\n [ 4.14956870e+00  2.67972035e+00]\n [-1.22132384e+00  1.96165234e+00]\n [ 7.63974718e-02  2.12831078e+00]\n [ 4.98587165e+00  3.10853071e+00]\n [ 3.35827152e+00  1.30862368e+00]\n [ 1.00344906e+01  6.96091443e+00]\n [ 1.89998430e+00  1.55460747e+00]\n [ 7.00270854e+00  2.91919965e+00]\n [ 3.40598744e+00  3.33335666e+00]\n [-2.84451575e-01 -5.14498298e-01]\n [ 1.80459108e+00  5.75090634e-01]\n [ 5.17975110e+00  4.17678917e+00]\n [ 3.38925189e+00  2.29768250e+00]\n [-6.80226625e-01 -2.57026989e+00]\n [ 1.85459196e+00  2.69966047e+00]\n [ 7.51512076e+00  4.49278295e+00]\n [ 4.91960836e+00  2.00141816e+00]\n [ 1.14246901e-01  6.34410412e-01]]\n</pre> <p>Transformations of low discrepancy sequences often have good properties, but may not be low discrepancy themselves, depending on your definition of discrepancy as shown by Yiou Li and Lulu Kang.</p> <p>Pause for questions</p> In\u00a0[12]: Copied! <pre>d = 5  #coded as parameters so that\ntol = 1e-3  #you can change here and propagate them through this example\nlattice = qmcpy.Lattice(d)\ngaussian_lattice = qmcpy.Gaussian(lattice, mean = 0, covariance = 1/2)  #mean and covariance of the distribution identified above\n</pre> d = 5  #coded as parameters so that tol = 1e-3  #you can change here and propagate them through this example lattice = qmcpy.Lattice(d) gaussian_lattice = qmcpy.Gaussian(lattice, mean = 0, covariance = 1/2)  #mean and covariance of the distribution identified above In\u00a0[13]: Copied! <pre>keister = qmcpy.Keister(gaussian_lattice) #transform the original integrand to the eventual one\n</pre> keister = qmcpy.Keister(gaussian_lattice) #transform the original integrand to the eventual one In\u00a0[14]: Copied! <pre>keister_lattice_gauss_g = qmcpy.CubQMCLatticeG(keister, abs_tol = tol)  #using Tony's stopping criterion\n</pre> keister_lattice_gauss_g = qmcpy.CubQMCLatticeG(keister, abs_tol = tol)  #using Tony's stopping criterion <p>Invoking the <code>integrate</code> method returns the numerical solution and a data object. Printing the data object provides a neat summary of the integration problem. For details of the output fields, see the online, searchable QMCPy Documentation at https://qmcpy.readthedocs.io/.</p> In\u00a0[15]: Copied! <pre>solution, data = keister_lattice_gauss_g.integrate()\nprint(data)\n</pre> solution, data = keister_lattice_gauss_g.integrate() print(data) <pre>Data (Data)\n    solution        1.135\n    comb_bound_low  1.135\n    comb_bound_high 1.136\n    comb_bound_diff 0.001\n    comb_flags      1\n    n_total         2^(17)\n    n               2^(17)\n    time_integrate  0.042\nCubQMCLatticeG (AbstractStoppingCriterion)\n    abs_tol         0.001\n    rel_tol         0\n    n_init          2^(10)\n    n_limit         2^(30)\nKeister (AbstractIntegrand)\nGaussian (AbstractTrueMeasure)\n    mean            0\n    covariance      2^(-1)\n    decomp_type     PCA\n    transform       Gaussian (AbstractTrueMeasure)\n                        mean            0\n                        covariance      2^(-1)\n                        decomp_type     PCA\nLattice (AbstractLDDiscreteDistribution)\n    d               5\n    replications    1\n    randomize       SHIFT\n    gen_vec_source  kuo.lattice-33002-1024-1048576.9125.txt\n    order           RADICAL INVERSE\n    n_limit         2^(20)\n    entropy         38563684764170563855999902143753109741\n</pre> In\u00a0[16]: Copied! <pre>warnings.simplefilter(\"ignore\")\nkeister_lattice_gauss_g_small_n = qmcpy.CubQMCLatticeG(keister, abs_tol = 1e-6, n_limit = 2**12)  #the default n_max is 2**35\nsolution, data = keister_lattice_gauss_g_small_n.integrate()\nprint(data)\n</pre> warnings.simplefilter(\"ignore\") keister_lattice_gauss_g_small_n = qmcpy.CubQMCLatticeG(keister, abs_tol = 1e-6, n_limit = 2**12)  #the default n_max is 2**35 solution, data = keister_lattice_gauss_g_small_n.integrate() print(data) <pre>Data (Data)\n    solution        1.129\n    comb_bound_low  1.116\n    comb_bound_high 1.142\n    comb_bound_diff 0.026\n    comb_flags      0\n    n_total         2^(12)\n    n               2^(12)\n    time_integrate  0.003\nCubQMCLatticeG (AbstractStoppingCriterion)\n    abs_tol         1.00e-06\n    rel_tol         0\n    n_init          2^(10)\n    n_limit         2^(12)\nKeister (AbstractIntegrand)\nGaussian (AbstractTrueMeasure)\n    mean            0\n    covariance      2^(-1)\n    decomp_type     PCA\n    transform       Gaussian (AbstractTrueMeasure)\n                        mean            0\n                        covariance      2^(-1)\n                        decomp_type     PCA\nLattice (AbstractLDDiscreteDistribution)\n    d               5\n    replications    1\n    randomize       SHIFT\n    gen_vec_source  kuo.lattice-33002-1024-1048576.9125.txt\n    order           RADICAL INVERSE\n    n_limit         2^(20)\n    entropy         38563684764170563855999902143753109741\n</pre> In\u00a0[17]: Copied! <pre>d = 5; tol = 2.5e-3  #re-construct the example\n#d = 7; tol = 3e-3  #if you change the dimension\n#d = 10; tol = 3e-2  #you may also wish to change the tolerance, since the value of the integral changes\nld_keister = qmcpy.Keister(qmcpy.Gaussian(qmcpy.Lattice(d), mean = 0, covariance = 1/2))  #mean and covariance of the distribution identified above\niid_keister =  qmcpy.Keister(qmcpy.Gaussian(qmcpy.IIDStdUniform(d), mean = 0, covariance = 1/2))\n\nn_tol = 7\nii_iid = 2  #make this larger to reduce the time required\ntol_vec = [tol*2**(ii) for ii in range(n_tol)]  #initialize\nld_time = [0]*n_tol; ld_n = [0]*n_tol  #low discrepancy time and number of function values\niid_time = [0]*n_tol; iid_n = [0]*n_tol  #IID time and number of function values\nfor ii in range(n_tol):\n  solution, data = qmcpy.CubQMCLatticeG(ld_keister, abs_tol = tol_vec[ii]).integrate()\n  if ii == 0:\n    print(f'\\nKeister integral = {solution}\\n')\n  ld_time[ii] = data.time_integrate\n  ld_n[ii] = data.n_total\n  if ii &gt;= ii_iid:\n    solution, data = qmcpy.CubMCG(iid_keister, abs_tol = tol_vec[ii]).integrate()\n    iid_time[ii] = data.time_integrate\n    iid_n[ii] = data.n_total\nfig,ax = pyplot.subplots(nrows=1,ncols=2,figsize=(13,5.5))\nax[0].scatter(tol_vec[0:n_tol],ld_time[0:n_tol],color='b');\nax[0].plot(tol_vec[0:n_tol],[(ld_time[0]*tol_vec[0])/tol_vec[jj] for jj in range(n_tol)],color='b')\nax[0].scatter(tol_vec[ii_iid:n_tol],iid_time[ii_iid:n_tol],color='g');\nax[0].plot(tol_vec[ii_iid:n_tol],[(iid_time[ii_iid]*(tol_vec[ii_iid]**2))/(tol_vec[jj]**2) for jj in range(ii_iid,n_tol)],color='g')\nax[0].set_ylim([0.001,1000]); ax[0].set_ylabel('Time (s)')\nax[1].scatter(tol_vec[0:n_tol],ld_n[0:n_tol],color='b');\nax[1].plot(tol_vec[0:n_tol],[(ld_n[0]*tol_vec[0])/tol_vec[jj] for jj in range(n_tol)],color='b')\nax[1].scatter(tol_vec[ii_iid:n_tol],iid_n[ii_iid:n_tol],color='g');\nax[1].plot(tol_vec[ii_iid:n_tol],[(iid_n[ii_iid]*(tol_vec[ii_iid]**2))/(tol_vec[jj]**2) for jj in range(ii_iid,n_tol)],color='g')\nax[1].set_ylim([1e2,1e8]); ax[1].set_ylabel('n')\nfor ii in range(2):\n  ax[ii].set_xlim([tol,100*tol]); ax[ii].set_xlabel('Tolerance, '+r'$\\varepsilon$')\n  ax[ii].set_xscale('log'); ax[ii].set_yscale('log')\n  ax[ii].legend([r'$\\mathcal{O}(\\varepsilon^{-1})$',r'$\\mathcal{O}(\\varepsilon^{-2})$','LD','IID'],frameon=False)\n  ax[ii].set_aspect(0.35)\n</pre> d = 5; tol = 2.5e-3  #re-construct the example #d = 7; tol = 3e-3  #if you change the dimension #d = 10; tol = 3e-2  #you may also wish to change the tolerance, since the value of the integral changes ld_keister = qmcpy.Keister(qmcpy.Gaussian(qmcpy.Lattice(d), mean = 0, covariance = 1/2))  #mean and covariance of the distribution identified above iid_keister =  qmcpy.Keister(qmcpy.Gaussian(qmcpy.IIDStdUniform(d), mean = 0, covariance = 1/2))  n_tol = 7 ii_iid = 2  #make this larger to reduce the time required tol_vec = [tol*2**(ii) for ii in range(n_tol)]  #initialize ld_time = [0]*n_tol; ld_n = [0]*n_tol  #low discrepancy time and number of function values iid_time = [0]*n_tol; iid_n = [0]*n_tol  #IID time and number of function values for ii in range(n_tol):   solution, data = qmcpy.CubQMCLatticeG(ld_keister, abs_tol = tol_vec[ii]).integrate()   if ii == 0:     print(f'\\nKeister integral = {solution}\\n')   ld_time[ii] = data.time_integrate   ld_n[ii] = data.n_total   if ii &gt;= ii_iid:     solution, data = qmcpy.CubMCG(iid_keister, abs_tol = tol_vec[ii]).integrate()     iid_time[ii] = data.time_integrate     iid_n[ii] = data.n_total fig,ax = pyplot.subplots(nrows=1,ncols=2,figsize=(13,5.5)) ax[0].scatter(tol_vec[0:n_tol],ld_time[0:n_tol],color='b'); ax[0].plot(tol_vec[0:n_tol],[(ld_time[0]*tol_vec[0])/tol_vec[jj] for jj in range(n_tol)],color='b') ax[0].scatter(tol_vec[ii_iid:n_tol],iid_time[ii_iid:n_tol],color='g'); ax[0].plot(tol_vec[ii_iid:n_tol],[(iid_time[ii_iid]*(tol_vec[ii_iid]**2))/(tol_vec[jj]**2) for jj in range(ii_iid,n_tol)],color='g') ax[0].set_ylim([0.001,1000]); ax[0].set_ylabel('Time (s)') ax[1].scatter(tol_vec[0:n_tol],ld_n[0:n_tol],color='b'); ax[1].plot(tol_vec[0:n_tol],[(ld_n[0]*tol_vec[0])/tol_vec[jj] for jj in range(n_tol)],color='b') ax[1].scatter(tol_vec[ii_iid:n_tol],iid_n[ii_iid:n_tol],color='g'); ax[1].plot(tol_vec[ii_iid:n_tol],[(iid_n[ii_iid]*(tol_vec[ii_iid]**2))/(tol_vec[jj]**2) for jj in range(ii_iid,n_tol)],color='g') ax[1].set_ylim([1e2,1e8]); ax[1].set_ylabel('n') for ii in range(2):   ax[ii].set_xlim([tol,100*tol]); ax[ii].set_xlabel('Tolerance, '+r'$\\varepsilon$')   ax[ii].set_xscale('log'); ax[ii].set_yscale('log')   ax[ii].legend([r'$\\mathcal{O}(\\varepsilon^{-1})$',r'$\\mathcal{O}(\\varepsilon^{-2})$','LD','IID'],frameon=False)   ax[ii].set_aspect(0.35) <pre>\nKeister integral = 1.1356452963607024\n\n</pre> In\u00a0[18]: Copied! <pre>lattice_rep = qmcpy.Lattice(d,replications=15)\ngaussian_lattice_rep = qmcpy.Gaussian(lattice_rep, mean = 0, covariance = 1/2)\nkeister_rep = qmcpy.Keister(gaussian_lattice_rep)\nkeister_lattice_gauss_CLT = qmcpy.CubQMCCLT(keister_rep, abs_tol = tol)  #using a CLT stopping criterion with random replications\nsolution, data = keister_lattice_gauss_CLT.integrate()\nprint(data)\n</pre> lattice_rep = qmcpy.Lattice(d,replications=15) gaussian_lattice_rep = qmcpy.Gaussian(lattice_rep, mean = 0, covariance = 1/2) keister_rep = qmcpy.Keister(gaussian_lattice_rep) keister_lattice_gauss_CLT = qmcpy.CubQMCCLT(keister_rep, abs_tol = tol)  #using a CLT stopping criterion with random replications solution, data = keister_lattice_gauss_CLT.integrate() print(data) <pre>Data (Data)\n    solution        1.135\n    comb_bound_low  1.134\n    comb_bound_high 1.136\n    comb_bound_diff 0.002\n    comb_flags      1\n    n_total         122880\n    n               122880\n    n_rep           2^(13)\n    time_integrate  0.029\nCubQMCRepStudentT (AbstractStoppingCriterion)\n    inflate         1\n    alpha           0.010\n    abs_tol         0.003\n    rel_tol         0\n    n_init          2^(8)\n    n_limit         2^(30)\nKeister (AbstractIntegrand)\nGaussian (AbstractTrueMeasure)\n    mean            0\n    covariance      2^(-1)\n    decomp_type     PCA\n    transform       Gaussian (AbstractTrueMeasure)\n                        mean            0\n                        covariance      2^(-1)\n                        decomp_type     PCA\nLattice (AbstractLDDiscreteDistribution)\n    d               5\n    replications    15\n    randomize       SHIFT\n    gen_vec_source  kuo.lattice-33002-1024-1048576.9125.txt\n    order           RADICAL INVERSE\n    n_limit         2^(20)\n    entropy         275034244784380212109446453686580990022\n</pre> <p>This answer agrees with the one above.</p> In\u00a0[19]: Copied! <pre>def my_Keister(x):  #this could be a functional of a solution to a PDE with random coefficients\n                    #or anything that you would like\n    \"\"\"\n    x: nxd numpy ndarray\n       n samples\n       d dimensions\n\n    returns n-vector of the Kesiter function\n    evaluated at the n input samples\n    \"\"\"\n    d = x.shape[1]\n    norm = np.sqrt((x**2).sum(1))\n    k = np.cos(norm)*np.exp(-norm**2)\n    return k  #size n vector\n\nld = qmcpy.Sobol(d)  #choose the LD points\nlebesgue = qmcpy.Lebesgue(qmcpy.Gaussian(ld))  #now choose the Lebesgue distribution\nf = qmcpy.CustomFun(lebesgue, g=my_Keister)\nkeister_lebesgue_ld_g = qmcpy.CubQMCSobolG(f, abs_tol = tol)  #the stopping criterion does need to match the LD points\nsolution, data = keister_lebesgue_ld_g.integrate()\nprint(data)\n</pre> def my_Keister(x):  #this could be a functional of a solution to a PDE with random coefficients                     #or anything that you would like     \"\"\"     x: nxd numpy ndarray        n samples        d dimensions      returns n-vector of the Kesiter function     evaluated at the n input samples     \"\"\"     d = x.shape[1]     norm = np.sqrt((x**2).sum(1))     k = np.cos(norm)*np.exp(-norm**2)     return k  #size n vector  ld = qmcpy.Sobol(d)  #choose the LD points lebesgue = qmcpy.Lebesgue(qmcpy.Gaussian(ld))  #now choose the Lebesgue distribution f = qmcpy.CustomFun(lebesgue, g=my_Keister) keister_lebesgue_ld_g = qmcpy.CubQMCSobolG(f, abs_tol = tol)  #the stopping criterion does need to match the LD points solution, data = keister_lebesgue_ld_g.integrate() print(data) <pre>Data (Data)\n    solution        1.136\n    comb_bound_low  1.133\n    comb_bound_high 1.138\n    comb_bound_diff 0.004\n    comb_flags      1\n    n_total         2^(16)\n    n               2^(16)\n    time_integrate  0.030\nCubQMCNetG (AbstractStoppingCriterion)\n    abs_tol         0.003\n    rel_tol         0\n    n_init          2^(10)\n    n_limit         2^(35)\nCustomFun (AbstractIntegrand)\nLebesgue (AbstractTrueMeasure)\n    transform       Gaussian (AbstractTrueMeasure)\n                        mean            0\n                        covariance      1\n                        decomp_type     PCA\nDigitalNetB2 (AbstractLDDiscreteDistribution)\n    d               5\n    replications    1\n    randomize       LMS DS\n    gen_mats_source joe_kuo.6.21201.txt\n    order           RADICAL INVERSE\n    t               63\n    alpha           1\n    n_limit         2^(32)\n    entropy         91580573475794728146679265749810645603\n</pre> <p>This answer agrees with the answers above for the same integration problem.</p> <p>The initial <code>DiscreteDistribution</code> does not need to mimic the standard uniform distribution as this next example shows.</p> In\u00a0[20]: Copied! <pre>iid = qmcpy.IIDStdUniform(d)  #choose the LD points\nlebesgue = qmcpy.Lebesgue(qmcpy.Gaussian(iid))  #now choose the Lebesgue distribution\nf = qmcpy.CustomFun(lebesgue, g=my_Keister)\nkeister_lebesgue_ld_g = qmcpy.CubMCCLT(f, abs_tol = 10*tol)  #the stopping criterion does need to match the points\nsolution, data = keister_lebesgue_ld_g.integrate()\nprint(data)\n</pre> iid = qmcpy.IIDStdUniform(d)  #choose the LD points lebesgue = qmcpy.Lebesgue(qmcpy.Gaussian(iid))  #now choose the Lebesgue distribution f = qmcpy.CustomFun(lebesgue, g=my_Keister) keister_lebesgue_ld_g = qmcpy.CubMCCLT(f, abs_tol = 10*tol)  #the stopping criterion does need to match the points solution, data = keister_lebesgue_ld_g.integrate() print(data) <pre>Data (Data)\n    solution        1.127\n    bound_low       1.099\n    bound_high      1.155\n    bound_diff      0.056\n    n_total         1694184\n    time_integrate  0.361\nCubMCCLT (AbstractStoppingCriterion)\n    abs_tol         0.025\n    rel_tol         0\n    n_init          2^(10)\n    n_limit         2^(30)\n    inflate         1.200\n    alpha           0.010\nCustomFun (AbstractIntegrand)\nLebesgue (AbstractTrueMeasure)\n    transform       Gaussian (AbstractTrueMeasure)\n                        mean            0\n                        covariance      1\n                        decomp_type     PCA\nIIDStdUniform (AbstractIIDDiscreteDistribution)\n    d               5\n    replications    1\n    entropy         110067297374675390351789060258764096093\n</pre> In\u00a0[21]: Copied! <pre>abs_tol = .05  #a nickel\nn_time_steps = 64  #being a power of 2 will help for multi-level\n\noptions = {  #there should be nothing magic about these choices\n    'interest_rate': .05,\n    'volatility': .5,\n    'start_price': 30,\n    'strike_price': 30\n}\n</pre> abs_tol = .05  #a nickel n_time_steps = 64  #being a power of 2 will help for multi-level  options = {  #there should be nothing magic about these choices     'interest_rate': .05,     'volatility': .5,     'start_price': 30,     'strike_price': 30 } In\u00a0[22]: Copied! <pre>iidBrownian = qmcpy.BrownianMotion(qmcpy.IIDStdUniform(n_time_steps))\npayoff = qmcpy.AsianOption(iidBrownian, **options)\nIIDstop = qmcpy.CubMCG(payoff,abs_tol=abs_tol)\nprice,data = IIDstop.integrate()\nprint(data)\n</pre> iidBrownian = qmcpy.BrownianMotion(qmcpy.IIDStdUniform(n_time_steps)) payoff = qmcpy.AsianOption(iidBrownian, **options) IIDstop = qmcpy.CubMCG(payoff,abs_tol=abs_tol) price,data = IIDstop.integrate() print(data) <pre>Data (Data)\n    solution        3.686\n    bound_low       3.636\n    bound_high      3.736\n    bound_diff      0.100\n    n_total         225848\n    time_integrate  0.834\nCubMCG (AbstractStoppingCriterion)\n    abs_tol         0.050\n    rel_tol         0\n    n_init          2^(10)\n    n_limit         2^(30)\n    inflate         1.200\n    alpha           0.010\n    kurtmax         1.478\nAsianOption (AbstractIntegrand)\n    option          ASIAN\n    call_put        CALL\n    volatility      2^(-1)\n    start_price     30\n    strike_price    30\n    interest_rate   0.050\n    t_final         1\n    asian_mean      ARITHMETIC\nBrownianMotion (AbstractTrueMeasure)\n    time_vec        [0.016 0.031 0.047 ... 0.969 0.984 1.   ]\n    drift           0\n    mean            [0. 0. 0. ... 0. 0. 0.]\n    covariance      [[0.016 0.016 0.016 ... 0.016 0.016 0.016]\n                     [0.016 0.031 0.031 ... 0.031 0.031 0.031]\n                     [0.016 0.031 0.047 ... 0.047 0.047 0.047]\n                     ...\n                     [0.016 0.031 0.047 ... 0.969 0.969 0.969]\n                     [0.016 0.031 0.047 ... 0.969 0.984 0.984]\n                     [0.016 0.031 0.047 ... 0.969 0.984 1.   ]]\n    decomp_type     PCA\n    transform       BrownianMotion (AbstractTrueMeasure)\n                        time_vec        [0.016 0.031 0.047 ... 0.969 0.984 1.   ]\n                        drift           0\n                        mean            [0. 0. 0. ... 0. 0. 0.]\n                        covariance      [[0.016 0.016 0.016 ... 0.016 0.016 0.016]\n                                         [0.016 0.031 0.031 ... 0.031 0.031 0.031]\n                                         [0.016 0.031 0.047 ... 0.047 0.047 0.047]\n                                         ...\n                                         [0.016 0.031 0.047 ... 0.969 0.969 0.969]\n                                         [0.016 0.031 0.047 ... 0.969 0.984 0.984]\n                                         [0.016 0.031 0.047 ... 0.969 0.984 1.   ]]\n                        decomp_type     PCA\nIIDStdUniform (AbstractIIDDiscreteDistribution)\n    d               2^(6)\n    replications    1\n    entropy         14445467058155448935378672762555839489\n</pre> In\u00a0[23]: Copied! <pre>giles_MLMC_stop = qmcpy.CubMCML(payoff,abs_tol=abs_tol)\nsolution,data = giles_MLMC_stop.integrate()\nprint(data)\n</pre> giles_MLMC_stop = qmcpy.CubMCML(payoff,abs_tol=abs_tol) solution,data = giles_MLMC_stop.integrate() print(data) <pre>Data (Data)\n    solution        3.681\n    n_total         430326\n    levels          3\n    n_level         [341502  61469  27319]\n    mean_level      [3.589 0.071 0.021]\n    var_level       [39.742  2.575  0.665]\n    cost_per_sample [2. 4. 8.]\n    alpha           1.769\n    beta            1.953\n    gamma           1.000\n    time_integrate  0.077\nCubMLMC (AbstractStoppingCriterion)\n    rmse_tol        0.019\n    n_init          2^(8)\n    levels_min      2^(1)\n    levels_max      10\n    theta           2^(-1)\nAsianOption (AbstractIntegrand)\n    option          ASIAN\n    call_put        CALL\n    volatility      2^(-1)\n    start_price     30\n    strike_price    30\n    interest_rate   0.050\n    t_final         1\n    asian_mean      ARITHMETIC\nBrownianMotion (AbstractTrueMeasure)\n    time_vec        [0.016 0.031 0.047 ... 0.969 0.984 1.   ]\n    drift           0\n    mean            [0. 0. 0. ... 0. 0. 0.]\n    covariance      [[0.016 0.016 0.016 ... 0.016 0.016 0.016]\n                     [0.016 0.031 0.031 ... 0.031 0.031 0.031]\n                     [0.016 0.031 0.047 ... 0.047 0.047 0.047]\n                     ...\n                     [0.016 0.031 0.047 ... 0.969 0.969 0.969]\n                     [0.016 0.031 0.047 ... 0.969 0.984 0.984]\n                     [0.016 0.031 0.047 ... 0.969 0.984 1.   ]]\n    decomp_type     PCA\n    transform       BrownianMotion (AbstractTrueMeasure)\n                        time_vec        [0.016 0.031 0.047 ... 0.969 0.984 1.   ]\n                        drift           0\n                        mean            [0. 0. 0. ... 0. 0. 0.]\n                        covariance      [[0.016 0.016 0.016 ... 0.016 0.016 0.016]\n                                         [0.016 0.031 0.031 ... 0.031 0.031 0.031]\n                                         [0.016 0.031 0.047 ... 0.047 0.047 0.047]\n                                         ...\n                                         [0.016 0.031 0.047 ... 0.969 0.969 0.969]\n                                         [0.016 0.031 0.047 ... 0.969 0.984 0.984]\n                                         [0.016 0.031 0.047 ... 0.969 0.984 1.   ]]\n                        decomp_type     PCA\nIIDStdUniform (AbstractIIDDiscreteDistribution)\n    d               2^(6)\n    replications    1\n    entropy         14445467058155448935378672762555839489\n</pre> In\u00a0[24]: Copied! <pre>sobol_brownian = qmcpy.BrownianMotion(qmcpy.Sobol(n_time_steps))\nintegrand = qmcpy.AsianOption(sobol_brownian, **options)\nstopping_criterion = qmcpy.CubQMCSobolG(integrand,abs_tol = abs_tol)\nsolution,data = stopping_criterion.integrate()\nprint(data)\n</pre> sobol_brownian = qmcpy.BrownianMotion(qmcpy.Sobol(n_time_steps)) integrand = qmcpy.AsianOption(sobol_brownian, **options) stopping_criterion = qmcpy.CubQMCSobolG(integrand,abs_tol = abs_tol) solution,data = stopping_criterion.integrate() print(data) <pre>Data (Data)\n    solution        3.697\n    comb_bound_low  3.672\n    comb_bound_high 3.721\n    comb_bound_diff 0.049\n    comb_flags      1\n    n_total         2^(10)\n    n               2^(10)\n    time_integrate  0.003\nCubQMCNetG (AbstractStoppingCriterion)\n    abs_tol         0.050\n    rel_tol         0\n    n_init          2^(10)\n    n_limit         2^(35)\nAsianOption (AbstractIntegrand)\n    option          ASIAN\n    call_put        CALL\n    volatility      2^(-1)\n    start_price     30\n    strike_price    30\n    interest_rate   0.050\n    t_final         1\n    asian_mean      ARITHMETIC\nBrownianMotion (AbstractTrueMeasure)\n    time_vec        [0.016 0.031 0.047 ... 0.969 0.984 1.   ]\n    drift           0\n    mean            [0. 0. 0. ... 0. 0. 0.]\n    covariance      [[0.016 0.016 0.016 ... 0.016 0.016 0.016]\n                     [0.016 0.031 0.031 ... 0.031 0.031 0.031]\n                     [0.016 0.031 0.047 ... 0.047 0.047 0.047]\n                     ...\n                     [0.016 0.031 0.047 ... 0.969 0.969 0.969]\n                     [0.016 0.031 0.047 ... 0.969 0.984 0.984]\n                     [0.016 0.031 0.047 ... 0.969 0.984 1.   ]]\n    decomp_type     PCA\n    transform       BrownianMotion (AbstractTrueMeasure)\n                        time_vec        [0.016 0.031 0.047 ... 0.969 0.984 1.   ]\n                        drift           0\n                        mean            [0. 0. 0. ... 0. 0. 0.]\n                        covariance      [[0.016 0.016 0.016 ... 0.016 0.016 0.016]\n                                         [0.016 0.031 0.031 ... 0.031 0.031 0.031]\n                                         [0.016 0.031 0.047 ... 0.047 0.047 0.047]\n                                         ...\n                                         [0.016 0.031 0.047 ... 0.969 0.969 0.969]\n                                         [0.016 0.031 0.047 ... 0.969 0.984 0.984]\n                                         [0.016 0.031 0.047 ... 0.969 0.984 1.   ]]\n                        decomp_type     PCA\nDigitalNetB2 (AbstractLDDiscreteDistribution)\n    d               2^(6)\n    replications    1\n    randomize       LMS DS\n    gen_mats_source joe_kuo.6.21201.txt\n    order           RADICAL INVERSE\n    t               63\n    alpha           1\n    n_limit         2^(32)\n    entropy         267658111069601541207388819540918772545\n</pre> In\u00a0[25]: Copied! <pre>abs_tol = .001\nn_time_steps = 64\n\noptions = {\n    'interest_rate': .05,\n    'volatility': .5,\n    'start_price': 30,\n    'strike_price': 40  #a larger strike price than before\n}\n</pre> abs_tol = .001 n_time_steps = 64  options = {     'interest_rate': .05,     'volatility': .5,     'start_price': 30,     'strike_price': 40  #a larger strike price than before } <p>First we price it as above using single level QMC.</p> In\u00a0[26]: Copied! <pre>sobol_brownian = qmcpy.BrownianMotion(qmcpy.Sobol(n_time_steps))\nintegrand = qmcpy.AsianOption(sobol_brownian, **options)\nstopping_criterion = qmcpy.CubQMCSobolG(integrand,abs_tol = abs_tol)\nsolution,data = stopping_criterion.integrate()\nprint(data)\n</pre> sobol_brownian = qmcpy.BrownianMotion(qmcpy.Sobol(n_time_steps)) integrand = qmcpy.AsianOption(sobol_brownian, **options) stopping_criterion = qmcpy.CubQMCSobolG(integrand,abs_tol = abs_tol) solution,data = stopping_criterion.integrate() print(data) <pre>Data (Data)\n    solution        1.018\n    comb_bound_low  1.017\n    comb_bound_high 1.019\n    comb_bound_diff 0.002\n    comb_flags      1\n    n_total         2^(15)\n    n               2^(15)\n    time_integrate  0.118\nCubQMCNetG (AbstractStoppingCriterion)\n    abs_tol         0.001\n    rel_tol         0\n    n_init          2^(10)\n    n_limit         2^(35)\nAsianOption (AbstractIntegrand)\n    option          ASIAN\n    call_put        CALL\n    volatility      2^(-1)\n    start_price     30\n    strike_price    40\n    interest_rate   0.050\n    t_final         1\n    asian_mean      ARITHMETIC\nBrownianMotion (AbstractTrueMeasure)\n    time_vec        [0.016 0.031 0.047 ... 0.969 0.984 1.   ]\n    drift           0\n    mean            [0. 0. 0. ... 0. 0. 0.]\n    covariance      [[0.016 0.016 0.016 ... 0.016 0.016 0.016]\n                     [0.016 0.031 0.031 ... 0.031 0.031 0.031]\n                     [0.016 0.031 0.047 ... 0.047 0.047 0.047]\n                     ...\n                     [0.016 0.031 0.047 ... 0.969 0.969 0.969]\n                     [0.016 0.031 0.047 ... 0.969 0.984 0.984]\n                     [0.016 0.031 0.047 ... 0.969 0.984 1.   ]]\n    decomp_type     PCA\n    transform       BrownianMotion (AbstractTrueMeasure)\n                        time_vec        [0.016 0.031 0.047 ... 0.969 0.984 1.   ]\n                        drift           0\n                        mean            [0. 0. 0. ... 0. 0. 0.]\n                        covariance      [[0.016 0.016 0.016 ... 0.016 0.016 0.016]\n                                         [0.016 0.031 0.031 ... 0.031 0.031 0.031]\n                                         [0.016 0.031 0.047 ... 0.047 0.047 0.047]\n                                         ...\n                                         [0.016 0.031 0.047 ... 0.969 0.969 0.969]\n                                         [0.016 0.031 0.047 ... 0.969 0.984 0.984]\n                                         [0.016 0.031 0.047 ... 0.969 0.984 1.   ]]\n                        decomp_type     PCA\nDigitalNetB2 (AbstractLDDiscreteDistribution)\n    d               2^(6)\n    replications    1\n    randomize       LMS DS\n    gen_mats_source joe_kuo.6.21201.txt\n    order           RADICAL INVERSE\n    t               63\n    alpha           1\n    n_limit         2^(32)\n    entropy         68741035845447190293894879002444698271\n</pre> <p>Next, we introduce an upward drift in the Brownian motion, which produces more stock price paths with positive payoffs.  This produces a smaller varation in the integrand and a generally faster run time.  (There still remains the question of how to automatically choose an optimal drift.)</p> In\u00a0[27]: Copied! <pre>sobol_drift_brownian = qmcpy.BrownianMotion(qmcpy.Sobol(n_time_steps), drift = 0.5)\nintegrand = qmcpy.AsianOption(sobol_drift_brownian, **options)\nstopping_criterion = qmcpy.CubQMCSobolG(integrand,abs_tol = abs_tol)\nsolution,data = stopping_criterion.integrate()\nprint(data)\n</pre> sobol_drift_brownian = qmcpy.BrownianMotion(qmcpy.Sobol(n_time_steps), drift = 0.5) integrand = qmcpy.AsianOption(sobol_drift_brownian, **options) stopping_criterion = qmcpy.CubQMCSobolG(integrand,abs_tol = abs_tol) solution,data = stopping_criterion.integrate() print(data) <pre>Data (Data)\n    solution        1.018\n    comb_bound_low  1.018\n    comb_bound_high 1.019\n    comb_bound_diff 0.001\n    comb_flags      1\n    n_total         2^(15)\n    n               2^(15)\n    time_integrate  0.115\nCubQMCNetG (AbstractStoppingCriterion)\n    abs_tol         0.001\n    rel_tol         0\n    n_init          2^(10)\n    n_limit         2^(35)\nAsianOption (AbstractIntegrand)\n    option          ASIAN\n    call_put        CALL\n    volatility      2^(-1)\n    start_price     30\n    strike_price    40\n    interest_rate   0.050\n    t_final         1\n    asian_mean      ARITHMETIC\nBrownianMotion (AbstractTrueMeasure)\n    time_vec        [0.016 0.031 0.047 ... 0.969 0.984 1.   ]\n    drift           0\n    mean            [0. 0. 0. ... 0. 0. 0.]\n    covariance      [[0.016 0.016 0.016 ... 0.016 0.016 0.016]\n                     [0.016 0.031 0.031 ... 0.031 0.031 0.031]\n                     [0.016 0.031 0.047 ... 0.047 0.047 0.047]\n                     ...\n                     [0.016 0.031 0.047 ... 0.969 0.969 0.969]\n                     [0.016 0.031 0.047 ... 0.969 0.984 0.984]\n                     [0.016 0.031 0.047 ... 0.969 0.984 1.   ]]\n    decomp_type     PCA\n    transform       BrownianMotion (AbstractTrueMeasure)\n                        time_vec        [0.016 0.031 0.047 ... 0.969 0.984 1.   ]\n                        drift           2^(-1)\n                        mean            [0.008 0.016 0.023 ... 0.484 0.492 0.5  ]\n                        covariance      [[0.016 0.016 0.016 ... 0.016 0.016 0.016]\n                                         [0.016 0.031 0.031 ... 0.031 0.031 0.031]\n                                         [0.016 0.031 0.047 ... 0.047 0.047 0.047]\n                                         ...\n                                         [0.016 0.031 0.047 ... 0.969 0.969 0.969]\n                                         [0.016 0.031 0.047 ... 0.969 0.984 0.984]\n                                         [0.016 0.031 0.047 ... 0.969 0.984 1.   ]]\n                        decomp_type     PCA\nDigitalNetB2 (AbstractLDDiscreteDistribution)\n    d               2^(6)\n    replications    1\n    randomize       LMS DS\n    gen_mats_source joe_kuo.6.21201.txt\n    order           RADICAL INVERSE\n    t               63\n    alpha           1\n    n_limit         2^(32)\n    entropy         195386499743864987571337198121595404839\n</pre> <p>Pause for questions</p> In\u00a0[28]: Copied! <pre>d=4; n=8\nx_gail = qmcpy.Lattice(d,order='linear',randomize=False).gen_samples(n,warn=False)\nprint('GAIL Samples')\nfor i in range(n): print(x_gail[i])\nx_mps = qmcpy.Lattice(d,order='natural',randomize=False).gen_samples(n,warn=False)\nprint('\\n\\nMPS Samples')\nfor i in range(n): print(x_mps[i])\nfig,ax = pyplot.subplots(nrows=1,ncols=2,figsize=(10,5.5))\nax[0].scatter(x_gail[0:n,0],x_gail[0:n,1],color='b')\nax[0].set_title('GAIL backend')\nax[1].scatter(x_mps[0:n,0],x_mps[0:n,1],color='b')\nax[1].set_title('MPS backend')\nfor ii in range(2):\n  ax[ii].set_xlim([0,1]); ax[ii].set_xticks([0,1]); ax[ii].set_xlabel('$x_{i,1}$')\n  ax[ii].set_ylim([0,1]); ax[ii].set_yticks([0,1]); ax[ii].set_ylabel('$x_{i,2}$')\n  ax[ii].set_aspect(1)\n</pre> d=4; n=8 x_gail = qmcpy.Lattice(d,order='linear',randomize=False).gen_samples(n,warn=False) print('GAIL Samples') for i in range(n): print(x_gail[i]) x_mps = qmcpy.Lattice(d,order='natural',randomize=False).gen_samples(n,warn=False) print('\\n\\nMPS Samples') for i in range(n): print(x_mps[i]) fig,ax = pyplot.subplots(nrows=1,ncols=2,figsize=(10,5.5)) ax[0].scatter(x_gail[0:n,0],x_gail[0:n,1],color='b') ax[0].set_title('GAIL backend') ax[1].scatter(x_mps[0:n,0],x_mps[0:n,1],color='b') ax[1].set_title('MPS backend') for ii in range(2):   ax[ii].set_xlim([0,1]); ax[ii].set_xticks([0,1]); ax[ii].set_xlabel('$x_{i,1}$')   ax[ii].set_ylim([0,1]); ax[ii].set_yticks([0,1]); ax[ii].set_ylabel('$x_{i,2}$')   ax[ii].set_aspect(1) <pre>GAIL Samples\n[0. 0. 0. 0.]\n[0.125 0.375 0.375 0.875]\n[0.25 0.75 0.75 0.75]\n[0.375 0.125 0.125 0.625]\n[0.5 0.5 0.5 0.5]\n[0.625 0.875 0.875 0.375]\n[0.75 0.25 0.25 0.25]\n[0.875 0.625 0.625 0.125]\n\n\nMPS Samples\n[0. 0. 0. 0.]\n[0.5 0.5 0.5 0.5]\n[0.25 0.75 0.75 0.75]\n[0.75 0.25 0.25 0.25]\n[0.125 0.375 0.375 0.875]\n[0.625 0.875 0.875 0.375]\n[0.375 0.125 0.125 0.625]\n[0.875 0.625 0.625 0.125]\n</pre> In\u00a0[29]: Copied! <pre>warnings.simplefilter('ignore')  #turn off warnings which stop execution in Colab\nn = 16\nldA = qmcpy.Lattice(2)\nldB = qmcpy.Lattice(2, randomize = False)\npointsA = ldA.gen_samples(n)  #construct some points\npointsB = ldB.gen_samples(n)  #construct some points\nprint(f'\\nRandomized LD Points with shape {pointsA.shape}\\n'+str(pointsA))  #these points have 15 significant digit precision but only three digits are shown\nprint(f'\\nNonrandomized LD Points with shape {pointsB.shape}\\n'+str(pointsB))  #these points have 15 significant digit precision but only three digits are shown\nfig,ax = pyplot.subplots(nrows=1,ncols=2,figsize=(10,5.5))\nax[0].scatter(pointsA[0:n,0],pointsA[0:n,1],color='b')\nax[1].scatter(pointsB[0:n,0],pointsB[0:n,1],color='b')\nfor ii in range(2):\n   ax[ii].set_xlim([0,1]); ax[ii].set_xticks([0,1]); ax[ii].set_xlabel('$x_{i,1}$')\n   ax[ii].set_ylim([0,1]); ax[ii].set_yticks([0,1]); ax[ii].set_ylabel('$x_{i,2}$')\n   ax[ii].set_aspect(1)\n</pre> warnings.simplefilter('ignore')  #turn off warnings which stop execution in Colab n = 16 ldA = qmcpy.Lattice(2) ldB = qmcpy.Lattice(2, randomize = False) pointsA = ldA.gen_samples(n)  #construct some points pointsB = ldB.gen_samples(n)  #construct some points print(f'\\nRandomized LD Points with shape {pointsA.shape}\\n'+str(pointsA))  #these points have 15 significant digit precision but only three digits are shown print(f'\\nNonrandomized LD Points with shape {pointsB.shape}\\n'+str(pointsB))  #these points have 15 significant digit precision but only three digits are shown fig,ax = pyplot.subplots(nrows=1,ncols=2,figsize=(10,5.5)) ax[0].scatter(pointsA[0:n,0],pointsA[0:n,1],color='b') ax[1].scatter(pointsB[0:n,0],pointsB[0:n,1],color='b') for ii in range(2):    ax[ii].set_xlim([0,1]); ax[ii].set_xticks([0,1]); ax[ii].set_xlabel('$x_{i,1}$')    ax[ii].set_ylim([0,1]); ax[ii].set_yticks([0,1]); ax[ii].set_ylabel('$x_{i,2}$')    ax[ii].set_aspect(1) <pre>\nRandomized LD Points with shape (16, 2)\n[[0.00944244 0.29799738]\n [0.50944244 0.79799738]\n [0.25944244 0.04799738]\n [0.75944244 0.54799738]\n [0.13444244 0.67299738]\n [0.63444244 0.17299738]\n [0.38444244 0.42299738]\n [0.88444244 0.92299738]\n [0.07194244 0.98549738]\n [0.57194244 0.48549738]\n [0.32194244 0.73549738]\n [0.82194244 0.23549738]\n [0.19694244 0.36049738]\n [0.69694244 0.86049738]\n [0.44694244 0.11049738]\n [0.94694244 0.61049738]]\n\nNonrandomized LD Points with shape (16, 2)\n[[0.     0.    ]\n [0.5    0.5   ]\n [0.25   0.75  ]\n [0.75   0.25  ]\n [0.125  0.375 ]\n [0.625  0.875 ]\n [0.375  0.125 ]\n [0.875  0.625 ]\n [0.0625 0.6875]\n [0.5625 0.1875]\n [0.3125 0.4375]\n [0.8125 0.9375]\n [0.1875 0.0625]\n [0.6875 0.5625]\n [0.4375 0.8125]\n [0.9375 0.3125]]\n</pre> In\u00a0[30]: Copied! <pre>warnings.simplefilter('ignore')  #turn off warnings which stop execution in Colab\nld = qmcpy.Lattice(6,randomize=False)\npoints = ld.gen_samples(4)  #construct some points\nprint(f'\\nLD Points with shape {points.shape}\\n'+str(points))  #these points have 15 significant digit precision but only three digits are shown\n</pre> warnings.simplefilter('ignore')  #turn off warnings which stop execution in Colab ld = qmcpy.Lattice(6,randomize=False) points = ld.gen_samples(4)  #construct some points print(f'\\nLD Points with shape {points.shape}\\n'+str(points))  #these points have 15 significant digit precision but only three digits are shown <pre>\nLD Points with shape (4, 6)\n[[0.   0.   0.   0.   0.   0.  ]\n [0.5  0.5  0.5  0.5  0.5  0.5 ]\n [0.25 0.75 0.75 0.75 0.25 0.75]\n [0.75 0.25 0.25 0.25 0.75 0.25]]\n</pre> In\u00a0[31]: Copied! <pre>warnings.simplefilter('ignore')  #turn off warnings which stop execution in Colab\nld = qmcpy.Lattice(6,randomize=False)\nunif_points = ld.gen_samples(2)  #construct some points\nprint(f'\\nLD Points with shape {unif_points.shape}\\n'+str(unif_points))\nld_gauss = qmcpy.Gaussian(ld)\ngauss_points = ld_gauss.gen_samples(2)\nprint(f'\\nLD Points with shape {gauss_points.shape}\\n'+str(gauss_points))\n</pre> warnings.simplefilter('ignore')  #turn off warnings which stop execution in Colab ld = qmcpy.Lattice(6,randomize=False) unif_points = ld.gen_samples(2)  #construct some points print(f'\\nLD Points with shape {unif_points.shape}\\n'+str(unif_points)) ld_gauss = qmcpy.Gaussian(ld) gauss_points = ld_gauss.gen_samples(2) print(f'\\nLD Points with shape {gauss_points.shape}\\n'+str(gauss_points)) <pre>\nLD Points with shape (2, 6)\n[[0.  0.  0.  0.  0.  0. ]\n [0.5 0.5 0.5 0.5 0.5 0.5]]\n\nLD Points with shape (2, 6)\n[[nan nan nan nan nan nan]\n [ 0.  0.  0.  0.  0.  0.]]\n</pre> In\u00a0[32]: Copied! <pre>solution_G_fixed = qmcpy.CubQMCSobolG(qmcpy.Keister(qmcpy.Gaussian(qmcpy.Sobol(2,seed=47),covariance=1/2))).integrate()[0]\nsolution_CLT_fixed = qmcpy.CubQMCCLT(qmcpy.Keister(qmcpy.Gaussian(qmcpy.Sobol(2,seed=47,replications=15),covariance=1/2))).integrate()[0]\nsolution_G = qmcpy.CubQMCSobolG(qmcpy.Keister(qmcpy.Gaussian(qmcpy.Sobol(2),covariance=1/2))).integrate()[0]\nsolution_CLT = qmcpy.CubQMCCLT(qmcpy.Keister(qmcpy.Gaussian(qmcpy.Sobol(2,replications=15),covariance=1/2))).integrate()[0]\nprint(f'Solution for GAIL stopping criterion with FIXED seed {solution_G_fixed}')\nprint(f'Solution for CLT stopping criterion with FIXED seed {solution_CLT_fixed}')\nprint(f'Solution for GAIL stopping criterion {solution_G}')\nprint(f'Solution for CLT stopping criterion {solution_CLT}')\n</pre> solution_G_fixed = qmcpy.CubQMCSobolG(qmcpy.Keister(qmcpy.Gaussian(qmcpy.Sobol(2,seed=47),covariance=1/2))).integrate()[0] solution_CLT_fixed = qmcpy.CubQMCCLT(qmcpy.Keister(qmcpy.Gaussian(qmcpy.Sobol(2,seed=47,replications=15),covariance=1/2))).integrate()[0] solution_G = qmcpy.CubQMCSobolG(qmcpy.Keister(qmcpy.Gaussian(qmcpy.Sobol(2),covariance=1/2))).integrate()[0] solution_CLT = qmcpy.CubQMCCLT(qmcpy.Keister(qmcpy.Gaussian(qmcpy.Sobol(2,replications=15),covariance=1/2))).integrate()[0] print(f'Solution for GAIL stopping criterion with FIXED seed {solution_G_fixed}') print(f'Solution for CLT stopping criterion with FIXED seed {solution_CLT_fixed}') print(f'Solution for GAIL stopping criterion {solution_G}') print(f'Solution for CLT stopping criterion {solution_CLT}') <pre>Solution for GAIL stopping criterion with FIXED seed 1.8088685362448778\nSolution for CLT stopping criterion with FIXED seed 1.806594507374441\nSolution for GAIL stopping criterion 1.807944316997558\nSolution for CLT stopping criterion 1.80672801265591\n</pre> In\u00a0[33]: Copied! <pre>help(qmcpy.Sobol)\ndir(qmcpy.Sobol)\n</pre> help(qmcpy.Sobol) dir(qmcpy.Sobol) <pre>Help on class DigitalNetB2 in module qmcpy.discrete_distribution.digital_net_b2.digital_net_b2:\n\nclass DigitalNetB2(qmcpy.discrete_distribution.abstract_discrete_distribution.AbstractLDDiscreteDistribution)\n |  DigitalNetB2(dimension=1, replications=None, seed=None, randomize='LMS DS', generating_matrices='joe_kuo.6.21201.txt', order='RADICAL INVERSE', t=63, alpha=1, msb=None, _verbose=False, graycode=None, t_max=None, t_lms=None)\n |\n |  Low discrepancy digital net in base 2.\n |\n |  Note:\n |      - Digital net sample sizes should be powers of $2$ e.g. $1$, $2$, $4$, $8$, $16$, $\\dots$.\n |      - The first point of an unrandomized digital nets is the origin.\n |      - `Sobol` is an alias for `DigitalNetB2`.\n |      - To use higher order digital nets, either:\n |\n |          - Pass in `generating_matrices` *without* interlacing and supply `alpha`&gt;1 to apply interlacing, or\n |          - Pass in `generating_matrices` *with* interlacing and set `alpha=1` to avoid additional interlacing\n |\n |          i.e. do *not* pass in interlaced `generating_matrices` and set `alpha&gt;1`, this will apply additional interlacing.\n |\n |  Examples:\n |      &gt;&gt;&gt; discrete_distrib = DigitalNetB2(2,seed=7)\n |      &gt;&gt;&gt; discrete_distrib(4)\n |      array([[0.72162356, 0.914955  ],\n |             [0.16345554, 0.42964856],\n |             [0.98676255, 0.03436384],\n |             [0.42956655, 0.55876342]])\n |      &gt;&gt;&gt; discrete_distrib(1) # first point in the sequence\n |      array([[0.72162356, 0.914955  ]])\n |      &gt;&gt;&gt; discrete_distrib\n |      DigitalNetB2 (AbstractLDDiscreteDistribution)\n |          d               2^(1)\n |          replications    1\n |          randomize       LMS DS\n |          gen_mats_source joe_kuo.6.21201.txt\n |          order           RADICAL INVERSE\n |          t               63\n |          alpha           1\n |          n_limit         2^(32)\n |          entropy         7\n |\n |      Replications of independent randomizations\n |\n |      &gt;&gt;&gt; x = DigitalNetB2(dimension=3,seed=7,replications=2)(4)\n |      &gt;&gt;&gt; x.shape\n |      (2, 4, 3)\n |      &gt;&gt;&gt; x\n |      array([[[0.24653277, 0.1821862 , 0.74732591],\n |              [0.68152903, 0.66169442, 0.42891961],\n |              [0.48139855, 0.79818233, 0.08201287],\n |              [0.91541325, 0.29520621, 0.77495809]],\n |      &lt;BLANKLINE&gt;\n |             [[0.44876891, 0.85899604, 0.50549679],\n |              [0.53635924, 0.04353443, 0.33564946],\n |              [0.23214143, 0.29281506, 0.06841036],\n |              [0.75295715, 0.60241448, 0.76962976]]])\n |\n |      Different orderings (avoid warnings that the first point is the origin)\n |\n |      &gt;&gt;&gt; DigitalNetB2(dimension=2,randomize=False,order=\"GRAY\")(n_min=2,n_max=4,warn=False)\n |      array([[0.75, 0.25],\n |             [0.25, 0.75]])\n |      &gt;&gt;&gt; DigitalNetB2(dimension=2,randomize=False,order=\"RADICAL INVERSE\")(n_min=2,n_max=4,warn=False)\n |      array([[0.25, 0.75],\n |             [0.75, 0.25]])\n |\n |      Generating matrices from [https://github.com/QMCSoftware/LDData/tree/main/dnet](https://github.com/QMCSoftware/LDData/tree/main/dnet)\n |\n |      &gt;&gt;&gt; DigitalNetB2(dimension=3,randomize=False,generating_matrices=\"mps.nx_s5_alpha2_m32.txt\")(8,warn=False)\n |      array([[0.        , 0.        , 0.        ],\n |             [0.75841841, 0.45284834, 0.48844557],\n |             [0.57679828, 0.13226272, 0.10061957],\n |             [0.31858402, 0.32113875, 0.39369111],\n |             [0.90278927, 0.45867532, 0.01803333],\n |             [0.14542431, 0.02548793, 0.4749614 ],\n |             [0.45587539, 0.33081476, 0.11474426],\n |             [0.71318879, 0.15377192, 0.37629925]])\n |\n |      All randomizations\n |\n |      &gt;&gt;&gt; DigitalNetB2(dimension=3,randomize='LMS DS',seed=5)(8)\n |      array([[0.69346401, 0.20118185, 0.64779396],\n |             [0.43998032, 0.90102467, 0.0936172 ],\n |             [0.86663563, 0.60910036, 0.26043276],\n |             [0.11327376, 0.30772653, 0.93959283],\n |             [0.62102883, 0.79169756, 0.77051637],\n |             [0.37451038, 0.1231324 , 0.46634012],\n |             [0.94785596, 0.38577413, 0.13377215],\n |             [0.20121617, 0.71843325, 0.56293458]])\n |      &gt;&gt;&gt; DigitalNetB2(dimension=3,randomize='LMS',seed=5)(8,warn=False)\n |      array([[0.        , 0.        , 0.        ],\n |             [0.75446077, 0.83265937, 0.69584079],\n |             [0.42329494, 0.65793842, 0.90427279],\n |             [0.67763292, 0.48937304, 0.33344964],\n |             [0.18550714, 0.97332905, 0.3772791 ],\n |             [0.93104851, 0.17195496, 0.82311652],\n |             [0.26221346, 0.31742386, 0.53093284],\n |             [0.50787715, 0.5172669 , 0.2101083 ]])\n |      &gt;&gt;&gt; DigitalNetB2(dimension=3,randomize='DS',seed=5)(8)\n |      array([[0.68383949, 0.04047995, 0.42903182],\n |             [0.18383949, 0.54047995, 0.92903182],\n |             [0.93383949, 0.79047995, 0.67903182],\n |             [0.43383949, 0.29047995, 0.17903182],\n |             [0.55883949, 0.66547995, 0.05403182],\n |             [0.05883949, 0.16547995, 0.55403182],\n |             [0.80883949, 0.41547995, 0.80403182],\n |             [0.30883949, 0.91547995, 0.30403182]])\n |      &gt;&gt;&gt; DigitalNetB2(dimension=3,randomize='OWEN',seed=5)(8)\n |      array([[0.33595486, 0.05834975, 0.30066401],\n |             [0.89110875, 0.84905188, 0.81833285],\n |             [0.06846074, 0.59997956, 0.67064205],\n |             [0.6693703 , 0.25824002, 0.10469644],\n |             [0.44586618, 0.99161977, 0.1873488 ],\n |             [0.84245267, 0.16445553, 0.56544372],\n |             [0.18546359, 0.44859876, 0.97389524],\n |             [0.61215442, 0.64341386, 0.44529863]])\n |\n |      Higher order net without randomization\n |\n |      &gt;&gt;&gt; DigitalNetB2(dimension=3,randomize='FALSE',seed=7,alpha=2)(4,warn=False)\n |      array([[0.    , 0.    , 0.    ],\n |             [0.75  , 0.75  , 0.75  ],\n |             [0.4375, 0.9375, 0.1875],\n |             [0.6875, 0.1875, 0.9375]])\n |\n |\n |      Higher order nets with randomizations and replications\n |\n |      &gt;&gt;&gt; DigitalNetB2(dimension=3,randomize='LMS DS',seed=7,replications=2,alpha=2)(4,warn=False)\n |      array([[[0.42955149, 0.89149058, 0.43867111],\n |              [0.68701828, 0.07601148, 0.51312447],\n |              [0.10088033, 0.16293661, 0.25144138],\n |              [0.85846252, 0.87103178, 0.70041789]],\n |      &lt;BLANKLINE&gt;\n |             [[0.27151905, 0.42406763, 0.21917369],\n |              [0.55035224, 0.67864387, 0.90033876],\n |              [0.19356758, 0.57589964, 0.00347701],\n |              [0.97235125, 0.32168581, 0.86920948]]])\n |      &gt;&gt;&gt; DigitalNetB2(dimension=3,randomize='LMS',seed=7,replications=2,alpha=2)(4,warn=False)\n |      array([[[0.        , 0.        , 0.        ],\n |              [0.75817062, 0.96603053, 0.94947625],\n |              [0.45367986, 0.80295638, 0.18778553],\n |              [0.71171791, 0.2295424 , 0.76175441]],\n |      &lt;BLANKLINE&gt;\n |             [[0.        , 0.        , 0.        ],\n |              [0.78664636, 0.75470215, 0.86876474],\n |              [0.45336727, 0.99953621, 0.22253579],\n |              [0.73996397, 0.24544824, 0.9008679 ]]])\n |      &gt;&gt;&gt; DigitalNetB2(dimension=3,randomize='DS',seed=7,replications=2,alpha=2)(4)\n |      array([[[0.04386058, 0.58727432, 0.3691824 ],\n |              [0.79386058, 0.33727432, 0.6191824 ],\n |              [0.48136058, 0.39977432, 0.4316824 ],\n |              [0.73136058, 0.64977432, 0.6816824 ]],\n |      &lt;BLANKLINE&gt;\n |             [[0.65212985, 0.69669968, 0.10605352],\n |              [0.40212985, 0.44669968, 0.85605352],\n |              [0.83962985, 0.25919968, 0.16855352],\n |              [0.08962985, 0.50919968, 0.91855352]]])\n |      &gt;&gt;&gt; DigitalNetB2(dimension=3,randomize='OWEN',seed=7,replications=2,alpha=2)(4)\n |      array([[[0.46368517, 0.03964427, 0.62172094],\n |              [0.7498683 , 0.76141348, 0.4243043 ],\n |              [0.01729754, 0.97968459, 0.65963223],\n |              [0.75365329, 0.1903774 , 0.34141493]],\n |      &lt;BLANKLINE&gt;\n |             [[0.52252547, 0.5679709 , 0.05949112],\n |              [0.27248656, 0.36488289, 0.81844058],\n |              [0.94219959, 0.39172304, 0.20285965],\n |              [0.19716391, 0.64741585, 0.92494554]]])\n |\n |  **References:**\n |\n |  1.  Marius Hofert and Christiane Lemieux.\n |      qrng: (Randomized) Quasi-Random Number Generators (2019).\n |      R package version 0.0-7.\n |      [https://CRAN.R-project.org/package=qrng](https://CRAN.R-project.org/package=qrng).\n |\n |  2.  Faure, Henri, and Christiane Lemieux.\n |      Implementation of Irreducible Sobol' Sequences in Prime Power Bases.\n |      Mathematics and Computers in Simulation 161 (2019): 13-22. Crossref. Web.\n |\n |  3.  F.Y. Kuo, D. Nuyens.\n |      Application of quasi-Monte Carlo methods to elliptic PDEs with random diffusion coefficients \\- a survey of analysis and implementation.\n |      Foundations of Computational Mathematics, 16(6):1631-1696, 2016.\n |      [https://link.springer.com/article/10.1007/s10208-016-9329-5](https://link.springer.com/article/10.1007/s10208-016-9329-5).\n |\n |  4.  D. Nuyens.\n |      The Magic Point Shop of QMC point generators and generating vectors.\n |      MATLAB and Python software, 2018.\n |      [https://people.cs.kuleuven.be/~dirk.nuyens/](https://people.cs.kuleuven.be/~dirk.nuyens/).\n |\n |  5.  R. Cools, F.Y. Kuo, D. Nuyens.\n |      Constructing embedded lattice rules for multivariate integration.\n |      SIAM J. Sci. Comput., 28(6), 2162-2188.\n |\n |  6.  I.M. Sobol', V.I. Turchaninov, Yu.L. Levitan, B.V. Shukhman.\n |      Quasi-Random Sequence Generators.\n |      Keldysh Institute of Applied Mathematics.\n |      Russian Academy of Sciences, Moscow (1992).\n |\n |  7.  Sobol, Ilya &amp; Asotsky, Danil &amp; Kreinin, Alexander &amp; Kucherenko, Sergei. (2011).\n |      Construction and Comparison of High-Dimensional Sobol' Generators. Wilmott. 2011.\n |      [10.1002/wilm.10056](https://onlinelibrary.wiley.com/doi/abs/10.1002/wilm.10056).\n |\n |  8.  Paul Bratley and Bennett L. Fox.\n |      Algorithm 659: Implementing Sobol's quasirandom sequence generator.\n |      ACM Trans. Math. Softw. 14, 1 (March 1988), 88-100. 1988.\n |      [https://doi.org/10.1145/42288.214372](https://doi.org/10.1145/42288.2143720).\n |\n |  Method resolution order:\n |      DigitalNetB2\n |      qmcpy.discrete_distribution.abstract_discrete_distribution.AbstractLDDiscreteDistribution\n |      qmcpy.discrete_distribution.abstract_discrete_distribution.AbstractDiscreteDistribution\n |      builtins.object\n |\n |  Methods defined here:\n |\n |  __init__(self, dimension=1, replications=None, seed=None, randomize='LMS DS', generating_matrices='joe_kuo.6.21201.txt', order='RADICAL INVERSE', t=63, alpha=1, msb=None, _verbose=False, graycode=None, t_max=None, t_lms=None)\n |      Args:\n |          dimension (Union[int,np.ndarray]): Dimension of the generator.\n |\n |              - If an `int` is passed in, use generating vector components at indices 0,...,`dimension`-1.\n |              - If an `np.ndarray` is passed in, use generating vector components at these indices.\n |\n |          replications (int): Number of independent randomizations of a pointset.\n |          seed (Union[None,int,np.random.SeedSeq): Seed the random number generator for reproducibility.\n |          randomize (str): Options are\n |\n |              - `'LMS DS'`: Linear matrix scramble with digital shift.\n |              - `'LMS'`: Linear matrix scramble only.\n |              - `'DS'`: Digital shift only.\n |              - `'NUS'`: Nested uniform scrambling. Also known as Owen scrambling.\n |              - `'FALSE'`: No randomization. In this case the first point will be the origin.\n |\n |          generating_matrices (Union[str,np.ndarray,int]: Specify the generating matrices.\n |\n |              - A `str` should be the name (or path) of a file from the LDData repo at [https://github.com/QMCSoftware/LDData/tree/main/dnet](https://github.com/QMCSoftware/LDData/tree/main/dnet).\n |              - An `np.ndarray` of integers with shape $(d,m_\\mathrm{max})$ or $(r,d,m_\\mathrm{max})$ where $d$ is the number of dimensions, $r$ is the number of replications, and $2^{m_\\mathrm{max}}$ is the maximum number of supported points. Setting `msb=False` will flip the bits of ints in the generating matrices.\n |\n |          order (str): `'RADICAL INVERSE'`, or `'GRAY'` ordering. See the doctest example above.\n |          t (int): Number of bits in integer represetation of points *after* randomization. The number of bits in the generating matrices is inferred based on the largest value.\n |          alpha (int): Interlacing factor for higher order nets.\n |              When `alpha`&gt;1, interlacing is performed regardless of the generating matrices,\n |              i.e., for `alpha`&gt;1 do *not* pass in generating matrices which are already interlaced.\n |              The Note for this class contains more info.\n |          msb (bool): Flag for Most Significant Bit (MSB) vs Least Significant Bit (LSB) integer representations in generating matrices. If `msb=False` (LSB order), then integers in generating matrices will be bit-reversed.\n |          _verbose (bool): If `True`, print linear matrix scrambling matrices.\n |\n |  ----------------------------------------------------------------------\n |  Methods inherited from qmcpy.discrete_distribution.abstract_discrete_distribution.AbstractLDDiscreteDistribution:\n |\n |  __repr__(self)\n |      Return repr(self).\n |\n |  ----------------------------------------------------------------------\n |  Methods inherited from qmcpy.discrete_distribution.abstract_discrete_distribution.AbstractDiscreteDistribution:\n |\n |  __call__(self, n=None, n_min=None, n_max=None, return_binary=False, warn=True)\n |      - If just `n` is supplied, generate samples from the sequence at indices 0,...,`n`-1.\n |      - If `n_min` and `n_max` are supplied, generate samples from the sequence at indices `n_min`,...,`n_max`-1.\n |      - If `n` and `n_min` are supplied, then generate samples from the sequence at indices `n`,...,`n_min`-1.\n |\n |      Args:\n |          n (Union[None,int]): Number of points to generate.\n |          n_min (Union[None,int]): Starting index of sequence.\n |          n_max (Union[None,int]): Final index of sequence.\n |          return_binary (bool): Only used for `DigitalNetB2`.\n |              If `True`, *only* return the integer representation `x_integer` of base 2 digital net.\n |          warn (bool): If `False`, disable warnings when generating samples.\n |\n |      Returns:\n |          x (np.ndarray): Samples from the sequence.\n |\n |              - If `replications` is `None` then this will be of size (`n_max`-`n_min`) $\\times$ `dimension`\n |              - If `replications` is a positive int, then `x` will be of size `replications` $\\times$ (`n_max`-`n_min`) $\\times$ `dimension`\n |\n |              Note that if `return_binary=True` then `x` is returned where `x` are integer representations of the digital net points.\n |\n |  gen_samples(self, n=None, n_min=None, n_max=None, return_binary=False, warn=True)\n |\n |  pdf(self, x)\n |\n |  spawn(self, s=1, dimensions=None)\n |      Spawn new instances of the current discrete distribution but with new seeds and dimensions.\n |      Used by multi-level QMC algorithms which require different seeds and dimensions on each level.\n |\n |      Note:\n |          Use `replications` instead of using `spawn` when possible, e.g., when spawning copies which all have the same dimension.\n |\n |      Args:\n |          s (int): Number of copies to spawn\n |          dimensions (np.ndarray): Length `s` array of dimensions for each copy. Defaults to the current dimension.\n |\n |      Returns:\n |          spawned_discrete_distribs (list): Discrete distributions with new seeds and dimensions.\n |\n |  ----------------------------------------------------------------------\n |  Data descriptors inherited from qmcpy.discrete_distribution.abstract_discrete_distribution.AbstractDiscreteDistribution:\n |\n |  __dict__\n |      dictionary for instance variables\n |\n |  __weakref__\n |      list of weak references to the object\n\n</pre> Out[33]: <pre>['__call__',\n '__class__',\n '__delattr__',\n '__dict__',\n '__dir__',\n '__doc__',\n '__eq__',\n '__format__',\n '__ge__',\n '__getattribute__',\n '__getstate__',\n '__gt__',\n '__hash__',\n '__init__',\n '__init_subclass__',\n '__le__',\n '__lt__',\n '__module__',\n '__ne__',\n '__new__',\n '__reduce__',\n '__reduce_ex__',\n '__repr__',\n '__setattr__',\n '__sizeof__',\n '__str__',\n '__subclasshook__',\n '__weakref__',\n '_gen_samples',\n '_spawn',\n 'gen_samples',\n 'pdf',\n 'spawn']</pre> In\u00a0[34]: Copied! <pre>sc = qmcpy.CubQMCLatticeG(qmcpy.Keister(qmcpy.Gaussian(qmcpy.Lattice(5,seed=7),covariance=1/2)),abs_tol=.01)\nprint(sc.integrate()[1],'\\n\\n')\nsc.set_tolerance(abs_tol=.001)  #changing the tolerance\nprint(sc.integrate()[1],'\\n\\n')\n</pre> sc = qmcpy.CubQMCLatticeG(qmcpy.Keister(qmcpy.Gaussian(qmcpy.Lattice(5,seed=7),covariance=1/2)),abs_tol=.01) print(sc.integrate()[1],'\\n\\n') sc.set_tolerance(abs_tol=.001)  #changing the tolerance print(sc.integrate()[1],'\\n\\n') <pre>Data (Data)\n    solution        1.135\n    comb_bound_low  1.129\n    comb_bound_high 1.142\n    comb_bound_diff 0.012\n    comb_flags      1\n    n_total         2^(13)\n    n               2^(13)\n    time_integrate  0.007\nCubQMCLatticeG (AbstractStoppingCriterion)\n    abs_tol         0.010\n    rel_tol         0\n    n_init          2^(10)\n    n_limit         2^(30)\nKeister (AbstractIntegrand)\nGaussian (AbstractTrueMeasure)\n    mean            0\n    covariance      2^(-1)\n    decomp_type     PCA\n    transform       Gaussian (AbstractTrueMeasure)\n                        mean            0\n                        covariance      2^(-1)\n                        decomp_type     PCA\nLattice (AbstractLDDiscreteDistribution)\n    d               5\n    replications    1\n    randomize       SHIFT\n    gen_vec_source  kuo.lattice-33002-1024-1048576.9125.txt\n    order           RADICAL INVERSE\n    n_limit         2^(20)\n    entropy         7 \n\n\nData (Data)\n    solution        1.136\n    comb_bound_low  1.135\n    comb_bound_high 1.136\n    comb_bound_diff 0.001\n    comb_flags      1\n    n_total         2^(17)\n    n               2^(17)\n    time_integrate  0.040\nCubQMCLatticeG (AbstractStoppingCriterion)\n    abs_tol         0.001\n    rel_tol         0\n    n_init          2^(10)\n    n_limit         2^(30)\nKeister (AbstractIntegrand)\nGaussian (AbstractTrueMeasure)\n    mean            0\n    covariance      2^(-1)\n    decomp_type     PCA\n    transform       Gaussian (AbstractTrueMeasure)\n                        mean            0\n                        covariance      2^(-1)\n                        decomp_type     PCA\nLattice (AbstractLDDiscreteDistribution)\n    d               5\n    replications    1\n    randomize       SHIFT\n    gen_vec_source  kuo.lattice-33002-1024-1048576.9125.txt\n    order           RADICAL INVERSE\n    n_limit         2^(20)\n    entropy         7 \n\n\n</pre> In\u00a0[35]: Copied! <pre>sc = qmcpy.CubQMCLatticeG(qmcpy.Keister(qmcpy.Gaussian(qmcpy.Lattice(5,seed=7),covariance=1/2)),abs_tol=.01)  #zero relative tolerance by default\nprint(sc.integrate()[1],'\\n\\n')\nsc.set_tolerance(abs_tol=0,rel_tol=0.005)  #nonzero relative tolerance and zero absolute tolerance\nprint(sc.integrate()[1])\n</pre> sc = qmcpy.CubQMCLatticeG(qmcpy.Keister(qmcpy.Gaussian(qmcpy.Lattice(5,seed=7),covariance=1/2)),abs_tol=.01)  #zero relative tolerance by default print(sc.integrate()[1],'\\n\\n') sc.set_tolerance(abs_tol=0,rel_tol=0.005)  #nonzero relative tolerance and zero absolute tolerance print(sc.integrate()[1]) <pre>Data (Data)\n    solution        1.135\n    comb_bound_low  1.129\n    comb_bound_high 1.142\n    comb_bound_diff 0.012\n    comb_flags      1\n    n_total         2^(13)\n    n               2^(13)\n    time_integrate  0.004\nCubQMCLatticeG (AbstractStoppingCriterion)\n    abs_tol         0.010\n    rel_tol         0\n    n_init          2^(10)\n    n_limit         2^(30)\nKeister (AbstractIntegrand)\nGaussian (AbstractTrueMeasure)\n    mean            0\n    covariance      2^(-1)\n    decomp_type     PCA\n    transform       Gaussian (AbstractTrueMeasure)\n                        mean            0\n                        covariance      2^(-1)\n                        decomp_type     PCA\nLattice (AbstractLDDiscreteDistribution)\n    d               5\n    replications    1\n    randomize       SHIFT\n    gen_vec_source  kuo.lattice-33002-1024-1048576.9125.txt\n    order           RADICAL INVERSE\n    n_limit         2^(20)\n    entropy         7 \n\n\nData (Data)\n    solution        1.135\n    comb_bound_low  1.131\n    comb_bound_high 1.139\n    comb_bound_diff 0.008\n    comb_flags      1\n    n_total         2^(14)\n    n               2^(14)\n    time_integrate  0.006\nCubQMCLatticeG (AbstractStoppingCriterion)\n    abs_tol         0\n    rel_tol         0.005\n    n_init          2^(10)\n    n_limit         2^(30)\nKeister (AbstractIntegrand)\nGaussian (AbstractTrueMeasure)\n    mean            0\n    covariance      2^(-1)\n    decomp_type     PCA\n    transform       Gaussian (AbstractTrueMeasure)\n                        mean            0\n                        covariance      2^(-1)\n                        decomp_type     PCA\nLattice (AbstractLDDiscreteDistribution)\n    d               5\n    replications    1\n    randomize       SHIFT\n    gen_vec_source  kuo.lattice-33002-1024-1048576.9125.txt\n    order           RADICAL INVERSE\n    n_limit         2^(20)\n    entropy         7\n</pre>"},{"location":"demos/talk_paper_demos/MCQMC_Tutorial_2020/MCQMC_2020_QMC_Software_Tutorial/#quasi-monte-carlo-qmc-software-in-qmcpy","title":"Quasi-Monte Carlo (QMC) Software in QMCPy\u00b6","text":"<p>A tutorial based on this notebook is at https://media.ed.ac.uk/playlist/dedicated/51612401/1_0z0wec2z/1_2k12mwiw.</p> <p>As an example of available QMC software, we introduce the Python library QMCPy.  This Jupyter notebook saves your typing.</p> <p>QMCPy is a community effort.  This early release includes contributions from</p> <ul> <li>Mike Giles MLMC and MLQMC software</li> <li>Marius Hofert and Christiane Lemieux's QRNG</li> <li>Pierre L'Ecuyer's Lattice Builder</li> <li>Dirk Nuyens's Magic Point Shop (MPS)</li> <li>Art Owen's Halton sequences</li> <li>PyTorch</li> <li>Guaranteed Automatic Integration Library (GAIL)</li> </ul> <p>and depends on the NumPy and SciPy Python packages.</p> <p>View the companion pdf slides at https://speakerdeck.com/fjhickernell/quasi-monte-carlo-software and the introductory QMCPy blog at https://qmcpy.wordpress.com.</p>"},{"location":"demos/talk_paper_demos/MCQMC_Tutorial_2020/MCQMC_2020_QMC_Software_Tutorial/#installation","title":"Installation\u00b6","text":"<p>QMCPy can be installed with <code>pip install qmcpy</code> or cloned from the  QMCSoftware GitHub repository.</p>"},{"location":"demos/talk_paper_demos/MCQMC_Tutorial_2020/MCQMC_2020_QMC_Software_Tutorial/#generating-low-discrepancy-ld-points-via-a-discretedistribution-object","title":"Generating low discrepancy (LD) points via a <code>DiscreteDistribution</code> object\u00b6","text":"<p>We generate some points used for quasi-Monte Carlo methods.  These points are called low discrepancy (LD for short) and are created as an instance of a <code>DiscreteDistribution</code> class.</p>"},{"location":"demos/talk_paper_demos/MCQMC_Tutorial_2020/MCQMC_2020_QMC_Software_Tutorial/#integration-lattices","title":"Integration lattices\u00b6","text":"<p>Here are some (randomly shifted) integration lattice points. This is a two step procees:</p> <p>i) construct the <code>DiscreteDistribution</code> object <code>lattice</code>, and then</p> <p>ii) construct a number of points from the sequence.</p> <p>The structure of these points favors <code>n</code> that is a power of 2.</p>"},{"location":"demos/talk_paper_demos/MCQMC_Tutorial_2020/MCQMC_2020_QMC_Software_Tutorial/#iid-uniform-points-do-not-fill-space-as-well","title":"IID uniform points do not fill space as well\u00b6","text":"<p>Contrast this with independent and identically distributed (IID) points.  Although successive points fill the square, they do so without knowledge of the others and produce clusters and gaps.  Think of it this way</p> <ul> <li>LD = evenly spread</li> <li>IID = points do not know about each other</li> </ul> <p>(Since the first parameter in the <code>DiscreteDistribution</code> object is the dimension, you need not identify it.)</p>"},{"location":"demos/talk_paper_demos/MCQMC_Tutorial_2020/MCQMC_2020_QMC_Software_Tutorial/#rows-and-columns-are-not-interchangeable-for-ld","title":"Rows and columns are not interchangeable for LD\u00b6","text":"<p>For LD sequences we must differentiate between the cooordinates of the point (column) and which point (row). The transpose of an LD array is not LD.  This differs from IID multivariate points with IID marginals.</p> <p>In the example below we reverse the roles of the rows and columns.  The transposed lattice points do not fill space at all, while the transposed IID points are as good as the originals.</p>"},{"location":"demos/talk_paper_demos/MCQMC_Tutorial_2020/MCQMC_2020_QMC_Software_Tutorial/#sobol-points","title":"Sobol' points\u00b6","text":"<p>Another LD sequence is the Sobol' points.  Again, new points fill in the gaps between existing points. Since these are randomly digitally shifted Sobol' points, rerunning this command gives a different set of points.  For Sobol' points as well, <code>n</code> should normally be a power of 2.</p>"},{"location":"demos/talk_paper_demos/MCQMC_Tutorial_2020/MCQMC_2020_QMC_Software_Tutorial/#halton-points","title":"Halton Points\u00b6","text":"<p>A third kind of LD sequence is Halton points, which are also randomized. Again, new points fill in the gaps between existing points.  For Halton points there are no favored numbers of points.</p>"},{"location":"demos/talk_paper_demos/MCQMC_Tutorial_2020/MCQMC_2020_QMC_Software_Tutorial/#timing","title":"Timing\u00b6","text":"<p>The time to generate LD points depends on several factors, including the hardware and the particular generator.  One can generate one million points in 64 dimensions in a matter of seconds, somewhat slower than IID uniform points.  You may replace the <code>DiscreteDistribution</code> object in the first line of code below to test the timings of other kinds of points.  Halton seems to be the slowest (probably because the backend is implemented in Python rather than C).</p>"},{"location":"demos/talk_paper_demos/MCQMC_Tutorial_2020/MCQMC_2020_QMC_Software_Tutorial/#transform-ld-sequences-to-mimic-other-distributions-via-a-truemeasure-object","title":"Transform LD sequences to mimic other distributions via a <code>TrueMeasure</code> object\u00b6","text":"<p>LD sequences mimic $\\mathcal{U}[0,1]^d$ by design.  If you want to mimic another distribution, LD points can be transformed accordingly.  This is done via a <code>TrueMeasure</code> object, which takes a <code>DiscreteDistribution</code> object as input. Choose the <code>TrueMeasure</code> according to the probability distribution that you wish to mimic, and input the LD <code>DiscreteDistribution</code> object that you wish to use.</p>"},{"location":"demos/talk_paper_demos/MCQMC_Tutorial_2020/MCQMC_2020_QMC_Software_Tutorial/#uniform-on-boldsymbolaboldsymbolb","title":"Uniform on $[\\boldsymbol{a},\\boldsymbol{b}]$\u00b6","text":"<p>An affine transformation can be used to turn $\\mathcal{U}[0,1]^d$ points into $\\mathcal{U}[\\boldsymbol{a},\\boldsymbol{b}]$ points.  The <code>TrueMeasure</code> object <code>qmcpy.Uniform</code> performs the transformation automatically.  Then we generate points as before, but now they mimic our new distribution.</p>"},{"location":"demos/talk_paper_demos/MCQMC_Tutorial_2020/MCQMC_2020_QMC_Software_Tutorial/#gaussian","title":"Gaussian\u00b6","text":"<p>A similar process can be followed for constructing points that mimic a multivariate Gaussian distribution. The transformation used here assigns the directions with the larger variance to the lower numbered coordinates of the LD sequence via principal component analysis (PCA) or equivalently an eigvenvector-eigenvalue decomposition of the covariance matrix.  This takes advantage of the fact that the lower numbered coordinates of an LD sequence tend to have better evenness.</p>"},{"location":"demos/talk_paper_demos/MCQMC_Tutorial_2020/MCQMC_2020_QMC_Software_Tutorial/#integration","title":"Integration\u00b6","text":"<p>Cubature\u2014the approximation of multivariate integrals\u2014is an important application area for QMC. To solve this problem we need the <code>Integrand</code> and <code>StoppingCriterion</code> classes.</p> <p>Consider the followng $d$-variate integral due to Keister: \\begin{equation*} \\mu = \\int_{\\mathbb{R}^d} \\cos(\\lVert\\boldsymbol{t}\\rVert) \\exp( - \\lVert \\boldsymbol{t} \\rVert^2) \\,  \\mathrm{d} \\boldsymbol{t}, \\end{equation*} where $\\lVert \\cdot \\rVert$ is the Euclidean norm.  To approximate this integral via (Q)MC methods, it needs to be written as the integral with respect to a probability measure, e.g., \\begin{equation*} \\mu = \\int_{\\mathbb{R}^d} \\underbrace{\\pi^{d/2} \\cos(\\lVert\\boldsymbol{t}\\rVert)}_{g(\\boldsymbol{t})} \\; \\underbrace{\\pi^{-d/2} \\exp( - \\lVert \\boldsymbol{t} \\rVert^2) \\,  \\mathrm{d} \\boldsymbol{t}}_{\\mathcal{N}(\\boldsymbol{0}_d,\\mathsf{I}_d/2) \\text{ measure}}. \\end{equation*} Using transformation techniques highlighted above, this integral can be further transformed to an integral over the unit cube, which is suitable for certain stopping criteria: \\begin{equation*} \\mu = \\int_{[0,1]^d} \\underbrace{\\pi^{d/2}  \\cos\\left(\\sqrt{ \\frac 12 \\sum_{j=1}^d \\bigl[\\Phi^{-1}(x_j)\\bigr]^2}\\right)}_{f(\\boldsymbol{x})}  \\, \\rm d \\boldsymbol{x}. \\end{equation*}</p> <p>Although it may seem counter-intuitive, we set up our numerical problem by</p> <ul> <li>first choosing the <code>DiscreteDistribution</code> object,</li> <li>next the <code>TrueMeasure</code> object,</li> <li>thirdly choosing <code>Integrand</code> object, and</li> <li>finally the <code>StoppingCriterion</code> object.</li> </ul>"},{"location":"demos/talk_paper_demos/MCQMC_Tutorial_2020/MCQMC_2020_QMC_Software_Tutorial/#integrand-objects","title":"<code>Integrand</code> objects\u00b6","text":"<p>The <code>TrueMeasure</code> object becomes input to an <code>Integrand</code> object, which transforms our original integrand, $g$, to our eventual integrand, $f$.  This transformation relies on the <code>TrueMeasure</code> object along with its corresponding <code>DiscreteDistribution</code> object.  The object <code>qmcpy.Keister</code> has already been coded as a use case in <code>qmcpy</code>.</p>"},{"location":"demos/talk_paper_demos/MCQMC_Tutorial_2020/MCQMC_2020_QMC_Software_Tutorial/#stoppingcriterion-objects-and-the-integrate-method","title":"<code>StoppingCriterion</code> objects and the <code>integrate</code> method\u00b6","text":"<p>Determining the sample size needed requires a stopping criterion, which typically depends on the error tolerance, <code>abs_tol</code>.  The stopping criterion attempts to produce the answer satisfying the error tolerance with not much more work than is truly needed. There are several <code>StoppingCriterion</code> objects available, but they tend to work for specific LD sequences.  This one comes from Tony Jim\u00e9nez Rugama.  It takes as its input the <code>Integrand</code> object, which carries information about the <code>TrueMeasure</code> object and its <code>DiscreteDistribution</code> object.</p>"},{"location":"demos/talk_paper_demos/MCQMC_Tutorial_2020/MCQMC_2020_QMC_Software_Tutorial/#fixed-sample-budget-computation","title":"Fixed sample budget computation\u00b6","text":"<p>If you are not concerned about meeting an error tolerance but can only afford <code>n_max</code> function values, then you can set an <code>abs_tol</code> small enough and set <code>n_max</code> to your desired sample size.  You will get an error bound.</p>"},{"location":"demos/talk_paper_demos/MCQMC_Tutorial_2020/MCQMC_2020_QMC_Software_Tutorial/#qmc-is-much-faster-than-mc-qmc-cost-is-mostly-dimension-dependent","title":"QMC is much faster than MC. (Q)MC cost is mostly dimension dependent\u00b6","text":"<p>Run this next code block to see how the run time and the number of function evaluations increase as the tolerance decreases. QMC using LD sequences uses much less time and much fewer function values than MC using IID sequences.</p> <p>Tensor product rules have a time or function value cost that is $\\mathcal{O}(\\varepsilon^{-d/r})$, where $\\varepsilon$ is the error tolerance and $r$ is bounded above by both the smoothness of the integrand and the quality of the algorithm.  Such rules have a curse of dimensionality because their cost blows up exponentially with dimension.</p> <p>Unlike tensor product cubature rules, the cost of (Q)MC cubature is essentially dimension independent: $\\mathcal{O}(\\varepsilon^{-2})$ for IID MC and typically $\\mathcal{O}(\\varepsilon^{-1-\\delta})$ for QMC.  Although Q(MC) is not particularly fast, its performance usually does not degrade as the number of variables of in the integrand increases.</p>"},{"location":"demos/talk_paper_demos/MCQMC_Tutorial_2020/MCQMC_2020_QMC_Software_Tutorial/#alternatives-for-stoppingcriterion","title":"Alternatives for <code>StoppingCriterion</code>\u00b6","text":"<p>Other <code>StoppingCriterion</code> objects are available.  Most are tied to particular <code>DiscreteDistribution</code> objects.  For LD points one can use replications and the Central Limit Theorem (CLT).</p>"},{"location":"demos/talk_paper_demos/MCQMC_Tutorial_2020/MCQMC_2020_QMC_Software_Tutorial/#alternatives-for-truemeasure","title":"Alternatives for <code>TrueMeasure</code>\u00b6","text":"<p>The Keister integrand may also be solved using a Lebesgue <code>TrueMeasure</code> object: $$\\mu = \\int_{\\mathbb{R}^d} \\underbrace{\\cos(\\lVert\\boldsymbol{t}\\rVert) \\exp( - \\lVert\\boldsymbol{t}\\rVert^2)}_{g(\\boldsymbol{t})} \\,  \\underbrace{\\mathrm{d} \\boldsymbol{t}}_{\\text{Lebesgue measure}}$$ The <code>TrueMeasure</code> object contains the appropriate information so that the <code>Integrand</code> object can obtain the correct eventual integrand, $f$, in terms of the original integrand, $g$.</p> <p>Since <code>TrueMeasure</code> is not a probability measure, so it cannot be mimicked, but it can be used to solve the integration problem.  The <code>Integrand</code> object maps the problem using an affine transformation for finite boxes and an inverse normal distribution function transformation for $\\mathbb{R}^d$.</p> <p>The code below also shows how to take our own integrand defined with a simple input and output, and turn it into a <code>qmcpy</code> ready integrand using the <code>qmcpy.CustomFun</code> object.</p>"},{"location":"demos/talk_paper_demos/MCQMC_Tutorial_2020/MCQMC_2020_QMC_Software_Tutorial/#multi-level-qmc","title":"Multi-level (Q)MC\u00b6","text":"<p>When the dimesion of the multivariate integral is high, multi-level (Quasi-)Monte Carlo (ML(Q)MC) methods may save computation time.  The cost of one integrand value depends on the number of input variables, $d$, which corresponds to the dimension of our integration problem.  ML(Q)MC methods allow us attain our accuracy requirements by evaluating low dimensional integrands many times and high dimensional integrands much fewer times.</p> <p>High or infinte dimesional integration problems arise when computing the  expectations of quantities coming stochastic differential equations (SDEs).  These problems arise in finance applications. The dimension of the integrand typically refers to the number of time steps used to discretize the SDE.</p> <p>Here are some parameters for the Asian option examples below.  Changing them here allows you to compare the run times of these examples in a fair way.</p>"},{"location":"demos/talk_paper_demos/MCQMC_Tutorial_2020/MCQMC_2020_QMC_Software_Tutorial/#single-level-mc","title":"Single Level MC\u00b6","text":"<p>The vanilla way to solve this problem is IID Monte Carlo.  The fair price of the option is an expectation or integral, and the dimension is the number of time steps of used to discretize the Brownian motion that drives the SDE describing the price of the underlying asset.  The number of time steps should be fairly large.</p> <p>This paper by Lan Jiang and collaborators describes the stopping criterion.</p>"},{"location":"demos/talk_paper_demos/MCQMC_Tutorial_2020/MCQMC_2020_QMC_Software_Tutorial/#adaptive-multilevel-mc-from-mike-giles","title":"Adaptive Multilevel MC from Mike Giles\u00b6","text":"<p>Mike Giles and his collaborators have developed several ML(Q)MC algorithms.   The ML IID MC algorithm and stopping criterion implemented here are from this paper and this code. The answer is expected to be different than above, even with the same parameters, as the <code>MLCallOptions</code> uses a different discretization. The algorithm considers the SDE for logarithm of the stock price, which allows exact time stepping for constant interest rates and volatirilities, while Giles uses a Milstein discretization for the SDE for the stock price iteself.</p>"},{"location":"demos/talk_paper_demos/MCQMC_Tutorial_2020/MCQMC_2020_QMC_Software_Tutorial/#single-level-qmc-baseline","title":"Single Level QMC Baseline\u00b6","text":"<p>Tony Jim\u00e9nez's stopping criterion for cubature via Sobol' sequences in  this paper does not yet work for multi-level problems.  Here it is treating the option pricing problem as a high dimensional integral.</p>"},{"location":"demos/talk_paper_demos/MCQMC_Tutorial_2020/MCQMC_2020_QMC_Software_Tutorial/#importance-sampling","title":"Importance Sampling\u00b6","text":"<p>For the option pricing problem, we may add a drift to the Brownian motion as an example of importance sampling.</p> <p>First, we need to change our problem to one for which importance sampling can show some benefit.  We consider an out-of-the-money call option.</p>"},{"location":"demos/talk_paper_demos/MCQMC_Tutorial_2020/MCQMC_2020_QMC_Software_Tutorial/#under-the-hood","title":"Under the hood\u00b6","text":"<p>The structure of <code>qmcpy</code> is that there are five major classes:</p> <ul> <li><code>DiscreteDistribution</code> used to generate LD sequences, primarily on $[0,1]^d$</li> <li><code>TrueMeasure</code> for using these LD sequences to mimic other distributions and to define integrals with respect to other measures</li> <li><code>Integrand</code> to define the integrand for the multivariate integration problems</li> <li><code>StoppingCriterion</code> to determine when the desired accuracy has been reached</li> <li><code>AccumulateData</code> the invisible class used to keep track of important data as you continue to sample</li> </ul> <p>We look at some of the important parameters that the corresponding objects have.</p>"},{"location":"demos/talk_paper_demos/MCQMC_Tutorial_2020/MCQMC_2020_QMC_Software_Tutorial/#ld-sequence-generators","title":"LD sequence generators\u00b6","text":"<p>The LD generators (<code>DiscreteDistribution</code> objects) implemented here are drawn from several sources, which are denoted <code>backend</code> (first listed is the default):</p> <ul> <li>Sobol: QRNG, MPS, &amp; PyTorch</li> <li>Lattice: GAIL &amp; MPS, with default generating vectors from Lattice Builder</li> <li>Halton: Art Owen's &amp; QRNG</li> <li>Korobov: QRNG</li> </ul> <p>We illustrate some of the features of these varous backends and some of the other parameters that you can set.</p>"},{"location":"demos/talk_paper_demos/MCQMC_Tutorial_2020/MCQMC_2020_QMC_Software_Tutorial/#different-lattice-backends-and-generators","title":"Different lattice backends and generators\u00b6","text":"<p>The <code>qmcpy.Lattice</code> generator using the GAIL and MPS <code>backends</code> with the same generating vectors yield the same points but in a different order.  The <code>stopping criterion</code> <code>qmcpy.CubLatticeG</code> requires the GAIL order, but not all stopping criteria do.</p>"},{"location":"demos/talk_paper_demos/MCQMC_Tutorial_2020/MCQMC_2020_QMC_Software_Tutorial/#default-lds-are-randomized","title":"Default LDs are randomized\u00b6","text":"<p>LDs can be purely deterministic or they can be randomized.  For lattices this corresponds to a random shift modulo one.  For Sobol' sequences this corresponds to a random digital shift.  (PyTorch also uses random linear scrambling.)</p> <p>This randomization is turned on by default, but can also be turned off. With randomization off, the points will always look the same. Below the first sequence of points is randomized and so is different every time the code is run.  The second sequence is not randomized and stay the same.</p> <p>Turning off randomization throws a warning, which stops execution in Colab so we disable the warnings.</p>"},{"location":"demos/talk_paper_demos/MCQMC_Tutorial_2020/MCQMC_2020_QMC_Software_Tutorial/#unrandomized-lds-start-with-boldsymbol0","title":"Unrandomized LDs start with $\\boldsymbol{0}$\u00b6","text":"<p>By definition, without randomization the first point in all the popluar LD sequences is the origin.  You can try.</p>"},{"location":"demos/talk_paper_demos/MCQMC_Tutorial_2020/MCQMC_2020_QMC_Software_Tutorial/#coordinate-values-of-zero-and-one-may-be-problemetic-if-transformed-to-mathbbrd","title":"Coordinate values of zero and one may be problemetic if transformed to $\\mathbb{R}^d$\u00b6","text":"<p>If a coordinate value of a point constructed on $[0,1]^d$ is zero or one, then then $0$ is mapped to $-\\infty$ and $1$ is mapped to $\\infty$ when these points are transformed to $\\mathbb{R}^d$, as is done when solving problems with Gaussian distributions. In Python, these may turn out to be <code>nan</code>.  For many applications, infinities or <code>nan</code> will be troublesome.</p>"},{"location":"demos/talk_paper_demos/MCQMC_Tutorial_2020/MCQMC_2020_QMC_Software_Tutorial/#replications-with-a-fixed-seed","title":"Replications with a fixed seed\u00b6","text":"<p>For debugging purposes, you may wish to fix the seed of your randomized points.  This gives the advantage of an unchanging answer while avoiding the boundaries of the unit cube.  When rerunning the code below, the answers are unchanged iff the seed is fixed. For Tony's stopping criterion from GAIL, the points are randomized, but the algorithm is deterministic. For the fixed-multilevel CLT stopping criterion, the points and the algorithm are both random.</p>"},{"location":"demos/talk_paper_demos/MCQMC_Tutorial_2020/MCQMC_2020_QMC_Software_Tutorial/#help","title":"Help\u00b6","text":"<p>You can obtain help on any object by the <code>help(...)</code> or <code>dir(...)</code> commands.</p>"},{"location":"demos/talk_paper_demos/MCQMC_Tutorial_2020/MCQMC_2020_QMC_Software_Tutorial/#stopping-criteria","title":"Stopping criteria\u00b6","text":"<p>The stopping criteria implemented come from several sources:</p> <ul> <li>There are Central Limit Theorem (CLT) criteria for IID and LD sampling</li> <li>Rigorous stopping criteria imported from GAIL have been implemented for<ul> <li>IID sampling</li> <li>Lattice sampling</li> <li>Sobol' sampling</li> </ul> </li> <li>Multilevel stopping criteria due to Giles are given for IID and LD sampling</li> </ul>"},{"location":"demos/talk_paper_demos/MCQMC_Tutorial_2020/MCQMC_2020_QMC_Software_Tutorial/#changing-the-tolerance","title":"Changing the tolerance\u00b6","text":"<p>If you want to change the error tolerance, without creating a new <code>StoppingCriterion</code> object, there is a <code>set_tolerance</code> method.</p>"},{"location":"demos/talk_paper_demos/MCQMC_Tutorial_2020/MCQMC_2020_QMC_Software_Tutorial/#relative-error-tolerances","title":"Relative error tolerances\u00b6","text":"<p>For some problems a relative error tolerance may make more sense.  By default <code>rel_tol</code> is zero.  The stopping criterion stops when either of the two tolerances is met.</p>"},{"location":"demos/talk_paper_demos/MCQMC_Tutorial_2020/MCQMC_2020_QMC_Software_Tutorial/#use-cases","title":"Use cases\u00b6","text":"<p>Only a few use cases have been coded so far.  These include</p> <ul> <li><code>qmcpy.keister</code> Keiser's example</li> <li>European and Asian option pricing (Single and Multi-level)</li> <li>Computing Q-Noisy Expected Improvement (qEI) for Bayesian Optimization, see the blog at https://qmcpy.wpcomstaging.com/2020/07/19/qei-with-qmcpy/ and the Colaboratory notebook at https://drive.google.com/drive/folders/1EOREUL7lx4hytuZRQXB50UV8aE9JYN_a</li> <li>Importance Sampling</li> <li>Acceptance Rejection Sampling</li> <li>Custom Sampling by Inverse CDF Transform</li> </ul> <p><code>qmcpy</code> would benefit from contributions.</p>"},{"location":"demos/talk_paper_demos/MCQMC_Tutorial_2020/MCQMC_2020_QMC_Software_Tutorial/#acknowledgements-and-contributing","title":"Acknowledgements and Contributing\u00b6","text":"<p>The <code>qmcpy</code> code has primarily been written by Aleksei Sorokin (BS/MS expected in 2021), with the generous financial support of SigOpt https://sigopt.com.  However, much of the code is adapted from that of our friends (see above)</p> <p>We hope that QMCPy will become supported by the community.  Your contribution will add value to QMCPy, while allowing you to take advantage of the contribution of others.</p> <p>We highlight the value of community owned software.</p>"},{"location":"demos/talk_paper_demos/MCQMC_Tutorial_2020/MCQMC_2020_QMC_Software_Tutorial/#benefitting-from-each-others-work-is-easier-when-we-share","title":"Benefitting from each other's work is easier when we share\u00b6","text":"<ul> <li><p>Most of us are very good at just one or two things:</p> <ul> <li>LD sequence generators</li> <li>Increasing efficiency (e.g., MLMC)</li> <li>Stopping criteria</li> <li>Realistic use cases</li> </ul> <p>Having a shared software library let's us take advantage of the best</p> </li> <li><p>Provides a consistent interface for different pieces from different places</p> </li> <li><p>Supports reproducible computational research</p> </li> <li><p>Tedious stuff only needs to be figured out once</p> </li> </ul>"},{"location":"demos/talk_paper_demos/MCQMC_Tutorial_2020/MCQMC_2020_QMC_Software_Tutorial/#a-community-helps-find-and-correct-code-errors","title":"A community helps find and correct code errors\u00b6","text":"<p>By having more eyes on code than just the developer's we are more likely to spot errors or idiosyncracies.  Here are two examples.</p>"},{"location":"demos/talk_paper_demos/MCQMC_Tutorial_2020/MCQMC_2020_QMC_Software_Tutorial/#matlabs-sobol-generator","title":"MATLAB's Sobol' generator\u00b6","text":"<p>Several years ago Llu\u00eds Antoni Jim\u00e9nez Rugama discovered that the scrambling of the Sobol' generators implemented in MATLAB's Statistics Toolbox was wrong.  After reporting the problem to the developers, it was corrected in MATLAB 2017a</p>"},{"location":"demos/talk_paper_demos/MCQMC_Tutorial_2020/MCQMC_2020_QMC_Software_Tutorial/#pytorchs-sobol-generator","title":"PyTorch's Sobol' generator\u00b6","text":"<p>PyTorch is a popular Python library with its own Sobol generator.  However, it should be used with care.</p> <ul> <li>As noted above, the first point is skipped.</li> <li>We found that unless you specify double precision, you get points that have 1 as a coordinate far too often.  The developer seemed unaware of this.</li> </ul> <p>These issues have been reported at https://github.com/pytorch/pytorch/issues/32047.</p>"},{"location":"demos/talk_paper_demos/MCQMC_Tutorial_2020/MCQMC_2020_QMC_Software_Tutorial/#how-you-can-contribute","title":"How you can contribute\u00b6","text":"<p>After trying QMCPy out help us out by</p> <p>Easy.</p> <p>Submit your bugs and feature requests as issues to https://github.com/QMCSoftware/QMCSoftware/issues</p> <p>Moderately Difficult.</p> <p>Ask your students or collaborators to try QMCPy for their own work and submit their bugs and feature requests</p> <p>Heroic.</p> <p>Add a feature or use case and make a pull request at  https://github.com/QMCSoftware/QMCSoftware/pulls so that we can included it in our next release</p> <p>Questions?  Email us at qmc-software@googlegroups.com</p>"},{"location":"demos/talk_paper_demos/MCQMC_Tutorial_2020/MCQMC_2020_QMC_Software_Tutorial/#references","title":"References\u00b6","text":"<ol> <li><p>S.-C. T. Choi, Y. Ding, F. J. Hickernell, L. Jiang, Ll. A. Jimenez Rugama, D. Li, Jagadeeswaran R., X. Tong, K. Zhang, Y. Zhang, and X. Zhou, GAIL: Guaranteed Automatic Integration Library (Version 2.3.1) [MATLAB Software], 2020. Available from <code>http://gailgithub.github.io/GAIL_Dev/</code></p> </li> <li><p>H. Faure and C. Lemieux. \u201cImplementation of Irreducible Sobol\u2019 Sequences in Prime Power Bases,\u201d Mathematics and Computers in Simulation 161 (2019): 13\u201322.</p> </li> <li><p>M. B. Giles. \"Multi-level Monte Carlo path simulation,\" Operations Research, 56(3):607-617, 2008. <code>http://people.maths.ox.ac.uk/~gilesm/files/OPRE_2008.pdf</code>.</p> </li> <li><p>M. B. Giles. \"Improved multilevel Monte Carlo convergence using the Milstein scheme,\" 343-358, in Monte Carlo and Quasi-Monte Carlo Methods 2006, Springer, 2008. <code>http://people.maths.ox.ac.uk/~gilesm/files/mcqmc06.pdf</code>.</p> </li> <li><p>M. B. Giles and B. J. Waterhouse. \"Multilevel quasi-Monte Carlo path simulation,\" pp.165-181 in Advanced Financial Modelling, in Radon Series on Computational and Applied Mathematics, de Gruyter, 2009. <code>http://people.maths.ox.ac.uk/~gilesm/files/radon.pdf</code></p> </li> <li><p>F. J. Hickernell, L. Jiang, Y. Liu, and A. B. Owen, \"Guaranteed conservative fixed width confidence intervals via Monte Carlo sampling,\" Monte Carlo and Quasi-Monte Carlo Methods 2012 (J. Dick, F.Y. Kuo, G. W. Peters, and I. H. Sloan, eds.), pp. 105-128, Springer-Verlag, Berlin, 2014. DOI: 10.1007/978-3-642-41095-6_5</p> </li> <li><p>F. J. Hickernell and Lluis Antoni Jimenez Rugama, \"Reliable adaptive cubature using digital sequences,\" Monte Carlo and Quasi-Monte Carlo Methods: MCQMC, Leuven, Belgium, April 2014 (R. Cools and D. Nuyens, eds.), Springer Proceedings in Mathematics and Statistics, vol. 163, Springer-Verlag, Berlin, 2016, arXiv:1410.8615 [math.NA], pp. 367-383.</p> </li> <li><p>M. Hofert and C. Lemieux (2019). qrng: (Randomized) Quasi-Random Number Generators. R package version 0.0-7. <code>https://CRAN.R-project.org/package=qrng</code>.</p> </li> <li><p>Ll. A. Jimenez Rugama and F. J. Hickernell, \"Adaptive multidimensional integration based on rank-1 lattices,\" Monte Carlo and Quasi-Monte Carlo  Methods: MCQMC, Leuven, Belgium, April 2014 (R. Cools and D. Nuyens, eds.), Springer Proceedings in Mathematics and Statistics, vol. 163, Springer-Verlag, Berlin, 2016, arXiv:1411.1966, pp. 407-422.</p> </li> <li><p>B. D. Keister, Multidimensional Quadrature Algorithms,  'Computers in Physics', 10, pp. 119-122, 1996.</p> </li> <li><p>F. Y. Kuo and D. Nuyens. \"Application of quasi-Monte Carlo methods to elliptic PDEs with random diffusion coefficients - a survey of analysis and implementation,\" Foundations of Computational Mathematics, 16(6):1631-1696, 2016. (springer link, arxiv link)</p> </li> <li><p>P. L\u2019Ecuyer and D. Munger, \"LatticeBuilder: A General Software Tool for Constructing Rank-1 Lattice Rules,\" ACM Transactions on Mathematical Software. 42 (2015). 10.1145/2754929.</p> </li> <li><p>Y. Li, L. Kang, L., and F. J. Hickernell, Is a Transformed Low Discrepancy Design Also Low Discrepancy? in Contemporary Experimental Design, Multivariate Analysis and Data Mining, Festschrift in Honour of Professor Kai-Tai Fang (J. Fan and J. Pan, eds.), p. 69\u201392, 2020, https://arxiv.org/abs/2004.09887.</p> </li> <li><p>A. B. Owen, \"A randomized Halton algorithm in R,\" 2017. arXiv:1706.02808 [stat.CO]</p> </li> </ol>"},{"location":"demos/talk_paper_demos/ProbFailureSorokinRao/prob_failure_gp_ci/","title":"2023 Probability of Failure Paper","text":"In\u00a0[\u00a0]: Copied! <pre>import qmcpy as qp\nimport numpy as np\nimport scipy.stats\nimport gpytorch\nimport torch\nimport os\nimport warnings\nimport pandas as pd\nfrom gpytorch.utils.warnings import NumericalWarning\nwarnings.filterwarnings(\"ignore\")\npd.set_option(\n    'display.max_rows', None,\n    'display.max_columns', None,\n    'display.width', 1000,\n    'display.colheader_justify', 'center',\n    'display.precision',2,\n    'display.float_format',lambda x:'%.1e'%x)\nfrom matplotlib import pyplot,gridspec\n</pre> import qmcpy as qp import numpy as np import scipy.stats import gpytorch import torch import os import warnings import pandas as pd from gpytorch.utils.warnings import NumericalWarning warnings.filterwarnings(\"ignore\") pd.set_option(     'display.max_rows', None,     'display.max_columns', None,     'display.width', 1000,     'display.colheader_justify', 'center',     'display.precision',2,     'display.float_format',lambda x:'%.1e'%x) from matplotlib import pyplot,gridspec In\u00a0[\u00a0]: Copied! <pre>import pandas as pd \npd.set_option(\n    'display.max_rows', None,\n    'display.max_columns', None,\n    'display.width', 1000,\n    'display.colheader_justify', 'center',\n    'display.precision',2,\n    'display.float_format',lambda x:'%.1e'%x)\nfrom matplotlib import pyplot\n</pre> import pandas as pd  pd.set_option(     'display.max_rows', None,     'display.max_columns', None,     'display.width', 1000,     'display.colheader_justify', 'center',     'display.precision',2,     'display.float_format',lambda x:'%.1e'%x) from matplotlib import pyplot In\u00a0[5]: Copied! <pre>gpytorch_use_gpu = torch.cuda.is_available()\ngpytorch_use_gpu\n</pre> gpytorch_use_gpu = torch.cuda.is_available() gpytorch_use_gpu Out[5]: <pre>False</pre> In\u00a0[6]: Copied! <pre>cols = 2\nnticks = 129\nnp.random.seed(7); torch.manual_seed(1)\nci_percentage = .95\nbeta = scipy.stats.norm.ppf(np.mean([ci_percentage,1]))\n\nf = lambda x: x*np.sin(4*np.pi*x)\nx = qp.Lattice(1,seed=7).gen_samples(4).squeeze()\ny = f(x)\n\ngp = qp.util.ExactGPyTorchRegressionModel(\n    x_t = x[:,None],\n    y_t = y,\n    prior_mean = gpytorch.means.ZeroMean(),\n    prior_cov = gpytorch.kernels.ScaleKernel(gpytorch.kernels.MaternKernel(nu=2.5)),\n    likelihood = gpytorch.likelihoods.GaussianLikelihood(noise_constraint = gpytorch.constraints.Interval(1e-12,1e-8))) \ngp.fit(\n    optimizer = torch.optim.Adam(gp.parameters(),lr=0.1),\n    mll = gpytorch.mlls.ExactMarginalLogLikelihood(gp.likelihood,gp),\n    training_iter = 100)\n\nxticks = np.linspace(0,1,nticks)\nyticks = f(xticks)\nyhatticks_mean,yhatticks_std = gp.predict(xticks[:,None])\nf_preds = gp(torch.from_numpy(xticks[:,None]))\nf_samples = f_preds.sample(sample_shape=torch.Size((cols,))).numpy()\n\nfig = pyplot.figure(tight_layout=True)\ngs = gridspec.GridSpec(2,cols)\nax_top = fig.add_subplot(gs[0,:])\nscatter_data = ax_top.scatter(x,y,color='r')\nplot_yticks, = ax_top.plot(xticks,yticks,color='c')\nplot_post_mean, = ax_top.plot(xticks,yhatticks_mean,color='k')\nplot_gp_ci = ax_top.fill_between(xticks,yhatticks_mean+beta*yhatticks_std,yhatticks_mean-beta*yhatticks_std,color='k',alpha=.25)\nax_top.axhline(y=0,color='lightgreen')\nfor spine in ['top','bottom']: ax_top.spines[spine].set_visible(False)\nax_top.set_xlim([0,1]); ax_top.set_xticks([0,1]); ax_top.set_yticks([])\nfor c in range(cols):\n    f_sample = f_samples[c]\n    ax = fig.add_subplot(gs[1,c]) if c==0 else fig.add_subplot(gs[1,c],sharey=ax)\n    ax.scatter(x,y,color='r')\n    ax.plot(xticks,yhatticks_mean,color='k')\n    ax.set_xlim([0,1]); ax.set_xticks([0,1])\n    plot_draw, = ax.plot(xticks,f_sample,c='b')\n    ymin,ymax = ax.get_ylim()#; ax.set_aspect(1/(ymax-ymin))\n    yminticks,ymaxticks = np.tile(ymin,nticks),np.tile(ymax,nticks)\n    tp = (f_sample&gt;=0)*(yhatticks_mean&gt;=0)\n    tn = (f_sample&lt;0)*(yhatticks_mean&lt;0)\n    fp = (f_sample&lt;0)*(yhatticks_mean&gt;=0)\n    fn = (f_sample&gt;=0)*(yhatticks_mean&lt;0)\n    plot_tp, = ax.plot(xticks,np.ma.masked_where(~tp,ymaxticks),color='m',linewidth=5)\n    plot_fp, = ax.plot(xticks,np.ma.masked_where(~fp,ymaxticks),color='orange',linewidth=5)\n    plot_tn, = ax.plot(xticks,np.ma.masked_where(~tn,yminticks),color='m',linewidth=5)\n    plot_fn, = ax.plot(xticks,np.ma.masked_where(~fn,yminticks),color='orange',linewidth=5)\n    ax.set_ylim([ymin,ymax])\n    for spine in ['top','bottom']: ax.spines[spine].set_visible(False)\n    ax.set_yticks([])\n    plot_failure_threshold = ax.axhline(y=0,color='lightgreen')\nfig.legend([scatter_data,plot_yticks,plot_post_mean,plot_gp_ci],\n           ['data','simulation','posterior mean','posterior 95\\% CI'],\n           ncol=4,\n           frameon=False,\n           loc='upper center',\n           bbox_to_anchor=(.5,1.01),\n           prop={'size':9})\nfig.legend([plot_failure_threshold,plot_draw,plot_tp,plot_fp,],\n           ['failure threshold','sample path','True','False'],\n           ncol=5,\n           frameon=False,\n           loc='upper center',\n           bbox_to_anchor=(.5,.58),\n           prop={'size':9})\nfig.savefig(\"outputs/TP_FP_TN_FN.png\",dpi=256,transparent=True,bbox_inches=\"tight\")\n</pre> cols = 2 nticks = 129 np.random.seed(7); torch.manual_seed(1) ci_percentage = .95 beta = scipy.stats.norm.ppf(np.mean([ci_percentage,1]))  f = lambda x: x*np.sin(4*np.pi*x) x = qp.Lattice(1,seed=7).gen_samples(4).squeeze() y = f(x)  gp = qp.util.ExactGPyTorchRegressionModel(     x_t = x[:,None],     y_t = y,     prior_mean = gpytorch.means.ZeroMean(),     prior_cov = gpytorch.kernels.ScaleKernel(gpytorch.kernels.MaternKernel(nu=2.5)),     likelihood = gpytorch.likelihoods.GaussianLikelihood(noise_constraint = gpytorch.constraints.Interval(1e-12,1e-8)))  gp.fit(     optimizer = torch.optim.Adam(gp.parameters(),lr=0.1),     mll = gpytorch.mlls.ExactMarginalLogLikelihood(gp.likelihood,gp),     training_iter = 100)  xticks = np.linspace(0,1,nticks) yticks = f(xticks) yhatticks_mean,yhatticks_std = gp.predict(xticks[:,None]) f_preds = gp(torch.from_numpy(xticks[:,None])) f_samples = f_preds.sample(sample_shape=torch.Size((cols,))).numpy()  fig = pyplot.figure(tight_layout=True) gs = gridspec.GridSpec(2,cols) ax_top = fig.add_subplot(gs[0,:]) scatter_data = ax_top.scatter(x,y,color='r') plot_yticks, = ax_top.plot(xticks,yticks,color='c') plot_post_mean, = ax_top.plot(xticks,yhatticks_mean,color='k') plot_gp_ci = ax_top.fill_between(xticks,yhatticks_mean+beta*yhatticks_std,yhatticks_mean-beta*yhatticks_std,color='k',alpha=.25) ax_top.axhline(y=0,color='lightgreen') for spine in ['top','bottom']: ax_top.spines[spine].set_visible(False) ax_top.set_xlim([0,1]); ax_top.set_xticks([0,1]); ax_top.set_yticks([]) for c in range(cols):     f_sample = f_samples[c]     ax = fig.add_subplot(gs[1,c]) if c==0 else fig.add_subplot(gs[1,c],sharey=ax)     ax.scatter(x,y,color='r')     ax.plot(xticks,yhatticks_mean,color='k')     ax.set_xlim([0,1]); ax.set_xticks([0,1])     plot_draw, = ax.plot(xticks,f_sample,c='b')     ymin,ymax = ax.get_ylim()#; ax.set_aspect(1/(ymax-ymin))     yminticks,ymaxticks = np.tile(ymin,nticks),np.tile(ymax,nticks)     tp = (f_sample&gt;=0)*(yhatticks_mean&gt;=0)     tn = (f_sample&lt;0)*(yhatticks_mean&lt;0)     fp = (f_sample&lt;0)*(yhatticks_mean&gt;=0)     fn = (f_sample&gt;=0)*(yhatticks_mean&lt;0)     plot_tp, = ax.plot(xticks,np.ma.masked_where(~tp,ymaxticks),color='m',linewidth=5)     plot_fp, = ax.plot(xticks,np.ma.masked_where(~fp,ymaxticks),color='orange',linewidth=5)     plot_tn, = ax.plot(xticks,np.ma.masked_where(~tn,yminticks),color='m',linewidth=5)     plot_fn, = ax.plot(xticks,np.ma.masked_where(~fn,yminticks),color='orange',linewidth=5)     ax.set_ylim([ymin,ymax])     for spine in ['top','bottom']: ax.spines[spine].set_visible(False)     ax.set_yticks([])     plot_failure_threshold = ax.axhline(y=0,color='lightgreen') fig.legend([scatter_data,plot_yticks,plot_post_mean,plot_gp_ci],            ['data','simulation','posterior mean','posterior 95\\% CI'],            ncol=4,            frameon=False,            loc='upper center',            bbox_to_anchor=(.5,1.01),            prop={'size':9}) fig.legend([plot_failure_threshold,plot_draw,plot_tp,plot_fp,],            ['failure threshold','sample path','True','False'],            ncol=5,            frameon=False,            loc='upper center',            bbox_to_anchor=(.5,.58),            prop={'size':9}) fig.savefig(\"outputs/TP_FP_TN_FN.png\",dpi=256,transparent=True,bbox_inches=\"tight\") In\u00a0[9]: Copied! <pre>mcispfgp = qp.PFGPCI(\n    integrand = qp.Sin1d(qp.DigitalNetB2(1,seed=17),k=3),\n    failure_threshold = 0,\n    failure_above_threshold=True,\n    abs_tol = 1e-2,\n    alpha = 1e-1,\n    n_init = 4,\n    init_samples = None,\n    batch_sampler = qp.PFSampleErrorDensityAR(verbose=True),\n    n_batch = 4,\n    n_limit = 20,\n    n_approx = 2**18,\n    gpytorch_prior_mean = gpytorch.means.ZeroMean(),\n    gpytorch_prior_cov = gpytorch.kernels.ScaleKernel(\n        gpytorch.kernels.MaternKernel(nu=2.5,\n            lengthscale_constraint = gpytorch.constraints.Interval(.01,.1)\n            ),\n        outputscale_constraint = gpytorch.constraints.Interval(1e-3,10)\n        ),\n    gpytorch_likelihood = gpytorch.likelihoods.GaussianLikelihood(noise_constraint = gpytorch.constraints.Interval(1e-12,1e-8)),\n    gpytorch_marginal_log_likelihood_func = lambda likelihood,gpyt_model: gpytorch.mlls.ExactMarginalLogLikelihood(likelihood,gpyt_model),\n    torch_optimizer_func = lambda gpyt_model: torch.optim.Adam(gpyt_model.parameters(),lr=0.1),\n    gpytorch_train_iter = 100,\n    gpytorch_use_gpu = False,\n    verbose = 50,\n    n_ref_approx = 2**22,\n    seed_ref_approx = None)\nsolution,data = mcispfgp.integrate(seed=7,refit=False)\nprint(data)\ndf = pd.DataFrame(data.get_results_dict())\nprint(\"\\nIteration Summary\")\nprint(df)\nfig = data.plot()\ndf.to_csv(\"outputs/Sin1D_df.csv\",index=False)\nfig.savefig(\"outputs/Sin1D_fig.png\",dpi=256,transparent=True,bbox_inches=\"tight\")\n</pre> mcispfgp = qp.PFGPCI(     integrand = qp.Sin1d(qp.DigitalNetB2(1,seed=17),k=3),     failure_threshold = 0,     failure_above_threshold=True,     abs_tol = 1e-2,     alpha = 1e-1,     n_init = 4,     init_samples = None,     batch_sampler = qp.PFSampleErrorDensityAR(verbose=True),     n_batch = 4,     n_limit = 20,     n_approx = 2**18,     gpytorch_prior_mean = gpytorch.means.ZeroMean(),     gpytorch_prior_cov = gpytorch.kernels.ScaleKernel(         gpytorch.kernels.MaternKernel(nu=2.5,             lengthscale_constraint = gpytorch.constraints.Interval(.01,.1)             ),         outputscale_constraint = gpytorch.constraints.Interval(1e-3,10)         ),     gpytorch_likelihood = gpytorch.likelihoods.GaussianLikelihood(noise_constraint = gpytorch.constraints.Interval(1e-12,1e-8)),     gpytorch_marginal_log_likelihood_func = lambda likelihood,gpyt_model: gpytorch.mlls.ExactMarginalLogLikelihood(likelihood,gpyt_model),     torch_optimizer_func = lambda gpyt_model: torch.optim.Adam(gpyt_model.parameters(),lr=0.1),     gpytorch_train_iter = 100,     gpytorch_use_gpu = False,     verbose = 50,     n_ref_approx = 2**22,     seed_ref_approx = None) solution,data = mcispfgp.integrate(seed=7,refit=False) print(data) df = pd.DataFrame(data.get_results_dict()) print(\"\\nIteration Summary\") print(df) fig = data.plot() df.to_csv(\"outputs/Sin1D_df.csv\",index=False) fig.savefig(\"outputs/Sin1D_fig.png\",dpi=256,transparent=True,bbox_inches=\"tight\") <pre>reference approximation with d=1: 0.4999997615814209\nbatch 0\n\tgpytorch model fitting\n\t\titer 50  of 100\n\t\t\tlikelihood.noise_covar.raw_noise.................. -8.37e-02\n\t\t\tcovar_module.raw_outputscale...................... -3.03e+00\n\t\t\tcovar_module.base_kernel.raw_lengthscale.......... 5.02e+00\n\t\titer 100 of 100\n\t\t\tlikelihood.noise_covar.raw_noise.................. -9.23e-02\n\t\t\tcovar_module.raw_outputscale...................... -2.96e+00\n\t\t\tcovar_module.base_kernel.raw_lengthscale.......... 6.27e+00\nbatch 1\n\tAR sampling with efficiency 2.8e-01, expect 14 draws: 12, 15, 18, 21, 24, \nbatch 2\n\tAR sampling with efficiency 1.7e-01, expect 23 draws: 16, 24, 28, \nbatch 3\n\tAR sampling with efficiency 9.4e-02, expect 42 draws: 28, \nbatch 4\n\tAR sampling with efficiency 6.0e-02, expect 66 draws: 48, 72, \nPFGPCIData (Data)\n    solution        0.501\n    error_bound     0.111\n    bound_low       0.390\n    bound_high      0.611\n    n_total         20\n    time_integrate  0.831\nPFGPCI (AbstractStoppingCriterion)\n    abs_tol         0.010\n    n_init          2^(2)\n    n_limit         20\n    n_batch         2^(2)\nSin1d (AbstractIntegrand)\nUniform (AbstractTrueMeasure)\n    lower_bound     0\n    upper_bound     18.850\nDigitalNetB2 (AbstractLDDiscreteDistribution)\n    d               1\n    replications    1\n    randomize       LMS_DS\n    gen_mats_source joe_kuo.6.21201.txt\n    order           NATURAL\n    t               63\n    alpha           1\n    n_limit         2^(32)\n    entropy         17\n\nIteration Summary\n   iter  n_sum  n_batch  error_bounds  ci_low  ci_high  solutions  solutions_ref  error_ref  in_ci\n0    0     4       4       5.4e-01    0.0e+00 1.0e+00   4.6e-01      5.0e-01      3.9e-02    True \n1    1     8       4       5.0e-01    0.0e+00 1.0e+00   5.0e-01      5.0e-01      4.2e-03    True \n2    2    12       4       4.7e-01    3.5e-02 9.8e-01   5.1e-01      5.0e-01      6.4e-03    True \n3    3    16       4       3.0e-01    2.1e-01 8.1e-01   5.1e-01      5.0e-01      9.4e-03    True \n4    4    20       4       1.1e-01    3.9e-01 6.1e-01   5.0e-01      5.0e-01      5.2e-04    True \n</pre> In\u00a0[10]: Copied! <pre>mcispfgp = qp.PFGPCI(\n    integrand = qp.Multimodal2d(qp.DigitalNetB2(2,seed=17)),\n    failure_threshold = 0,\n    failure_above_threshold=True,\n    abs_tol = 1e-2,\n    alpha = 1e-1,\n    n_init = 64,\n    init_samples = None,\n    batch_sampler = qp.PFSampleErrorDensityAR(verbose=True),\n    n_batch = 16,\n    n_limit = 128,\n    n_approx = 2**18,\n    gpytorch_prior_mean = gpytorch.means.ZeroMean(),\n    gpytorch_prior_cov = gpytorch.kernels.ScaleKernel(\n        gpytorch.kernels.MaternKernel(nu=1.5,\n            lengthscale_constraint = gpytorch.constraints.Interval(.1,1)\n            ),\n        outputscale_constraint = gpytorch.constraints.Interval(1e-3,.5)\n        ),\n    gpytorch_likelihood = gpytorch.likelihoods.GaussianLikelihood(noise_constraint = gpytorch.constraints.Interval(1e-12,1e-8)),\n    gpytorch_marginal_log_likelihood_func = lambda likelihood,gpyt_model: gpytorch.mlls.ExactMarginalLogLikelihood(likelihood,gpyt_model),\n    torch_optimizer_func = lambda gpyt_model: torch.optim.Adam(gpyt_model.parameters(),lr=0.1),\n    gpytorch_train_iter = 800,\n    gpytorch_use_gpu = gpytorch_use_gpu,\n    verbose = 200,\n    n_ref_approx = 2**22,\n    seed_ref_approx = None)\nsolution,data = mcispfgp.integrate(seed=7,refit=False)\nprint(data)\ndf = pd.DataFrame(data.get_results_dict())\nprint(\"\\nIteration Summary\")\nprint(df)\nfig = data.plot()\ndf.to_csv(\"outputs/Multimodal2D_df.csv\",index=False)\nfig.savefig(\"outputs/Multimodal2D_fig.png\",dpi=256,transparent=True,bbox_inches=\"tight\")\n</pre> mcispfgp = qp.PFGPCI(     integrand = qp.Multimodal2d(qp.DigitalNetB2(2,seed=17)),     failure_threshold = 0,     failure_above_threshold=True,     abs_tol = 1e-2,     alpha = 1e-1,     n_init = 64,     init_samples = None,     batch_sampler = qp.PFSampleErrorDensityAR(verbose=True),     n_batch = 16,     n_limit = 128,     n_approx = 2**18,     gpytorch_prior_mean = gpytorch.means.ZeroMean(),     gpytorch_prior_cov = gpytorch.kernels.ScaleKernel(         gpytorch.kernels.MaternKernel(nu=1.5,             lengthscale_constraint = gpytorch.constraints.Interval(.1,1)             ),         outputscale_constraint = gpytorch.constraints.Interval(1e-3,.5)         ),     gpytorch_likelihood = gpytorch.likelihoods.GaussianLikelihood(noise_constraint = gpytorch.constraints.Interval(1e-12,1e-8)),     gpytorch_marginal_log_likelihood_func = lambda likelihood,gpyt_model: gpytorch.mlls.ExactMarginalLogLikelihood(likelihood,gpyt_model),     torch_optimizer_func = lambda gpyt_model: torch.optim.Adam(gpyt_model.parameters(),lr=0.1),     gpytorch_train_iter = 800,     gpytorch_use_gpu = gpytorch_use_gpu,     verbose = 200,     n_ref_approx = 2**22,     seed_ref_approx = None) solution,data = mcispfgp.integrate(seed=7,refit=False) print(data) df = pd.DataFrame(data.get_results_dict()) print(\"\\nIteration Summary\") print(df) fig = data.plot() df.to_csv(\"outputs/Multimodal2D_df.csv\",index=False) fig.savefig(\"outputs/Multimodal2D_fig.png\",dpi=256,transparent=True,bbox_inches=\"tight\") <pre>reference approximation with d=2: 0.3020758628845215\nbatch 0\n\tgpytorch model fitting\n\t\titer 200 of 800\n\t\t\tlikelihood.noise_covar.raw_noise.................. 1.45e+00\n\t\t\tcovar_module.raw_outputscale...................... 2.79e+00\n\t\t\tcovar_module.base_kernel.raw_lengthscale.......... -3.02e+00\n\t\titer 400 of 800\n\t\t\tlikelihood.noise_covar.raw_noise.................. 1.48e+00\n\t\t\tcovar_module.raw_outputscale...................... 3.54e+00\n\t\t\tcovar_module.base_kernel.raw_lengthscale.......... -3.33e+00\n\t\titer 600 of 800\n\t\t\tlikelihood.noise_covar.raw_noise.................. 1.52e+00\n\t\t\tcovar_module.raw_outputscale...................... 4.07e+00\n\t\t\tcovar_module.base_kernel.raw_lengthscale.......... -3.46e+00\n\t\titer 800 of 800\n\t\t\tlikelihood.noise_covar.raw_noise.................. 1.56e+00\n\t\t\tcovar_module.raw_outputscale...................... 4.48e+00\n\t\t\tcovar_module.base_kernel.raw_lengthscale.......... -3.51e+00\nbatch 1\n\tAR sampling with efficiency 4.9e-02, expect 323 draws: 224, 364, 392, 406, \nbatch 2\n\tAR sampling with efficiency 3.3e-02, expect 485 draws: 336, 609, 735, 798, 819, 840, 861, 882, \nbatch 3\n\tAR sampling with efficiency 2.1e-02, expect 764 draws: 528, 891, 1089, \nbatch 4\n\tAR sampling with efficiency 2.3e-02, expect 692 draws: 480, 660, 750, 810, \nPFGPCIData (Data)\n    solution        0.300\n    error_bound     0.090\n    bound_low       0.210\n    bound_high      0.390\n    n_total         2^(7)\n    time_integrate  6.875\nPFGPCI (AbstractStoppingCriterion)\n    abs_tol         0.010\n    n_init          2^(6)\n    n_limit         2^(7)\n    n_batch         2^(4)\nMultimodal2d (AbstractIntegrand)\nUniform (AbstractTrueMeasure)\n    lower_bound     [-4 -3]\n    upper_bound     [7 8]\nDigitalNetB2 (AbstractLDDiscreteDistribution)\n    d               2^(1)\n    replications    1\n    randomize       LMS_DS\n    gen_mats_source joe_kuo.6.21201.txt\n    order           NATURAL\n    t               63\n    alpha           1\n    n_limit         2^(32)\n    entropy         17\n\nIteration Summary\n   iter  n_sum  n_batch  error_bounds  ci_low  ci_high  solutions  solutions_ref  error_ref  in_ci\n0    0     64     64       2.5e-01    2.7e-02 5.2e-01   2.7e-01      3.0e-01      2.8e-02    True \n1    1     80     16       1.6e-01    1.2e-01 4.5e-01   2.9e-01      3.0e-01      1.5e-02    True \n2    2     96     16       1.0e-01    1.8e-01 3.9e-01   2.9e-01      3.0e-01      1.3e-02    True \n3    3    112     16       1.2e-01    1.8e-01 4.1e-01   3.0e-01      3.0e-01      6.1e-03    True \n4    4    128     16       9.0e-02    2.1e-01 3.9e-01   3.0e-01      3.0e-01      2.4e-03    True \n</pre> In\u00a0[11]: Copied! <pre>mcispfgp = qp.PFGPCI(\n    integrand = qp.FourBranch2d(qp.DigitalNetB2(2,seed=17)),\n    failure_threshold = 0,\n    failure_above_threshold=True,\n    abs_tol = 1e-2,\n    alpha = 1e-1,\n    n_init = 64,\n    init_samples = None,\n    batch_sampler = qp.PFSampleErrorDensityAR(verbose=True),\n    n_batch = 12,\n    n_limit = 200,\n    n_approx = 2**18,\n    gpytorch_prior_mean = gpytorch.means.ZeroMean(),\n    gpytorch_prior_cov = gpytorch.kernels.ScaleKernel(\n        gpytorch.kernels.MaternKernel(nu=1.5,\n            lengthscale_constraint = gpytorch.constraints.Interval(.5,1)\n            ),\n        outputscale_constraint = gpytorch.constraints.Interval(1e-8,.5)\n        ),\n    gpytorch_likelihood = gpytorch.likelihoods.GaussianLikelihood(noise_constraint = gpytorch.constraints.Interval(1e-12,1e-8)),\n    gpytorch_marginal_log_likelihood_func = lambda likelihood,gpyt_model: gpytorch.mlls.ExactMarginalLogLikelihood(likelihood,gpyt_model),\n    torch_optimizer_func = lambda gpyt_model: torch.optim.Adam(gpyt_model.parameters(),lr=0.1),\n    gpytorch_train_iter = 800,\n    gpytorch_use_gpu = gpytorch_use_gpu,\n    verbose = 200,\n    n_ref_approx = 2**22,\n    seed_ref_approx = None)\nsolution,data = mcispfgp.integrate(seed=7,refit=False)\nprint(data)\ndf = pd.DataFrame(data.get_results_dict())\nprint(\"\\nIteration Summary\")\nprint(df)\nfig = data.plot()\ndf.to_csv(\"outputs/FourBranch2D_df.csv\",index=False)\nfig.savefig(\"outputs/FourBranch2D_fig.png\",dpi=256,transparent=True,bbox_inches=\"tight\")\n</pre> mcispfgp = qp.PFGPCI(     integrand = qp.FourBranch2d(qp.DigitalNetB2(2,seed=17)),     failure_threshold = 0,     failure_above_threshold=True,     abs_tol = 1e-2,     alpha = 1e-1,     n_init = 64,     init_samples = None,     batch_sampler = qp.PFSampleErrorDensityAR(verbose=True),     n_batch = 12,     n_limit = 200,     n_approx = 2**18,     gpytorch_prior_mean = gpytorch.means.ZeroMean(),     gpytorch_prior_cov = gpytorch.kernels.ScaleKernel(         gpytorch.kernels.MaternKernel(nu=1.5,             lengthscale_constraint = gpytorch.constraints.Interval(.5,1)             ),         outputscale_constraint = gpytorch.constraints.Interval(1e-8,.5)         ),     gpytorch_likelihood = gpytorch.likelihoods.GaussianLikelihood(noise_constraint = gpytorch.constraints.Interval(1e-12,1e-8)),     gpytorch_marginal_log_likelihood_func = lambda likelihood,gpyt_model: gpytorch.mlls.ExactMarginalLogLikelihood(likelihood,gpyt_model),     torch_optimizer_func = lambda gpyt_model: torch.optim.Adam(gpyt_model.parameters(),lr=0.1),     gpytorch_train_iter = 800,     gpytorch_use_gpu = gpytorch_use_gpu,     verbose = 200,     n_ref_approx = 2**22,     seed_ref_approx = None) solution,data = mcispfgp.integrate(seed=7,refit=False) print(data) df = pd.DataFrame(data.get_results_dict()) print(\"\\nIteration Summary\") print(df) fig = data.plot() df.to_csv(\"outputs/FourBranch2D_df.csv\",index=False) fig.savefig(\"outputs/FourBranch2D_fig.png\",dpi=256,transparent=True,bbox_inches=\"tight\") <pre>reference approximation with d=2: 0.208723783493042\nbatch 0\n\tgpytorch model fitting\n\t\titer 200 of 800\n\t\t\tlikelihood.noise_covar.raw_noise.................. 2.88e+00\n\t\t\tcovar_module.raw_outputscale...................... 3.85e+00\n\t\t\tcovar_module.base_kernel.raw_lengthscale.......... -4.50e+00\n\t\titer 400 of 800\n\t\t\tlikelihood.noise_covar.raw_noise.................. 3.63e+00\n\t\t\tcovar_module.raw_outputscale...................... 4.75e+00\n\t\t\tcovar_module.base_kernel.raw_lengthscale.......... -5.44e+00\n\t\titer 600 of 800\n\t\t\tlikelihood.noise_covar.raw_noise.................. 4.16e+00\n\t\t\tcovar_module.raw_outputscale...................... 5.33e+00\n\t\t\tcovar_module.base_kernel.raw_lengthscale.......... -6.04e+00\n\t\titer 800 of 800\n\t\t\tlikelihood.noise_covar.raw_noise.................. 4.58e+00\n\t\t\tcovar_module.raw_outputscale...................... 5.77e+00\n\t\t\tcovar_module.base_kernel.raw_lengthscale.......... -6.48e+00\nbatch 1\n\tAR sampling with efficiency 8.2e-03, expect 1470 draws: 1020, 1360, 1530, 1615, \nbatch 2\n\tAR sampling with efficiency 5.1e-03, expect 2344 draws: 1632, 2312, 2856, 3400, 3944, 4080, 4216, 4352, 4488, 4624, \nbatch 3\n\tAR sampling with efficiency 3.5e-03, expect 3474 draws: 2412, 3216, 3618, 3819, 4020, \nbatch 4\n\tAR sampling with efficiency 2.5e-03, expect 4789 draws: 3324, 5540, 6925, \nbatch 5\n\tAR sampling with efficiency 2.1e-03, expect 5641 draws: 3912, 5868, 6194, 6520, 6846, \nPFGPCIData (Data)\n    solution        0.209\n    error_bound     0.008\n    bound_low       0.202\n    bound_high      0.217\n    n_total         124\n    time_integrate  6.893\nPFGPCI (AbstractStoppingCriterion)\n    abs_tol         0.010\n    n_init          2^(6)\n    n_limit         200\n    n_batch         12\nFourBranch2d (AbstractIntegrand)\nUniform (AbstractTrueMeasure)\n    lower_bound     -8\n    upper_bound     2^(3)\nDigitalNetB2 (AbstractLDDiscreteDistribution)\n    d               2^(1)\n    replications    1\n    randomize       LMS_DS\n    gen_mats_source joe_kuo.6.21201.txt\n    order           NATURAL\n    t               63\n    alpha           1\n    n_limit         2^(32)\n    entropy         17\n\nIteration Summary\n   iter  n_sum  n_batch  error_bounds  ci_low  ci_high  solutions  solutions_ref  error_ref  in_ci\n0    0     64     64       4.1e-02    1.7e-01 2.5e-01   2.1e-01      2.1e-01      3.0e-03    True \n1    1     76     12       2.6e-02    1.9e-01 2.4e-01   2.1e-01      2.1e-01      5.4e-03    True \n2    2     88     12       1.7e-02    1.9e-01 2.3e-01   2.1e-01      2.1e-01      1.9e-03    True \n3    3    100     12       1.3e-02    2.0e-01 2.2e-01   2.1e-01      2.1e-01      1.0e-03    True \n4    4    112     12       1.1e-02    2.0e-01 2.2e-01   2.1e-01      2.1e-01      8.5e-05    True \n5    5    124     12       7.6e-03    2.0e-01 2.2e-01   2.1e-01      2.1e-01      5.5e-04    True \n</pre> In\u00a0[\u00a0]: Copied! <pre>mcispfgp = qp.PFGPCI(\n    integrand = qp.Ishigami(qp.DigitalNetB2(3,seed=17)),\n    failure_threshold = 0,\n    failure_above_threshold=False,\n    abs_tol = 1e-2,\n    alpha = 1e-1,\n    n_init = 128,\n    init_samples = None,\n    batch_sampler = qp.PFSampleErrorDensityAR(verbose=True),\n    n_batch = 16,\n    n_limit = 256,\n    n_approx = 2**18,\n    gpytorch_prior_mean = gpytorch.means.ZeroMean(),\n    gpytorch_prior_cov = gpytorch.kernels.ScaleKernel(\n        gpytorch.kernels.MaternKernel(nu=2.5,\n            lengthscale_constraint = gpytorch.constraints.Interval(.5,1)\n            ),\n        outputscale_constraint = gpytorch.constraints.Interval(1e-8,.5)\n        ),\n    gpytorch_likelihood = gpytorch.likelihoods.GaussianLikelihood(noise_constraint = gpytorch.constraints.Interval(1e-12,1e-8)),\n    gpytorch_marginal_log_likelihood_func = lambda likelihood,gpyt_model: gpytorch.mlls.ExactMarginalLogLikelihood(likelihood,gpyt_model),\n    torch_optimizer_func = lambda gpyt_model: torch.optim.Adam(gpyt_model.parameters(),lr=0.1),\n    gpytorch_train_iter = 800,\n    gpytorch_use_gpu = gpytorch_use_gpu,\n    verbose = 200,\n    n_ref_approx = 2**22,\n    seed_ref_approx = None)\nsolution,data = mcispfgp.integrate(seed=7,refit=False)\nprint(data)\ndf = pd.DataFrame(data.get_results_dict())\nprint(\"\\nIteration Summary\")\nprint(df)\nfig = data.plot()\nfig.tight_layout()\ndf.to_csv(\"outputs/Ishigami3D_df.csv\",index=False)\nfig.savefig(\"outputs/Ishigami3D_fig.png\",dpi=256,transparent=True,bbox_inches=\"tight\")\n</pre> mcispfgp = qp.PFGPCI(     integrand = qp.Ishigami(qp.DigitalNetB2(3,seed=17)),     failure_threshold = 0,     failure_above_threshold=False,     abs_tol = 1e-2,     alpha = 1e-1,     n_init = 128,     init_samples = None,     batch_sampler = qp.PFSampleErrorDensityAR(verbose=True),     n_batch = 16,     n_limit = 256,     n_approx = 2**18,     gpytorch_prior_mean = gpytorch.means.ZeroMean(),     gpytorch_prior_cov = gpytorch.kernels.ScaleKernel(         gpytorch.kernels.MaternKernel(nu=2.5,             lengthscale_constraint = gpytorch.constraints.Interval(.5,1)             ),         outputscale_constraint = gpytorch.constraints.Interval(1e-8,.5)         ),     gpytorch_likelihood = gpytorch.likelihoods.GaussianLikelihood(noise_constraint = gpytorch.constraints.Interval(1e-12,1e-8)),     gpytorch_marginal_log_likelihood_func = lambda likelihood,gpyt_model: gpytorch.mlls.ExactMarginalLogLikelihood(likelihood,gpyt_model),     torch_optimizer_func = lambda gpyt_model: torch.optim.Adam(gpyt_model.parameters(),lr=0.1),     gpytorch_train_iter = 800,     gpytorch_use_gpu = gpytorch_use_gpu,     verbose = 200,     n_ref_approx = 2**22,     seed_ref_approx = None) solution,data = mcispfgp.integrate(seed=7,refit=False) print(data) df = pd.DataFrame(data.get_results_dict()) print(\"\\nIteration Summary\") print(df) fig = data.plot() fig.tight_layout() df.to_csv(\"outputs/Ishigami3D_df.csv\",index=False) fig.savefig(\"outputs/Ishigami3D_fig.png\",dpi=256,transparent=True,bbox_inches=\"tight\") <pre>reference approximation with d=3: 0.16236066818237305\nbatch 0\n\tgpytorch model fitting\n\t\titer 200 of 800\n\t\t\tlikelihood.noise_covar.raw_noise.................. 2.21e+00\n\t\t\tcovar_module.raw_outputscale...................... 3.42e+00\n\t\t\tcovar_module.base_kernel.raw_lengthscale.......... -4.08e+00\n\t\titer 400 of 800\n\t\t\tlikelihood.noise_covar.raw_noise.................. 2.70e+00\n\t\t\tcovar_module.raw_outputscale...................... 4.25e+00\n\t\t\tcovar_module.base_kernel.raw_lengthscale.......... -4.98e+00\n\t\titer 600 of 800\n\t\t\tlikelihood.noise_covar.raw_noise.................. 3.12e+00\n\t\t\tcovar_module.raw_outputscale...................... 4.81e+00\n\t\t\tcovar_module.base_kernel.raw_lengthscale.......... -5.56e+00\n\t\titer 800 of 800\n\t\t\tlikelihood.noise_covar.raw_noise.................. 3.48e+00\n\t\t\tcovar_module.raw_outputscale...................... 5.23e+00\n\t\t\tcovar_module.base_kernel.raw_lengthscale.......... -5.99e+00\nbatch 1\n\tAR sampling with efficiency 6.2e-03, expect 2589 draws: 1792, 2240, 2576, \nbatch 2\n\tAR sampling with efficiency 5.1e-03, expect 3135 draws: 2176, 2992, 3672, 4080, 4352, 4624, 4896, \nbatch 3\n\tAR sampling with efficiency 4.3e-03, expect 3700 draws: 2560, 3040, \nbatch 4\n\tAR sampling with efficiency 3.8e-03, expect 4245 draws: 2944, 4600, 4968, \nbatch 5\n\tAR sampling with efficiency 3.3e-03, expect 4897 draws: 3392, 4876, 5936, 6360, 6572, \nbatch 6\n\tAR sampling with efficiency 2.9e-03, expect 5482 draws: 3808, 4284, \nbatch 7\n\tAR sampling with efficiency 2.6e-03, expect 6246 draws: 4336, 5691, \nbatch 8\n\tAR sampling with efficiency 2.3e-03, expect 6824 draws: 4736, 5328, \nPFGPCIData (Data)\n    solution        0.162\n    error_bound     0.010\n    bound_low       0.152\n    bound_high      0.172\n    n_total         2^(8)\n    time_integrate  20.302\nPFGPCI (AbstractStoppingCriterion)\n    abs_tol         0.010\n    n_init          2^(7)\n    n_limit         2^(8)\n    n_batch         2^(4)\nIshigami (AbstractIntegrand)\nUniform (AbstractTrueMeasure)\n    lower_bound     -3.142\n    upper_bound     3.142\nDigitalNetB2 (AbstractLDDiscreteDistribution)\n    d               3\n    replications    1\n    randomize       LMS_DS\n    gen_mats_source joe_kuo.6.21201.txt\n    order           NATURAL\n    t               63\n    alpha           1\n    n_limit         2^(32)\n    entropy         17\n\nIteration Summary\n   iter  n_sum  n_batch  error_bounds  ci_low  ci_high  solutions  solutions_ref  error_ref  in_ci\n0    0    128     128      3.1e-02    1.4e-01 2.0e-01   1.7e-01      1.6e-01      8.6e-03    True \n1    1    144      16      2.6e-02    1.4e-01 2.0e-01   1.7e-01      1.6e-01      7.7e-03    True \n2    2    160      16      2.2e-02    1.5e-01 1.9e-01   1.7e-01      1.6e-01      5.7e-03    True \n3    3    176      16      1.9e-02    1.5e-01 1.8e-01   1.7e-01      1.6e-01      3.0e-03    True \n4    4    192      16      1.6e-02    1.5e-01 1.8e-01   1.7e-01      1.6e-01      2.8e-03    True \n5    5    208      16      1.5e-02    1.5e-01 1.8e-01   1.6e-01      1.6e-01      1.8e-03    True \n6    6    224      16      1.3e-02    1.5e-01 1.8e-01   1.6e-01      1.6e-01      1.4e-03    True \n7    7    240      16      1.2e-02    1.5e-01 1.7e-01   1.6e-01      1.6e-01      8.9e-04    True \n8    8    256      16      1.0e-02    1.5e-01 1.7e-01   1.6e-01      1.6e-01      3.2e-04    True \n</pre> In\u00a0[\u00a0]: Copied! <pre>mcispfgp = qp.PFGPCI(\n    integrand = qp.Hartmann6d(qp.DigitalNetB2(6,seed=17)),\n    failure_threshold = -2,\n    failure_above_threshold=False,\n    abs_tol = 2.5e-3,\n    alpha = 1e-1,\n    n_init = 512,\n    init_samples = None,\n    batch_sampler = qp.PFSampleErrorDensityAR(verbose=True),\n    n_batch = 64,\n    n_limit = 2500,\n    n_approx = 2**18,\n    gpytorch_prior_mean = gpytorch.means.ZeroMean(),\n    gpytorch_prior_cov = gpytorch.kernels.ScaleKernel(gpytorch.kernels.MaternKernel(nu=2.5)),\n    gpytorch_likelihood = gpytorch.likelihoods.GaussianLikelihood(noise_constraint = gpytorch.constraints.Interval(1e-12,1e-8)),\n    gpytorch_marginal_log_likelihood_func = lambda likelihood,gpyt_model: gpytorch.mlls.ExactMarginalLogLikelihood(likelihood,gpyt_model),\n    torch_optimizer_func = lambda gpyt_model: torch.optim.Adam(gpyt_model.parameters(),lr=0.1),\n    gpytorch_train_iter = 150,\n    gpytorch_use_gpu = gpytorch_use_gpu,\n    verbose = 50,\n    n_ref_approx = 2**23,\n    seed_ref_approx = None)\nsolution,data = mcispfgp.integrate(seed=7,refit=False)\nprint(data)\ndf = pd.DataFrame(data.get_results_dict())\nprint(\"\\nIteration Summary\")\nprint(df)\nfig = data.plot()\nfig.tight_layout()\ndf.to_csv(\"outputs/Hartmann6D_df.csv\",index=False)\nfig.savefig(\"outputs/Hartmann6D_fig.png\",dpi=256,transparent=True,bbox_inches=\"tight\")\n</pre> mcispfgp = qp.PFGPCI(     integrand = qp.Hartmann6d(qp.DigitalNetB2(6,seed=17)),     failure_threshold = -2,     failure_above_threshold=False,     abs_tol = 2.5e-3,     alpha = 1e-1,     n_init = 512,     init_samples = None,     batch_sampler = qp.PFSampleErrorDensityAR(verbose=True),     n_batch = 64,     n_limit = 2500,     n_approx = 2**18,     gpytorch_prior_mean = gpytorch.means.ZeroMean(),     gpytorch_prior_cov = gpytorch.kernels.ScaleKernel(gpytorch.kernels.MaternKernel(nu=2.5)),     gpytorch_likelihood = gpytorch.likelihoods.GaussianLikelihood(noise_constraint = gpytorch.constraints.Interval(1e-12,1e-8)),     gpytorch_marginal_log_likelihood_func = lambda likelihood,gpyt_model: gpytorch.mlls.ExactMarginalLogLikelihood(likelihood,gpyt_model),     torch_optimizer_func = lambda gpyt_model: torch.optim.Adam(gpyt_model.parameters(),lr=0.1),     gpytorch_train_iter = 150,     gpytorch_use_gpu = gpytorch_use_gpu,     verbose = 50,     n_ref_approx = 2**23,     seed_ref_approx = None) solution,data = mcispfgp.integrate(seed=7,refit=False) print(data) df = pd.DataFrame(data.get_results_dict()) print(\"\\nIteration Summary\") print(df) fig = data.plot() fig.tight_layout() df.to_csv(\"outputs/Hartmann6D_df.csv\",index=False) fig.savefig(\"outputs/Hartmann6D_fig.png\",dpi=256,transparent=True,bbox_inches=\"tight\") <pre>reference approximation with d=6: 0.007369875907897949\nbatch 0\n\tgpytorch model fitting\n\t\titer 50  of 150\n\t\t\tlikelihood.noise_covar.raw_noise.................. 1.28e+00\n\t\t\tcovar_module.raw_outputscale...................... 5.62e-01\n\t\t\tcovar_module.base_kernel.raw_lengthscale.......... 1.90e-01\n\t\titer 100 of 150\n\t\t\tlikelihood.noise_covar.raw_noise.................. 2.28e+00\n\t\t\tcovar_module.raw_outputscale...................... 5.14e-01\n\t\t\tcovar_module.base_kernel.raw_lengthscale.......... 1.83e-01\n\t\titer 150 of 150\n\t\t\tlikelihood.noise_covar.raw_noise.................. 2.84e+00\n\t\t\tcovar_module.raw_outputscale...................... 5.12e-01\n\t\t\tcovar_module.base_kernel.raw_lengthscale.......... 1.83e-01\nbatch 1\n\tAR sampling with efficiency 2.5e-03, expect 25281 draws: 17536, 19180, \nbatch 2\n\tAR sampling with efficiency 2.5e-03, expect 25500 draws: 17664, 25944, 27600, \nbatch 3\n\tAR sampling with efficiency 2.0e-03, expect 31766 draws: 22016, 30272, 31992, 33368, 34056, 34744, 35088, 35432, 35776, 36120, 36464, 36808, \nbatch 4\n\tAR sampling with efficiency 1.7e-03, expect 36697 draws: 25472, 31840, 34626, 35024, 35422, \nbatch 5\n\tAR sampling with efficiency 1.5e-03, expect 42097 draws: 29184, 42408, 46056, \nbatch 6\n\tAR sampling with efficiency 1.3e-03, expect 50094 draws: 34752, 45612, 46155, 46698, \nbatch 7\n\tAR sampling with efficiency 1.1e-03, expect 55935 draws: 38784, 49086, 53328, \nbatch 8\n\tAR sampling with efficiency 1.0e-03, expect 63192 draws: 43840, 56855, \nbatch 9\n\tAR sampling with efficiency 9.4e-04, expect 68419 draws: 47424, 57798, 63726, 65949, 68172, \nbatch 10\n\tAR sampling with efficiency 8.8e-04, expect 72820 draws: 50496, 65487, 67854, 69432, 71010, 71799, 72588, \nbatch 11\n\tAR sampling with efficiency 8.1e-04, expect 79266 draws: 54976, 65284, 69579, 70438, 71297, \nbatch 12\n\tAR sampling with efficiency 7.6e-04, expect 84428 draws: 58560, 61305, 63135, 64050, 64965, \nbatch 13\n\tAR sampling with efficiency 7.0e-04, expect 91332 draws: 63296, 74175, 75164, 76153, 77142, 78131, \nbatch 14\n\tAR sampling with efficiency 6.7e-04, expect 95110 draws: 65920, 79310, 83430, \nbatch 15\n\tAR sampling with efficiency 6.4e-04, expect 99354 draws: 68864, 88232, 90384, 92536, 94688, \nbatch 16\n\tAR sampling with efficiency 6.2e-04, expect 103811 draws: 71936, 104532, 115772, 116896, 118020, \nbatch 17\n\tAR sampling with efficiency 6.0e-04, expect 106125 draws: 73600, 83950, 86250, \nbatch 18\n\tAR sampling with efficiency 5.7e-04, expect 111408 draws: 77248, 90525, 95353, \nbatch 19\n\tAR sampling with efficiency 5.5e-04, expect 116498 draws: 80768, 99698, 112318, 116104, 118628, \nbatch 20\n\tAR sampling with efficiency 5.3e-04, expect 120838 draws: 83776, 102102, 111265, 112574, \nbatch 21\n\tAR sampling with efficiency 5.1e-04, expect 124745 draws: 86464, 114835, 131047, 133749, 135100, 136451, \nPFGPCIData (Data)\n    solution        0.007\n    error_bound     0.002\n    bound_low       0.005\n    bound_high      0.010\n    n_total         1856\n    time_integrate  414.202\nPFGPCI (AbstractStoppingCriterion)\n    abs_tol         0.003\n    n_init          2^(9)\n    n_limit         2500\n    n_batch         2^(6)\nHartmann6d (AbstractIntegrand)\nUniform (AbstractTrueMeasure)\n    lower_bound     0\n    upper_bound     1\nDigitalNetB2 (AbstractLDDiscreteDistribution)\n    d               6\n    replications    1\n    randomize       LMS_DS\n    gen_mats_source joe_kuo.6.21201.txt\n    order           NATURAL\n    t               63\n    alpha           1\n    n_limit         2^(32)\n    entropy         17\n\nIteration Summary\n    iter  n_sum  n_batch  error_bounds  ci_low  ci_high  solutions  solutions_ref  error_ref  in_ci\n0     0    512     512      1.3e-02    0.0e+00 1.6e-02   3.3e-03      7.4e-03      4.1e-03    True \n1     1    576      64      1.3e-02    0.0e+00 1.9e-02   6.9e-03      7.4e-03      5.0e-04    True \n2     2    640      64      1.0e-02    0.0e+00 1.7e-02   7.2e-03      7.4e-03      1.4e-04    True \n3     3    704      64      8.7e-03    0.0e+00 1.6e-02   7.4e-03      7.4e-03      7.7e-06    True \n4     4    768      64      7.6e-03    0.0e+00 1.5e-02   7.4e-03      7.4e-03      1.2e-05    True \n5     5    832      64      6.4e-03    1.0e-03 1.4e-02   7.4e-03      7.4e-03      5.7e-05    True \n6     6    896      64      5.7e-03    1.7e-03 1.3e-02   7.5e-03      7.4e-03      9.5e-05    True \n7     7    960      64      5.1e-03    2.4e-03 1.3e-02   7.5e-03      7.4e-03      9.5e-05    True \n8     8   1024      64      4.7e-03    2.7e-03 1.2e-02   7.4e-03      7.4e-03      3.8e-05    True \n9     9   1088      64      4.4e-03    3.0e-03 1.2e-02   7.4e-03      7.4e-03      3.8e-05    True \n10   10   1152      64      4.0e-03    3.4e-03 1.1e-02   7.4e-03      7.4e-03      1.9e-05    True \n11   11   1216      64      3.8e-03    3.6e-03 1.1e-02   7.4e-03      7.4e-03      3.4e-05    True \n12   12   1280      64      3.5e-03    3.9e-03 1.1e-02   7.4e-03      7.4e-03      3.1e-05    True \n13   13   1344      64      3.4e-03    4.0e-03 1.1e-02   7.4e-03      7.4e-03      3.4e-05    True \n14   14   1408      64      3.2e-03    4.2e-03 1.1e-02   7.4e-03      7.4e-03      3.1e-05    True \n15   15   1472      64      3.1e-03    4.3e-03 1.0e-02   7.4e-03      7.4e-03      1.5e-05    True \n16   16   1536      64      3.0e-03    4.4e-03 1.0e-02   7.4e-03      7.4e-03      2.3e-05    True \n17   17   1600      64      2.9e-03    4.5e-03 1.0e-02   7.4e-03      7.4e-03      3.4e-05    True \n18   18   1664      64      2.7e-03    4.6e-03 1.0e-02   7.4e-03      7.4e-03      1.5e-05    True \n19   19   1728      64      2.6e-03    4.7e-03 1.0e-02   7.4e-03      7.4e-03      1.5e-05    True \n20   20   1792      64      2.6e-03    4.8e-03 9.9e-03   7.4e-03      7.4e-03      3.7e-06    True \n21   21   1856      64      2.5e-03    4.9e-03 9.9e-03   7.4e-03      7.4e-03      3.7e-06    True \n</pre> In\u00a0[17]: Copied! <pre>import umbridge\n</pre> import umbridge In\u00a0[18]: Copied! <pre>!docker run --name tsunami -d -it -p 4242:4242 linusseelinger/model-exahype-tsunami:latest # https://um-bridge-benchmarks.readthedocs.io/en/docs/models/exahype-tsunami.html\n</pre> !docker run --name tsunami -d -it -p 4242:4242 linusseelinger/model-exahype-tsunami:latest # https://um-bridge-benchmarks.readthedocs.io/en/docs/models/exahype-tsunami.html <pre>6aaec8dbceffcdae00e17362c385684ca72ef5e5308701505d9e3c135c9447c2\n</pre> In\u00a0[19]: Copied! <pre>ld_sampler = qp.DigitalNetB2(2,seed=7)\norigin_distrib = qp.Uniform(ld_sampler,lower_bound=[-239,-339],upper_bound=[739,339])\numbridge_tsu_model = umbridge.HTTPModel('http://localhost:4242','forward')\numbridge_config = {'d': origin_distrib.d, 'level':1}\nqmcpy_umbridge_tsu_model = qp.UMBridgeWrapper(origin_distrib,umbridge_tsu_model,umbridge_config,parallel=True)\n</pre> ld_sampler = qp.DigitalNetB2(2,seed=7) origin_distrib = qp.Uniform(ld_sampler,lower_bound=[-239,-339],upper_bound=[739,339]) umbridge_tsu_model = umbridge.HTTPModel('http://localhost:4242','forward') umbridge_config = {'d': origin_distrib.d, 'level':1} qmcpy_umbridge_tsu_model = qp.UMBridgeWrapper(origin_distrib,umbridge_tsu_model,umbridge_config,parallel=True) In\u00a0[28]: Copied! <pre>class TsunamiMaxWaveHeightBouy1(qp.Integrand):\n    # https://um-bridge-benchmarks.readthedocs.io/en/docs/models/exahype-tsunami.html#\n    def __init__(self, qmcpy_umbridge_tsu_model):\n        self.tsu_model = qmcpy_umbridge_tsu_model\n        self.true_measure = self.sampler = self.tsu_model.true_measure\n        assert self.tsu_model.d == 2\n        super(TsunamiMaxWaveHeightBouy1,self).__init__(\n            dimension_indv = (),\n            dimension_comb = (),\n            parallel = False,\n            threadpool = False)\n    def g(self, t):\n        assert t.ndim==2 and t.shape[1]==self.d\n        y = self.tsu_model.g(t)\n        y_buoy1_height = 1000*y[1] # max wave height at buoy (meters SSHA)\n        height_above_thresh = y_buoy1_height\n        return height_above_thresh\ntmwhb1 = TsunamiMaxWaveHeightBouy1(qmcpy_umbridge_tsu_model)\n</pre> class TsunamiMaxWaveHeightBouy1(qp.Integrand):     # https://um-bridge-benchmarks.readthedocs.io/en/docs/models/exahype-tsunami.html#     def __init__(self, qmcpy_umbridge_tsu_model):         self.tsu_model = qmcpy_umbridge_tsu_model         self.true_measure = self.sampler = self.tsu_model.true_measure         assert self.tsu_model.d == 2         super(TsunamiMaxWaveHeightBouy1,self).__init__(             dimension_indv = (),             dimension_comb = (),             parallel = False,             threadpool = False)     def g(self, t):         assert t.ndim==2 and t.shape[1]==self.d         y = self.tsu_model.g(t)         y_buoy1_height = 1000*y[1] # max wave height at buoy (meters SSHA)         height_above_thresh = y_buoy1_height         return height_above_thresh tmwhb1 = TsunamiMaxWaveHeightBouy1(qmcpy_umbridge_tsu_model) In\u00a0[29]: Copied! <pre>mcispfgp = qp.PFGPCI(\n    integrand = tmwhb1,\n    failure_threshold = 3,\n    failure_above_threshold=True,\n    abs_tol = 1e-3,\n    alpha = 1e-1,\n    n_init = 64,\n    init_samples = None,\n    batch_sampler = qp.PFSampleErrorDensityAR(verbose=True),\n    n_batch = 8,\n    n_limit = 128,\n    n_approx = 2**20,\n    gpytorch_prior_mean = gpytorch.means.ConstantMean(),\n    gpytorch_prior_cov = gpytorch.kernels.ScaleKernel(\n        gpytorch.kernels.MaternKernel(nu=2.5,\n            lengthscale_constraint = gpytorch.constraints.Interval(.1,1)),\n        outputscale_constraint = gpytorch.constraints.Interval(1e-8,1)),\n    gpytorch_likelihood = gpytorch.likelihoods.GaussianLikelihood(noise_constraint = gpytorch.constraints.Interval(1e-12,1e-10)),\n    gpytorch_marginal_log_likelihood_func = lambda likelihood,gpyt_model: gpytorch.mlls.ExactMarginalLogLikelihood(likelihood,gpyt_model),\n    torch_optimizer_func = lambda gpyt_model: torch.optim.Adam(gpyt_model.parameters(),lr=0.1),\n    gpytorch_train_iter = 1000,\n    gpytorch_use_gpu = gpytorch_use_gpu,\n    verbose = 1000,\n    n_ref_approx = 0)\nsolution,data = mcispfgp.integrate(seed=11,refit=False)\nprint(data)\ndf = pd.DataFrame(data.get_results_dict())\nprint(\"\\nIteration Summary\")\nprint(df)\nfig = data.plot()\ndf.to_csv(\"outputs/Tsunami2D_df.csv\",index=False)\nfig.savefig(\"outputs/Tsunami2D_fig.png\",dpi=256,transparent=True,bbox_inches=\"tight\")\n</pre> mcispfgp = qp.PFGPCI(     integrand = tmwhb1,     failure_threshold = 3,     failure_above_threshold=True,     abs_tol = 1e-3,     alpha = 1e-1,     n_init = 64,     init_samples = None,     batch_sampler = qp.PFSampleErrorDensityAR(verbose=True),     n_batch = 8,     n_limit = 128,     n_approx = 2**20,     gpytorch_prior_mean = gpytorch.means.ConstantMean(),     gpytorch_prior_cov = gpytorch.kernels.ScaleKernel(         gpytorch.kernels.MaternKernel(nu=2.5,             lengthscale_constraint = gpytorch.constraints.Interval(.1,1)),         outputscale_constraint = gpytorch.constraints.Interval(1e-8,1)),     gpytorch_likelihood = gpytorch.likelihoods.GaussianLikelihood(noise_constraint = gpytorch.constraints.Interval(1e-12,1e-10)),     gpytorch_marginal_log_likelihood_func = lambda likelihood,gpyt_model: gpytorch.mlls.ExactMarginalLogLikelihood(likelihood,gpyt_model),     torch_optimizer_func = lambda gpyt_model: torch.optim.Adam(gpyt_model.parameters(),lr=0.1),     gpytorch_train_iter = 1000,     gpytorch_use_gpu = gpytorch_use_gpu,     verbose = 1000,     n_ref_approx = 0) solution,data = mcispfgp.integrate(seed=11,refit=False) print(data) df = pd.DataFrame(data.get_results_dict()) print(\"\\nIteration Summary\") print(df) fig = data.plot() df.to_csv(\"outputs/Tsunami2D_df.csv\",index=False) fig.savefig(\"outputs/Tsunami2D_fig.png\",dpi=256,transparent=True,bbox_inches=\"tight\") <pre>batch 0\n\tgpytorch model fitting\n\t\titer 1000 of 1000\n\t\t\tlikelihood.noise_covar.raw_noise.................. 1.17e+00\n\t\t\tmean_module.raw_constant.......................... -2.25e+00\n\t\t\tcovar_module.raw_outputscale...................... 1.99e+00\n\t\t\tcovar_module.base_kernel.raw_lengthscale.......... -3.02e+00\nbatch 1\n\tAR sampling with efficiency 1.9e-02, expect 426 draws: 296, 444, 555, \nbatch 2\n\tAR sampling with efficiency 5.5e-03, expect 1450 draws: 1008, 1512, 1638, \nbatch 3\n\tAR sampling with efficiency 1.1e-02, expect 757 draws: 528, 924, 990, \nbatch 4\n\tAR sampling with efficiency 3.2e-03, expect 2489 draws: 1728, 3024, \nbatch 5\n\tAR sampling with efficiency 2.5e-03, expect 3227 draws: 2240, 2800, 3360, 3640, \nbatch 6\n\tAR sampling with efficiency 1.4e-03, expect 5698 draws: 3952, \nbatch 7\n\tAR sampling with efficiency 1.7e-03, expect 4841 draws: 3360, 3780, \nbatch 8\n\tAR sampling with efficiency 1.6e-03, expect 4905 draws: 3400, 4675, 5100, \nPFGPCIData (Data)\n    solution        0.057\n    error_bound     0.005\n    bound_low       0.053\n    bound_high      0.062\n    n_total         2^(7)\n    time_integrate  2099.126\nPFGPCI (AbstractStoppingCriterion)\n    abs_tol         0.001\n    n_init          2^(6)\n    n_limit         2^(7)\n    n_batch         2^(3)\nTsunamiMaxWaveHeightBouy1 (AbstractIntegrand)\nUniform (AbstractTrueMeasure)\n    lower_bound     [-239 -339]\n    upper_bound     [739 339]\nDigitalNetB2 (AbstractLDDiscreteDistribution)\n    d               2^(1)\n    replications    1\n    randomize       LMS_DS\n    gen_mats_source joe_kuo.6.21201.txt\n    order           NATURAL\n    t               63\n    alpha           1\n    n_limit         2^(32)\n    entropy         7\n\nIteration Summary\n   iter  n_sum  n_batch  error_bounds  ci_low  ci_high  solutions\n0    0     64     64       9.4e-02    0.0e+00 1.5e-01   5.3e-02  \n1    1     72      8       2.8e-02    3.5e-02 9.0e-02   6.2e-02  \n2    2     80      8       5.3e-02    1.9e-02 1.2e-01   7.1e-02  \n3    3     88      8       1.6e-02    5.0e-02 8.2e-02   6.6e-02  \n4    4     96      8       1.2e-02    4.9e-02 7.4e-02   6.2e-02  \n5    5    104      8       7.0e-03    5.4e-02 6.8e-02   6.1e-02  \n6    6    112      8       8.3e-03    5.1e-02 6.8e-02   5.9e-02  \n7    7    120      8       8.2e-03    5.2e-02 6.8e-02   6.0e-02  \n8    8    128      8       4.5e-03    5.3e-02 6.2e-02   5.7e-02  \n</pre> In\u00a0[30]: Copied! <pre>!docker rm -f tsunami\n</pre> !docker rm -f tsunami <pre>python(15698) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n</pre> <pre>tsunami\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"demos/talk_paper_demos/ProbFailureSorokinRao/prob_failure_gp_ci/#probability-of-failure-estimation-with-gaussian-processes","title":"Probability of Failure Estimation with Gaussian Processes\u00b6","text":""},{"location":"demos/talk_paper_demos/ProbFailureSorokinRao/prob_failure_gp_ci/#method-visual","title":"Method Visual\u00b6","text":""},{"location":"demos/talk_paper_demos/ProbFailureSorokinRao/prob_failure_gp_ci/#sin-1d-problem","title":"Sin 1d Problem\u00b6","text":""},{"location":"demos/talk_paper_demos/ProbFailureSorokinRao/prob_failure_gp_ci/#multimodal-2d-problem","title":"Multimodal 2d Problem\u00b6","text":""},{"location":"demos/talk_paper_demos/ProbFailureSorokinRao/prob_failure_gp_ci/#four-branch-2d-problem","title":"Four Branch 2d Problem\u00b6","text":""},{"location":"demos/talk_paper_demos/ProbFailureSorokinRao/prob_failure_gp_ci/#ishigami-3d-problem","title":"Ishigami 3d Problem\u00b6","text":""},{"location":"demos/talk_paper_demos/ProbFailureSorokinRao/prob_failure_gp_ci/#hartmann-6d-problem","title":"Hartmann 6d Problem\u00b6","text":""},{"location":"demos/talk_paper_demos/ProbFailureSorokinRao/prob_failure_gp_ci/#tsunami","title":"Tsunami\u00b6","text":""},{"location":"demos/talk_paper_demos/Purdue_Talk_2023_March/Purdue_Talk_Figures/","title":"2023 Purdue Talk","text":"In\u00a0[1]: Copied! <pre>import matplotlib.pyplot as plt\nimport numpy as np\nimport qmcpy as qp\nimport time  #timing routines\nimport warnings  #to suppress warnings when needed\nimport pickle  #write output to a file and load it back in\nfrom copy import deepcopy\n\nplt.rc('font', size=16)  #set defaults so that the plots are readable\nplt.rc('axes', titlesize=16)\nplt.rc('axes', labelsize=16)\nplt.rc('xtick', labelsize=16)\nplt.rc('ytick', labelsize=16)\nplt.rc('legend', fontsize=16)\nplt.rc('figure', titlesize=16)\n\n#a helpful plotting method to show increasing numbers of points\ndef plot_successive_points(distrib,ld_name,first_n=64,n_cols=1,\n                           pt_clr=['tab:blue', 'tab:green', 'k', 'tab:cyan', 'tab:purple', 'tab:orange'],\n                           xlim=[0,1],ylim=[0,1]):\n  fig,ax = plt.subplots(nrows=1,ncols=n_cols,figsize=(5*n_cols,5.5))\n  if n_cols==1: ax = [ax]\n  last_n = first_n*(2**n_cols)\n  points = distrib.gen_samples(n=last_n)\n  for i in range(n_cols):\n    n = first_n\n    nstart = 0\n    for j in range(i+1):\n      n = first_n*(2**j)\n      ax[i].scatter(points[nstart:n,0],points[nstart:n,1],color=pt_clr[j])\n      nstart = n\n    ax[i].set_title('n = %d'%n)\n    ax[i].set_xlim(xlim); ax[i].set_xticks(xlim); ax[i].set_xlabel('$x_{i,1}$')\n    ax[i].set_ylim(ylim); ax[i].set_yticks(ylim); ax[i].set_ylabel('$x_{i,2}$')\n    ax[i].set_aspect((xlim[1]-xlim[0])/(ylim[1]-ylim[0]))\n  fig.suptitle('%s Points'%ld_name, y=0.87)\n  return fig\n\nprint('QMCPy Version',qp.__version__)\n</pre> import matplotlib.pyplot as plt import numpy as np import qmcpy as qp import time  #timing routines import warnings  #to suppress warnings when needed import pickle  #write output to a file and load it back in from copy import deepcopy  plt.rc('font', size=16)  #set defaults so that the plots are readable plt.rc('axes', titlesize=16) plt.rc('axes', labelsize=16) plt.rc('xtick', labelsize=16) plt.rc('ytick', labelsize=16) plt.rc('legend', fontsize=16) plt.rc('figure', titlesize=16)  #a helpful plotting method to show increasing numbers of points def plot_successive_points(distrib,ld_name,first_n=64,n_cols=1,                            pt_clr=['tab:blue', 'tab:green', 'k', 'tab:cyan', 'tab:purple', 'tab:orange'],                            xlim=[0,1],ylim=[0,1]):   fig,ax = plt.subplots(nrows=1,ncols=n_cols,figsize=(5*n_cols,5.5))   if n_cols==1: ax = [ax]   last_n = first_n*(2**n_cols)   points = distrib.gen_samples(n=last_n)   for i in range(n_cols):     n = first_n     nstart = 0     for j in range(i+1):       n = first_n*(2**j)       ax[i].scatter(points[nstart:n,0],points[nstart:n,1],color=pt_clr[j])       nstart = n     ax[i].set_title('n = %d'%n)     ax[i].set_xlim(xlim); ax[i].set_xticks(xlim); ax[i].set_xlabel('$x_{i,1}$')     ax[i].set_ylim(ylim); ax[i].set_yticks(ylim); ax[i].set_ylabel('$x_{i,2}$')     ax[i].set_aspect((xlim[1]-xlim[0])/(ylim[1]-ylim[0]))   fig.suptitle('%s Points'%ld_name, y=0.87)   return fig  print('QMCPy Version',qp.__version__) <pre>QMCPy Version 1.6.3c\n</pre> In\u00a0[2]: Copied! <pre>figpath = '' #this path sends the figures to the directory that you want\n</pre> figpath = '' #this path sends the figures to the directory that you want In\u00a0[3]: Copied! <pre>d = 5 #dimension\nn = 16 #number of points\nld = qp.Lattice(d) #define the generator\nxpts = ld.gen_samples(n) #generate points\nprint(xpts)\nfig = plot_successive_points(ld,'Lattice',first_n=n,n_cols=4)\nfig.savefig(figpath+'latticepts.eps',format='eps')\n</pre> d = 5 #dimension n = 16 #number of points ld = qp.Lattice(d) #define the generator xpts = ld.gen_samples(n) #generate points print(xpts) fig = plot_successive_points(ld,'Lattice',first_n=n,n_cols=4) fig.savefig(figpath+'latticepts.eps',format='eps') <pre>[[0.40132838 0.98330345 0.11124065 0.76370523 0.74520191]\n [0.90132838 0.48330345 0.61124065 0.26370523 0.24520191]\n [0.65132838 0.73330345 0.86124065 0.51370523 0.99520191]\n [0.15132838 0.23330345 0.36124065 0.01370523 0.49520191]\n [0.52632838 0.35830345 0.48624065 0.63870523 0.37020191]\n [0.02632838 0.85830345 0.98624065 0.13870523 0.87020191]\n [0.77632838 0.10830345 0.23624065 0.38870523 0.62020191]\n [0.27632838 0.60830345 0.73624065 0.88870523 0.12020191]\n [0.46382838 0.67080345 0.29874065 0.20120523 0.55770191]\n [0.96382838 0.17080345 0.79874065 0.70120523 0.05770191]\n [0.71382838 0.42080345 0.04874065 0.95120523 0.80770191]\n [0.21382838 0.92080345 0.54874065 0.45120523 0.30770191]\n [0.58882838 0.04580345 0.67374065 0.07620523 0.18270191]\n [0.08882838 0.54580345 0.17374065 0.57620523 0.68270191]\n [0.83882838 0.79580345 0.42374065 0.82620523 0.43270191]\n [0.33882838 0.29580345 0.92374065 0.32620523 0.93270191]]\n</pre> In\u00a0[4]: Copied! <pre>ld = qp.Sobol(d) #define the generator\nxpts_Sobol = ld.gen_samples(n) #generate points\nfig = plot_successive_points(ld,'Sobol\\'',first_n=n,n_cols=4)\nfig.savefig(figpath+'sobolpts.eps',format='eps')\n</pre> ld = qp.Sobol(d) #define the generator xpts_Sobol = ld.gen_samples(n) #generate points fig = plot_successive_points(ld,'Sobol\\'',first_n=n,n_cols=4) fig.savefig(figpath+'sobolpts.eps',format='eps') In\u00a0[5]: Copied! <pre>iid = qp.IIDStdUniform(d) #define the generator\nxpts = ld.gen_samples(n) #generate points\nxpts\nfig = plot_successive_points(iid,'IID',first_n=n,n_cols=4)\nfig.savefig(figpath+'iidpts.eps',format='eps')\n</pre> iid = qp.IIDStdUniform(d) #define the generator xpts = ld.gen_samples(n) #generate points xpts fig = plot_successive_points(iid,'IID',first_n=n,n_cols=4) fig.savefig(figpath+'iidpts.eps',format='eps') In\u00a0[6]: Copied! <pre>with open(figpath+'iid_ld.pkl','rb') as myfile: tol_vec,n_tol,ii_iid,ld_time,ld_n,iid_time,iid_n,best_solution_i = pickle.load(myfile)\nprint(best_solution_i)\nfig,ax = plt.subplots(nrows=1,ncols=2,figsize=(13,5.5))\nax[0].scatter(tol_vec[0:n_tol],ld_time[0:n_tol],color='b'); \nax[0].plot(tol_vec[0:n_tol],[(ld_time[0]*tol_vec[0])/tol_vec[jj] for jj in range(n_tol)],color='b')\nax[0].scatter(tol_vec[ii_iid:n_tol],iid_time[ii_iid:n_tol],color='g'); \nax[0].plot(tol_vec[ii_iid:n_tol],[(iid_time[ii_iid]*(tol_vec[ii_iid]**2))/(tol_vec[jj]**2) for jj in range(ii_iid,n_tol)],color='g')\nax[0].set_ylim([0.001,1000]); ax[0].set_ylabel('Time (s)')\nax[1].scatter(tol_vec[0:n_tol],ld_n[0:n_tol],color='b'); \nax[1].plot(tol_vec[0:n_tol],[(ld_n[0]*tol_vec[0])/tol_vec[jj] for jj in range(n_tol)],color='b')\nax[1].scatter(tol_vec[ii_iid:n_tol],iid_n[ii_iid:n_tol],color='g'); \nax[1].plot(tol_vec[ii_iid:n_tol],[(iid_n[ii_iid]*(tol_vec[ii_iid]**2))/(tol_vec[jj]**2) for jj in range(ii_iid,n_tol)],color='g')\nax[1].set_ylim([1e2,1e8]); ax[1].set_ylabel('n')\nfor ii in range(2):\n  ax[ii].set_xlim([0.007,100]); ax[ii].set_xlabel('Tolerance, '+r'$\\varepsilon$')\n  ax[ii].set_xscale('log'); ax[ii].set_yscale('log')\n  ax[ii].legend([r'$\\mathcal{O}(\\varepsilon^{-1})$',r'$\\mathcal{O}(\\varepsilon^{-2})$','LD','IID'],frameon=False)\n  ax[ii].set_aspect(0.65)\nax[0].set_yticks([1, 60, 3600], labels = ['1 sec', '1 min', '1 hr'])    \nfig.savefig(figpath+'iidldbeam.eps',format='eps',bbox_inches='tight')\n</pre> with open(figpath+'iid_ld.pkl','rb') as myfile: tol_vec,n_tol,ii_iid,ld_time,ld_n,iid_time,iid_n,best_solution_i = pickle.load(myfile) print(best_solution_i) fig,ax = plt.subplots(nrows=1,ncols=2,figsize=(13,5.5)) ax[0].scatter(tol_vec[0:n_tol],ld_time[0:n_tol],color='b');  ax[0].plot(tol_vec[0:n_tol],[(ld_time[0]*tol_vec[0])/tol_vec[jj] for jj in range(n_tol)],color='b') ax[0].scatter(tol_vec[ii_iid:n_tol],iid_time[ii_iid:n_tol],color='g');  ax[0].plot(tol_vec[ii_iid:n_tol],[(iid_time[ii_iid]*(tol_vec[ii_iid]**2))/(tol_vec[jj]**2) for jj in range(ii_iid,n_tol)],color='g') ax[0].set_ylim([0.001,1000]); ax[0].set_ylabel('Time (s)') ax[1].scatter(tol_vec[0:n_tol],ld_n[0:n_tol],color='b');  ax[1].plot(tol_vec[0:n_tol],[(ld_n[0]*tol_vec[0])/tol_vec[jj] for jj in range(n_tol)],color='b') ax[1].scatter(tol_vec[ii_iid:n_tol],iid_n[ii_iid:n_tol],color='g');  ax[1].plot(tol_vec[ii_iid:n_tol],[(iid_n[ii_iid]*(tol_vec[ii_iid]**2))/(tol_vec[jj]**2) for jj in range(ii_iid,n_tol)],color='g') ax[1].set_ylim([1e2,1e8]); ax[1].set_ylabel('n') for ii in range(2):   ax[ii].set_xlim([0.007,100]); ax[ii].set_xlabel('Tolerance, '+r'$\\varepsilon$')   ax[ii].set_xscale('log'); ax[ii].set_yscale('log')   ax[ii].legend([r'$\\mathcal{O}(\\varepsilon^{-1})$',r'$\\mathcal{O}(\\varepsilon^{-2})$','LD','IID'],frameon=False)   ax[ii].set_aspect(0.65) ax[0].set_yticks([1, 60, 3600], labels = ['1 sec', '1 min', '1 hr'])     fig.savefig(figpath+'iidldbeam.eps',format='eps',bbox_inches='tight') <pre>[1037.12106673]\n</pre> In\u00a0[7]: Copied! <pre>with open(figpath+'ld_parallel.pkl','rb') as myfile: tol_vec,n_tol,ld_time,ld_n,ld_p_time,ld_p_n,best_solution = pickle.load(myfile)\nprint(best_solution_i)\nfig,ax = plt.subplots(nrows=1,ncols=2,figsize=(11,4))\nax[0].scatter(tol_vec[0:n_tol],ld_time[0:n_tol],color='tab:blue'); \nax[0].plot(tol_vec[0:n_tol],[(ld_time[0]*tol_vec[0])/tol_vec[jj] for jj in range(n_tol)],color='tab:blue')\nax[0].scatter(tol_vec[0:n_tol],ld_p_time[0:n_tol],color='tab:orange',marker = '+',s=100,linewidths=2); \n#ax[0].plot(tol_vec[0:n_tol],[(ld_p_time[0]*tol_vec[0])/tol_vec[jj] for jj in range(n_tol)],color='tab:orange')\nax[0].set_ylim([0.05,500]); ax[0].set_ylabel('Time')\nax[1].scatter(tol_vec[0:n_tol],ld_n[0:n_tol],color='tab:blue'); \nax[1].plot(tol_vec[0:n_tol],[(ld_n[0]*tol_vec[0])/tol_vec[jj] for jj in range(n_tol)],color='tab:blue')\nax[1].scatter(tol_vec[0:n_tol],ld_p_n[0:n_tol],color='tab:orange',marker = '+',s=100,linewidths=2); \n#ax[1].plot(tol_vec[0:n_tol],[(ld_p_n[0]*tol_vec[0])/tol_vec[jj] for jj in range(n_tol)],color='tab:orange')\nax[1].set_ylim([10,1e5]); ax[1].set_ylabel('n')\nfor ii in range(2):\n  ax[ii].set_xlim([0.007,4]); ax[ii].set_xlabel('Tolerance, '+r'$\\varepsilon$')\n  ax[ii].set_xscale('log'); ax[ii].set_yscale('log')\n  ax[ii].legend(['Lattice',r'$\\mathcal{O}(\\varepsilon^{-1})$','Lattice Parallel',r'$\\mathcal{O}(\\varepsilon^{-1})$'],frameon=False)\n  ax[ii].set_aspect(0.6)\nax[0].set_yticks([1, 60], labels = ['1 sec', '1 min'])\nfig.savefig(figpath+'ldparallelbeam.eps',format='eps',bbox_inches='tight')\n</pre> with open(figpath+'ld_parallel.pkl','rb') as myfile: tol_vec,n_tol,ld_time,ld_n,ld_p_time,ld_p_n,best_solution = pickle.load(myfile) print(best_solution_i) fig,ax = plt.subplots(nrows=1,ncols=2,figsize=(11,4)) ax[0].scatter(tol_vec[0:n_tol],ld_time[0:n_tol],color='tab:blue');  ax[0].plot(tol_vec[0:n_tol],[(ld_time[0]*tol_vec[0])/tol_vec[jj] for jj in range(n_tol)],color='tab:blue') ax[0].scatter(tol_vec[0:n_tol],ld_p_time[0:n_tol],color='tab:orange',marker = '+',s=100,linewidths=2);  #ax[0].plot(tol_vec[0:n_tol],[(ld_p_time[0]*tol_vec[0])/tol_vec[jj] for jj in range(n_tol)],color='tab:orange') ax[0].set_ylim([0.05,500]); ax[0].set_ylabel('Time') ax[1].scatter(tol_vec[0:n_tol],ld_n[0:n_tol],color='tab:blue');  ax[1].plot(tol_vec[0:n_tol],[(ld_n[0]*tol_vec[0])/tol_vec[jj] for jj in range(n_tol)],color='tab:blue') ax[1].scatter(tol_vec[0:n_tol],ld_p_n[0:n_tol],color='tab:orange',marker = '+',s=100,linewidths=2);  #ax[1].plot(tol_vec[0:n_tol],[(ld_p_n[0]*tol_vec[0])/tol_vec[jj] for jj in range(n_tol)],color='tab:orange') ax[1].set_ylim([10,1e5]); ax[1].set_ylabel('n') for ii in range(2):   ax[ii].set_xlim([0.007,4]); ax[ii].set_xlabel('Tolerance, '+r'$\\varepsilon$')   ax[ii].set_xscale('log'); ax[ii].set_yscale('log')   ax[ii].legend(['Lattice',r'$\\mathcal{O}(\\varepsilon^{-1})$','Lattice Parallel',r'$\\mathcal{O}(\\varepsilon^{-1})$'],frameon=False)   ax[ii].set_aspect(0.6) ax[0].set_yticks([1, 60], labels = ['1 sec', '1 min']) fig.savefig(figpath+'ldparallelbeam.eps',format='eps',bbox_inches='tight') <pre>[1037.12106673]\n</pre> In\u00a0[8]: Copied! <pre>fig,ax = plt.subplots(figsize=(6,3))\nax.plot(best_solution,'-')\nax.set_xlim([0,len(best_solution)-1]); ax.set_xlabel('Position')\nax.set_ylim([1050,-50]);  ax.set_ylabel('Mean Deflection');\nax.set_aspect(0.02)\nfig.suptitle('Cantilevered Beam')\nfig.savefig(figpath+'cantileveredbeamwords.eps',format='eps',bbox_inches='tight')\n</pre> fig,ax = plt.subplots(figsize=(6,3)) ax.plot(best_solution,'-') ax.set_xlim([0,len(best_solution)-1]); ax.set_xlabel('Position') ax.set_ylim([1050,-50]);  ax.set_ylabel('Mean Deflection'); ax.set_aspect(0.02) fig.suptitle('Cantilevered Beam') fig.savefig(figpath+'cantileveredbeamwords.eps',format='eps',bbox_inches='tight') In\u00a0[9]: Copied! <pre>qp.util.stop_notebook()\n</pre> qp.util.stop_notebook() <p>Below is long-running code, that we rarely wish to run</p> In\u00a0[11]: Copied! <pre>import umbridge #this is the connector\n!docker run --name muqbp -d -it -p 4243:4243 linusseelinger/benchmark-muq-beam-propagation:latest #get beam example\nd = 3 #dimension of the randomness\nlb = 1 #lower bound on randomness\nub = 1.2 #upper bound on randomness\numbridge_config = {\"d\": d}\nmodel = umbridge.HTTPModel('http://localhost:4243','forward') #this is the original model\noutindex = -1 #choose last element of the vector of beam deflections\nmodeli = deepcopy(model) #and construct a model for just that deflection\nmodeli.get_output_sizes = lambda *args : [1]\nmodeli.get_output_sizes()\nmodeli.__call__ = lambda *args,**kwargs: [[model.__call__(*args,**kwargs)[0][outindex]]]\n</pre> import umbridge #this is the connector !docker run --name muqbp -d -it -p 4243:4243 linusseelinger/benchmark-muq-beam-propagation:latest #get beam example d = 3 #dimension of the randomness lb = 1 #lower bound on randomness ub = 1.2 #upper bound on randomness umbridge_config = {\"d\": d} model = umbridge.HTTPModel('http://localhost:4243','forward') #this is the original model outindex = -1 #choose last element of the vector of beam deflections modeli = deepcopy(model) #and construct a model for just that deflection modeli.get_output_sizes = lambda *args : [1] modeli.get_output_sizes() modeli.__call__ = lambda *args,**kwargs: [[model.__call__(*args,**kwargs)[0][outindex]]] <pre>docker: Error response from daemon: Conflict. The container name \"/muqbp\" is already in use by container \"3bf2d2f6046f7ed0b096ce6945a8c3ee4662fb05f481c81b0e68b3345c125125\". You have to remove (or rename) that container to be able to reuse that name.\nSee 'docker run --help'.\n</pre> In\u00a0[\u00a0]: Copied! <pre>ld = qp.Uniform(qp.Lattice(d,seed=7),lower_bound=lb,upper_bound=ub) #lattice points for this problem\nld_integ = qp.UMBridgeWrapper(ld,modeli,umbridge_config,parallel=False) #integrand\niid = qp.Uniform(qp.IIDStdUniform(d),lower_bound=lb,upper_bound=ub) #iid points for this problem\niid_integ = qp.UMBridgeWrapper(iid,modeli,umbridge_config,parallel=False) #integrand\ntol = 0.01  #smallest tolerance\n\nn_tol = 14  #number of different tolerances\nii_iid = 9  #make this larger to reduce the time required by not running all cases for IID\ntol_vec = [tol*(2**ii) for ii in range(n_tol)]  #initialize vector of tolerances\nld_time = [0]*n_tol; ld_n = [0]*n_tol  #low discrepancy time and number of function values\niid_time = [0]*n_tol; iid_n = [0]*n_tol  #IID time and number of function values\nprint(f'\\nCantilever Beam\\n')\nprint('iteration ', end = '')\nfor ii in range(n_tol):\n  solution, data = qp.CubQMCLatticeG(ld_integ, abs_tol = tol_vec[ii]).integrate()\n  if ii == 0:\n    best_solution_i = solution\n  ld_time[ii] = data.time_integrate\n  ld_n[ii] = data.n_total\n  if ii &gt;= ii_iid:\n    solution, data = qp.CubMCG(iid_integ, abs_tol = tol_vec[ii]).integrate()\n    iid_time[ii] = data.time_integrate\n    iid_n[ii] = data.n_total\n  print(ii, end = ' ')\nwith open(figpath+'iid_ld.pkl','wb') as myfile:pickle.dump([tol_vec,n_tol,ii_iid,ld_time,ld_n,iid_time,iid_n,best_solution_i],myfile)\n</pre> ld = qp.Uniform(qp.Lattice(d,seed=7),lower_bound=lb,upper_bound=ub) #lattice points for this problem ld_integ = qp.UMBridgeWrapper(ld,modeli,umbridge_config,parallel=False) #integrand iid = qp.Uniform(qp.IIDStdUniform(d),lower_bound=lb,upper_bound=ub) #iid points for this problem iid_integ = qp.UMBridgeWrapper(iid,modeli,umbridge_config,parallel=False) #integrand tol = 0.01  #smallest tolerance  n_tol = 14  #number of different tolerances ii_iid = 9  #make this larger to reduce the time required by not running all cases for IID tol_vec = [tol*(2**ii) for ii in range(n_tol)]  #initialize vector of tolerances ld_time = [0]*n_tol; ld_n = [0]*n_tol  #low discrepancy time and number of function values iid_time = [0]*n_tol; iid_n = [0]*n_tol  #IID time and number of function values print(f'\\nCantilever Beam\\n') print('iteration ', end = '') for ii in range(n_tol):   solution, data = qp.CubQMCLatticeG(ld_integ, abs_tol = tol_vec[ii]).integrate()   if ii == 0:     best_solution_i = solution   ld_time[ii] = data.time_integrate   ld_n[ii] = data.n_total   if ii &gt;= ii_iid:     solution, data = qp.CubMCG(iid_integ, abs_tol = tol_vec[ii]).integrate()     iid_time[ii] = data.time_integrate     iid_n[ii] = data.n_total   print(ii, end = ' ') with open(figpath+'iid_ld.pkl','wb') as myfile:pickle.dump([tol_vec,n_tol,ii_iid,ld_time,ld_n,iid_time,iid_n,best_solution_i],myfile) In\u00a0[\u00a0]: Copied! <pre>ld_integ = qp.UMBridgeWrapper(ld,model,umbridge_config,parallel=False) #integrand\nld_integ_p = qp.UMBridgeWrapper(ld,model,umbridge_config,parallel=8) #integrand with parallel processing\n\ntol = 0.01\nn_tol = 9  #number of different tolerances\ntol_vec = [tol*(2**ii) for ii in range(n_tol)]  #initialize vector of tolerances\nld_time = [0]*n_tol; ld_n = [0]*n_tol  #low discrepancy time and number of function values\nld_p_time = [0]*n_tol; ld_p_n = [0]*n_tol  #low discrepancy time and number of function values with parallel\nprint(f'\\nCantilever Beam\\n')\nprint('iteration ', end = '')\nfor ii in range(n_tol):\n  solution, data = qp.CubQMCLatticeG(ld_integ, abs_tol = tol_vec[ii]).integrate()\n  if ii == 0:\n    best_solution = solution\n  ld_time[ii] = data.time_integrate\n  ld_n[ii] = data.n_total\n  solution, data = qp.CubQMCLatticeG(ld_integ_p, abs_tol = tol_vec[ii]).integrate()\n  ld_p_time[ii] = data.time_integrate\n  ld_p_n[ii] = data.n_total\n  print(ii, end = ' ') \nwith open(figpath+'ld_parallel.pkl','wb') as myfile:pickle.dump([tol_vec,n_tol,ld_time,ld_n,ld_p_time,ld_p_n,best_solution],myfile)\n</pre> ld_integ = qp.UMBridgeWrapper(ld,model,umbridge_config,parallel=False) #integrand ld_integ_p = qp.UMBridgeWrapper(ld,model,umbridge_config,parallel=8) #integrand with parallel processing  tol = 0.01 n_tol = 9  #number of different tolerances tol_vec = [tol*(2**ii) for ii in range(n_tol)]  #initialize vector of tolerances ld_time = [0]*n_tol; ld_n = [0]*n_tol  #low discrepancy time and number of function values ld_p_time = [0]*n_tol; ld_p_n = [0]*n_tol  #low discrepancy time and number of function values with parallel print(f'\\nCantilever Beam\\n') print('iteration ', end = '') for ii in range(n_tol):   solution, data = qp.CubQMCLatticeG(ld_integ, abs_tol = tol_vec[ii]).integrate()   if ii == 0:     best_solution = solution   ld_time[ii] = data.time_integrate   ld_n[ii] = data.n_total   solution, data = qp.CubQMCLatticeG(ld_integ_p, abs_tol = tol_vec[ii]).integrate()   ld_p_time[ii] = data.time_integrate   ld_p_n[ii] = data.n_total   print(ii, end = ' ')  with open(figpath+'ld_parallel.pkl','wb') as myfile:pickle.dump([tol_vec,n_tol,ld_time,ld_n,ld_p_time,ld_p_n,best_solution],myfile) In\u00a0[\u00a0]: Copied! <pre>!docker rm -f muqbp #shut down docker image\n</pre> !docker rm -f muqbp #shut down docker image In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"demos/talk_paper_demos/Purdue_Talk_2023_March/Purdue_Talk_Figures/#purdue-university-colloquium-talk","title":"Purdue University Colloquium Talk\u00b6","text":"<p>Computations and Figures for Department of Statistics Colloquium at Purdue University</p> <p>presented on Friday, March 3, 2023, slides here</p>"},{"location":"demos/talk_paper_demos/Purdue_Talk_2023_March/Purdue_Talk_Figures/#import-the-necessary-packages-and-set-up-plotting-routines","title":"Import the necessary packages and set up plotting routines\u00b6","text":""},{"location":"demos/talk_paper_demos/Purdue_Talk_2023_March/Purdue_Talk_Figures/#set-the-path-to-save-the-figures-here","title":"Set the path to save the figures here\u00b6","text":""},{"location":"demos/talk_paper_demos/Purdue_Talk_2023_March/Purdue_Talk_Figures/#here-are-some-plots-of-iid-and-low-discrepancy-ld-points","title":"Here are some plots of IID and Low Discrepancy (LD) Points\u00b6","text":""},{"location":"demos/talk_paper_demos/Purdue_Talk_2023_March/Purdue_Talk_Figures/#lattice-points-first","title":"Lattice points first\u00b6","text":""},{"location":"demos/talk_paper_demos/Purdue_Talk_2023_March/Purdue_Talk_Figures/#next-sobol-points","title":"Next Sobol' points\u00b6","text":""},{"location":"demos/talk_paper_demos/Purdue_Talk_2023_March/Purdue_Talk_Figures/#compare-to-iid","title":"Compare to IID\u00b6","text":"<p>Note that there are more gaps and clusters</p>"},{"location":"demos/talk_paper_demos/Purdue_Talk_2023_March/Purdue_Talk_Figures/#beam-example-figures","title":"Beam Example Figures\u00b6","text":"<p>Using computations done below</p>"},{"location":"demos/talk_paper_demos/Purdue_Talk_2023_March/Purdue_Talk_Figures/#plot-the-time-and-sample-size-required-to-solve-for-the-deflection-of-the-end-point-using-iid-and-low-discrepancy","title":"Plot the time and sample size required to solve for the deflection of the end point using IID and low discrepancy\u00b6","text":""},{"location":"demos/talk_paper_demos/Purdue_Talk_2023_March/Purdue_Talk_Figures/#plot-the-time-and-sample-size-required-to-solve-for-the-deflection-of-the-whole-beam-using-low-discrepancy-with-and-without-parallel","title":"Plot the time and sample size required to solve for the deflection of the whole beam using low discrepancy with and without parallel\u00b6","text":""},{"location":"demos/talk_paper_demos/Purdue_Talk_2023_March/Purdue_Talk_Figures/#plot-of-beam-solution","title":"Plot of beam solution\u00b6","text":""},{"location":"demos/talk_paper_demos/Purdue_Talk_2023_March/Purdue_Talk_Figures/#beam-example-computations","title":"Beam Example Computations\u00b6","text":""},{"location":"demos/talk_paper_demos/Purdue_Talk_2023_March/Purdue_Talk_Figures/#set-up-the-problem-using-a-docker-container-to-solve-the-ode","title":"Set up the problem using a docker container to solve the ODE\u00b6","text":"<p>To run this, you need to be running the docker application, https://www.docker.com/products/docker-desktop/</p>"},{"location":"demos/talk_paper_demos/Purdue_Talk_2023_March/Purdue_Talk_Figures/#first-we-compute-the-time-required-to-solve-for-the-deflection-of-the-end-point-using-iid-and-low-discrepancy","title":"First we compute the time required to solve for the deflection of the end point using IID and low discrepancy\u00b6","text":""},{"location":"demos/talk_paper_demos/Purdue_Talk_2023_March/Purdue_Talk_Figures/#next-we-compute-the-time-required-to-solve-for-the-deflection-of-the-whole-beam-using-low-discrepancy-with-and-without-parallel","title":"Next, we compute the time required to solve for the deflection of the whole beam using low discrepancy with and without parallel\u00b6","text":""},{"location":"demos/talk_paper_demos/Purdue_Talk_2023_March/Purdue_Talk_Figures/#shut-down-docker","title":"Shut down docker\u00b6","text":""},{"location":"demos/talk_paper_demos/why_add_q_to_mc_blog/why_add_q_to_mc_blog/","title":"2020 Why Add Q to MC?","text":"In\u00a0[1]: Copied! <pre>from qmcpy import *\nfrom matplotlib import pyplot\n\npyplot.rc('font', size=16)          # controls default text sizes\npyplot.rc('axes', titlesize=16)     # fontsize of the axes title\npyplot.rc('axes', labelsize=16)    # fontsize of the x and y labels\npyplot.rc('xtick', labelsize=16)    # fontsize of the tick labels\npyplot.rc('ytick', labelsize=16)    # fontsize of the tick labels\npyplot.rc('legend', fontsize=16)    # legend fontsize\npyplot.rc('figure', titlesize=16)  # fontsize of the figure title\n\nn = 64\n\npts_sets = [\n    IIDStdUniform(2,seed=7).gen_samples(n),\n    Lattice(2,seed=7).gen_samples(n)]\ntitles = ['$U[0,1]^2$','Shifted Lattice']\nsymbols = ['T','X']\noutput_files = ['iid_uniform_pts','lattice_pts']\n\nfor pts,title,symbol,out_f in zip(pts_sets,titles,symbols,output_files):\n    fig,ax = pyplot.subplots(nrows=1, ncols=1, figsize=(5,5))\n    ax.scatter(pts[:,0],pts[:,1],color='b')\n    ax.set_xlabel('$%s_{i1}$'%symbol)\n    ax.set_xlim([0,1])\n    ax.set_xticks([0,1])\n    ax.set_ylabel('$%s_{i2}$'%symbol)\n    ax.set_ylim([0,1])\n    ax.set_yticks([0,1])\n    ax.set_title(title)\n    ax.set_aspect('equal')\n    pyplot.tight_layout()\n    fig.savefig('./outputs/%s.png'%out_f,dpi=200)\n</pre> from qmcpy import * from matplotlib import pyplot  pyplot.rc('font', size=16)          # controls default text sizes pyplot.rc('axes', titlesize=16)     # fontsize of the axes title pyplot.rc('axes', labelsize=16)    # fontsize of the x and y labels pyplot.rc('xtick', labelsize=16)    # fontsize of the tick labels pyplot.rc('ytick', labelsize=16)    # fontsize of the tick labels pyplot.rc('legend', fontsize=16)    # legend fontsize pyplot.rc('figure', titlesize=16)  # fontsize of the figure title  n = 64  pts_sets = [     IIDStdUniform(2,seed=7).gen_samples(n),     Lattice(2,seed=7).gen_samples(n)] titles = ['$U[0,1]^2$','Shifted Lattice'] symbols = ['T','X'] output_files = ['iid_uniform_pts','lattice_pts']  for pts,title,symbol,out_f in zip(pts_sets,titles,symbols,output_files):     fig,ax = pyplot.subplots(nrows=1, ncols=1, figsize=(5,5))     ax.scatter(pts[:,0],pts[:,1],color='b')     ax.set_xlabel('$%s_{i1}$'%symbol)     ax.set_xlim([0,1])     ax.set_xticks([0,1])     ax.set_ylabel('$%s_{i2}$'%symbol)     ax.set_ylim([0,1])     ax.set_yticks([0,1])     ax.set_title(title)     ax.set_aspect('equal')     pyplot.tight_layout()     fig.savefig('./outputs/%s.png'%out_f,dpi=200)  In\u00a0[\u00a0]: Copied! <pre>\n</pre>"}]}