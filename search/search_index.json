{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Quasi Monte Carlo Software","text":""},{"location":"#qmcpy-quasi-monte-carlo-community-software-in-python","title":"QMCPy: Quasi-Monte Carlo Community Software in Python","text":"<p>Quasi-Monte Carlo (QMC) methods are used to approximate multivariate integrals. They have four main components: a discrete distribution, a true measure of randomness, an integrand, and a stopping criterion. Information about the integrand is obtained as a sequence of values of the function sampled at the data-sites of the discrete distribution. The stopping criterion tells the algorithm when the user-specified error tolerance has been satisfied. We are developing a framework that allows collaborators in the QMC community to develop plug-and-play modules in an effort to produce more efficient and portable QMC software. Each of the above four components is an abstract class. Abstract classes specify the common properties and methods of all subclasses. The ways in which the four kinds of classes interact with each other are also specified. Subclasses then flesh out different integrands, sampling schemes, and stopping criteria. Besides providing developers a way to link their new ideas with those implemented by the rest of the QMC community, we also aim to provide practitioners with state-of-the-art QMC software for their applications.</p>"},{"location":"#resources","title":"Resources","text":"<p>The QMCPy documentation contains a detailed package reference documenting functions and classes including thorough doctests. A number of example notebook demos are also rendered into the documentation from <code>QMCSoftware/demos/</code>. We recommend the following resources to start learning more about QMCPy</p> <ul> <li>mathematical description of QMCPy software and components.</li> <li>Aleksei Sorokin's 2023 PyData Chicago video tutorial and corresponding notebook</li> <li>Fred Hickernell's 2020 MCQMC video tutorial and corresponding notebook</li> <li>The QMCPy introduction notebook and quickstart notebook</li> </ul>"},{"location":"#installation","title":"Installation","text":"<pre><code>pip install qmcpy\n</code></pre> <p>To install from source, please see the contributing guidelines.</p>"},{"location":"#citation","title":"Citation","text":"<p>If you find QMCPy helpful in your work, please support us by citing the following work, which is also available as a QMCPy BibTex citation</p> <pre><code>Sou-Cheng T. Choi, Fred J. Hickernell, Michael McCourt, Jagadeeswaran Rathinavel, Aleksei G. Sorokin,\nQMCPy: A Quasi-Monte Carlo Python Library. 2026.\nhttps://qmcsoftware.github.io/QMCSoftware/\n</code></pre> <p>We maintain a list of publications on the development and use of QMCPy as well as a list of select references upon which QMCPy was built.</p>"},{"location":"#development","title":"Development","text":"<p>Want to contribute to QMCPy? Please see our guidelines for contributors which includes instructions on installation for developers, running tests, and compiling documentation.</p> <p>This software would not be possible without the efforts of the QMCPy community including our steering council, collaborators, contributors, and sponsors.</p> <p>QMCPy is distributed under an Apache 2.0 license from the Illinois Institute of Technology.</p>"},{"location":"CONTRIBUTING/","title":"Contributing","text":""},{"location":"CONTRIBUTING/#contributing","title":"Contributing","text":"<p>Thank you for your interest in contributing to the QMCPy package!</p> <p>Please submit pull requests to the <code>develop</code> branch and issues using a template from <code>.github/ISSUE_TEMPLATE/</code></p> <p>If you develop a new component please consider writing a blog for qmcpy.org</p> <p>Join team communications by reaching out to us at qmc-software@googlegroups.com</p>"},{"location":"CONTRIBUTING/#installation","title":"Installation","text":"<p>In a git enabled terminal (e.g. bash for Windows) with miniconda installed and C compilers enabled (Windows users may need to install Microsoft C++ Build Tools), run</p> <pre><code>git clone https://github.com/QMCSoftware/QMCSoftware.git\ncd QMCSoftware\ngit checkout develop\nconda create --name qmcpy python=3.12\nconda activate qmcpy\npip install -e .[dev]\n</code></pre> <p>While <code>dev</code> contains the most complete set of install dependencies, a number of other install dependency groups can be found in our <code>pyproject.toml</code> file. If running in the <code>zsh</code> terminal you may need to use</p> <pre><code>pip install -e \".[dev]\"\n</code></pre>"},{"location":"CONTRIBUTING/#using-qmcpy-in-courses-class-extra","title":"\ud83d\udcda Using <code>qmcpy</code> in courses (<code>class</code> extra)","text":"<p><code>qmcpy</code> provides a <code>class</code> optional dependency group that installs a complete teaching environment (JupyterLab, plotting, statistics, and utilities) in addition to <code>qmcpy</code> itself.</p> <p>For a typical course setup, you can do: </p><pre><code>git clone https://github.com/QMCSoftware/QMCSoftware.git\ncd QMCSoftware\npip install -e \".[class]\"\n</code></pre><p></p> <p>or for a heavy-duty version </p><pre><code>pip install -e \".[class,dev]\"\n</code></pre><p></p>"},{"location":"CONTRIBUTING/#tests","title":"Tests","text":"<p>Doctests and unittests take a few minute to run with</p> <pre><code>make tests_no_docker\n</code></pre> <p>Optionally, you may install Docker and then run all tests with</p> <pre><code>make tests\n</code></pre> <p>Please see the targets in the makefile for more granular control over tests.</p>"},{"location":"CONTRIBUTING/#documentation","title":"Documentation","text":"<pre><code>pip install -e \".[doc]\"\n</code></pre> <p>This installs the documentation extras, including <code>pylint</code>.</p>"},{"location":"CONTRIBUTING/#ensure-pyreverse-is-on-your-path","title":"Ensure <code>pyreverse</code> is on your PATH","text":"<p><code>pyreverse</code> must be available as a command-line tool. If it is not, verify your PATH as below.</p> <ul> <li>MacOS / Linux </li> </ul> <pre><code>conda activate qmcpy\n# check that pyreverse is found\nwhich pyreverse || echo \"pyreverse not found\"\npyreverse --help\n</code></pre> <p>Alternative:</p> <pre><code>    # add user scripts dir to PATH (zsh example; use ~/.bashrc for bash)\n    echo 'export PATH=\"$HOME/.local/bin:$PATH\"' &gt;&gt; ~/.zshrc\n    # preferred: open a new terminal so the new PATH is picked up\n    source ~/.zshrc\n</code></pre> <ul> <li>Windows (cmd or PowerShell)</li> </ul> <pre><code>conda activate qmcpy\n# check that pyreverse is found\nwhere pyreverse\npyreverse --help\n</code></pre> <p>If <code>where pyreverse</code> cannot find the command, ensure your Python Scripts directory is on your <code>PATH</code>. A common way to locate it is:</p> <pre><code>python -m site --user-base\n# then add \"&lt;that-path&gt;\\Scripts\" to your PATH\n</code></pre> <p>You can update PATH via System settings or in your PowerShell profile (<code>$PROFILE</code>).</p>"},{"location":"CONTRIBUTING/#build-the-documentation","title":"Build the documentation","text":"<p>On MacOS / Linux (and on Windows via Git Bash, WSL, or any environment with <code>make</code>):</p> <pre><code>make doc\n</code></pre>"},{"location":"CONTRIBUTING/#download-pdf-documentation","title":"Download PDF documentation","text":"<p>In the built HTML documentation:</p> <ol> <li>Navigate to the \u201cPrintable Docs\u201d section.</li> <li>Use your browser\u2019s print dialog:</li> <li>Windows / Linux: <code>Ctrl+P</code> </li> <li>MacOS: <code>Cmd+P</code></li> <li>Choose \u201cSave as PDF\u201d and save to your preferred location.</li> </ol>"},{"location":"CONTRIBUTING/#demos","title":"Demos","text":"<p>Demos are Jupyter notebooks which may be launched using the command</p> <pre><code>jupyter-lab\n</code></pre>"},{"location":"CONTRIBUTING/#other-developer-tools","title":"Other Developer Tools","text":"<p>The Developers Tools page on qmcpy.org documents additional tools we have found helpful for mathematical software development and presentation.</p>"},{"location":"CONTRIBUTING/#vscode-tips","title":"VSCode Tips","text":"<p>VSCode (Visual Studio Code) is the IDE of choice for many of our developers. Here we compile some helpful notes regarding additional setup for VSCode.</p> <ul> <li>Run <code>CMD</code>+<code>p</code> then <code>&gt; Python: Select Interpreter</code> then select the <code>('qmcpy')</code> choice from the dropdown to link the qmcpy environment into your workspace. Now when you open a terminal, your command line should read <code>(qmcpy) username@...</code> which indicates the qmcpy environment has been automatically activated. Also, when debugging the qmcpy environment will be automatically used.</li> <li>Go to <code>File</code> and click <code>Save Workspace as...</code> to save a <code>qmcpy</code> workspace for future development.</li> </ul> <p>Some VSCode extensions we found useful include</p> <ul> <li>Python</li> <li>Jupyter</li> <li>Markdown Preview Enhanced</li> <li>eps-preview, which requires<ul> <li>Postscript Language</li> <li>pdf2svg</li> </ul> </li> <li>Git Graph</li> <li>Code Spell Checker</li> </ul>"},{"location":"RELEASE/","title":"Release Manual","text":""},{"location":"RELEASE/#qmcpy-release-documentation","title":"QMCPy Release Documentation","text":"<p>The following is a guide for those in charge of making new releases of QMCPy.</p>"},{"location":"RELEASE/#setup","title":"Setup","text":"<p>In order to publish a release, you will need to have an account on the two websites:</p> <ul> <li>https://pypi.org</li> <li>https://test.pypi.org</li> </ul> <p>Notes:</p> <ul> <li>These accounts will be different from each other. PyPI is the standard Python package index where users can <code>pip install qmcpy</code> from. TestPyPI is a separate package index meant for developers to test their release before publishing on the actual PyPI. In order to release <code>qmcpy</code>, you will need to contact Aleksei to request access to the actual <code>qmcpy</code> project on both websites.</li> <li>When setting up your PyPI account, please have another team member invite you as a collaborator. However, you will need to have two factor authentication (2FA) setup for your account before you can be added. We will not go over the steps on setting up 2FA as that depends on the vendor and method (mobile app or security key) you choose. Please research on the setup procedure for the 2FA vendor and method you pick. Some common 2FA authenticators you can install on your smartphone include Google Authenticator and Microsoft Authenticator.</li> </ul> <p>Please setup and save your PyPI and TestPyPI API keys (do not commit to a public git repo). They should look something like the following </p> <pre><code>[pypi]\n  username = __token__\n  password = ...\n\n[testpypi]\n  username = __token__\n  password = ...\n</code></pre>"},{"location":"RELEASE/#sanity-checks","title":"Sanity Checks","text":"<pre><code>git checkout develop\ngit pull \npip install -e .\nmake doctests_no_docker\nmake doc\n</code></pre> <p>Ensure all tests are passing and the docs are compiling locally and look good. Again, if the GitHub actions are passing then of course everything should work locally, but it is always good to check. </p>"},{"location":"RELEASE/#testpypi-release","title":"TestPyPI release","text":"<p>To set the release version, edit the <code>__version__</code> attribute in <code>qmcpy/__init__.py</code>. Note that versions ending in a letter, e.g. <code>2.1.1a</code>, are alpha releases that are not automatically downloaded. Note also that the version number must be greater than the current version number. For example, if the latest version was <code>2.1</code> then you could do <code>2.1.1</code> or <code>2.2</code> but cannot go back to <code>2.0.1</code>. See this guide of Python versioning for more details. </p> <p>Please install the PDM Python package and dependency manager, the build system we use as specified in the <code>pyproject.toml</code> file.</p> <p>With PDM installed, you can follow the PDM publish guide. Specifically, we first publish to TestPyPI with the command </p> <pre><code>pdm publish --repository testpypi\n</code></pre> <p>When prompted for a username put <code>__token__</code>, and when prompted for a password put the password for your API token for TestPyPI. If successful, you will get a link to the release on TestPyPI. Note that locally, your build may be found at <code>QMCSoftware/dist</code>. </p> <p>To actually test our TestPyPI release, it is a good idea to pretend you are a new user and create a fresh environment in which you try some basic QMCPy commands. Here are some commands to run to create a fresh environment and run some basic tests </p> <pre><code>conda create --name tmp python=3.12 \nconda activate tmp \npip install -i https://test.pypi.org/simple/ qmcpy==???\npython\n</code></pre> <p>Of course you will need to replace ??? with the version name. If the <code>pip install -i ...</code> command fails, try </p> <pre><code>pip install --index-url https://pypi.org/simple --extra-index-url https://test.pypi.org/simple \"qmcpy==2.1\"\n</code></pre> <p>Once in the Python console in the <code>tmp</code> environment, try running some basic commands such as </p> <pre><code>import qmcpy as qp \nimport numpy as np\nqp.DigitalNetB2(3)(8) \nqp.DigitalNetB2(3,replications=2)(8) \nqp.KernelShiftInvar(2)(np.random.rand(3,1,2),np.random.rand(1,4,2))\nexit()\n</code></pre> <p>It is also good to check things work with torch. Still in the <code>tmp</code> env, run </p> <pre><code>pip install torch \npython\n</code></pre> <p>Back in the Python console, run </p> <pre><code>import qmcpy as qp \nimport torch \nqp.fwht_torch(torch.rand(8))\nqp.KernelShiftInvar(2,torchify=True)(torch.rand(3,1,2),torch.rand(1,4,2))\n</code></pre> <p>If you find errors, please fix them and try another release on TestPyPI until everything looks good. Note that you will need to increase the version number if making a new release, so it is good practice to make only small increments, e.g., if you want to release version <code>2.2</code> but that one failed, make the next try <code>2.2.0.1</code> and the one after that <code>2.2.0.2</code> and so on. Otherwise, if you do say <code>2.3</code>, then the next time someone goes to make a release of <code>2.3</code> it will give errors because that version has already been released on TestPyPI. </p>"},{"location":"RELEASE/#pypi-release","title":"PyPI Release","text":"<p>Back in the <code>qmcpy</code> environment, to make the actual PyPI release that users <code>pip install qmcpy</code> with, it is as simple as running </p> <pre><code>pdm publish\n</code></pre> <p>As with TestPyPI, the username is <code>__token__</code> and you will need your PyPI API password. </p> <p>Back in your <code>tmp</code> environment, you may try a </p> <pre><code>pip install qmcpy\npython\n</code></pre> <p>just to make sure everything gets installed ok. In the Python console, you can run the same sanity checks as above, but it is probably not necessary. It is probably sufficient just to make sure the correct version is installed with </p> <pre><code>import qmcpy as qp\nqp.__version__\n</code></pre>"},{"location":"RELEASE/#pull-develop-branch-into-master","title":"Pull <code>develop</code> branch into <code>master</code>","text":"<p>Make a pull request from <code>develop</code> into <code>master</code> and merge it in. </p>"},{"location":"RELEASE/#github-release","title":"GitHub Release","text":"<p>The PyPI release contains barebones tools from <code>QMCSoftware/qmcpy/</code>. The GitHub release contains additional project components such as tests and demos. To create a GitHub release, navigate to QMCPy's GitHub release page and click <code>Draft a new release</code>. For <code>Tag:Select tag</code>, click <code>Create new tag</code> and enter, for example, <code>v2.1</code>. The target should be the master branch. Give the release a title like <code>QMCPy v2.1</code>. Click <code>Generate release notes</code> to automatically collect PR and other release notes. </p>"},{"location":"RELEASE/#cleanup","title":"Cleanup","text":"<p>Delete the <code>tmp</code> environment with </p> <pre><code>conda env remove --name tmp \n</code></pre>"},{"location":"RELEASE/#advertising-qmcpy","title":"Advertising QMCPy","text":"<p>Please share the QMCPy release with team members and the greater community if possible. In the past we have posted both the PyPI and GitHub release links to the group on Slack.  </p>"},{"location":"RELEASE/#additional-post-release-tasks","title":"Additional Post-Release Tasks","text":"<ul> <li>Update this document if necessary.</li> <li>Draft an announcement in <code>docs/release</code> for NA Digest, <code>qmcpy.org</code>, etc.  Add it to <code>mkdocs.yml</code> for team to review.</li> <li>Coordinate branch cleanup and communicate to contributors.</li> <li>Review Release Issue; close or reassign sub\u2011issues to a future release as appropriate.</li> <li>If critical issues appear, prepare and publish a patch release (e.g., vX.Y.1).</li> </ul>"},{"location":"booktests/","title":"Unit tests on Jupyter Notebooks","text":""},{"location":"booktests/#notebook-tests-in-qmcpy","title":"Notebook Tests in QMCPy","text":""},{"location":"booktests/#contents","title":"Contents","text":"<ul> <li><code>.gitignore</code>: ignores temporary files and generated test outputs in this directory.</li> <li><code>__init__.py</code>: package marker for the <code>test.booktests</code> module.</li> <li><code>generate_test.py</code>: script that generates <code>tb_*.py</code> test files from notebooks in <code>demos/</code>.</li> <li><code>parsl_test_runner.py</code>: helper harness used to run Parsl-based notebook tests and coordinate workers.</li> <li><code>README.md</code>: this documentation file describing how to run and manage the notebook tests.</li> </ul>"},{"location":"booktests/#overview","title":"Overview","text":"<p>To execute an individual testbook file, e.g., <code>tb_acm_toms_sorokin_2025.py</code>, run the following command in a terminal:</p> <pre><code>    cd test/booktests &amp;&amp; python -m pytest tb_acm_toms_sorokin_2025.py -v\n</code></pre> <p>To execute all testbook files sequentially, run the following command in a terminal:</p> <pre><code>    make booktests_no_docker\n</code></pre> <p>To execute all testbook files in parallel using Parsl, run the following command in a terminal:</p> <pre><code>    make booktests_parallel_no_docker\n</code></pre> <p>To execute say two testbook files sequentially, run the following command in a terminal:</p> <pre><code>    make booktests_no_docker TESTS=\"tb_Argonne_2023_Talk_Figures tb_Purdue_Talk_Figures\"\n</code></pre> <p>To execute say two testbook files in parallel, run the following command in a terminal:</p> <pre><code>    make booktests_parallel_no_docker TESTS=\"tb_Argonne_2023_Talk_Figures tb_Purdue_Talk_Figures\"\n</code></pre> <p>For a demo, see the Jupyter notebook, <code>demos/talk_paper_demos/Parslfest_2025/</code>.</p>"},{"location":"booktests/#design-patterns-using-setuphelpers-vs-testbook-decorator","title":"Design Patterns: Using <code>setUp/helpers</code> vs. <code>@testbook</code> Decorator**","text":"<ul> <li>Generated files such as <code>tb_iris.py</code> uses <code>@testbook</code> for a standalone notebook, requiring no special setup.</li> <li>GBM notebooks such as <code>gbm_examples.py</code> rely on local modules and sometimes have broken symlinks, needing setup for correct imports.</li> <li>Running notebooks from their directory (via <code>setUp</code>) ensures consistent relative paths and imports, which the decorator doesn't reliably handle.</li> <li><code>BaseNotebookTest</code>'s <code>setUp/tearDown</code> methods handle resource management, beneficial for long-running demos.</li> <li>Shared helper functions in <code>__init__.py</code> centralize complex logic (location, symlink fixing, running) for reusability and clarity.</li> </ul>"},{"location":"community/","title":"Community","text":""},{"location":"community/#community","title":"Community","text":"<p>This document outlines a few key roles within the QMCPy Community. The purpose of this document is to provide guidance and clarity to community members on their responsibilities and the roles they play in the development and maintenance of the software. The key roles are the Steering Council, Collaborators, and Contributors. By working together, the community can ensure that the software continues to evolve and meet the needs of the scientific community.</p>"},{"location":"community/#steering-council","title":"Steering Council","text":"<p>The Steering Council is responsible for the overall direction and governance of the software. This includes defining the vision and goals of the software project, establishing policies and procedures, and ensuring that the community operates in a transparent and democratic manner. Their responsibilities include the following:</p> <ul> <li>Provide leadership and guidance to the community</li> <li>Set priorities and allocate resources</li> <li>Ensure that the community operates in an ethical and transparent manner</li> <li>Foster collaboration and communication between community members</li> <li>Represent the community to external stakeholders</li> </ul> <p>The current council members are as follows (listed in alphabetical order of last names):</p> <ul> <li>Sou-Cheng T. Choi</li> <li>Fred J. Hickernell</li> <li>Michael McCourt</li> <li>Jagadeeswaran Rathinavel</li> <li>Aleksei Sorokin</li> </ul>"},{"location":"community/#collaborators","title":"Collaborators","text":"<p>Collaborators play a key role in the development of QMCPy by contributing their expertise in scientific research. They provide valuable insights, support, and feedback on the software\u2019s functionality and ensure that it meets the needs of the scientific community. Their responsibilities include the following:</p> <ul> <li>Offer suggestions for new features and enhancements based on their research and knowledge domain</li> <li>Provide feedback on the software\u2019s functionality and design</li> <li>Act as a liaison between the software community and the academic community</li> </ul> <p>The following are our collaborators (listed in alphabetical order of last names):</p> <ul> <li>Yuhan Ding</li> <li>Adrian Ebert</li> <li>Mike Giles</li> <li>Marius Hofert</li> <li>Lan Jiang</li> <li>Sergei Kucherenko</li> <li>Pierre L\u2019Ecuyer</li> <li>Christiane Lemieux</li> <li>Dirk Nuyens</li> <li>Onyekachi Osisiogu</li> <li>Art Owen</li> <li>Pieterjan Robbe</li> <li>Xuan Zhou</li> </ul>"},{"location":"community/#contributors","title":"Contributors","text":"<p>Contributors are individuals who actively participate in the development of the software. They may contribute code, documentation, bug reports, and other forms of support.  Their responsibilities include the following:</p> <ul> <li>Participate in the development of the software</li> <li>Resolve bugs and suggest improvements</li> <li>Develop tests and documentation</li> <li>Provide support to other community members</li> <li>Participate in community discussions and decision-making processes</li> </ul> <p>The contributors to our GitHub are:</p> <p> </p> <p>For a list of contributors to QMCPY.org, please refer to https://qmcpy.org/contributors/.</p>"},{"location":"community/#sponsors","title":"Sponsors","text":"<ul> <li>Illinois Tech</li> </ul> <ul> <li>Kamakura Corporation, acquired by SAS Institute Inc. in June 2022</li> </ul> <ul> <li>SigOpt, Inc.</li> </ul>"},{"location":"community/#select-references","title":"Select References","text":"<ol> <li> <p>F. Y. Kuo and D. Nuyens. \"Application of quasi-Monte Carlo methods to elliptic PDEs with random diffusion coefficients - a survey of analysis and implementation,\" Foundations of Computational Mathematics, 16(6):1631-1696, 2016. https://link.springer.com/article/10.1007/s10208-016-9329-5. https://arxiv.org/abs/1606.06613.</p> </li> <li> <p>Fred J. Hickernell, Lan Jiang, Yuewei Liu, and Art B. Owen, \"Guaranteed conservative fixed width confidence intervals via Monte Carlo   sampling,\" Monte Carlo and Quasi-Monte Carlo Methods 2012 (J. Dick, F.Y. Kuo, G. W.  Peters, and I. H. Sloan, eds.), pp. 105-128, Springer-Verlag, Berlin, 2014. DOI: 10.1007/978-3-642-41095-6_5</p> </li> <li> <p>Sou-Cheng T. Choi, Yuhan Ding, Fred J. Hickernell, Lan Jiang, Lluis Antoni Jimenez Rugama, Da Li, Jagadeeswaran Rathinavel, Xin Tong, Kan Zhang, Yizhi Zhang, and Xuan Zhou, GAIL: Guaranteed Automatic Integration Library (Version 2.3.1) [MATLAB Software], 2020. http://gailgithub.github.io/GAIL_Dev/.</p> </li> <li> <p>Sou-Cheng T. Choi, \"MINRES-QLP Pack and Reliable Reproducible Research via Supportable Scientific Software,\" Journal of Open Research Software, Volume 2, Number 1, e22, pp. 1-7, 2014.</p> </li> <li> <p>Sou-Cheng T. Choi and Fred J. Hickernell, \"IIT MATH-573 Reliable Mathematical Software\" [Course Slides], Illinois Institute of Technology, Chicago, IL, 2013. Available from http://gailgithub.github.io/GAIL_Dev/.</p> </li> <li> <p>Daniel S. Katz, Sou-Cheng T. Choi, Hilmar Lapp, Ketan Maheshwari, Frank Loffler, Matthew Turk, Marcus D. Hanwell, Nancy Wilkins-Diehr, James Hetherington, James Howison, Shel Swenson, Gabrielle D. Allen, Anne C. Elster, Bruce Berriman, Colin Venters, \"Summary of the First Workshop On Sustainable Software for Science: Practice and Experiences (WSSSPE1),\" Journal of Open Research Software, Volume 2, Number 1, e6, pp. 1-21, 2014.</p> </li> <li> <p>Fang, K.-T., and Wang, Y. (1994). Number-theoretic Methods in Statistics. London, UK: CHAPMAN &amp; HALL</p> </li> <li> <p>Lan Jiang, Guaranteed Adaptive Monte Carlo Methods for Estimating Means of Random Variables, PhD Thesis, Illinois Institute of Technology, 2016.</p> </li> <li> <p>Lluis Antoni Jimenez Rugama and Fred J. Hickernell, \"Adaptive multidimensional integration based on rank-1 lattices,\" Monte Carlo and Quasi-Monte Carlo  Methods: MCQMC, Leuven, Belgium, April 2014 (R. Cools and D. Nuyens, eds.), Springer Proceedings in Mathematics and Statistics, vol. 163, Springer-Verlag, Berlin, 2016, arXiv:1411.1966, pp. 407-422.</p> </li> <li> <p>Kai-Tai Fang and Yuan Wang, Number-theoretic Methods in Statistics, Chapman &amp; Hall, London, 1994.</p> </li> <li> <p>Fred J. Hickernell and Lluis Antoni Jimenez Rugama, \"Reliable adaptive cubature using digital sequences,\" Monte Carlo and Quasi-Monte Carlo Methods: MCQMC, Leuven, Belgium, April 2014 (R. Cools and D. Nuyens, eds.), Springer Proceedings in Mathematics and Statistics, vol. 163, Springer-Verlag, Berlin, 2016, arXiv:1410.8615 [math.NA], pp. 367-383.</p> </li> <li> <p>Marius Hofert and Christiane Lemieux (2019). qrng: (Randomized) Quasi-Random Number Generators. R package version 0.0-7. https://CRAN.R-project.org/package=qrng.</p> </li> <li> <p>Faure, Henri, and Christiane Lemieux. \u201cImplementation of Irreducible Sobol\u2019 Sequences in Prime Power Bases,\u201d Mathematics and Computers in Simulation 161 (2019): 13\u201322. </p> </li> <li> <p>M. B. Giles. \"Multi-level Monte Carlo path simulation,\" Operations Research, 56(3):607-617, 2008. http://people.maths.ox.ac.uk/~gilesm/files/OPRE_2008.pdf.</p> </li> <li> <p>M. B. Giles. \"Improved multilevel Monte Carlo convergence using the Milstein scheme,\" 343-358, in Monte Carlo and Quasi-Monte Carlo Methods 2006, Springer, 2008. http://people.maths.ox.ac.uk/~gilesm/files/mcqmc06.pdf.</p> </li> <li> <p>M. B. Giles and B. J. Waterhouse. \"Multilevel quasi-Monte Carlo path simulation,\" pp.165-181 in Advanced Financial Modelling, in Radon Series on Computational and Applied Mathematics, de Gruyter, 2009. http://people.maths.ox.ac.uk/~gilesm/files/radon.pdf.</p> </li> <li> <p>Owen, A. B. \"A randomized Halton algorithm in R,\" 2017. arXiv:1706.02808 [stat.CO]</p> </li> <li> <p>B. D. Keister, Multidimensional Quadrature Algorithms,  'Computers in Physics', 10, pp. 119-122, 1996.</p> </li> <li> <p>L\u2019Ecuyer, Pierre &amp; Munger, David. (2015). LatticeBuilder: A General Software Tool for Constructing Rank-1 Lattice Rules. ACM Transactions on Mathematical Software. 42. 10.1145/2754929. </p> </li> <li> <p>Fischer, Gregory &amp; Carmon, Ziv &amp; Zauberman, Gal &amp; L\u2019Ecuyer, Pierre. (1999). Good Parameters and Implementations for Combined Multiple Recursive Random Number Generators. Operations Research. 47. 159-164. 10.1287/opre.47.1.159. </p> </li> <li> <p>I.M. Sobol', V.I. Turchaninov, Yu.L. Levitan, B.V. Shukhman: \"Quasi-Random Sequence Generators\" Keldysh Institute of Applied Mathematics, Russian Academy of Sciences, Moscow (1992).</p> </li> <li> <p>Sobol, Ilya &amp; Asotsky, Danil &amp; Kreinin, Alexander &amp; Kucherenko, Sergei. (2011). Construction and Comparison of High-Dimensional Sobol' Generators. Wilmott. 2011. 10.1002/wilm.10056. </p> </li> <li> <p>Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., \u2026 Chintala, S. (2019). PyTorch: An Imperative Style, High-Performance Deep Learning Library. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d extquotesingle Alch'e-Buc, E. Fox, &amp; R. Garnett (Eds.), Advances in Neural Information Processing Systems 32 (pp. 8024\u20138035). Curran Associates, Inc. Retrieved from http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf</p> </li> <li> <p>S. Joe and F. Y. Kuo, Constructing Sobol sequences with better two-dimensional projections, SIAM J. Sci. Comput. 30, 2635-2654 (2008).</p> </li> <li> <p>Paul Bratley and Bennett L. Fox. 1988. Algorithm 659: Implementing Sobol's quasirandom sequence generator. ACM Trans. Math. Softw. 14, 1 (March 1988), 88\u2013100. DOI:https://doi.org/10.1145/42288.214372</p> </li> <li> <p>P. L'Ecuyer, P. Marion, M. Godin, and F. Puchhammer, \"A Tool for Custom Construction of QMC and RQMC Point Sets,\" Monte Carlo and Quasi-Monte Carlo Methods 2020.</p> </li> <li> <p>P Kumaraswamy, A generalized probability density function for double-bounded random processes. J. Hydrol. 46, 79\u201388 (1980).</p> </li> <li> <p>D Li, Reliable quasi-Monte Carlo with control variates. Master\u2019s thesis, Illinois Institute of Technology (2016)</p> </li> <li> <p>D.H. Bailey, J.M. Borwein, R.E. Crandall, Box integrals, Journal of Computational and Applied Mathematics, Volume 206, Issue 1, 2007, Pages 196-208, ISSN 0377-0427, https://doi.org/10.1016/j.cam.2006.06.010.</p> </li> <li> <p>Art B. Owen.Monte Carlo theory, methods and examples. 2013.</p> </li> </ol>"},{"location":"components/","title":"Components","text":""},{"location":"components/#components-of-qmcpy-quasi-monte-carlo-software-in-python","title":"Components of QMCPy: Quasi-Monte Carlo Software in Python","text":"<p>Monte Carlo (MC) methods approximate the true mean (expectation) \\(\\mu\\) of a random variable \\(g(\\boldsymbol{T})\\) by the sample mean \\(\\hat{\\mu}_n = \\frac{1}{n} \\sum_{i=0}^{n-1} g(\\boldsymbol{T}_i)\\) for some samples \\(\\boldsymbol{T}_0,\\dots,\\boldsymbol{T}_{n-1}\\). We call the \\(d\\)-dimensional vector random variable \\(\\boldsymbol{T}\\) the true measure and we call \\(g\\) the integrand. As most computer-generated random numbers are uniformly distributed, we use a transform \\(\\boldsymbol{\\psi}\\) to write \\(\\boldsymbol{T} \\sim \\boldsymbol{\\psi}(\\boldsymbol{X})\\) where \\(\\boldsymbol{X} \\sim \\mathcal{U}[0,1]^d\\). The resulting Monte Carlo approximation is written in terms of the transformed integrand \\(f(\\boldsymbol{x}) = g(\\boldsymbol{\\psi}(\\boldsymbol{x}))\\) as</p> \\[\\mu = \\mathbb{E}[f(\\boldsymbol{X})] = \\int_{[0,1]^d} f(\\boldsymbol{x}) \\mathrm{d} \\boldsymbol{x} \\approx \\int_{[0,1]^d} f(\\boldsymbol{x}) \\hat{\\lambda}_n(\\mathrm{d} \\boldsymbol{x}) = \\frac{1}{n} \\sum_{i=0}^{n-1} f(\\boldsymbol{x}_i) = \\hat{\\mu}, \\qquad \\boldsymbol{X} \\sim \\mathcal{U}[0,1]^d\\] <p>for some discrete distribution \\(\\hat{\\lambda}_n\\) defined by samples \\(\\boldsymbol{x}_0,\\dots,\\boldsymbol{x}_{n-1} \\in [0,1]^d\\) (formally \\(\\hat{\\lambda}_n(A)\\) measures the proportion of points \\((\\boldsymbol{x}_i)_{i=0}^{n-1}\\) which lie in some set \\(A\\)). The error of this approximation is</p> \\[E_n = \\lvert \\mu - \\hat{\\mu}_n \\rvert.\\] <p>Classic Monte Carlo methods choose IID (independent and identically distributed) samples \\(\\boldsymbol{x}_0,\\dots,\\boldsymbol{x}_{n-1} \\overset{\\mathrm{IID}}{\\sim} \\mathcal{U}[0,1]^d\\) and have error \\(E_n\\) like \\(\\mathcal{O}(n^{-1/2})\\). Quasi-Monte Carlo (QMC) methods achieve a significantly better error rate of \\(\\mathcal{O}(n^{-1})\\) by using low discrepancy (LD) sequences for \\((\\boldsymbol{x}_i)_{i=0}^{n-1}\\) which more evenly fill the unit cube than IID points.</p> The first \\(32\\) points of each sequence are shown as purple starts, the next \\(32\\) points are shown as green triangles, and the \\(64\\) points after that are shown as blue circles. Notice the gaps and clusters of IID points compared to the more uniform coverage of LD sequences. <p>Often practitioners would like to run their (Quasi-)Monte Carlo method until the error \\(E_n\\) is below a desired error tolerance \\(\\varepsilon\\) and/or until they have expired their sample budget \\(B\\). For example, one may wish to estimate the expected discounted payoff of a financial option to within a tolerance of one penny, \\(\\varepsilon = 0.01\\), or until \\(1\\) million option paths have been simulated, \\(B=10^6\\). Stopping criterion deploy (Quasi-)Monte Carlo methods under such constraints by utilizing adaptive sampling schemes and efficient error estimation procedures.  </p> <p><code>QMCPy</code> is organized into into the four main components below. Details for each of these classes are available in the linked guides and API docs.</p>"},{"location":"components/#discrete-distributions","title":"Discrete Distributions","text":"<p>These generates IID or LD points \\(\\boldsymbol{x}_0,\\boldsymbol{x}_1,\\dots\\). Supported LD sequences include</p> <ul> <li>Lattices with<ul> <li>extensible constructions</li> <li>random shifts</li> </ul> </li> <li>Digital Nets in base \\(b=2\\) with<ul> <li>extensible constructions</li> <li>digital shifts</li> <li>linear matrix scrambling</li> <li>nested uniform scrambling (also called Owen scrambling)</li> <li>higher order constructions via digital interlacing</li> </ul> </li> <li>Halton point sets with<ul> <li>extensible constructions</li> <li>digital shifts</li> <li>permutation scrambling</li> <li>linear matrix scrambling</li> <li>nested uniform scrambling</li> </ul> </li> </ul>"},{"location":"components/#true-measures","title":"True Measures","text":"<p>These define \\(\\boldsymbol{T}\\), for which <code>QMCPy</code> will automatically choose an appropriate transform \\(\\boldsymbol{\\psi}\\) so that \\(\\boldsymbol{T} \\sim \\boldsymbol{\\psi}(\\boldsymbol{X})\\) with \\(\\boldsymbol{X} \\sim \\mathcal{U}[0,1]^d\\). Some popular true measures are</p> <ul> <li>Uniform \\(\\boldsymbol{T} \\sim \\mathcal{U}[\\boldsymbol{l},\\boldsymbol{u}]\\) with elementwise \\(\\boldsymbol{l} \\leq \\boldsymbol{u}\\) for which \\(\\boldsymbol{\\psi}(\\boldsymbol{x}) = \\boldsymbol{l}+(\\boldsymbol{u}-\\boldsymbol{l}) \\odot \\boldsymbol{x}\\) with \\(\\odot\\) the Hadamard (elementwise) product.</li> <li>Gaussian \\(\\boldsymbol{T} \\sim \\mathcal{N}(\\boldsymbol{m},\\mathsf{\\Sigma})\\) for which \\(\\boldsymbol{\\psi}(\\boldsymbol{x}) = \\boldsymbol{m}+\\mathsf{A}\\Phi^{-1}(\\boldsymbol{x})\\) where \\(\\Phi^{-1}\\) is the inverse CDF of the standard Gaussian distribution applied elementwise and the covariance \\(\\mathsf{\\Sigma} = \\mathsf{A} \\mathsf{A}^T\\) may be decomposed using either<ul> <li>the Cholesky decomposition or</li> <li>the eigendecomposition.</li> </ul> </li> <li>Brownian Motion observed with an initial value \\(B_0\\), drift \\(\\gamma\\), and diffusion \\(\\sigma^2\\) at times \\(\\boldsymbol{t} := (t_1,\\dots,t_d)^T\\) satisfying \\(0 \\leq t_1 &lt; t_2 &lt; \\dots &lt; t_d\\) is a Gaussian with mean and covariance</li> </ul> \\[\\boldsymbol{m} = B_0 + \\gamma \\boldsymbol{t}\\] \\[\\mathsf{\\Sigma} = \\sigma^2 \\left(\\min\\{t_i,t_{i'}\\}\\right)_{i,i'=1}^{d}\\] <ul> <li>Independent Marginals have \\(\\boldsymbol{T} = (T_1,\\dots,T_d)^T\\) with \\(T_1,\\dots,T_d\\) independent. We support (continuous) marginal distributions from <code>scipy.stats</code>.</li> </ul>"},{"location":"components/#integrands","title":"Integrands","text":"<p>These define \\(g\\), which <code>QMCPy</code> will use to define \\(f = g \\circ \\boldsymbol{\\psi}\\). Some popular integrands are</p> <ul> <li>User Defined Integrands, where the user provides a function handle for \\(g\\)</li> <li>Financial Options, including the European option, Asian option, and Barrier option</li> <li><code>UM-Bridge</code> Functions. From their docs, <code>UM-Bridge</code> is a universal interface that makes any numerical model accessible from any programming language or higher-level software through the use of containerized environments. <code>UM-Bridge</code> also enables simulations to scale to supercomputers or the cloud with minimal effort.</li> </ul>"},{"location":"components/#stopping-criteria","title":"Stopping Criteria","text":"The cost of IID-Monte Carlo algorithms is \\(n = \\mathcal{O}(1/\\varepsilon^2)\\) in the number of samples \\(n\\) and error tolerance \\(\\varepsilon\\) while Quasi-Monte Carlo algorithms only cost around \\(n=\\mathcal{O}(1/\\varepsilon)\\). Both IID-Monte Carlo and Quasi-Monte Carlo stopping criterion consistently determine approximations which meet the desired error tolerance. <p>These deploy (Quasi-)Monte Carlo methods under error tolerance and budgetary constraints by utilizing adaptive sampling schemes and efficient error estimation procedures. Common stopping criteria include</p> <ul> <li>Quasi-Monte Carlo via tracking the decay of coefficients in an orthogonal basis expansion. These methods are guaranteed for cones of functions whose coefficients decay in a regular manner.  Efficient procedures exist to estimate coefficients when<ul> <li>pairing lattices with the Fourier expansion or</li> <li>pairing digital nets with the Walsh expansion.</li> </ul> </li> <li>Quasi-Monte Carlo via efficient Bayesian cubature methods which assume \\(f\\) is a draw from a Gaussian process so the posterior expectation has an analytic expression. While classic Bayesian cubature would require \\(\\mathcal{O}(n^2)\\) storage and \\(\\mathcal{O}(n^3)\\) computations, when matching certain LD sequences to special kernels the Gram matrices become nicely structured to permit Bayesian cubature with only \\(\\mathcal{O}(n)\\) storage and \\(\\mathcal{O}(n \\log n)\\) computations. Specifically,<ul> <li>pairing lattices with shift-invariant kernels gives circulant Gram matrices which are diagonalizable by the Fast Fourier Transform (FFT), and</li> <li>pairing digital nets with digitally-shift-invariant kernels gives Gram matrices which are diagonalizable by the Fast Walsh-Hadamard Transform (FWHT).</li> </ul> </li> <li>Quasi-Monte Carlo via multiple independent randomizations of an LD point set and Student's \\(t\\) confidence intervals.</li> <li>IID Monte Carlo via a two step procedure using the Central Limit Theorem (CLT). Error estimates are not guaranteed as CLT is asymptotic in \\(n\\) and the variance must be estimated.</li> <li>IID Monte Carlo via a two step procedure using Berry-Esseen inequalities to account for finite sample sizes. Error estimates are guaranteed for functions with bounded Kurtosis.</li> <li>Multilevel IID Monte Carlo and Quasi-Monte Carlo which more efficiently integrate expensive functions by exploiting a telescoping sum over lower fidelity models.</li> </ul>"},{"location":"tests/","title":"Doc tests and unit tests on `qmcpy`","text":""},{"location":"tests/#test-targets-guide","title":"Test Targets Guide","text":"<p>This document describes the available test targets in the Makefile for QMCSoftware. All targets use pytest with parallel execution (via <code>pytest-xdist</code>) when available.</p>"},{"location":"tests/#quick-reference","title":"Quick Reference","text":"Target Purpose Speed Use Case <code>make tests_fast</code> \u26a1 Recommended: All tests in parallel Fast Default choice: doctests + unittests + booktests concurrently <code>make tests_no_docker</code> All tests sequentially (no Docker) Slow Conservative validation; isolates flaky tests <code>make tests</code> All tests with Docker support Very Slow Complete validation with GPU/Docker dependencies <code>make unittests</code> Unit tests only Fast Quick feedback on code changes <code>make doctests_no_docker</code> Doctests (excludes GPU/Docker) Moderate Validate docstring examples <code>make doctests</code> All doctests with Docker Slow Full docstring validation <code>make booktests_no_docker</code> Jupyter notebook tests Slow Validate demo notebooks <code>make booktests_parallel_no_docker</code> Notebook tests with Parsl parallelization Variable Distributed notebook execution <code>make coverage</code> Display coverage report Instant View test coverage summary <code>make delcoverage</code> Reset coverage tracking Instant Start fresh coverage analysis"},{"location":"tests/#detailed-descriptions","title":"Detailed Descriptions","text":""},{"location":"tests/#scope","title":"Scope","text":"<p>This short guide highlights four practical areas: - Local tests in the <code>test/</code> directory and the Makefile targets that run them (for example <code>make unittests</code>, <code>make booktests_no_docker</code>, and <code>make doctests_no_docker</code>). - Local coverage targets in the Makefile (for example <code>make coverage</code> and <code>make delcoverage</code>) and how to collect coverage locally using <code>--cov-append</code> or <code>coverage run --append</code>. - Remote CI: the GitHub Actions workflow at <code>.github/workflows/alltests.yml</code> (referred to here as <code>alltests.yml</code>) which runs matrix jobs and invokes the Makefile targets for full validation. - Remote coverage publishing to Codecov and how to add a Codecov badge to the project <code>README.md</code>.</p>"},{"location":"tests/#core-test-targets-recommended","title":"Core Test Targets (Recommended)","text":""},{"location":"tests/#make-tests_fast-recommended","title":"<code>make tests_fast</code> \u26a1 RECOMMENDED","text":"<p>Fastest option: Runs doctests, unittests, and booktests concurrently in background processes. - Parallelization: All three test families run simultaneously, not sequentially - Cleanup: Removes invalid distribution artifacts before running - Time: ~30\u201360 seconds (depending on CPU cores and notebook complexity) - Coverage: Full summary report at the end - Use when: You want comprehensive testing with maximum speed</p>"},{"location":"tests/#make-tests_no_docker","title":"<code>make tests_no_docker</code>","text":"<p>Runs all tests sequentially: doctests, unittests, and generates coverage reports (excludes Docker-dependent tests). - Cleans invalid distribution artifacts before running (via <code>scripts/cleanup_invalid_dist.py</code>) - Sequence: doctests_no_docker \u2192 unittests \u2192 coverage report - Time: ~60\u2013120 seconds - Coverage: Full summary report - Use when: Pre-commit or CI/CD validation (without Docker)</p>"},{"location":"tests/#make-tests","title":"<code>make tests</code>","text":"<p>Runs all tests sequentially with full Docker support (for GPU-heavy and UMBridge tests). - Includes: Full doctests suite (with umbridge and markdown validation) - Time: ~120\u2013180+ seconds - Coverage: Full summary report - Use when: Complete validation with Docker dependencies available</p>"},{"location":"tests/#doctest-targets","title":"Doctest Targets","text":""},{"location":"tests/#make-doctests_no_docker-composite","title":"<code>make doctests_no_docker</code> (Composite)","text":"<p>Runs doctests excluding GPU and Docker dependencies. - Composition: <code>doctests_minimal</code> + <code>doctests_torch</code> + <code>doctests_gpytorch</code> + <code>doctests_botorch</code> - Time: ~15\u201330 seconds - Use when: Validating docstring examples</p>"},{"location":"tests/#make-doctests-composite","title":"<code>make doctests</code> (Composite)","text":"<p>Runs all doctests including Docker-dependent UMBridge and markdown validation. - Composition: <code>doctests_markdown</code> + <code>doctests_minimal</code> + <code>doctests_torch</code> + <code>doctests_gpytorch</code> + <code>doctests_botorch</code> + <code>doctests_umbridge</code> - Time: Variable (Docker startup overhead) - Use when: Full docstring validation with Docker available</p>"},{"location":"tests/#make-doctests_minimal-building-block","title":"<code>make doctests_minimal</code> (Building Block)","text":"<p>Core doctests excluding all optional dependencies (PyTorch, GPyTorch, BoTorch, UMBridge). - Modules tested: Main qmcpy modules - Time: ~5\u201310 seconds - Note: Usually called via <code>doctests_no_docker</code> or <code>doctests</code>; rarely used standalone</p>"},{"location":"tests/#make-doctests_torch-building-block","title":"<code>make doctests_torch</code> (Building Block)","text":"<p>Doctests for PyTorch-dependent modules. - Modules tested: <code>qmcpy/fast_transform/ft_pytorch.py</code>, <code>qmcpy/kernel/*.py</code>, <code>qmcpy/util/*shift*.py</code> - Time: ~3\u20135 seconds - Note: Usually called via <code>doctests_no_docker</code>; rarely used standalone</p>"},{"location":"tests/#make-doctests_gpytorch-building-block","title":"<code>make doctests_gpytorch</code> (Building Block)","text":"<p>Doctests for GPyTorch integration. - Modules tested: <code>qmcpy/stopping_criterion/pf_gp_ci.py</code> - Time: ~3\u20135 seconds - Note: Usually called via <code>doctests_no_docker</code>; rarely used standalone</p>"},{"location":"tests/#make-doctests_botorch-building-block","title":"<code>make doctests_botorch</code> (Building Block)","text":"<p>Doctests for BoTorch integration. - Modules tested: <code>qmcpy/integrand/hartmann6d.py</code> - Time: ~2\u20133 seconds - Note: Usually called via <code>doctests_no_docker</code>; rarely used standalone</p>"},{"location":"tests/#make-doctests_umbridge","title":"<code>make doctests_umbridge</code>","text":"<p>Doctests for UMBridge wrapper (requires Docker). - Modules tested: <code>qmcpy/integrand/umbridge_wrapper.py</code> - Time: ~10\u201315 seconds (includes Docker startup) - Dependencies: Docker must be running - Note: Usually called via <code>doctests</code>; rarely used standalone</p>"},{"location":"tests/#make-doctests_markdown","title":"<code>make doctests_markdown</code>","text":"<p>Validates embedded Python code in markdown files under <code>docs/</code>. - Tools: Uses <code>phmutest</code> (markdown test utility) - Time: ~3\u20135 seconds - Note: Usually called via <code>doctests</code>; rarely used standalone</p>"},{"location":"tests/#unit-notebook-test-targets","title":"Unit &amp; Notebook Test Targets","text":""},{"location":"tests/#make-unittests","title":"<code>make unittests</code>","text":"<p>Runs unit tests from the <code>test/</code> directory using pytest with parallel workers (when <code>pytest-xdist</code> is installed). - Time: ~13\u201330 seconds (depending on CPU cores) - Coverage: Incremental coverage report appended to <code>.coverage</code> - Use when: Quick feedback on code changes</p>"},{"location":"tests/#make-booktests_no_docker","title":"<code>make booktests_no_docker</code>","text":"<p>Generates and runs tests from Jupyter notebooks in the <code>demos/</code> folder using unittest discovery. - Automatically generates missing test files in <code>test/booktests/</code> - Cleans cache and temporary files before running - Time: Highly variable (5 seconds to 5+ minutes depending on notebook complexity) - Coverage: Incremental - Use when: Validating demo notebooks or documentation</p>"},{"location":"tests/#make-booktests_parallel_no_docker","title":"<code>make booktests_parallel_no_docker</code>","text":"<p>Runs notebook tests with Parsl distributed parallelization for compute-heavy demos. - Parallelization: Uses Parsl framework for task scheduling - Cleanup: Removes temporary outputs (EPS, JPG, PDF, PNG files, logs, etc.) - Time: Highly variable (depends on Parsl workers and notebook complexity) - Optional parameters: <code>TESTS=\"tb_notebook1 tb_notebook2\"</code> to run specific tests - Dependencies: Parsl must be installed and configured - Use when: Running large notebook suites with distributed compute resources</p>"},{"location":"tests/#make-tests_parallel_no_docker","title":"<code>make tests_parallel_no_docker</code>","text":"<p>Runs only unit tests with parallel pytest workers (no doctests or booktests). - Time: ~13\u201320 seconds - Coverage: Incremental - Use when: Testing unit tests only in parallel mode</p>"},{"location":"tests/#helper-internal-targets","title":"Helper / Internal Targets","text":""},{"location":"tests/#make-check_booktests","title":"<code>make check_booktests</code>","text":"<p>Validates that all Jupyter notebooks in <code>demos/</code> have corresponding test files in <code>test/booktests/</code>. - Output: Lists missing test files and notebook/test file counts - Note: Called automatically by <code>booktests_no_docker</code>; rarely used standalone</p>"},{"location":"tests/#make-generate_booktests","title":"<code>make generate_booktests</code>","text":"<p>Auto-generates missing test stub files for notebooks. - Output: Reports any generated files - Note: Called automatically by <code>booktests_no_docker</code>; rarely used standalone</p>"},{"location":"tests/#make-coverage","title":"<code>make coverage</code>","text":"<p>Displays the current coverage report (must run other targets first to accumulate coverage data). - Output: Terminal summary of coverage percentages per file/module - Note: Coverage data is appended from previous runs; use <code>make delcoverage</code> to reset</p>"},{"location":"tests/#make-delcoverage","title":"<code>make delcoverage</code>","text":"<p>Deletes <code>.coverage</code> and <code>coverage.json</code> files to reset coverage tracking. - Use before: Running a fresh coverage report without accumulated data</p>"},{"location":"tests/#redundancy-analysis-status","title":"Redundancy Analysis &amp; Status","text":""},{"location":"tests/#removed-redundant-target","title":"Removed Redundant Target \u2705","text":""},{"location":"tests/#make-tests_parallel_no_docker-removed","title":"<code>make tests_parallel_no_docker</code> (REMOVED)","text":"<ul> <li>Was redundant: Ran only unit tests in parallel. <code>make tests_fast</code> is a strict superset (doctests + unittests + booktests in parallel).</li> <li>Status: Removed from Makefile to simplify maintenance and reduce user confusion.</li> <li>Migration: Users should use <code>make tests_fast</code> instead (faster, more comprehensive).</li> </ul>"},{"location":"tests/#currently-active-targets-justification","title":"Currently Active Targets: Justification","text":""},{"location":"tests/#target-dependency-graph","title":"Target Dependency Graph","text":"<pre><code>tests_fast \u26a1 (RECOMMENDED)\n\u251c\u2500\u2500 doctests_no_docker\n\u2502   \u251c\u2500\u2500 doctests_minimal\n\u2502   \u251c\u2500\u2500 doctests_torch\n\u2502   \u251c\u2500\u2500 doctests_gpytorch\n\u2502   \u2514\u2500\u2500 doctests_botorch\n\u251c\u2500\u2500 unittests\n\u2514\u2500\u2500 booktests_no_docker\n    \u251c\u2500\u2500 check_booktests\n    \u251c\u2500\u2500 generate_booktests\n    \u2514\u2500\u2500 clean_local_only_files\n\ntests_no_docker\n\u251c\u2500\u2500 doctests_no_docker\n\u251c\u2500\u2500 unittests\n\u2514\u2500\u2500 coverage\n\ntests (full with Docker)\n\u251c\u2500\u2500 doctests\n\u2502   \u251c\u2500\u2500 doctests_markdown\n\u2502   \u251c\u2500\u2500 doctests_minimal\n\u2502   \u251c\u2500\u2500 doctests_torch\n\u2502   \u251c\u2500\u2500 doctests_gpytorch\n\u2502   \u251c\u2500\u2500 doctests_botorch\n\u2502   \u2514\u2500\u2500 doctests_umbridge\n\u251c\u2500\u2500 unittests\n\u2514\u2500\u2500 coverage\n\nbooktests_parallel_no_docker\n\u251c\u2500\u2500 check_booktests\n\u251c\u2500\u2500 generate_booktests\n\u251c\u2500\u2500 clean_local_only_files\n\u2514\u2500\u2500 [Parsl distributed execution]\n</code></pre>"},{"location":"tests/#automatic-parallel-execution","title":"Automatic Parallel Execution","text":"<ul> <li>If <code>pytest-xdist</code> is installed, tests run with <code>-n auto</code> (detected by <code>scripts/pytest_xdist.py</code>)</li> <li>Override: <code>make PYTEST_XDIST=\"-n 4\" unittests</code> to force 4 workers</li> </ul>"},{"location":"tests/#selective-notebook-tests","title":"Selective Notebook Tests","text":"<p>Run specific notebook tests: </p><pre><code>make booktests_no_docker TESTS=\"tb_quickstart tb_pricing_options\"\n</code></pre><p></p>"},{"location":"tests/#environment-cleanup","title":"Environment Cleanup","text":"<p>The test targets automatically call <code>scripts/cleanup_invalid_dist.py --apply</code> to remove corrupted distribution artifacts (e.g., invalid seaborn entries).</p>"},{"location":"tests/#reproducibility","title":"Reproducibility","text":"<p>All tests use deterministic seeds (e.g., <code>seed=42</code>, <code>seed=7</code>) where applicable to ensure reproducible results across runs.</p>"},{"location":"tests/#workflow-recommendations","title":"Workflow Recommendations","text":""},{"location":"tests/#for-development","title":"For Development","text":"<pre><code>make unittests              # Quick feedback (13\u201330s)\nmake doctests_no_docker     # Validate docstrings (15\u201330s)\n</code></pre>"},{"location":"tests/#before-committing","title":"Before Committing","text":"<pre><code>make tests_fast             # Comprehensive, fast (30\u201360s)\n</code></pre>"},{"location":"tests/#for-cicd-or-full-validation","title":"For CI/CD or Full Validation","text":"<pre><code>make tests_no_docker        # Sequential, safe (60\u2013120s)\n</code></pre>"},{"location":"tests/#troubleshooting","title":"Troubleshooting","text":"<p>Issue: Tests fail with \"command not found\" or pytest not recognized - Solution: Ensure the conda environment is activated: <code>conda activate qmcpy</code></p> <p>Issue: Tests are slow or not parallelized - Solution: Install <code>pytest-xdist</code>: <code>pip install pytest-xdist</code></p> <p>Issue: Warnings about invalid distributions - Solution: Cleanup runs automatically; if needed manually: <code>python scripts/cleanup_invalid_dist.py --apply</code></p> <p>Issue: Coverage numbers seem low or cumulative - Solution: Reset coverage with <code>make delcoverage</code>, then run tests</p>"},{"location":"tests/#coverage-report-strategy","title":"Coverage Report Strategy","text":""},{"location":"tests/#overview","title":"Overview","text":"<p>QMCSoftware uses a multi-platform unified coverage report approach in GitHub Actions CI. Coverage data from all test types (doctests, unittests, booktests) running on all platforms (Ubuntu, macOS, Windows) is combined into a single  coverage percentage.</p>"},{"location":"tests/#official-coverage-metric-unit-tests-only","title":"Official Coverage Metric (Unit Tests Only)","text":"<p>Although QMCSoftware executes doctests, unit tests, and booktests, only unit tests are used as the official coverage metric for evaluating code coverage.</p> <p>This distinction is intentional and follows standard software engineering best practices.</p>"},{"location":"tests/#rationale","title":"Rationale","text":"<ul> <li>Unit tests (<code>test/</code>) are:</li> <li>Deterministic and isolated</li> <li>Designed to explicitly exercise control flow, edge cases, and error paths</li> <li>Stable across platforms and environments</li> <li> <p>The industry-standard basis for coverage metrics</p> </li> <li> <p>Doctests validate correctness of documented examples, but:</p> </li> <li>Often execute high-level workflows</li> <li>May implicitly cover large portions of code without asserting behavior</li> <li> <p>Can inflate coverage without increasing test rigor</p> </li> <li> <p>Booktests (notebook tests):</p> </li> <li>Execute demonstration notebooks end-to-end</li> <li>Are environment- and runtime-dependent</li> <li>Primarily validate documentation and reproducibility, not fine-grained logic</li> </ul> <p>For these reasons, doctests and booktests are excluded from the official coverage percentage to avoid overstating test completeness.</p>"},{"location":"tests/#local-vs-ci-coverage","title":"Local vs CI Coverage","text":"<ul> <li>Local development coverage:</li> <li>Developers may accumulate coverage across test types for diagnostic purposes</li> <li> <p>HTML reports may include additional lines exercised by doctests or booktests</p> </li> <li> <p>Official coverage evaluation:</p> </li> <li>Coverage reported for review, grading, or quality metrics is derived only from unit tests</li> <li>This ensures coverage reflects intentional, maintainable, and reviewable tests</li> </ul> <p>This policy ensures that coverage metrics remain meaningful, reproducible, and aligned with software quality goals, rather than reflecting incidental execution paths.</p>"},{"location":"tests/#architecture","title":"Architecture","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Matrix Job: tests (windows-latest, macos-latest, ubuntu-latest)  \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502  1. Clean old coverage files (.coverage*, coverage.json)\u2502\n\u2502  2. Run doctests (with --cov-append)                    \u2502\n\u2502  3. Run unittests (with --cov-append)                   \u2502\n\u2502  4. Run booktests (with --cov-append)                   \u2502\n\u2502  5. Upload .coverage &amp; coverage.json as artifacts       \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"tests/#key-syntax-configuration","title":"Key Syntax &amp; Configuration","text":""},{"location":"tests/#1-coverage-configuration-coveragerc","title":"1. Coverage Configuration (<code>.coveragerc</code>)","text":"<p>Cross-platform coverage combining requires relative paths to handle different OS path formats (Windows <code>C:\\</code>, macOS <code>/Users/</code>, Ubuntu <code>/home/</code>):</p> <p><code>.coveragerc</code> settings: - <code>relative_files = True</code> \u2013 Store paths relative to project root - <code>[paths]</code> section maps all OS path variants to common source location</p>"},{"location":"tests/#2-makefile-test-targets-coverage-append-mode","title":"2. Makefile Test Targets (Coverage Append Mode)","text":"<p>All test targets use <code>--cov-append</code> (pytest) or <code>coverage run --append</code> to accumulate coverage within each OS runner:</p> <p>Doctest targets: <code>doctests_minimal</code>, <code>doctests_torch</code>, <code>doctests_gpytorch</code>, <code>doctests_botorch</code>, <code>doctests_umbridge</code> \u2013 all use <code>--cov-append</code></p> <p>Unit test target: <code>unittests</code> \u2013 uses <code>--cov-append</code></p> <p>Notebook test targets: - <code>booktests_no_docker</code> \u2013 uses <code>coverage run --append</code> - <code>booktests_parallel_no_docker</code> \u2013 Parsl runner internally uses <code>coverage run --append</code> - <code>booktests_parallel_pytest</code> \u2013 uses <code>--cov-append</code></p> <p>Key flags: - <code>--cov qmcpy/</code> \u2013 Target package for coverage measurement - <code>--cov-append</code> \u2013 Append to existing <code>.coverage</code> data (pytest-cov) - <code>coverage run --append</code> \u2013 Append mode for unittest-based tests - <code>--cov-report term</code> \u2013 Terminal output after each test run - <code>--cov-report json</code> \u2013 Generate <code>coverage.json</code> for tracking</p>"},{"location":"tests/#3-github-actions-workflow","title":"3. GitHub Actions Workflow","text":"<ul> <li>Clean coverage at start of each matrix job:</li> </ul>"},{"location":"tests/#ci-coverage-summary","title":"CI &amp; Coverage (summary)","text":"<ul> <li> <p>GitHub Actions: The main CI workflow is <code>.github/workflows/alltests.yml</code> (referred to in this document as <code>alltests.yml</code>). It runs a matrix across OSes, and calls Makefile targets </p> <p>Note: The project CI is configured to upload coverage to Codecov.</p> </li> </ul> <p>A second workflow, <code>.github/workflows/unittests.yml</code>, runs a matrix across Python versions and is triggered by updates to <code>develop</code> and <code>master</code> branches only.</p>"},{"location":"tests/#local-coverage-workflow","title":"Local Coverage Workflow","text":"<p>Run tests and view coverage locally:</p> <ul> <li>Clean old coverage</li> <li>Run tests (accumulates coverage with --cov-append)</li> </ul>"},{"location":"tests/#benefits","title":"Benefits","text":"<ol> <li>Comprehensive test coverage \u2013 Includes doctests, unittests, and notebook tests</li> <li>Artifact persistence \u2013 HTML and XML reports available </li> <li>Incremental local testing \u2013 <code>--cov-append</code> allows building coverage across multiple test runs</li> <li>CI/CD integration ready \u2013 XML output compatible with Codecov, Coveralls, etc.</li> </ol>"},{"location":"tests/#troubleshooting_1","title":"Troubleshooting","text":"<p>Coverage numbers seem wrong or incomplete: - Run <code>make delcoverage</code> to clean old data before starting fresh - Ensure all test commands use <code>--cov-append</code> or <code>coverage run --append</code></p> <p>Coverage combining fails locally: - Ensure <code>coverage</code> package is installed: <code>pip install coverage</code></p>"},{"location":"tests/#see-also","title":"See Also","text":"<ul> <li><code>Makefile</code> \u2013 Full test target definitions (in project root)</li> <li><code>.github/workflows/alltests.yml</code> \u2013 CI all test workflow</li> <li><code>.github/workflows/unittests.yml</code> - CI unit test workflow</li> <li><code>scripts/cleanup_invalid_dist.py</code> \u2013 Artifact cleanup utility</li> <li><code>scripts/pytest_xdist.py</code> \u2013 Parallel execution detection helper</li> </ul>"},{"location":"api/discrete_distributions/","title":"Discrete Distributions","text":""},{"location":"api/discrete_distributions/#discrete-distributions","title":"Discrete Distributions","text":""},{"location":"api/discrete_distributions/#uml-overview","title":"UML Overview","text":""},{"location":"api/discrete_distributions/#abstractdiscretedistribution","title":"<code>AbstractDiscreteDistribution</code>","text":"<p>               Bases: <code>object</code></p> Source code in <code>qmcpy/discrete_distribution/abstract_discrete_distribution.py</code> <pre><code>def __init__(self, dimension, replications, seed, d_limit, n_limit):\n    self.mimics = \"StdUniform\"\n    if not hasattr(self, \"parameters\"):\n        self.parameters = []\n    self.d_limit = d_limit\n    self.n_limit = n_limit\n    if not (\n        np.isscalar(self.d_limit)\n        and self.d_limit &gt; 0\n        and np.isscalar(self.n_limit)\n        and self.n_limit &gt; 0\n    ):\n        raise ParameterError(\"d_limit and n_limit must be greater than 0\")\n    self.d_limit = np.inf if self.d_limit == np.inf else int(self.d_limit)\n    self.n_limit = np.inf if self.n_limit == np.inf else int(self.n_limit)\n    self.no_replications = replications is None\n    self.replications = 1 if self.no_replications else int(replications)\n    if self.replications &lt; 0:\n        raise ParameterError(\"replications must be None or a postive int\")\n    if (\n        isinstance(dimension, list)\n        or isinstance(dimension, tuple)\n        or isinstance(dimension, np.ndarray)\n    ):\n        self.dvec = np.array(dimension, dtype=int)\n        self.d = len(self.dvec)\n        if not (self.dvec.ndim == 1 and len(np.unique(self.dvec)) == self.d):\n            raise ParameterError(\"dimension must be a 1d array of unique values\")\n    else:\n        self.d = int(dimension)\n        self.dvec = np.arange(self.d)\n    if any(self.dvec &gt; self.d_limit):\n        raise ParameterError(\n            \"dimension greater than dimension limit %d\" % self.d_limit\n        )\n    self._base_seed = (\n        seed\n        if isinstance(seed, np.random.SeedSequence)\n        else np.random.SeedSequence(seed)\n    )\n    self.entropy = self._base_seed.entropy\n    self.spawn_key = self._base_seed.spawn_key\n    self.rng = np.random.Generator(np.random.SFC64(self._base_seed))\n</code></pre>"},{"location":"api/discrete_distributions/#qmcpy.discrete_distribution.abstract_discrete_distribution.AbstractDiscreteDistribution.__call__","title":"__call__","text":"<pre><code>__call__(n=None, n_min=None, n_max=None, return_binary=False, warn=True)\n</code></pre> <ul> <li>If just <code>n</code> is supplied, generate samples from the sequence at indices 0,...,<code>n</code>-1.</li> <li>If <code>n_min</code> and <code>n_max</code> are supplied, generate samples from the sequence at indices <code>n_min</code>,...,<code>n_max</code>-1.</li> <li>If <code>n</code> and <code>n_min</code> are supplied, then generate samples from the sequence at indices <code>n</code>,...,<code>n_min</code>-1.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>Union[None, int]</code> <p>Number of points to generate.</p> <code>None</code> <code>n_min</code> <code>Union[None, int]</code> <p>Starting index of sequence.</p> <code>None</code> <code>n_max</code> <code>Union[None, int]</code> <p>Final index of sequence.</p> <code>None</code> <code>return_binary</code> <code>bool</code> <p>Only used for <code>DigitalNetB2</code>. If <code>True</code>, only return the integer representation <code>x_integer</code> of base 2 digital net.</p> <code>False</code> <code>warn</code> <code>bool</code> <p>If <code>False</code>, disable warnings when generating samples.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>x</code> <code>ndarray</code> <p>Samples from the sequence.</p> <ul> <li>If <code>replications</code> is <code>None</code> then this will be of size (<code>n_max</code>-<code>n_min</code>) \\(\\times\\) <code>dimension</code></li> <li>If <code>replications</code> is a positive int, then <code>x</code> will be of size <code>replications</code> \\(\\times\\) (<code>n_max</code>-<code>n_min</code>) \\(\\times\\) <code>dimension</code></li> </ul> <p>Note that if <code>return_binary=True</code> then <code>x</code> is returned where <code>x</code> are integer representations of the digital net points.</p> Source code in <code>qmcpy/discrete_distribution/abstract_discrete_distribution.py</code> <pre><code>def __call__(self, n=None, n_min=None, n_max=None, return_binary=False, warn=True):\n    r\"\"\"\n    - If just `n` is supplied, generate samples from the sequence at indices 0,...,`n`-1.\n    - If `n_min` and `n_max` are supplied, generate samples from the sequence at indices `n_min`,...,`n_max`-1.\n    - If `n` and `n_min` are supplied, then generate samples from the sequence at indices `n`,...,`n_min`-1.\n\n    Args:\n        n (Union[None, int]): Number of points to generate.\n        n_min (Union[None, int]): Starting index of sequence.\n        n_max (Union[None, int]): Final index of sequence.\n        return_binary (bool): Only used for `DigitalNetB2`.\n            If `True`, *only* return the integer representation `x_integer` of base 2 digital net.\n        warn (bool): If `False`, disable warnings when generating samples.\n\n    Returns:\n        x (np.ndarray): Samples from the sequence.\n\n            - If `replications` is `None` then this will be of size (`n_max`-`n_min`) $\\times$ `dimension`\n            - If `replications` is a positive int, then `x` will be of size `replications` $\\times$ (`n_max`-`n_min`) $\\times$ `dimension`\n\n            Note that if `return_binary=True` then `x` is returned where `x` are integer representations of the digital net points.\n    \"\"\"\n    return self.gen_samples(\n        n=n, n_min=n_min, n_max=n_max, return_binary=return_binary, warn=warn\n    )\n</code></pre>"},{"location":"api/discrete_distributions/#qmcpy.discrete_distribution.abstract_discrete_distribution.AbstractDiscreteDistribution.spawn","title":"spawn","text":"<pre><code>spawn(s=1, dimensions=None)\n</code></pre> <p>Spawn new instances of the current discrete distribution but with new seeds and dimensions. Used by multi-level QMC algorithms which require different seeds and dimensions on each level.</p> Note <p>Use <code>replications</code> instead of using <code>spawn</code> when possible, e.g., when spawning copies which all have the same dimension.</p> <p>Parameters:</p> Name Type Description Default <code>s</code> <code>int</code> <p>Number of copies to spawn</p> <code>1</code> <code>dimensions</code> <code>ndarray</code> <p>Length <code>s</code> array of dimensions for each copy. Defaults to the current dimension.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>spawned_discrete_distribs</code> <code>list</code> <p>Discrete distributions with new seeds and dimensions.</p> Source code in <code>qmcpy/discrete_distribution/abstract_discrete_distribution.py</code> <pre><code>def spawn(self, s=1, dimensions=None):\n    r\"\"\"\n    Spawn new instances of the current discrete distribution but with new seeds and dimensions.\n    Used by multi-level QMC algorithms which require different seeds and dimensions on each level.\n\n    Note:\n        Use `replications` instead of using `spawn` when possible, e.g., when spawning copies which all have the same dimension.\n\n    Args:\n        s (int): Number of copies to spawn\n        dimensions (np.ndarray): Length `s` array of dimensions for each copy. Defaults to the current dimension.\n\n    Returns:\n        spawned_discrete_distribs (list): Discrete distributions with new seeds and dimensions.\n    \"\"\"\n    s = int(s)\n    if s &lt;= 0:\n        raise ParameterError(\"Must spawn s&gt;0 instances\")\n    if dimensions is None:\n        dimensions = np.tile(self.d, s)\n    elif (\n        isinstance(dimensions, list)\n        or isinstance(dimensions, tuple)\n        or isinstance(dimensions, np.ndarray)\n    ):\n        dimensions = np.array(dimensions, dtype=int)\n    else:\n        dimensions = np.tile(dimensions, s)\n    if not (dimensions.ndim == 1 and len(dimensions) == s):\n        raise ParameterError(\"dimensions must be a length s np.ndarray\")\n    child_seeds = self._base_seed.spawn(s)\n    spawned_discrete_distribs = [\n        self._spawn(child_seeds[i], int(dimensions[i])) for i in range(s)\n    ]\n    return spawned_discrete_distribs\n</code></pre>"},{"location":"api/discrete_distributions/#digitalnetb2","title":"<code>DigitalNetB2</code>","text":"<p>               Bases: <code>AbstractLDDiscreteDistribution</code></p> <p>Low discrepancy digital net in base 2.</p> Note <ul> <li>Digital net sample sizes should be powers of \\(2\\) e.g. \\(1\\), \\(2\\), \\(4\\), \\(8\\), \\(16\\), \\(\\dots\\).</li> <li>The first point of an unrandomized digital nets is the origin.</li> <li><code>Sobol</code> is an alias for <code>DigitalNetB2</code>.</li> <li> <p>To use higher order digital nets, either:</p> <ul> <li>Pass in <code>generating_matrices</code> without interlacing and supply <code>alpha</code>&gt;1 to apply interlacing, or</li> <li>Pass in <code>generating_matrices</code> with interlacing and set <code>alpha=1</code> to avoid additional interlacing</li> </ul> <p>i.e. do not pass in interlaced <code>generating_matrices</code> and set <code>alpha&gt;1</code>, this will apply additional interlacing.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; discrete_distrib = DigitalNetB2(2,seed=7)\n&gt;&gt;&gt; discrete_distrib(4)\narray([[0.72162356, 0.914955  ],\n       [0.16345554, 0.42964856],\n       [0.98676255, 0.03436384],\n       [0.42956655, 0.55876342]])\n&gt;&gt;&gt; discrete_distrib(1) # first point in the sequence\narray([[0.72162356, 0.914955  ]])\n&gt;&gt;&gt; discrete_distrib\nDigitalNetB2 (AbstractLDDiscreteDistribution)\n    d               2^(1)\n    replications    1\n    randomize       LMS DS\n    gen_mats_source joe_kuo.6.21201.txt\n    order           RADICAL INVERSE\n    t               63\n    alpha           1\n    n_limit         2^(32)\n    entropy         7\n</code></pre> <p>Replications of independent randomizations</p> <pre><code>&gt;&gt;&gt; x = DigitalNetB2(dimension=3,seed=7,replications=2)(4)\n&gt;&gt;&gt; x.shape\n(2, 4, 3)\n&gt;&gt;&gt; x\narray([[[0.24653277, 0.1821862 , 0.74732591],\n        [0.68152903, 0.66169442, 0.42891961],\n        [0.48139855, 0.79818233, 0.08201287],\n        [0.91541325, 0.29520621, 0.77495809]],\n\n       [[0.44876891, 0.85899604, 0.50549679],\n        [0.53635924, 0.04353443, 0.33564946],\n        [0.23214143, 0.29281506, 0.06841036],\n        [0.75295715, 0.60241448, 0.76962976]]])\n</code></pre> <p>Different orderings (avoid warnings that the first point is the origin)</p> <pre><code>&gt;&gt;&gt; DigitalNetB2(dimension=2,randomize=False,order=\"GRAY\")(n_min=2,n_max=4,warn=False)\narray([[0.75, 0.25],\n       [0.25, 0.75]])\n&gt;&gt;&gt; DigitalNetB2(dimension=2,randomize=False,order=\"RADICAL INVERSE\")(n_min=2,n_max=4,warn=False)\narray([[0.25, 0.75],\n       [0.75, 0.25]])\n</code></pre> <p>Generating matrices from https://github.com/QMCSoftware/LDData/tree/main/dnet</p> <pre><code>&gt;&gt;&gt; DigitalNetB2(dimension=3,randomize=False,generating_matrices=\"mps.nx_s5_alpha2_m32.txt\")(8,warn=False)\narray([[0.        , 0.        , 0.        ],\n       [0.75841841, 0.45284834, 0.48844557],\n       [0.57679828, 0.13226272, 0.10061957],\n       [0.31858402, 0.32113875, 0.39369111],\n       [0.90278927, 0.45867532, 0.01803333],\n       [0.14542431, 0.02548793, 0.4749614 ],\n       [0.45587539, 0.33081476, 0.11474426],\n       [0.71318879, 0.15377192, 0.37629925]])\n</code></pre> <p>All randomizations</p> <pre><code>&gt;&gt;&gt; DigitalNetB2(dimension=3,randomize='LMS DS',seed=5)(8)\narray([[0.69346401, 0.20118185, 0.64779396],\n       [0.43998032, 0.90102467, 0.0936172 ],\n       [0.86663563, 0.60910036, 0.26043276],\n       [0.11327376, 0.30772653, 0.93959283],\n       [0.62102883, 0.79169756, 0.77051637],\n       [0.37451038, 0.1231324 , 0.46634012],\n       [0.94785596, 0.38577413, 0.13377215],\n       [0.20121617, 0.71843325, 0.56293458]])\n&gt;&gt;&gt; DigitalNetB2(dimension=3,randomize='LMS',seed=5)(8,warn=False)\narray([[0.        , 0.        , 0.        ],\n       [0.75446077, 0.83265937, 0.69584079],\n       [0.42329494, 0.65793842, 0.90427279],\n       [0.67763292, 0.48937304, 0.33344964],\n       [0.18550714, 0.97332905, 0.3772791 ],\n       [0.93104851, 0.17195496, 0.82311652],\n       [0.26221346, 0.31742386, 0.53093284],\n       [0.50787715, 0.5172669 , 0.2101083 ]])\n&gt;&gt;&gt; DigitalNetB2(dimension=3,randomize='DS',seed=5)(8)\narray([[0.68383949, 0.04047995, 0.42903182],\n       [0.18383949, 0.54047995, 0.92903182],\n       [0.93383949, 0.79047995, 0.67903182],\n       [0.43383949, 0.29047995, 0.17903182],\n       [0.55883949, 0.66547995, 0.05403182],\n       [0.05883949, 0.16547995, 0.55403182],\n       [0.80883949, 0.41547995, 0.80403182],\n       [0.30883949, 0.91547995, 0.30403182]])\n&gt;&gt;&gt; DigitalNetB2(dimension=3,randomize='OWEN',seed=5)(8)\narray([[0.33595486, 0.05834975, 0.30066401],\n       [0.89110875, 0.84905188, 0.81833285],\n       [0.06846074, 0.59997956, 0.67064205],\n       [0.6693703 , 0.25824002, 0.10469644],\n       [0.44586618, 0.99161977, 0.1873488 ],\n       [0.84245267, 0.16445553, 0.56544372],\n       [0.18546359, 0.44859876, 0.97389524],\n       [0.61215442, 0.64341386, 0.44529863]])\n</code></pre> <p>Higher order net without randomization</p> <pre><code>&gt;&gt;&gt; DigitalNetB2(dimension=3,randomize='FALSE',seed=7,alpha=2)(4,warn=False)\narray([[0.    , 0.    , 0.    ],\n       [0.75  , 0.75  , 0.75  ],\n       [0.4375, 0.9375, 0.1875],\n       [0.6875, 0.1875, 0.9375]])\n</code></pre> <p>Higher order nets with randomizations and replications</p> <pre><code>&gt;&gt;&gt; DigitalNetB2(dimension=3,randomize='LMS DS',seed=7,replications=2,alpha=2)(4,warn=False)\narray([[[0.42955149, 0.89149058, 0.43867111],\n        [0.68701828, 0.07601148, 0.51312447],\n        [0.10088033, 0.16293661, 0.25144138],\n        [0.85846252, 0.87103178, 0.70041789]],\n\n       [[0.27151905, 0.42406763, 0.21917369],\n        [0.55035224, 0.67864387, 0.90033876],\n        [0.19356758, 0.57589964, 0.00347701],\n        [0.97235125, 0.32168581, 0.86920948]]])\n&gt;&gt;&gt; DigitalNetB2(dimension=3,randomize='LMS',seed=7,replications=2,alpha=2)(4,warn=False)\narray([[[0.        , 0.        , 0.        ],\n        [0.75817062, 0.96603053, 0.94947625],\n        [0.45367986, 0.80295638, 0.18778553],\n        [0.71171791, 0.2295424 , 0.76175441]],\n\n       [[0.        , 0.        , 0.        ],\n        [0.78664636, 0.75470215, 0.86876474],\n        [0.45336727, 0.99953621, 0.22253579],\n        [0.73996397, 0.24544824, 0.9008679 ]]])\n&gt;&gt;&gt; DigitalNetB2(dimension=3,randomize='DS',seed=7,replications=2,alpha=2)(4)\narray([[[0.04386058, 0.58727432, 0.3691824 ],\n        [0.79386058, 0.33727432, 0.6191824 ],\n        [0.48136058, 0.39977432, 0.4316824 ],\n        [0.73136058, 0.64977432, 0.6816824 ]],\n\n       [[0.65212985, 0.69669968, 0.10605352],\n        [0.40212985, 0.44669968, 0.85605352],\n        [0.83962985, 0.25919968, 0.16855352],\n        [0.08962985, 0.50919968, 0.91855352]]])\n&gt;&gt;&gt; DigitalNetB2(dimension=3,randomize='OWEN',seed=7,replications=2,alpha=2)(4)\narray([[[0.46368517, 0.03964427, 0.62172094],\n        [0.7498683 , 0.76141348, 0.4243043 ],\n        [0.01729754, 0.97968459, 0.65963223],\n        [0.75365329, 0.1903774 , 0.34141493]],\n\n       [[0.52252547, 0.5679709 , 0.05949112],\n        [0.27248656, 0.36488289, 0.81844058],\n        [0.94219959, 0.39172304, 0.20285965],\n        [0.19716391, 0.64741585, 0.92494554]]])\n</code></pre> <p>References:</p> <ol> <li> <p>Marius Hofert and Christiane Lemieux.     qrng: (Randomized) Quasi-Random Number Generators (2019).     R package version 0.0-7.     https://CRAN.R-project.org/package=qrng.</p> </li> <li> <p>Faure, Henri, and Christiane Lemieux.     Implementation of Irreducible Sobol' Sequences in Prime Power Bases.     Mathematics and Computers in Simulation 161 (2019): 13-22. Crossref. Web.</p> </li> <li> <p>F.Y. Kuo, D. Nuyens.     Application of quasi-Monte Carlo methods to elliptic PDEs with random diffusion coefficients - a survey of analysis and implementation.     Foundations of Computational Mathematics, 16(6):1631-1696, 2016.     https://link.springer.com/article/10.1007/s10208-016-9329-5.</p> </li> <li> <p>D. Nuyens.     The Magic Point Shop of QMC point generators and generating vectors.     MATLAB and Python software, 2018.     https://people.cs.kuleuven.be/~dirk.nuyens/.</p> </li> <li> <p>R. Cools, F.Y. Kuo, D. Nuyens.     Constructing embedded lattice rules for multivariate integration.     SIAM J. Sci. Comput., 28(6), 2162-2188.</p> </li> <li> <p>I.M. Sobol', V.I. Turchaninov, Yu.L. Levitan, B.V. Shukhman.     Quasi-Random Sequence Generators.     Keldysh Institute of Applied Mathematics.     Russian Academy of Sciences, Moscow (1992).</p> </li> <li> <p>Sobol, Ilya &amp; Asotsky, Danil &amp; Kreinin, Alexander &amp; Kucherenko, Sergei. (2011).     Construction and Comparison of High-Dimensional Sobol' Generators. Wilmott. 2011.     10.1002/wilm.10056.</p> </li> <li> <p>Paul Bratley and Bennett L. Fox.     Algorithm 659: Implementing Sobol's quasirandom sequence generator.     ACM Trans. Math. Softw. 14, 1 (March 1988), 88-100. 1988.     https://doi.org/10.1145/42288.214372.</p> </li> </ol> <p>Parameters:</p> Name Type Description Default <code>dimension</code> <code>Union[int, ndarray]</code> <p>Dimension of the generator.</p> <ul> <li>If an <code>int</code> is passed in, use generating vector components at indices 0,...,<code>dimension</code>-1.</li> <li>If an <code>np.ndarray</code> is passed in, use generating vector components at these indices.</li> </ul> <code>1</code> <code>replications</code> <code>int</code> <p>Number of independent randomizations of a pointset.</p> <code>None</code> <code>seed</code> <code>Union[None, int, np.random.SeedSeq</code> <p>Seed the random number generator for reproducibility.</p> <code>None</code> <code>randomize</code> <code>str</code> <p>Options are</p> <ul> <li><code>'LMS DS'</code>: Linear matrix scramble with digital shift.</li> <li><code>'LMS'</code>: Linear matrix scramble only.</li> <li><code>'DS'</code>: Digital shift only.</li> <li><code>'NUS'</code>: Nested uniform scrambling. Also known as Owen scrambling.</li> <li><code>'FALSE'</code>: No randomization. In this case the first point will be the origin.</li> </ul> <code>'LMS DS'</code> <code>generating_matrices</code> <code>Union[str, ndarray, int]</code> <p>Specify the generating matrices.</p> <ul> <li>A <code>str</code> should be the name (or path) of a file from the LDData repo at https://github.com/QMCSoftware/LDData/tree/main/dnet.</li> <li>An <code>np.ndarray</code> of integers with shape \\((d,m_\\mathrm{max})\\) or \\((r,d,m_\\mathrm{max})\\) where \\(d\\) is the number of dimensions, \\(r\\) is the number of replications, and \\(2^{m_\\mathrm{max}}\\) is the maximum number of supported points. Setting <code>msb=False</code> will flip the bits of ints in the generating matrices.</li> </ul> <code>'joe_kuo.6.21201.txt'</code> <code>order</code> <code>str</code> <p><code>'RADICAL INVERSE'</code>, or <code>'GRAY'</code> ordering. See the doctest example above.</p> <code>'RADICAL INVERSE'</code> <code>t</code> <code>int</code> <p>Number of bits in integer represetation of points after randomization. The number of bits in the generating matrices is inferred based on the largest value.</p> <code>63</code> <code>alpha</code> <code>int</code> <p>Interlacing factor for higher order nets. When <code>alpha</code>&gt;1, interlacing is performed regardless of the generating matrices, i.e., for <code>alpha</code>&gt;1 do not pass in generating matrices which are already interlaced. The Note for this class contains more info.</p> <code>1</code> <code>msb</code> <code>bool</code> <p>Flag for Most Significant Bit (MSB) vs Least Significant Bit (LSB) integer representations in generating matrices. If <code>msb=False</code> (LSB order), then integers in generating matrices will be bit-reversed.</p> <code>None</code> <code>_verbose</code> <code>bool</code> <p>If <code>True</code>, print linear matrix scrambling matrices.</p> <code>False</code> Source code in <code>qmcpy/discrete_distribution/digital_net_b2/digital_net_b2.py</code> <pre><code>def __init__(\n    self,\n    dimension=1,\n    replications=None,\n    seed=None,\n    randomize=\"LMS DS\",\n    generating_matrices=\"joe_kuo.6.21201.txt\",\n    order=\"RADICAL INVERSE\",\n    t=63,\n    alpha=1,\n    msb=None,\n    _verbose=False,\n    # deprecated\n    graycode=None,\n    t_max=None,\n    t_lms=None,\n):\n    r\"\"\"\n    Args:\n        dimension (Union[int, np.ndarray]): Dimension of the generator.\n\n            - If an `int` is passed in, use generating vector components at indices 0,...,`dimension`-1.\n            - If an `np.ndarray` is passed in, use generating vector components at these indices.\n\n        replications (int): Number of independent randomizations of a pointset.\n        seed (Union[None, int, np.random.SeedSeq): Seed the random number generator for reproducibility.\n        randomize (str): Options are\n\n            - `'LMS DS'`: Linear matrix scramble with digital shift.\n            - `'LMS'`: Linear matrix scramble only.\n            - `'DS'`: Digital shift only.\n            - `'NUS'`: Nested uniform scrambling. Also known as Owen scrambling.\n            - `'FALSE'`: No randomization. In this case the first point will be the origin.\n\n        generating_matrices (Union[str, np.ndarray, int]): Specify the generating matrices.\n\n            - A `str` should be the name (or path) of a file from the LDData repo at [https://github.com/QMCSoftware/LDData/tree/main/dnet](https://github.com/QMCSoftware/LDData/tree/main/dnet).\n            - An `np.ndarray` of integers with shape $(d,m_\\mathrm{max})$ or $(r,d,m_\\mathrm{max})$ where $d$ is the number of dimensions, $r$ is the number of replications, and $2^{m_\\mathrm{max}}$ is the maximum number of supported points. Setting `msb=False` will flip the bits of ints in the generating matrices.\n\n        order (str): `'RADICAL INVERSE'`, or `'GRAY'` ordering. See the doctest example above.\n        t (int): Number of bits in integer represetation of points *after* randomization. The number of bits in the generating matrices is inferred based on the largest value.\n        alpha (int): Interlacing factor for higher order nets.\n            When `alpha`&gt;1, interlacing is performed regardless of the generating matrices,\n            i.e., for `alpha`&gt;1 do *not* pass in generating matrices which are already interlaced.\n            The Note for this class contains more info.\n        msb (bool): Flag for Most Significant Bit (MSB) vs Least Significant Bit (LSB) integer representations in generating matrices. If `msb=False` (LSB order), then integers in generating matrices will be bit-reversed.\n        _verbose (bool): If `True`, print linear matrix scrambling matrices.\n    \"\"\"\n    if graycode is not None:\n        order = \"GRAY\" if graycode else \"RADICAL INVERSE\"\n        warnings.warn(\n            \"graycode argument deprecated, set order='GRAY' or order='RADICAL INVERSE' instead. Using order='%s'\"\n            % order,\n            ParameterWarning,\n        )\n    if t_lms is not None:\n        t = t_lms\n        warnings.warn(\n            \"t_lms argument deprecated. Set t instead. Using t = %d\" % t,\n            ParameterWarning,\n        )\n    if t_max is not None:\n        warnings.warn(\n            \"t_max is deprecated as it can be inferred from the generating matrices. Set t to change the number of bits after randomization.\",\n            ParameterWarning,\n        )\n    self.parameters = [\n        \"randomize\",\n        \"gen_mats_source\",\n        \"order\",\n        \"t\",\n        \"alpha\",\n        \"n_limit\",\n    ]\n    self.input_generating_matrices = deepcopy(generating_matrices)\n    self.input_t = deepcopy(t)\n    self.input_msb = deepcopy(msb)\n    if (\n        isinstance(generating_matrices, str)\n        and generating_matrices == \"joe_kuo.6.21201.txt\"\n    ):\n        self.gen_mats_source = generating_matrices\n        if np.isscalar(dimension) and dimension &lt;= 1024 and alpha == 1:\n            gen_mats = np.load(\n                dirname(abspath(__file__))\n                + \"/generating_matrices/joe_kuo.6.1024.npy\"\n            )[None, :]\n            d_limit = 1024\n        else:\n            gen_mats = np.load(\n                dirname(abspath(__file__))\n                + \"/generating_matrices/joe_kuo.6.21201.npy\"\n            )[None, :]\n            d_limit = 21201\n        msb = True\n        n_limit = 4294967296\n        self._t_curr = 32\n        compat_shift = self._t_curr - t if self._t_curr &gt;= t else 0\n        if compat_shift &gt; 0:\n            warnings.warn(\n                \"Truncating ints in generating matrix to have t = %d bits.\" % t,\n                ParameterWarning,\n            )\n        gen_mats = gen_mats &gt;&gt; compat_shift\n    elif isinstance(generating_matrices, str):\n        self.gen_mats_source = generating_matrices\n        assert generating_matrices[-4:] == \".txt\"\n        local_root = dirname(abspath(__file__)) + \"/generating_matrices/\"\n        repos = DataSource()\n        if repos.exists(local_root + generating_matrices):\n            datafile = repos.open(local_root + generating_matrices)\n        elif repos.exists(\n            \"https://raw.githubusercontent.com/QMCSoftware/LDData/refs/heads/main/dnet/\"\n            + generating_matrices\n        ):\n            datafile = repos.open(\n                \"https://raw.githubusercontent.com/QMCSoftware/LDData/refs/heads/main/dnet/\"\n                + generating_matrices\n            )\n        elif repos.exists(\n            \"https://raw.githubusercontent.com/QMCSoftware/LDData/refs/heads/main/\"\n            + generating_matrices\n        ):\n            datafile = repos.open(\n                \"https://raw.githubusercontent.com/QMCSoftware/LDData/refs/heads/main/\"\n                + generating_matrices\n            )\n        elif repos.exists(\n            \"https://raw.githubusercontent.com/QMCSoftware/LDData/refs/heads/main/dnet/\"\n            + generating_matrices[7:]\n        ):\n            datafile = repos.open(\n                \"https://raw.githubusercontent.com/QMCSoftware/LDData/refs/heads/main/dnet/\"\n                + generating_matrices[7:]\n            )\n        elif repos.exists(\n            \"https://raw.githubusercontent.com/QMCSoftware/\" + generating_matrices\n        ):\n            datafile = repos.open(\n                \"https://raw.githubusercontent.com/QMCSoftware/\"\n                + generating_matrices\n            )\n        elif repos.exists(generating_matrices):\n            datafile = repos.open(generating_matrices)\n        else:\n            raise ParameterError(\"LDData path %s not found\" % generating_matrices)\n        contents = [line.rstrip(\"\\n\").strip() for line in datafile.readlines()]\n        contents = [line.split(\"#\", 1)[0] for line in contents if line[0] != \"#\"]\n        datafile.close()\n        msb = True\n        assert int(contents[0]) == 2, \"DigitalNetB2 requires base=2 \"  # base 2\n        d_limit = int(contents[1])\n        n_limit = int(contents[2])\n        self._t_curr = int(contents[3])\n        compat_shift = self._t_curr - t if self._t_curr &gt;= t else 0\n        if compat_shift &gt; 0:\n            warnings.warn(\n                \"Truncating ints in generating matrix to have t = %d bits.\" % t,\n                ParameterWarning,\n            )\n        gen_mats = np.array(\n            [\n                [int(v) &gt;&gt; compat_shift for v in line.split(\" \")]\n                for line in contents[4:]\n            ],\n            dtype=np.uint64,\n        )[None, :]\n    elif isinstance(generating_matrices, np.ndarray):\n        self.gen_mats_source = \"custom\"\n        assert generating_matrices.ndim == 2 or generating_matrices.ndim == 3\n        gen_mats = (\n            generating_matrices[None, :, :]\n            if generating_matrices.ndim == 2\n            else generating_matrices\n        )\n        assert isinstance(\n            msb, bool\n        ), \"when generating_matrices is a np.ndarray you must set either msb=True (for most significant bit ordering) or msb=False (for least significant bit ordering which will require a bit reversal)\"\n        gen_mat_max = gen_mats.max()\n        assert gen_mat_max &gt; 0, \"generating matrix must have positive ints\"\n        self._t_curr = int(np.ceil(np.log2(gen_mat_max + 1)))\n        d_limit = gen_mats.shape[1]\n        n_limit = int(2 ** (gen_mats.shape[2]))\n    else:\n        raise ParameterError(\n            \"invalid generating_matrices, must be a string or np.ndarray.\"\n        )\n    super(DigitalNetB2, self).__init__(\n        dimension, replications, seed, d_limit, n_limit\n    )\n    assert (\n        gen_mats.ndim == 3\n        and gen_mats.shape[1] &gt;= self.d\n        and (gen_mats.shape[0] == 1 or gen_mats.shape[0] == self.replications)\n        and gen_mats.shape[2] &gt; 0\n    ), \"invalid gen_mats.shape = %s\" % str(gen_mats.shape)\n    self.m_max = int(gen_mats.shape[-1])\n    if isinstance(generating_matrices, np.ndarray) and (not msb):\n        qmctoolscl.dnb2_gmat_lsb_to_msb(\n            np.uint64(gen_mats.shape[0]),\n            np.uint64(self.d),\n            np.uint64(self.m_max),\n            np.tile(np.uint64(self._t_curr), int(gen_mats.shape[0])),\n            gen_mats,\n            gen_mats,\n            backend=\"c\",\n        )\n    self.order = str(order).upper().strip().replace(\"_\", \" \")\n    if self.order == \"GRAY CODE\":\n        self.order = \"GRAY\"\n    if self.order == \"NATURAL\":\n        self.order = \"RADICAL INVERSE\"\n    assert self.order in [\"RADICAL INVERSE\", \"GRAY\"]\n    assert isinstance(t, int) and t &gt; 0\n    assert self._t_curr &lt;= t &lt;= 64, (\n        \"t must no more than 64 and no less than %d (the number of bits used to represent the generating matrices)\"\n        % (self._t_curr)\n    )\n    assert isinstance(alpha, int) and alpha &gt; 0\n    self.alpha = alpha\n    if self.alpha &gt; 1:\n        assert (\n            self.dvec == np.arange(self.d)\n        ).all(), \"digital interlacing requires dimension is an int\"\n        if self.m_max != self._t_curr:\n            warnings.warn(\n                \"Digital interlacing is often performed on matrices with the number of columns (m_max = %d) equal to the number of bits in each int (%d), but this is not the case. Ensure you are NOT setting alpha&gt;1 when generating matrices are already interlaced.\"\n                % (self.m_max, self._t_curr),\n                ParameterWarning,\n            )\n    self._verbose = _verbose\n    self.randomize = str(randomize).upper().strip().replace(\"_\", \" \")\n    if self.randomize == \"TRUE\":\n        self.randomize = \"LMS DS\"\n    if self.randomize == \"OWEN\":\n        self.randomize = \"NUS\"\n    if self.randomize == \"NONE\":\n        self.randomize = \"FALSE\"\n    if self.randomize == \"NO\":\n        self.randomize = \"FALSE\"\n    assert self.randomize in [\"LMS DS\", \"LMS\", \"DS\", \"NUS\", \"FALSE\"]\n    self.dtalpha = self.alpha * self.d\n    if self.randomize == \"FALSE\":\n        if self.alpha == 1:\n            self.gen_mats = gen_mats[:, self.dvec, :]\n            self.t = self._t_curr\n        else:\n            t_alpha = min(self.alpha * self._t_curr, t)\n            gen_mat_ho = np.empty(\n                (gen_mats.shape[0], self.d, self.m_max), dtype=np.uint64\n            )\n            qmctoolscl.dnb2_interlace(\n                np.uint64(gen_mats.shape[0]),\n                np.uint64(self.d),\n                np.uint64(self.m_max),\n                np.uint64(self.dtalpha),\n                np.uint64(self._t_curr),\n                np.uint64(t_alpha),\n                np.uint64(self.alpha),\n                gen_mats[:, : self.dtalpha, :].copy(),\n                gen_mat_ho,\n                backend=\"c\",\n            )\n            self.gen_mats = gen_mat_ho\n            self._t_curr = t_alpha\n            self.t = self._t_curr\n    elif self.randomize == \"DS\":\n        if self.alpha == 1:\n            self.gen_mats = gen_mats[:, self.dvec, :]\n            self.t = t\n        else:\n            t_alpha = min(self.alpha * self._t_curr, t)\n            gen_mat_ho = np.empty(\n                (gen_mats.shape[0], self.d, self.m_max), dtype=np.uint64\n            )\n            qmctoolscl.dnb2_interlace(\n                np.uint64(gen_mats.shape[0]),\n                np.uint64(self.d),\n                np.uint64(self.m_max),\n                np.uint64(self.dtalpha),\n                np.uint64(self._t_curr),\n                np.uint64(t_alpha),\n                np.uint64(self.alpha),\n                gen_mats[:, : self.dtalpha, :].copy(),\n                gen_mat_ho,\n                backend=\"c\",\n            )\n            self.gen_mats = gen_mat_ho\n            self._t_curr = t_alpha\n            self.t = t\n        self.rshift = qmctoolscl.random_tbit_uint64s(\n            self.rng, self.t, (self.replications, self.d)\n        )\n    elif self.randomize in [\"LMS\", \"LMS DS\"]:\n        if self.alpha == 1:\n            gen_mat_lms = np.empty(\n                (self.replications, self.d, self.m_max), dtype=np.uint64\n            )\n            S = qmctoolscl.dnb2_get_linear_scramble_matrix(\n                self.rng,\n                np.uint64(self.replications),\n                np.uint64(self.d),\n                np.uint64(self._t_curr),\n                np.uint64(t),\n                np.uint64(self._verbose),\n            )\n            qmctoolscl.dnb2_linear_matrix_scramble(\n                np.uint64(self.replications),\n                np.uint64(self.d),\n                np.uint64(self.m_max),\n                np.uint64(gen_mats.shape[0]),\n                np.uint64(t),\n                S,\n                gen_mats[:, self.dvec, :].copy(),\n                gen_mat_lms,\n                backend=\"c\",\n            )\n            self.gen_mats = gen_mat_lms\n            self._t_curr = t\n            self.t = self._t_curr\n        else:\n            gen_mat_lms = np.empty(\n                (self.replications, self.dtalpha, self.m_max), dtype=np.uint64\n            )\n            S = qmctoolscl.dnb2_get_linear_scramble_matrix(\n                self.rng,\n                np.uint64(self.replications),\n                np.uint64(self.dtalpha),\n                np.uint64(self._t_curr),\n                np.uint64(t),\n                np.uint64(self._verbose),\n            )\n            qmctoolscl.dnb2_linear_matrix_scramble(\n                np.uint64(self.replications),\n                np.uint64(self.dtalpha),\n                np.uint64(self.m_max),\n                np.uint64(gen_mats.shape[0]),\n                np.uint64(t),\n                S,\n                gen_mats[:, : self.dtalpha, :].copy(),\n                gen_mat_lms,\n                backend=\"c\",\n            )\n            gen_mat_lms_ho = np.empty(\n                (self.replications, self.d, self.m_max), dtype=np.uint64\n            )\n            qmctoolscl.dnb2_interlace(\n                np.uint64(self.replications),\n                np.uint64(self.d),\n                np.uint64(self.m_max),\n                np.uint64(self.dtalpha),\n                np.uint64(t),\n                np.uint64(t),\n                np.uint64(self.alpha),\n                gen_mat_lms,\n                gen_mat_lms_ho,\n                backend=\"c\",\n            )\n            self.gen_mats = gen_mat_lms_ho\n            self._t_curr = t\n            self.t = self._t_curr\n        if self.randomize == \"LMS DS\":\n            self.rshift = qmctoolscl.random_tbit_uint64s(\n                self.rng, self.t, (self.replications, self.d)\n            )\n    elif self.randomize == \"NUS\":\n        if alpha == 1:\n            new_seeds = self._base_seed.spawn(self.replications * self.d)\n            self.rngs = np.array(\n                [\n                    np.random.Generator(np.random.SFC64(new_seeds[j]))\n                    for j in range(self.replications * self.d)\n                ]\n            ).reshape(self.replications, self.d)\n            self.root_nodes = np.array(\n                [\n                    qmctoolscl.NUSNode_dnb2()\n                    for i in range(self.replications * self.d)\n                ]\n            ).reshape(self.replications, self.d)\n            self.gen_mats = gen_mats[:, self.dvec, :].copy()\n            self.t = t\n        else:\n            self.dtalpha = self.alpha * self.d\n            new_seeds = self._base_seed.spawn(self.replications * self.dtalpha)\n            self.rngs = np.array(\n                [\n                    np.random.Generator(np.random.SFC64(new_seeds[j]))\n                    for j in range(self.replications * self.dtalpha)\n                ]\n            ).reshape(self.replications, self.dtalpha)\n            self.root_nodes = np.array(\n                [\n                    qmctoolscl.NUSNode_dnb2()\n                    for i in range(self.replications * self.dtalpha)\n                ]\n            ).reshape(self.replications, self.dtalpha)\n            self.gen_mats = gen_mats[:, : self.dtalpha, :].copy()\n            self.t = t\n    else:\n        raise ParameterError(\"self.randomize parsing error\")\n    self.gen_mats = np.ascontiguousarray(self.gen_mats)\n    gen_mat_max = self.gen_mats.max()\n    assert gen_mat_max &gt; 0, \"generating matrix must have positive ints\"\n    assert self._t_curr == int(np.ceil(np.log2(gen_mat_max + 1)))\n    assert (\n        0 &lt; self._t_curr &lt;= self.t &lt;= 64\n    ), \"invalid 0 &lt;= self._t_curr (%d) &lt;= self.t (%d) &lt;= 64\" % (\n        self._t_curr,\n        self.t,\n    )\n    if self.randomize == \"FALSE\":\n        assert self.gen_mats.shape[0] == self.replications, (\n            \"randomize='FALSE' but replications = %d does not equal the number of sets of generating matrices %d\"\n            % (self.replications, self.gen_mats.shape[0])\n        )\n</code></pre>"},{"location":"api/discrete_distributions/#lattice","title":"<code>Lattice</code>","text":"<p>               Bases: <code>AbstractLDDiscreteDistribution</code></p> <p>Low discrepancy lattice sequence.</p> Note <ul> <li>Lattice sample sizes should be powers of \\(2\\) e.g. \\(1\\), \\(2\\), \\(4\\), \\(8\\), \\(16\\), \\(\\dots\\).</li> <li>The first point of an unrandomized lattice is the origin.</li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; discrete_distrib = Lattice(2,seed=7)\n&gt;&gt;&gt; discrete_distrib(4)\narray([[0.04386058, 0.58727432],\n       [0.54386058, 0.08727432],\n       [0.29386058, 0.33727432],\n       [0.79386058, 0.83727432]])\n&gt;&gt;&gt; discrete_distrib(1) # first point in the sequence\narray([[0.04386058, 0.58727432]])\n&gt;&gt;&gt; discrete_distrib\nLattice (AbstractLDDiscreteDistribution)\n    d               2^(1)\n    replications    1\n    randomize       SHIFT\n    gen_vec_source  kuo.lattice-33002-1024-1048576.9125.txt\n    order           RADICAL INVERSE\n    n_limit         2^(20)\n    entropy         7\n</code></pre> <p>Replications of independent randomizations</p> <pre><code>&gt;&gt;&gt; x = Lattice(3,seed=7,replications=2)(4)\n&gt;&gt;&gt; x.shape\n(2, 4, 3)\n&gt;&gt;&gt; x\narray([[[0.04386058, 0.58727432, 0.3691824 ],\n        [0.54386058, 0.08727432, 0.8691824 ],\n        [0.29386058, 0.33727432, 0.1191824 ],\n        [0.79386058, 0.83727432, 0.6191824 ]],\n\n       [[0.65212985, 0.69669968, 0.10605352],\n        [0.15212985, 0.19669968, 0.60605352],\n        [0.90212985, 0.44669968, 0.85605352],\n        [0.40212985, 0.94669968, 0.35605352]]])\n</code></pre> <p>Different orderings (avoid warnings that the first point is the origin).</p> <pre><code>&gt;&gt;&gt; Lattice(dimension=2,randomize=False,order='RADICAL INVERSE')(4,warn=False)\narray([[0.  , 0.  ],\n       [0.5 , 0.5 ],\n       [0.25, 0.75],\n       [0.75, 0.25]])\n&gt;&gt;&gt; Lattice(dimension=2,randomize=False,order='GRAY')(4,warn=False)\narray([[0.  , 0.  ],\n       [0.5 , 0.5 ],\n       [0.75, 0.25],\n       [0.25, 0.75]])\n&gt;&gt;&gt; Lattice(dimension=2,randomize=False,order='LINEAR')(4,warn=False)\narray([[0.  , 0.  ],\n       [0.25, 0.75],\n       [0.5 , 0.5 ],\n       [0.75, 0.25]])\n</code></pre> <p>Generating vector from https://github.com/QMCSoftware/LDData/tree/main/lattice</p> <pre><code>&gt;&gt;&gt; Lattice(dimension=3,randomize=False,generating_vector=\"mps.exod2_base2_m20_CKN.txt\")(8,warn=False)\narray([[0.   , 0.   , 0.   ],\n       [0.5  , 0.5  , 0.5  ],\n       [0.25 , 0.75 , 0.75 ],\n       [0.75 , 0.25 , 0.25 ],\n       [0.125, 0.375, 0.375],\n       [0.625, 0.875, 0.875],\n       [0.375, 0.125, 0.125],\n       [0.875, 0.625, 0.625]])\n</code></pre> <p>Random generating vector supporting \\(2^{25}\\) points</p> <pre><code>&gt;&gt;&gt; discrete_distrib = Lattice(3,generating_vector=25,seed=55,randomize=False)\n&gt;&gt;&gt; discrete_distrib.gen_vec\narray([[       1, 11961679, 12107519]], dtype=uint64)\n&gt;&gt;&gt; discrete_distrib(4,warn=False)\narray([[0.  , 0.  , 0.  ],\n       [0.5 , 0.5 , 0.5 ],\n       [0.25, 0.75, 0.75],\n       [0.75, 0.25, 0.25]])\n</code></pre> <p>Two random generating vectors both supporting \\(2^{25}\\) points along with independent random shifts</p> <pre><code>&gt;&gt;&gt; discrete_distrib = Lattice(3,seed=7,generating_vector=25,replications=2)\n&gt;&gt;&gt; discrete_distrib.gen_vec\narray([[       1, 32809149,  1471719],\n       [       1,   275319, 19705657]], dtype=uint64)\n&gt;&gt;&gt; discrete_distrib(4)\narray([[[0.3691824 , 0.65212985, 0.69669968],\n        [0.8691824 , 0.15212985, 0.19669968],\n        [0.6191824 , 0.90212985, 0.44669968],\n        [0.1191824 , 0.40212985, 0.94669968]],\n\n       [[0.10605352, 0.63025643, 0.13630282],\n        [0.60605352, 0.13025643, 0.63630282],\n        [0.35605352, 0.38025643, 0.38630282],\n        [0.85605352, 0.88025643, 0.88630282]]])\n</code></pre> <p>References</p> <ol> <li> <p>Sou-Cheng T. Choi, Yuhan Ding, Fred J. Hickernell, Lan Jiang, Lluis Antoni Jimenez Rugama, Da Li, Jagadeeswaran Rathinavel, Xin Tong, Kan Zhang, Yizhi Zhang, and Xuan Zhou.     GAIL: Guaranteed Automatic Integration Library (Version 2.3), MATLAB Software, 2019.     http://gailgithub.github.io/GAIL_Dev/.</p> </li> <li> <p>F.Y. Kuo, D. Nuyens.     Application of quasi-Monte Carlo methods to elliptic PDEs with random diffusion coefficients - a survey of analysis and implementation.     Foundations of Computational Mathematics, 16(6):1631-1696, 2016.     https://link.springer.com/article/10.1007/s10208-016-9329-5.</p> </li> <li> <p>D. Nuyens.     The Magic Point Shop of QMC point generators and generating vectors.     MATLAB and Python software, 2018.     https://people.cs.kuleuven.be/~dirk.nuyens/.</p> </li> <li> <p>R. Cools, F.Y. Kuo, D. Nuyens.     Constructing embedded lattice rules for multivariate integration.     SIAM J. Sci. Comput., 28(6), 2162-2188.</p> </li> <li> <p>P. L'Ecuyer, D. Munger.     LatticeBuilder: A General Software Tool for Constructing Rank-1 Lattice Rules.     ACM Transactions on Mathematical Software. 42. (2015).     10.1145/2754929.</p> </li> </ol> <p>Parameters:</p> Name Type Description Default <code>dimension</code> <code>Union[int, ndarray]</code> <p>Dimension of the generator.</p> <ul> <li>If an <code>int</code> is passed in, use generating vector components at indices 0,...,<code>dimension</code>-1.</li> <li>If an <code>np.ndarray</code> is passed in, use generating vector components at these indices.</li> </ul> <code>1</code> <code>replications</code> <code>int</code> <p>Number of independent randomizations.</p> <code>None</code> <code>seed</code> <code>Union[None, int, np.random.SeedSeq</code> <p>Seed the random number generator for reproducibility.</p> <code>None</code> <code>randomize</code> <code>str</code> <p>Options are</p> <ul> <li><code>'SHIFT'</code>: Random shift.</li> <li><code>'FALSE'</code>: No randomization. In this case the first point will be the origin.</li> </ul> <code>'SHIFT'</code> <code>generating_vector</code> <code>Union[str, ndarray, int]</code> <p>Specify the generating vector.</p> <ul> <li>A <code>str</code> should be the name (or path) of a file from the LDData repo at https://github.com/QMCSoftware/LDData/tree/main/lattice.</li> <li>A <code>np.ndarray</code> of integers with shape \\((d,)\\) or \\((r,d)\\) where \\(d\\) is the number of dimensions and \\(r\\) is the number of replications.     Must supply <code>m_max</code> where \\(2^{m_\\mathrm{max}}\\) is the max number of supported samples.</li> <li>An <code>int</code>, call it \\(M\\), gives the random generating vector \\((1,v_1,\\dots,v_{d-1})^T\\) where \\(d\\) is the dimension and \\(v_i\\) are randomly selected from \\(\\{3,5,\\dots,2^M-1\\}\\) uniformly and independently. We require require \\(1 &lt; M &lt; 27\\).</li> </ul> <code>'kuo.lattice-33002-1024-1048576.9125.txt'</code> <code>order</code> <code>str</code> <p><code>'LINEAR'</code>, <code>'RADICAL INVERSE'</code>, or <code>'GRAY'</code> ordering. See the doctest example above.</p> <code>'RADICAL INVERSE'</code> <code>m_max</code> <code>int</code> <p>\\(2^{m_\\mathrm{max}}\\) is the maximum number of supported samples.</p> <code>None</code> Source code in <code>qmcpy/discrete_distribution/lattice/lattice.py</code> <pre><code>def __init__(\n    self,\n    dimension=1,\n    replications=None,\n    seed=None,\n    randomize=\"SHIFT\",\n    generating_vector=\"kuo.lattice-33002-1024-1048576.9125.txt\",\n    order=\"RADICAL INVERSE\",\n    m_max=None,\n):\n    r\"\"\"\n    Args:\n        dimension (Union[int, np.ndarray]): Dimension of the generator.\n\n            - If an `int` is passed in, use generating vector components at indices 0,...,`dimension`-1.\n            - If an `np.ndarray` is passed in, use generating vector components at these indices.\n\n        replications (int): Number of independent randomizations.\n        seed (Union[None, int, np.random.SeedSeq): Seed the random number generator for reproducibility.\n        randomize (str): Options are\n\n            - `'SHIFT'`: Random shift.\n            - `'FALSE'`: No randomization. In this case the first point will be the origin.\n\n        generating_vector (Union[str, np.ndarray, int]): Specify the generating vector.\n\n            - A `str` should be the name (or path) of a file from the LDData repo at [https://github.com/QMCSoftware/LDData/tree/main/lattice](https://github.com/QMCSoftware/LDData/tree/main/lattice).\n            - A `np.ndarray` of integers with shape $(d,)$ or $(r,d)$ where $d$ is the number of dimensions and $r$ is the number of replications.\n                Must supply `m_max` where $2^{m_\\mathrm{max}}$ is the max number of supported samples.\n            - An `int`, call it $M$,\n            gives the random generating vector $(1,v_1,\\dots,v_{d-1})^T$\n            where $d$ is the dimension and $v_i$ are randomly selected from $\\{3,5,\\dots,2^M-1\\}$ uniformly and independently.\n            We require require $1 &lt; M &lt; 27$.\n\n        order (str): `'LINEAR'`, `'RADICAL INVERSE'`, or `'GRAY'` ordering. See the doctest example above.\n        m_max (int): $2^{m_\\mathrm{max}}$ is the maximum number of supported samples.\n    \"\"\"\n    self.parameters = [\"randomize\", \"gen_vec_source\", \"order\", \"n_limit\"]\n    self.input_generating_vector = deepcopy(generating_vector)\n    self.input_m_max = deepcopy(m_max)\n    if (\n        isinstance(generating_vector, str)\n        and generating_vector == \"kuo.lattice-33002-1024-1048576.9125.txt\"\n    ):\n        self.gen_vec_source = generating_vector\n        gen_vec = np.load(\n            dirname(abspath(__file__))\n            + \"/generating_vectors/kuo.lattice-33002-1024-1048576.9125.npy\"\n        )[None, :]\n        d_limit = 9125\n        n_limit = 1048576\n    elif isinstance(generating_vector, str):\n        self.gen_vec_source = generating_vector\n        assert generating_vector[-4:] == \".txt\"\n        local_root = dirname(abspath(__file__)) + \"/generating_vectors/\"\n        repos = DataSource()\n        if repos.exists(local_root + generating_vector):\n            datafile = repos.open(local_root + generating_vector)\n        elif repos.exists(\n            \"https://raw.githubusercontent.com/QMCSoftware/LDData/refs/heads/main/lattice/\"\n            + generating_vector\n        ):\n            datafile = repos.open(\n                \"https://raw.githubusercontent.com/QMCSoftware/LDData/refs/heads/main/lattice/\"\n                + generating_vector\n            )\n        elif repos.exists(\n            \"https://raw.githubusercontent.com/QMCSoftware/LDData/refs/heads/main/\"\n            + generating_vector\n        ):\n            datafile = repos.open(\n                \"https://raw.githubusercontent.com/QMCSoftware/LDData/refs/heads/main/\"\n                + generating_vector\n            )\n        elif repos.exists(\n            \"https://raw.githubusercontent.com/QMCSoftware/LDData/refs/heads/main/lattice/\"\n            + generating_vector[7:]\n        ):\n            datafile = repos.open(\n                \"https://raw.githubusercontent.com/QMCSoftware/LDData/refs/heads/main/lattice/\"\n                + generating_vector[7:]\n            )\n        elif repos.exists(\n            \"https://raw.githubusercontent.com/QMCSoftware/\" + generating_vector\n        ):\n            datafile = repos.open(\n                \"https://raw.githubusercontent.com/QMCSoftware/\" + generating_vector\n            )\n        elif repos.exists(generating_vector):\n            datafile = repos.open(generating_vector)\n        else:\n            raise ParameterError(\"LDData path %s not found\" % generating_vector)\n        contents = [\n            int(line.rstrip(\"\\n\").strip().split(\"#\", 1)[0])\n            for line in datafile.readlines()\n            if line[0] != \"#\"\n        ]\n        datafile.close()\n        d_limit = int(contents[0])\n        n_limit = int(contents[1])\n        gen_vec = np.array(contents[2:], dtype=np.uint64)[None, :]\n    elif isinstance(generating_vector, np.ndarray):\n        self.gen_vec_source = \"custom\"\n        gen_vec = generating_vector\n        if m_max is None:\n            raise ParameterError(\n                \"m_max must be supplied when generating_vector is a np.ndarray\"\n            )\n        n_limit = int(2**m_max)\n        d_limit = int(gen_vec.shape[-1])\n    elif isinstance(generating_vector, int):\n        assert 1 &lt; generating_vector &lt; 27, \"int generating vector out of range\"\n        n_limit = 2**generating_vector\n        assert isinstance(\n            dimension, int\n        ), \"random generating vector requires int dimension\"\n        d_limit = dimension\n    else:\n        raise ParameterError(\n            \"invalid generating_vector, must be a string, numpy.ndarray, or int\"\n        )\n    super(Lattice, self).__init__(dimension, replications, seed, d_limit, n_limit)\n    if isinstance(generating_vector, int):\n        self.gen_vec_source = \"random\"\n        m_max = int(np.log2(self.n_limit))\n        gen_vec = np.hstack(\n            [\n                np.ones((self.replications, 1), dtype=np.uint64),\n                2\n                * self.rng.integers(\n                    1,\n                    2 ** (m_max - 1),\n                    size=(self.replications, dimension - 1),\n                    dtype=np.uint64,\n                )\n                + 1,\n            ]\n        ).copy()\n    assert isinstance(gen_vec, np.ndarray)\n    gen_vec = np.atleast_2d(gen_vec)\n    assert (\n        gen_vec.ndim == 2\n        and gen_vec.shape[1] &gt;= self.d\n        and (gen_vec.shape[0] == 1 or gen_vec.shape[0] == self.replications)\n    ), \"invalid gen_vec.shape = %s\" % str(gen_vec.shape)\n    self.gen_vec = gen_vec[:, self.dvec].copy()\n    self.order = str(order).upper().strip().replace(\"_\", \" \")\n    if self.order == \"GRAY CODE\":\n        self.order = \"GRAY\"\n    if self.order == \"NATURAL\":\n        self.order = \"RADICAL INVERSE\"\n    assert self.order in [\"LINEAR\", \"RADICAL INVERSE\", \"GRAY\"]\n    self.randomize = str(randomize).upper()\n    if self.randomize == \"TRUE\":\n        self.randomize = \"SHIFT\"\n    if self.randomize == \"NONE\":\n        self.randomize = \"FALSE\"\n    if self.randomize == \"NO\":\n        self.randomize = \"FALSE\"\n    assert self.randomize in [\"SHIFT\", \"FALSE\"]\n    if self.randomize == \"SHIFT\":\n        self.shift = self.rng.uniform(size=(self.replications, self.d))\n    if self.randomize == \"FALSE\":\n        assert self.gen_vec.shape[0] == self.replications, (\n            \"randomize='FALSE' but replications = %d does not equal the number of sets of generating vectors %d\"\n            % (self.replications, self.gen_vec.shape[0])\n        )\n</code></pre>"},{"location":"api/discrete_distributions/#halton","title":"<code>Halton</code>","text":"<p>               Bases: <code>DigitalNetAnyBases</code></p> <p>Low discrepancy Halton points.</p> Note <ul> <li>The first point of an unrandomized Halton sequence is the origin.</li> <li>QRNG does not support multiple replications (independent randomizations).</li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; discrete_distrib = Halton(2,seed=7)\n&gt;&gt;&gt; discrete_distrib(4)\narray([[0.83790457, 0.89981478],\n       [0.00986102, 0.4610941 ],\n       [0.62236343, 0.02796307],\n       [0.29427505, 0.79909098]])\n&gt;&gt;&gt; discrete_distrib\nHalton (AbstractLDDiscreteDistribution)\n    d               2^(1)\n    replications    1\n    randomize       LMS DP\n    t               63\n    n_limit         2^(32)\n    entropy         7\n</code></pre> <p>Replications of independent randomizations </p> <pre><code>&gt;&gt;&gt; x = Halton(3,seed=7,replications=2)(4)\n&gt;&gt;&gt; x.shape\n(2, 4, 3)\n&gt;&gt;&gt; x\narray([[[0.70988236, 0.18180876, 0.54073621],\n        [0.38178158, 0.61168824, 0.64684354],\n        [0.98597752, 0.70650871, 0.31479029],\n        [0.15795399, 0.28162992, 0.98945647]],\n\n       [[0.620398  , 0.57025403, 0.46336542],\n        [0.44021889, 0.69926312, 0.60133428],\n        [0.89132308, 0.12030255, 0.35715804],\n        [0.04025218, 0.44304244, 0.10724799]]])\n</code></pre> <p>Unrandomized Halton </p> <pre><code>&gt;&gt;&gt; Halton(2,randomize=\"FALSE\",seed=7)(4,warn=False)\narray([[0.        , 0.        ],\n       [0.5       , 0.33333333],\n       [0.25      , 0.66666667],\n       [0.75      , 0.11111111]])\n</code></pre> <p>All randomizations </p> <pre><code>&gt;&gt;&gt; Halton(2,randomize=\"LMS DP\",seed=7)(4)\narray([[0.83790457, 0.89981478],\n       [0.00986102, 0.4610941 ],\n       [0.62236343, 0.02796307],\n       [0.29427505, 0.79909098]])\n&gt;&gt;&gt; Halton(2,randomize=\"LMS DS\",seed=7)(4)\narray([[0.82718745, 0.90603116],\n       [0.0303368 , 0.44704107],\n       [0.60182684, 0.03580544],\n       [0.30505343, 0.78367016]])\n&gt;&gt;&gt; Halton(2,randomize=\"LMS\",seed=7)(4,warn=False)\narray([[0.        , 0.        ],\n       [0.82822666, 0.92392942],\n       [0.28838899, 0.46493682],\n       [0.6165384 , 0.2493814 ]])\n&gt;&gt;&gt; Halton(2,randomize=\"DP\",seed=7)(4)\narray([[0.11593484, 0.99232505],\n       [0.61593484, 0.65899172],\n       [0.36593484, 0.32565839],\n       [0.86593484, 0.77010283]])\n&gt;&gt;&gt; Halton(2,randomize=\"DS\",seed=7)(4)\narray([[0.56793849, 0.04063513],\n       [0.06793849, 0.37396846],\n       [0.81793849, 0.7073018 ],\n       [0.31793849, 0.15174624]])\n&gt;&gt;&gt; Halton(2,randomize=\"NUS\",seed=7)(4)\narray([[0.141964  , 0.99285569],\n       [0.65536579, 0.51938353],\n       [0.46955206, 0.11342811],\n       [0.78505432, 0.87032345]])\n&gt;&gt;&gt; Halton(2,randomize=\"QRNG\",seed=7)(4)\narray([[0.35362988, 0.38733489],\n       [0.85362988, 0.72066823],\n       [0.10362988, 0.05400156],\n       [0.60362988, 0.498446  ]])\n</code></pre> <p>Replications of randomizations </p> <pre><code>&gt;&gt;&gt; Halton(3,randomize=\"LMS DP\",seed=7,replications=2)(4)\narray([[[0.70988236, 0.18180876, 0.54073621],\n        [0.38178158, 0.61168824, 0.64684354],\n        [0.98597752, 0.70650871, 0.31479029],\n        [0.15795399, 0.28162992, 0.98945647]],\n\n       [[0.620398  , 0.57025403, 0.46336542],\n        [0.44021889, 0.69926312, 0.60133428],\n        [0.89132308, 0.12030255, 0.35715804],\n        [0.04025218, 0.44304244, 0.10724799]]])\n&gt;&gt;&gt; Halton(3,randomize=\"LMS DS\",seed=7,replications=2)(4)\narray([[[4.57465163e-01, 5.75419751e-04, 7.47353067e-01],\n        [6.29314800e-01, 9.24349881e-01, 8.47915779e-01],\n        [2.37544271e-01, 4.63986168e-01, 1.78817056e-01],\n        [9.09318567e-01, 2.48566227e-01, 3.17475640e-01]],\n\n       [[6.04003127e-01, 9.92849835e-01, 4.21625151e-01],\n        [4.57027115e-01, 1.97310094e-01, 2.43670150e-01],\n        [8.76467351e-01, 4.22339232e-01, 1.05777101e-01],\n        [5.46933622e-02, 7.79075280e-01, 9.29409300e-01]]])\n&gt;&gt;&gt; Halton(3,randomize=\"LMS\",seed=7,replications=2)(4,warn=False)\narray([[[0.        , 0.        , 0.        ],\n        [0.82822666, 0.92392942, 0.34057871],\n        [0.28838899, 0.46493682, 0.47954399],\n        [0.6165384 , 0.2493814 , 0.77045601]],\n\n       [[0.        , 0.        , 0.        ],\n        [0.93115665, 0.57483093, 0.87170952],\n        [0.48046642, 0.8122114 , 0.69381851],\n        [0.58055977, 0.28006957, 0.55586147]]])\n&gt;&gt;&gt; Halton(3,randomize=\"DS\",seed=7,replications=2)(4)\narray([[[0.56793849, 0.04063513, 0.74276256],\n        [0.06793849, 0.37396846, 0.94276256],\n        [0.81793849, 0.7073018 , 0.14276256],\n        [0.31793849, 0.15174624, 0.34276256]],\n\n       [[0.98309816, 0.80260469, 0.17299622],\n        [0.48309816, 0.13593802, 0.37299622],\n        [0.73309816, 0.46927136, 0.57299622],\n        [0.23309816, 0.9137158 , 0.77299622]]])\n&gt;&gt;&gt; Halton(3,randomize=\"DP\",seed=7,replications=2)(4)\narray([[[0.11593484, 0.99232505, 0.6010751 ],\n        [0.61593484, 0.65899172, 0.0010751 ],\n        [0.36593484, 0.32565839, 0.4010751 ],\n        [0.86593484, 0.77010283, 0.8010751 ]],\n\n       [[0.26543198, 0.12273092, 0.20202896],\n        [0.76543198, 0.45606426, 0.60202896],\n        [0.01543198, 0.78939759, 0.40202896],\n        [0.51543198, 0.23384203, 0.00202896]]])\n&gt;&gt;&gt; Halton(3,randomize=\"NUS\",seed=7,replications=2)(4)\narray([[[0.141964  , 0.99285569, 0.77722918],\n        [0.65536579, 0.51938353, 0.22797442],\n        [0.46955206, 0.11342811, 0.9975298 ],\n        [0.78505432, 0.87032345, 0.57696123]],\n\n       [[0.04813634, 0.16158904, 0.56038465],\n        [0.89364888, 0.33578478, 0.36145822],\n        [0.34111023, 0.84596814, 0.0292313 ],\n        [0.71866903, 0.23852281, 0.80431142]]])\n</code></pre> <p>References:</p> <ol> <li> <p>Marius Hofert and Christiane Lemieux.     qrng: (Randomized) Quasi-Random Number Generators.     R package version 0.0-7. (2019). https://CRAN.R-project.org/package=qrng.</p> </li> <li> <p>A. B. Owen.     A randomized Halton algorithm in R. arXiv:1706.02808 [stat.CO]. 2017. </p> </li> <li> <p>A. B. Owen and Z. Pan.     Gain coefficients for scrambled Halton points. arXiv:2308.08035 [stat.CO]. 2023.</p> </li> </ol> Source code in <code>qmcpy/discrete_distribution/digital_net_any_bases/digital_net_any_bases.py</code> <pre><code>def __init__(self,\n             dimension = 1,\n             replications = None,\n             seed = None, \n             randomize = 'LMS DP',\n             bases_generating_matrices = None,\n             t = None,\n             alpha = 1,\n             n_lim = 2**32,\n             warn = True):\n    r\"\"\"\n    Args:\n        dimension (Union[int,np.ndarray]): Dimension of the generator.\n\n            - If an `int` is passed in, use generating vector components at indices 0,...,`dimension`-1.\n            - If an `np.ndarray` is passed in, use generating vector components at these indices.\n\n        replications (int): Number of independent randomizations of a pointset.\n        seed (Union[None,int,np.random.SeedSeq): Seed the random number generator for reproducibility.\n        randomize (str): Options are\n\n            - `'LMS DP'`: Linear matrix scramble with digital permutation.\n            - `'LMS DS'`: Linear matrix scramble with digital shift.\n            - `'LMS'`: Linear matrix scramble only.\n            - `'DP'`: Digital permutation scramble only.\n            - `'DS'`: Digital shift only.\n            - `'NUS'`: Nested uniform scrambling.\n            - `'QRNG'`: Deterministic permutation scramble and random digital shift from QRNG [1] (with `generalize=True`). Does *not* support replications&gt;1.\n            - `None`: No randomization. In this case the first point will be the origin. \n        bases_generating_matrices (Union[str, tuple]: Specify the bases and the generating matrices.\n\n            - `\"HALTON\"` will use Halton generating matrices.\n            - `\"FAURE\"` will use Faure generating matrices .\n            - `bases,generating_matrices` requires \n\n                - `bases` is an `np.ndarray` of integers with shape $(,d)$ or $(r,d)$ where $d$ is the number of dimensions and $r$ is the number of replications.\n                - `generating_matrices` is an `np.ndarray` of integers with shape $(d,m_\\mathrm{max},t_\\mathrm{max})$ or $(r,d,m_\\mathrm{max},t_\\mathrm{max})$ where $d$ is the number of dimensions, $r$ is the number of replications, and $2^{m_\\mathrm{max}}$ is the maximum number of supported points.\n\n        t (int): Number of digits *after* randomization. The number of digits in the generating matrices is inferred.\n        alpha (int): Interlacing factor for higher order nets.  \n            When `alpha`&gt;1, interlacing is performed regardless of the generating matrices,  \n            i.e., for `alpha`&gt;1 do *not* pass in generating matrices which are already interlaced.  \n            The Note for this class contains more info.  \n        n_lim (int): Maximum number of compatible points, determines the number of rows in the generating matrices. \n        warn (bool): If `False`, suppress warnings in construction \n    \"\"\"\n    self.parameters = ['randomize','t','n_limit']\n    self.all_primes = np.array([2, 3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, 47, 53, 59, 61, 67, 71, 73, 79, 83, 89, 97, 101, 103, 107, 109, 113, 127, 131, 137, 139, 149, 151, 157, 163, 167, 173, 179, 181, 191, 193, 197, 199, 211, 223, 227, 229, 233, 239, 241, 251, 257, 263, 269, 271, 277, 281, 283, 293, 307, 311, 313, 317, 331, 337, 347, 349, 353, 359, 367, 373, 379, 383, 389, 397, 401, 409, 419, 421, 431, 433, 439, 443, 449, 457, 461, 463, 467, 479, 487, 491, 499, 503, 509, 521, 523, 541, 547, 557, 563, 569, 571, 577, 587, 593, 599, 601, 607, 613, 617, 619, 631, 641, 643, 647, 653, 659, 661, 673, 677, 683, 691, 701, 709, 719, 727, 733, 739, 743, 751, 757, 761, 769, 773, 787, 797, 809, 811, 821, 823, 827, 829, 839, 853, 857, 859, 863, 877, 881, 883, 887, 907, 911, 919, 929, 937, 941, 947, 953, 967, 971, 977, 983, 991, 997, 1009, 1013, 1019, 1021, 1031, 1033, 1039, 1049, 1051, 1061, 1063, 1069, 1087, 1091, 1093, 1097, 1103, 1109, 1117, 1123, 1129, 1151, 1153, 1163, 1171, 1181, 1187, 1193, 1201, 1213, 1217, 1223, 1229, 1231, 1237, 1249, 1259, 1277, 1279, 1283, 1289, 1291, 1297, 1301, 1303, 1307, 1319, 1321, 1327, 1361, 1367, 1373, 1381, 1399, 1409, 1423, 1427, 1429, 1433, 1439, 1447, 1451, 1453, 1459, 1471, 1481, 1483, 1487, 1489, 1493, 1499, 1511, 1523, 1531, 1543, 1549, 1553, 1559, 1567, 1571, 1579, 1583, 1597, 1601, 1607, 1609, 1613, 1619, 1621, 1627, 1637, 1657, 1663, 1667, 1669, 1693, 1697, 1699, 1709, 1721, 1723, 1733, 1741, 1747, 1753, 1759, 1777, 1783, 1787, 1789, 1801, 1811, 1823, 1831, 1847, 1861, 1867, 1871, 1873, 1877, 1879, 1889, 1901, 1907, 1913, 1931, 1933, 1949, 1951, 1973, 1979, 1987, 1993, 1997, 1999, 2003, 2011, 2017, 2027, 2029, 2039, 2053, 2063, 2069, 2081, 2083, 2087, 2089, 2099, 2111, 2113, 2129, 2131, 2137, 2141, 2143, 2153, 2161, 2179, 2203, 2207, 2213, 2221, 2237, 2239, 2243, 2251, 2267, 2269, 2273, 2281, 2287, 2293, 2297, 2309, 2311, 2333, 2339, 2341, 2347, 2351, 2357, 2371, 2377, 2381, 2383, 2389, 2393, 2399, 2411, 2417, 2423, 2437, 2441, 2447, 2459, 2467, 2473, 2477, 2503, 2521, 2531, 2539, 2543, 2549, 2551, 2557, 2579, 2591, 2593, 2609, 2617, 2621, 2633, 2647, 2657, 2659, 2663, 2671, 2677, 2683, 2687, 2689, 2693, 2699, 2707, 2711, 2713, 2719, 2729, 2731, 2741, 2749, 2753, 2767, 2777, 2789, 2791, 2797, 2801, 2803, 2819, 2833, 2837, 2843, 2851, 2857, 2861, 2879, 2887, 2897, 2903, 2909, 2917, 2927, 2939, 2953, 2957, 2963, 2969, 2971, 2999, 3001, 3011, 3019, 3023, 3037, 3041, 3049, 3061, 3067, 3079, 3083, 3089, 3109, 3119, 3121, 3137, 3163, 3167, 3169, 3181, 3187, 3191, 3203, 3209, 3217, 3221, 3229, 3251, 3253, 3257, 3259, 3271, 3299, 3301, 3307, 3313, 3319, 3323, 3329, 3331, 3343, 3347, 3359, 3361, 3371, 3373, 3389, 3391, 3407, 3413, 3433, 3449, 3457, 3461, 3463, 3467, 3469, 3491, 3499, 3511, 3517, 3527, 3529, 3533, 3539, 3541, 3547, 3557, 3559, 3571, 3581, 3583, 3593, 3607, 3613, 3617, 3623, 3631, 3637, 3643, 3659, 3671, 3673, 3677, 3691, 3697, 3701, 3709, 3719, 3727, 3733, 3739, 3761, 3767, 3769, 3779, 3793, 3797, 3803, 3821, 3823, 3833, 3847, 3851, 3853, 3863, 3877, 3881, 3889, 3907, 3911, 3917, 3919, 3923, 3929, 3931, 3943, 3947, 3967, 3989, 4001, 4003, 4007, 4013, 4019, 4021, 4027, 4049, 4051, 4057, 4073, 4079, 4091, 4093, 4099, 4111, 4127, 4129, 4133, 4139, 4153, 4157, 4159, 4177, 4201, 4211, 4217, 4219, 4229, 4231, 4241, 4243, 4253, 4259, 4261, 4271, 4273, 4283, 4289, 4297, 4327, 4337, 4339, 4349, 4357, 4363, 4373, 4391, 4397, 4409, 4421, 4423, 4441, 4447, 4451, 4457, 4463, 4481, 4483, 4493, 4507, 4513, 4517, 4519, 4523, 4547, 4549, 4561, 4567, 4583, 4591, 4597, 4603, 4621, 4637, 4639, 4643, 4649, 4651, 4657, 4663, 4673, 4679, 4691, 4703, 4721, 4723, 4729, 4733, 4751, 4759, 4783, 4787, 4789, 4793, 4799, 4801, 4813, 4817, 4831, 4861, 4871, 4877, 4889, 4903, 4909, 4919, 4931, 4933, 4937, 4943, 4951, 4957, 4967, 4969, 4973, 4987, 4993, 4999, 5003, 5009, 5011, 5021, 5023, 5039, 5051, 5059, 5077, 5081, 5087, 5099, 5101, 5107, 5113, 5119, 5147, 5153, 5167, 5171, 5179, 5189, 5197, 5209, 5227, 5231, 5233, 5237, 5261, 5273, 5279, 5281, 5297, 5303, 5309, 5323, 5333, 5347, 5351, 5381, 5387, 5393, 5399, 5407, 5413, 5417, 5419, 5431, 5437, 5441, 5443, 5449, 5471, 5477, 5479, 5483, 5501, 5503, 5507, 5519, 5521, 5527, 5531, 5557, 5563, 5569, 5573, 5581, 5591, 5623, 5639, 5641, 5647, 5651, 5653, 5657, 5659, 5669, 5683, 5689, 5693, 5701, 5711, 5717, 5737, 5741, 5743, 5749, 5779, 5783, 5791, 5801, 5807, 5813, 5821, 5827, 5839, 5843, 5849, 5851, 5857, 5861, 5867, 5869, 5879, 5881, 5897, 5903, 5923, 5927, 5939, 5953, 5981, 5987, 6007, 6011, 6029, 6037, 6043, 6047, 6053, 6067, 6073, 6079, 6089, 6091, 6101, 6113, 6121, 6131, 6133, 6143, 6151, 6163, 6173, 6197, 6199, 6203, 6211, 6217, 6221, 6229, 6247, 6257, 6263, 6269, 6271, 6277, 6287, 6299, 6301, 6311, 6317, 6323, 6329, 6337, 6343, 6353, 6359, 6361, 6367, 6373, 6379, 6389, 6397, 6421, 6427, 6449, 6451, 6469, 6473, 6481, 6491, 6521, 6529, 6547, 6551, 6553, 6563, 6569, 6571, 6577, 6581, 6599, 6607, 6619, 6637, 6653, 6659, 6661, 6673, 6679, 6689, 6691, 6701, 6703, 6709, 6719, 6733, 6737, 6761, 6763, 6779, 6781, 6791, 6793, 6803, 6823, 6827, 6829, 6833, 6841, 6857, 6863, 6869, 6871, 6883, 6899, 6907, 6911, 6917, 6947, 6949, 6959, 6961, 6967, 6971, 6977, 6983, 6991, 6997, 7001, 7013, 7019, 7027, 7039, 7043, 7057, 7069, 7079, 7103, 7109, 7121, 7127, 7129, 7151, 7159, 7177, 7187, 7193, 7207, 7211, 7213, 7219, 7229, 7237, 7243, 7247, 7253, 7283, 7297, 7307, 7309, 7321, 7331, 7333, 7349, 7351, 7369, 7393, 7411, 7417, 7433, 7451, 7457, 7459, 7477, 7481, 7487, 7489, 7499, 7507, 7517, 7523, 7529, 7537, 7541, 7547, 7549, 7559, 7561, 7573, 7577, 7583, 7589, 7591, 7603, 7607, 7621, 7639, 7643, 7649, 7669, 7673, 7681, 7687, 7691, 7699, 7703, 7717, 7723, 7727, 7741, 7753, 7757, 7759, 7789, 7793, 7817, 7823, 7829, 7841, 7853, 7867, 7873, 7877, 7879, 7883, 7901, 7907, 7919],dtype=np.uint64)\n    if bases_generating_matrices is None:\n        if self.DEFAULT_GENERATING_MATRICES==\"HALTON\":\n            self.type_bases_generating_matrices = \"HALTON\"\n            d_limit = len(self.all_primes)\n        elif self.DEFAULT_GENERATING_MATRICES==\"FAURE\":\n            self.type_bases_generating_matrices = \"FAURE\"\n            d_limit = int(self.all_primes[-1])\n        else:\n            raise ParameterError(\"must supply bases_generating_matrices\")\n    else:\n        self.type_bases_generating_matrices = \"CUSTOM\"\n        assert len(bases_generating_matrices)==2\n        bases,generating_matrices = bases_generating_matrices\n        assert isinstance(generating_matrices,np.ndarray)\n        assert generating_matrices.ndim==3 or generating_matrices.ndim==4 \n        d_limit = generating_matrices.shape[1]\n        if np.isscalar(bases):\n            assert bases&gt;0\n            assert bases%1==0 \n            bases = int(bases)*np.ones(d_limit,dtype=int)\n        assert bases.ndim==1 or bases.ndim==2\n    self.input_t = deepcopy(t) \n    self.input_bases_generating_matrices = deepcopy(bases_generating_matrices)\n    super(DigitalNetAnyBases,self).__init__(dimension,replications,seed,d_limit,n_lim)\n    self.randomize = str(randomize).upper().strip().replace(\"_\",\" \")\n    if self.randomize==\"TRUE\": self.randomize = \"LMS DP\"\n    if self.randomize==\"LMS PERM\": self.randomize = \"LMS DP\"\n    if self.randomize==\"PERM\": self.randomize = \"DP\"\n    if self.randomize==\"OWEN\": self.randomize = \"NUS\"\n    if self.randomize==\"NONE\": self.randomize = \"FALSE\"\n    if self.randomize==\"NO\": self.randomize = \"FALSE\"\n    assert self.randomize in [\"LMS DP\",\"LMS DS\",\"LMS\",\"DP\",\"DS\",\"NUS\",\"QRNG\",\"FALSE\"]\n    if self.randomize==\"QRNG\":\n        assert self.type_bases_generating_matrices==\"HALTON\", \"QRNG randomization is only applicable for the Halton generator.\"\n        from .._c_lib import _load_c_lib\n        assert self.replications==1, \"QRNG requires replications=1\"\n        self.randu_d_32 = self.rng.uniform(size=(self.d,32))\n        _c_lib = _load_c_lib()\n        import ctypes\n        self.halton_cf_qrng = _c_lib.halton_qrng\n        self.halton_cf_qrng.argtypes = [\n            ctypes.c_int,  # n\n            ctypes.c_int,  # d\n            ctypes.c_int, # n0\n            ctypes.c_int, # generalized\n            np.ctypeslib.ndpointer(ctypes.c_double, flags='C_CONTIGUOUS'), # res\n            np.ctypeslib.ndpointer(ctypes.c_double, flags='C_CONTIGUOUS'), # randu_d_32\n            np.ctypeslib.ndpointer(ctypes.c_int, flags='C_CONTIGUOUS')]  # dvec\n        self.halton_cf_qrng.restype = None\n    self.alpha = alpha\n    assert self.alpha&gt;=1\n    assert self.alpha%1==0\n    if self.alpha&gt;1:\n        assert (self.dvec==np.arange(self.d)).all(), \"digital interlacing requires dimension is an int\"\n    self.dtalpha = self.alpha*self.d \n    if self.type_bases_generating_matrices==\"HALTON\":\n        self.bases = self.all_primes[self.dvec][None,:]\n        self.m_max = int(np.ceil(np.log(self.n_limit)/np.log(self.bases.min())))\n        self._t_curr = self.m_max\n        if t is None: t = int(np.ceil(-np.log(2**(-63))/np.log(self.bases.min())))\n        self.t = self.m_max if self.m_max&gt;t else t\n        self.C = qmctoolscl.gdn_get_halton_generating_matrix(np.uint64(1),np.uint64(self.d),np.uint64(self._t_curr))\n    elif self.type_bases_generating_matrices==\"FAURE\":\n        assert (self.dvec==np.arange(self.d)).all(), \"Faure requires dimension is an int\"\n        p = self.all_primes[np.argmax(self.all_primes&gt;=self.d)]\n        self.bases = p*np.ones((1,self.dtalpha),dtype=np.uint64)\n        self.m_max = int(np.ceil(np.log(self.n_limit)/np.log(p)))\n        if t is None: t = int(np.ceil(-np.log(2**(-63))/np.log(p)))\n        self._t_curr = self.m_max\n        self.t = self.m_max if self.m_max&gt;t else t\n        self.C = np.ones((self.dtalpha,1,1),dtype=np.uint64)*np.eye(self._t_curr,dtype=np.uint64)\n        if self.dtalpha&gt;1:\n            for a in range(self._t_curr):\n                for b in range(a+1):\n                    self.C[1,a,b] = comb(a,b)%p\n        if self.dtalpha&gt;2:\n            for k in range(2,self.dtalpha):\n                for a in range(self._t_curr):\n                    for b in range(a+1):\n                        self.C[k,a,b] = (int(self.C[1,a,b])*((k**(a-b))%p))%p\n        self.C = self.C[None,:,:,:]\n        self.faure_base = p\n    else:\n        self.bases = bases.astype(np.uint64)\n        if self.bases.ndim==1: self.bases = self.bases[None,:]\n        assert self.bases.shape[1]&gt;=self.dtalpha\n        if self.alpha==1:\n            self.bases = self.bases[:,self.dvec]\n        else:\n            self.bases = self.bases[:,:self.dtalpha]\n        self.C = generating_matrices.astype(np.uint64)\n        if self.C.ndim==3: self.C = self.C[None,:,:,:]\n        assert self.C.shape[1]&gt;=self.dtalpha\n        if self.alpha==1:\n            self.C = self.C[:,self.dvec,:,:]\n        else:\n            self.C = self.C[:,:self.dtalpha,:,:]\n        self.m_max,self._t_curr = self.C.shape[-2:]\n        if t is None: t = int(np.ceil(-np.log(2**(-63))/np.log(self.bases.min())))\n        self.t = self.m_max if self.m_max&gt;t else t\n        assert (0&lt;=self.C).all()\n        assert (self.C&lt;self.bases[:,:,None,None]).all()\n        self.C = np.ascontiguousarray(self.C) \n        self.bases = np.ascontiguousarray(self.bases)\n    if self.alpha&gt;1:\n        assert (self.bases==self.bases[0,0]).all(), \"alpha&gt;1 performs digital interlacing which requires the same base across dimensions and replications.\"\n        if warn and self.m_max!=self._t_curr:\n            warnings.warn(\"Digital interlacing is often performed on generating matrices with the number of columns (m_max = %d) equal to the number of rows (_t_curr = %d), but this is not the case. Ensure you are NOT setting alpha&gt;1 when generating matrices are already interlaced.\"%(self.m_max,self._t_curr),ParameterWarning)\n    assert self.bases.ndim==2\n    assert self.bases.shape[-1]==self.dtalpha\n    assert self.bases.shape[0]==1 or self.bases.shape[0]==self.replications\n    assert self.C.ndim==4\n    assert self.C.shape[-3:]==(self.dtalpha,self.m_max,self._t_curr)\n    assert self.C.shape[0]==1 or self.C.shape[0]==self.replications\n    r_b = self.bases.shape[0]\n    r_C = self.C.shape[0]\n    if self.randomize==\"FALSE\":\n        if self.alpha&gt;1:\n            t_alpha = min(self.t,self._t_curr*self.alpha)\n            C_ho = np.empty((self.replications,self.d,self.m_max,t_alpha),dtype=np.uint64)\n            qmctoolscl.gdn_interlace(np.uint64(self.replications),np.uint64(self.d),np.uint64(self.m_max),np.uint64(self.dtalpha),np.uint64(self._t_curr),np.uint64(t_alpha),np.uint64(self.alpha),self.C,C_ho)\n            self.C = C_ho\n            self._t_curr = t_alpha\n            self.t = t_alpha\n            self.bases = self.bases[:,:self.d]\n    elif self.randomize==\"DP\":\n        self.perms = qmctoolscl.gdn_get_digital_permutations(self.rng,np.uint64(self.replications),np.uint64(self.d),self.t,np.uint64(r_b),self.bases)\n    elif self.randomize==\"DS\":\n        self.rshift = qmctoolscl.gdn_get_digital_shifts(self.rng,np.uint64(self.replications),np.uint64(self.d),self.t,np.uint64(r_b),self.bases)\n    elif self.randomize in [\"LMS\",\"LMS DS\",\"LMS DP\"]:\n        if self.alpha==1:\n            S = qmctoolscl.gdn_get_linear_scramble_matrix(self.rng,np.uint64(self.replications),np.uint64(self.d),np.uint64(self._t_curr),np.uint64(self.t),np.uint64(r_b),self.bases)\n            C_lms = np.empty((self.replications,self.d,self.m_max,self.t),dtype=np.uint64)\n            qmctoolscl.gdn_linear_matrix_scramble(np.uint64(self.replications),np.uint64(self.d),np.uint64(self.m_max),np.uint64(r_C),np.uint64(r_b),np.uint64(self._t_curr),np.uint64(self.t),self.bases,S,self.C,C_lms,backend=\"c\")\n            self.C = C_lms\n            self._t_curr = self.t\n        else:\n            t_dig = np.ceil(max(self.t/self.alpha,self._t_curr))\n            S = qmctoolscl.gdn_get_linear_scramble_matrix(self.rng,np.uint64(self.replications),np.uint64(self.dtalpha),np.uint64(self._t_curr),np.uint64(t_dig),np.uint64(r_b),self.bases)\n            C_lms = np.empty((self.replications,self.dtalpha,self.m_max,self.t),dtype=np.uint64)\n            qmctoolscl.gdn_linear_matrix_scramble(np.uint64(self.replications),np.uint64(self.dtalpha),np.uint64(self.m_max),np.uint64(r_C),np.uint64(r_b),np.uint64(self._t_curr),np.uint64(t_dig),self.bases,S,self.C,C_lms,backend=\"c\")\n            C_lms_ho = np.empty((self.replications,self.d,self.m_max,self.t),dtype=np.uint64)\n            qmctoolscl.gdn_interlace(np.uint64(self.replications),np.uint64(self.d),np.uint64(self.m_max),np.uint64(self.dtalpha),np.uint64(t_dig),np.uint64(self.t),np.uint64(self.alpha),C_lms,C_lms_ho)\n            self.C = C_lms_ho\n            self._t_curr = self.t\n            self.t = self._t_curr\n            self.bases = self.bases[:,:self.d]\n        if self.randomize==\"LMS DP\":\n            self.perms = qmctoolscl.gdn_get_digital_permutations(self.rng,np.uint64(self.replications),np.uint64(self.d),self.t,np.uint64(r_b),self.bases)\n        elif self.randomize==\"LMS DS\":\n            self.rshift = qmctoolscl.gdn_get_digital_shifts(self.rng,np.uint64(self.replications),np.uint64(self.d),self.t,np.uint64(r_b),self.bases)\n    elif self.randomize==\"NUS\":\n        if self.alpha==1:\n            new_seeds = self._base_seed.spawn(self.replications*self.d)\n            self.rngs = np.array([np.random.Generator(np.random.SFC64(new_seeds[j])) for j in range(self.replications*self.d)]).reshape(self.replications,self.d)\n            self.root_nodes = np.array([qmctoolscl.NUSNode_gdn() for i in range(self.replications*self.d)]).reshape(self.replications,self.d)\n        else:\n            new_seeds = self._base_seed.spawn(self.replications*self.dtalpha)\n            self.rngs = np.array([np.random.Generator(np.random.SFC64(new_seeds[j])) for j in range(self.replications*self.dtalpha)]).reshape(self.replications,self.dtalpha)\n            self.root_nodes = np.array([qmctoolscl.NUSNode_gdn() for i in range(self.replications*self.dtalpha)]).reshape(self.replications,self.dtalpha)\n    assert self.C.ndim==4 and (self.C.shape[0]==1 or self.C.shape[0]==self.replications) and self.C.shape[1]==(self.dtalpha if self.randomize==\"NUS\" else self.d) and self.C.shape[2]==self.m_max and self.C.shape[3]==self._t_curr\n    assert self.bases.ndim==2 and (self.bases.shape[0]==1 or self.bases.shape[0]==self.replications) and self.bases.shape[1]==(self.dtalpha if self.randomize==\"NUS\" else self.d)\n    assert 0&lt;self._t_curr&lt;=self.t&lt;=64\n    if self.randomize==\"FALSE\": assert self.C.shape[0]==self.replications, \"randomize='FALSE' but replications = %d does not equal the number of sets of generating vectors %d\"%(self.replications,self.C.shape[0])\n    if warn and (self.bases==2).all():\n        warnings.warn(\"It is more efficient to use DigitalNetB2 instead of DigitalNetAnyBases when all bases are 2\")\n    self.warn = warn\n</code></pre>"},{"location":"api/discrete_distributions/#faure","title":"<code>Faure</code>","text":"<p>               Bases: <code>DigitalNetAnyBases</code></p> <p>Low discrepancy Faure points.</p> Note <ul> <li>The first point of an unrandomized Faure sequence is the origin.</li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; discrete_distrib = Faure(4,seed=7)\n&gt;&gt;&gt; discrete_distrib(25)\narray([[0.33534484, 0.26765076, 0.49135906, 0.6052543 ],\n       [0.45194823, 0.53502655, 0.93195248, 0.5024511 ],\n       [0.69367212, 0.09800441, 0.77275989, 0.35304609],\n       [0.0086832 , 0.96517227, 0.01157255, 0.85055011],\n       [0.97041531, 0.63254601, 0.25235643, 0.18869841],\n       [0.20862319, 0.45844039, 0.64733082, 0.97109116],\n       [0.57003274, 0.12560935, 0.08772865, 0.02771165],\n       [0.73495613, 0.91292101, 0.32692686, 0.68483526],\n       [0.05156003, 0.78796242, 0.56754229, 0.54342666],\n       [0.89329109, 0.21506683, 0.80647015, 0.27293526],\n       [0.25277941, 0.07296122, 0.23030907, 0.46332812],\n       [0.4941915 , 0.94839898, 0.46928813, 0.39348452],\n       [0.60952104, 0.69550381, 0.90983954, 0.81009581],\n       [0.17092444, 0.37887902, 0.74911456, 0.10721583],\n       [0.93584834, 0.40585698, 0.18944839, 0.72587572],\n       [0.36989645, 0.85447136, 0.97641039, 0.14826857],\n       [0.53450751, 0.7376398 , 0.61725827, 0.64572148],\n       [0.65143961, 0.28501343, 0.05601974, 0.58393268],\n       [0.09284913, 0.59218182, 0.29689116, 0.23247628],\n       [0.80817253, 0.02749358, 0.53742056, 0.88960099],\n       [0.29406032, 0.64634796, 0.15459067, 0.31205785],\n       [0.40906423, 0.35338995, 0.39383649, 0.93066193],\n       [0.77079531, 0.50849426, 0.43438996, 0.06810554],\n       [0.13540741, 0.17593145, 0.87500744, 0.76631324],\n       [0.85233694, 0.81903638, 0.71417647, 0.42286144]])\n&gt;&gt;&gt; discrete_distrib\nFaure (AbstractLDDiscreteDistribution)\n    d               2^(2)\n    replications    1\n    randomize       LMS DP\n    t               28\n    n_limit         2^(32)\n    entropy         7\n</code></pre> <p>Replications of independent randomizations </p> <pre><code>&gt;&gt;&gt; x = Faure(3,seed=7,replications=2)(9)\n&gt;&gt;&gt; x.shape\n(2, 9, 3)\n&gt;&gt;&gt; x\narray([[[0.46995809, 0.81347921, 0.84921511],\n        [0.22573669, 0.29702269, 0.03044283],\n        [0.68125682, 0.38949622, 0.65611464],\n        [0.58494536, 0.14367836, 0.43908813],\n        [0.00739063, 0.63809438, 0.95363225],\n        [0.79609189, 0.71822727, 0.1472069 ],\n        [0.36690976, 0.47240941, 0.2470566 ],\n        [0.12284101, 0.96819212, 0.55174327],\n        [0.91154206, 0.05940035, 0.73295342]],\n\n       [[0.44633162, 0.60684282, 0.96393795],\n        [0.13904548, 0.73709456, 0.37711571],\n        [0.79070871, 0.15560342, 0.12328662],\n        [0.40389155, 0.97863405, 0.0114599 ],\n        [0.05951757, 0.10889081, 0.85272281],\n        [0.71514612, 0.52312704, 0.59877454],\n        [0.64931396, 0.23505158, 0.48710589],\n        [0.30097968, 0.36957094, 0.23358374],\n        [0.99369356, 0.78380717, 0.74090153]]])\n</code></pre> <p>Unrandomized Faure </p> <pre><code>&gt;&gt;&gt; Faure(4,randomize=\"FALSE\",seed=7)(25,warn=False)\narray([[0.  , 0.  , 0.  , 0.  ],\n       [0.2 , 0.2 , 0.2 , 0.2 ],\n       [0.4 , 0.4 , 0.4 , 0.4 ],\n       [0.6 , 0.6 , 0.6 , 0.6 ],\n       [0.8 , 0.8 , 0.8 , 0.8 ],\n       [0.04, 0.24, 0.44, 0.64],\n       [0.24, 0.44, 0.64, 0.84],\n       [0.44, 0.64, 0.84, 0.04],\n       [0.64, 0.84, 0.04, 0.24],\n       [0.84, 0.04, 0.24, 0.44],\n       [0.08, 0.48, 0.88, 0.28],\n       [0.28, 0.68, 0.08, 0.48],\n       [0.48, 0.88, 0.28, 0.68],\n       [0.68, 0.08, 0.48, 0.88],\n       [0.88, 0.28, 0.68, 0.08],\n       [0.12, 0.72, 0.32, 0.92],\n       [0.32, 0.92, 0.52, 0.12],\n       [0.52, 0.12, 0.72, 0.32],\n       [0.72, 0.32, 0.92, 0.52],\n       [0.92, 0.52, 0.12, 0.72],\n       [0.16, 0.96, 0.76, 0.56],\n       [0.36, 0.16, 0.96, 0.76],\n       [0.56, 0.36, 0.16, 0.96],\n       [0.76, 0.56, 0.36, 0.16],\n       [0.96, 0.76, 0.56, 0.36]])\n</code></pre> <p>All randomizations </p> <pre><code>&gt;&gt;&gt; Faure(3,randomize=\"LMS DP\",seed=7)(9)\narray([[0.60869072, 0.76096155, 0.79807281],\n       [0.03946861, 0.56443723, 0.39462162],\n       [0.84046405, 0.17459934, 0.31263449],\n       [0.39080182, 0.42253604, 0.09713822],\n       [0.15491328, 0.22747922, 0.90356545],\n       [0.95606114, 0.84998662, 0.50057213],\n       [0.50599254, 0.09792123, 0.60600892],\n       [0.26995157, 0.88915344, 0.20257439],\n       [0.73776586, 0.51292533, 0.68846996]])\n&gt;&gt;&gt; Faure(3,randomize=\"LMS DS\",seed=7)(9)\narray([[0.09363194, 0.63822128, 0.52606134],\n       [0.66021198, 0.02837507, 0.7155204 ],\n       [0.85642224, 0.83340554, 0.22549083],\n       [0.31989976, 0.30119144, 0.00893035],\n       [0.55299403, 0.70247804, 0.63048602],\n       [0.7492045 , 0.49633053, 0.81948782],\n       [0.20034171, 0.96431985, 0.92797189],\n       [0.43358859, 0.3653974 , 0.11741381],\n       [0.9629798 , 0.17028087, 0.41752644]])\n&gt;&gt;&gt; Faure(3,randomize=\"LMS\",seed=7)(9,warn=False)\narray([[0.        , 0.        , 0.        ],\n       [0.5706961 , 0.84756013, 0.63390413],\n       [0.80394278, 0.65243798, 0.82336297],\n       [0.22678173, 0.67547662, 0.93144564],\n       [0.46399184, 0.50916041, 0.12088811],\n       [0.69723852, 0.31536484, 0.40911072],\n       [0.11956371, 0.33845241, 0.52953528],\n       [0.35692624, 0.18443172, 0.70664719],\n       [0.92335406, 0.97711587, 0.22942236]])\n&gt;&gt;&gt; Faure(3,randomize=\"DP\",seed=7)(9)\narray([[0.62331102, 0.65595286, 0.5208953 ],\n       [0.28997769, 0.98928619, 0.18756197],\n       [0.95664436, 0.32261952, 0.85422863],\n       [0.51219991, 0.87817508, 0.96533975],\n       [0.17886658, 0.21150841, 0.63200641],\n       [0.84553325, 0.54484175, 0.29867308],\n       [0.4010888 , 0.1003973 , 0.07645086],\n       [0.06775547, 0.43373064, 0.74311752],\n       [0.73442213, 0.76706397, 0.40978419]])\n&gt;&gt;&gt; Faure(3,randomize=\"DS\",seed=7)(9)\narray([[0.68058188, 0.90130449, 0.36582045],\n       [0.01391521, 0.23463782, 0.69915378],\n       [0.34724854, 0.56797116, 0.03248711],\n       [0.79169299, 0.0124156 , 0.14359822],\n       [0.12502632, 0.34574893, 0.47693156],\n       [0.45835966, 0.67908227, 0.81026489],\n       [0.9028041 , 0.45686005, 0.921376  ],\n       [0.23613743, 0.79019338, 0.25470933],\n       [0.56947077, 0.12352671, 0.58804267]])\n&gt;&gt;&gt; Faure(3,randomize=\"NUS\",seed=7)(9)\narray([[0.077534  , 0.99285569, 0.64186774],\n       [0.82225806, 0.4963679 , 0.86453228],\n       [0.48498632, 0.0833722 , 0.06011164],\n       [0.15691802, 0.60236449, 0.11226224],\n       [0.95445636, 0.2796012 , 0.54752025],\n       [0.61890325, 0.84183901, 0.70654863],\n       [0.25089638, 0.17805972, 0.95988146],\n       [0.68344029, 0.77065782, 0.26676153],\n       [0.4322891 , 0.40799837, 0.34911626]])\n</code></pre> <p>Replications of randomizations </p> <pre><code>&gt;&gt;&gt; Faure(3,randomize=\"LMS DP\",seed=7,replications=2)(9)\narray([[[0.46995809, 0.81347921, 0.84921511],\n        [0.22573669, 0.29702269, 0.03044283],\n        [0.68125682, 0.38949622, 0.65611464],\n        [0.58494536, 0.14367836, 0.43908813],\n        [0.00739063, 0.63809438, 0.95363225],\n        [0.79609189, 0.71822727, 0.1472069 ],\n        [0.36690976, 0.47240941, 0.2470566 ],\n        [0.12284101, 0.96819212, 0.55174327],\n        [0.91154206, 0.05940035, 0.73295342]],\n\n       [[0.44633162, 0.60684282, 0.96393795],\n        [0.13904548, 0.73709456, 0.37711571],\n        [0.79070871, 0.15560342, 0.12328662],\n        [0.40389155, 0.97863405, 0.0114599 ],\n        [0.05951757, 0.10889081, 0.85272281],\n        [0.71514612, 0.52312704, 0.59877454],\n        [0.64931396, 0.23505158, 0.48710589],\n        [0.30097968, 0.36957094, 0.23358374],\n        [0.99369356, 0.78380717, 0.74090153]]])\n&gt;&gt;&gt; Faure(3,randomize=\"LMS DS\",seed=7,replications=2)(9)\narray([[[1.90132648e-01, 7.86261604e-01, 1.43261685e-01],\n        [4.27494577e-01, 6.21249393e-01, 4.43831847e-01],\n        [9.93922386e-01, 9.24890107e-02, 9.54261973e-01],\n        [8.29146531e-02, 4.49180940e-01, 7.37185290e-01],\n        [6.53610094e-01, 2.95352182e-01, 2.47631786e-01],\n        [8.86856776e-01, 7.55465004e-01, 5.48200701e-01],\n        [2.96893357e-01, 1.12311024e-01, 6.56285230e-01],\n        [5.34102847e-01, 9.58324412e-01, 8.45725185e-01],\n        [7.67349545e-01, 4.29366454e-01, 3.51846617e-02]],\n\n       [[9.98454735e-01, 2.53648344e-01, 7.13434357e-01],\n        [3.05609133e-01, 4.45718366e-01, 6.36691290e-01],\n        [6.54114646e-01, 6.91349078e-01, 1.11471149e-01],\n        [7.11691963e-01, 6.25495629e-01, 2.23136805e-01],\n        [5.60878749e-02, 8.17408217e-01, 8.25559119e-01],\n        [4.00275568e-01, 5.89230529e-02, 4.15633066e-01],\n        [7.91236327e-01, 9.93070231e-01, 5.25932625e-01],\n        [1.39697286e-01, 1.89249857e-01, 7.14552095e-04],\n        [4.46998476e-01, 4.30617921e-01, 9.36315716e-01]]])\n&gt;&gt;&gt; Faure(3,randomize=\"LMS\",seed=7,replications=2)(9,warn=False)\narray([[[0.        , 0.        , 0.        ],\n        [0.5706961 , 0.84756013, 0.63390413],\n        [0.80394278, 0.65243798, 0.82336297],\n        [0.22678173, 0.67547662, 0.93144564],\n        [0.46399184, 0.50916041, 0.12088811],\n        [0.69723852, 0.31536484, 0.40911072],\n        [0.11956371, 0.33845241, 0.52953528],\n        [0.35692624, 0.18443172, 0.70664719],\n        [0.92335406, 0.97711587, 0.22942236]],\n\n       [[0.        , 0.        , 0.        ],\n        [0.34851179, 0.57896969, 0.93575568],\n        [0.69285188, 0.80808804, 0.52582983],\n        [0.20726949, 0.37184791, 0.6375009 ],\n        [0.55146283, 0.94654367, 0.11228285],\n        [0.8628786 , 0.17977726, 0.71455089],\n        [0.2908789 , 0.74353714, 0.82500264],\n        [0.59818511, 0.31838531, 0.41492625],\n        [0.94242359, 0.54735751, 0.22303965]]])\n&gt;&gt;&gt; Faure(3,randomize=\"DS\",seed=7,replications=2)(9)\narray([[[0.68058188, 0.90130449, 0.36582045],\n        [0.01391521, 0.23463782, 0.69915378],\n        [0.34724854, 0.56797116, 0.03248711],\n        [0.79169299, 0.0124156 , 0.14359822],\n        [0.12502632, 0.34574893, 0.47693156],\n        [0.45835966, 0.67908227, 0.81026489],\n        [0.9028041 , 0.45686005, 0.921376  ],\n        [0.23613743, 0.79019338, 0.25470933],\n        [0.56947077, 0.12352671, 0.58804267]],\n\n       [[0.95466555, 0.56062266, 0.29687589],\n        [0.28799888, 0.89395599, 0.63020922],\n        [0.62133222, 0.22728933, 0.96354256],\n        [0.73244333, 0.67173377, 0.74132033],\n        [0.06577666, 0.0050671 , 0.07465367],\n        [0.39911   , 0.33840044, 0.407987  ],\n        [0.84355444, 0.11617822, 0.51909811],\n        [0.17688777, 0.44951155, 0.85243145],\n        [0.51022111, 0.78284488, 0.18576478]]])\n&gt;&gt;&gt; Faure(3,randomize=\"DP\",seed=7,replications=2)(9)\narray([[[0.62331102, 0.65595286, 0.5208953 ],\n        [0.28997769, 0.98928619, 0.18756197],\n        [0.95664436, 0.32261952, 0.85422863],\n        [0.51219991, 0.87817508, 0.96533975],\n        [0.17886658, 0.21150841, 0.63200641],\n        [0.84553325, 0.54484175, 0.29867308],\n        [0.4010888 , 0.1003973 , 0.07645086],\n        [0.06775547, 0.43373064, 0.74311752],\n        [0.73442213, 0.76706397, 0.40978419]],\n\n       [[0.65992302, 0.22190291, 0.57359374],\n        [0.32658969, 0.88856957, 0.90692707],\n        [0.99325635, 0.55523624, 0.2402604 ],\n        [0.54881191, 0.77745846, 0.12914929],\n        [0.21547857, 0.44412513, 0.46248263],\n        [0.88214524, 0.1107918 , 0.79581596],\n        [0.4377008 , 0.66634735, 0.68470485],\n        [0.10436746, 0.33301402, 0.01803818],\n        [0.77103413, 0.99968068, 0.35137152]]])\n&gt;&gt;&gt; Faure(3,randomize=\"NUS\",seed=7,replications=2)(9)\narray([[[0.077534  , 0.99285569, 0.64186774],\n        [0.82225806, 0.4963679 , 0.86453228],\n        [0.48498632, 0.0833722 , 0.06011164],\n        [0.15691802, 0.60236449, 0.11226224],\n        [0.95445636, 0.2796012 , 0.54752025],\n        [0.61890325, 0.84183901, 0.70654863],\n        [0.25089638, 0.17805972, 0.95988146],\n        [0.68344029, 0.77065782, 0.26676153],\n        [0.4322891 , 0.40799837, 0.34911626]],\n\n       [[0.03739061, 0.16158904, 0.85379075],\n        [0.7635801 , 0.40075819, 0.56644152],\n        [0.49937232, 0.74097216, 0.02663559],\n        [0.24837929, 0.59622709, 0.12164937],\n        [0.79067346, 0.83319946, 0.67040549],\n        [0.40808716, 0.31605175, 0.51589378],\n        [0.15774296, 0.96060354, 0.35917763],\n        [0.96867006, 0.05032588, 0.24759375],\n        [0.59326363, 0.50120469, 0.9906825 ]]])\n</code></pre> <p>Higher order Faure</p> <pre><code>&gt;&gt;&gt; Faure(3,randomize=\"LMS DP\",seed=7,alpha=2)(9)\narray([[0.07060326, 0.24965078, 0.49971375],\n       [0.9104272 , 0.77359118, 0.02813304],\n       [0.52709818, 0.51562414, 0.97220901],\n       [0.23236901, 0.55967785, 0.28909699],\n       [0.88746566, 0.08382775, 0.77956468],\n       [0.39256201, 0.81762837, 0.43144077],\n       [0.20940728, 0.91289379, 0.71118926],\n       [0.71589223, 0.4203758 , 0.56884549],\n       [0.59136462, 0.16673034, 0.219807  ]])\n&gt;&gt;&gt; Faure(3,randomize=\"LMS DS\",seed=7,alpha=2)(9)\narray([[0.27004662, 0.64652898, 0.04873353],\n       [0.35569696, 0.06066804, 0.54773651],\n       [0.88670561, 0.79188841, 0.90363807],\n       [0.03415605, 0.99796556, 0.62789873],\n       [0.5267559 , 0.39579989, 0.76064852],\n       [0.94711696, 0.14327166, 0.11150335],\n       [0.20948284, 0.29996665, 0.83571342],\n       [0.62847216, 0.71019873, 0.30272608],\n       [0.67845718, 0.45371208, 0.36140179]])\n&gt;&gt;&gt; Faure(3,randomize=\"LMS\",seed=7,alpha=2)(9,warn=False)\narray([[0.        , 0.        , 0.        ],\n       [0.53148918, 0.85880934, 0.51685452],\n       [0.95184459, 0.60232457, 0.98293576],\n       [0.21282738, 0.35205188, 0.58389855],\n       [0.59616274, 0.20653524, 0.72441959],\n       [0.68272695, 0.94187019, 0.19189138],\n       [0.2733855 , 0.7034865 , 0.79264466],\n       [0.35996672, 0.5457835 , 0.27107157],\n       [0.85440183, 0.28913878, 0.43628397]])\n&gt;&gt;&gt; Faure(3,randomize=\"NUS\",seed=7,alpha=2)(9)\narray([[0.25822173, 0.41712395, 0.27892432],\n       [0.88275133, 0.97811616, 0.46301445],\n       [0.3846734 , 0.21568889, 0.71276888],\n       [0.16342299, 0.03041244, 0.40224624],\n       [0.71060873, 0.62350799, 0.97150042],\n       [0.57999438, 0.78144529, 0.21321557],\n       [0.10439042, 0.71635743, 0.78194125],\n       [0.89714231, 0.26123842, 0.01970874],\n       [0.52510616, 0.46613858, 0.65895473]])\n&gt;&gt;&gt; Faure(3,randomize=\"False\",seed=7,alpha=2)(9,warn=False)\narray([[0.        , 0.        , 0.        ],\n       [0.44444444, 0.44444444, 0.44444444],\n       [0.88888889, 0.88888889, 0.88888889],\n       [0.16049383, 0.71604938, 0.60493827],\n       [0.60493827, 0.16049383, 0.71604938],\n       [0.71604938, 0.60493827, 0.16049383],\n       [0.32098765, 0.43209877, 0.87654321],\n       [0.43209877, 0.87654321, 0.32098765],\n       [0.87654321, 0.32098765, 0.43209877]])\n</code></pre> <p>Replications of higher order Faure</p> <pre><code>&gt;&gt;&gt; Faure(3,randomize=\"LMS DP\",seed=7,alpha=2,replications=2)(9)\narray([[[0.65006542, 0.84004771, 0.39377772],\n        [0.73541117, 0.25289783, 0.11639162],\n        [0.11843564, 0.44363425, 0.98988211],\n        [0.37561943, 0.50110654, 0.26965135],\n        [0.79429224, 0.90984833, 0.69823833],\n        [0.32592209, 0.08813056, 0.53196334],\n        [0.47565296, 0.15886247, 0.84891661],\n        [0.96885711, 0.55947605, 0.57425894],\n        [0.05559154, 0.74599626, 0.07691998]],\n\n       [[0.01647589, 0.79799093, 0.98866597],\n        [0.79175933, 0.39051971, 0.1440758 ],\n        [0.57923112, 0.31651741, 0.39977363],\n        [0.19716729, 0.14023077, 0.0888261 ],\n        [0.9681839 , 0.73296284, 0.5775844 ],\n        [0.41014543, 0.6588137 , 0.83313225],\n        [0.28590138, 0.45066659, 0.52218222],\n        [0.74006249, 0.04320102, 0.67774517],\n        [0.51107317, 0.96909703, 0.26664273]]])\n&gt;&gt;&gt; Faure(3,randomize=\"LMS DS\",seed=7,alpha=2,replications=2)(9)\narray([[[0.82257501, 0.14639929, 0.60116121],\n        [0.24249012, 0.98867933, 0.77186064],\n        [0.43925164, 0.40287298, 0.12683077],\n        [0.91956054, 0.44754172, 0.85119388],\n        [0.08067361, 0.30216242, 0.31317864],\n        [0.49966237, 0.71234433, 0.33572993],\n        [0.75789644, 0.7949646 , 0.05999059],\n        [0.17827381, 0.65360268, 0.52607183],\n        [0.55976324, 0.05143265, 0.91398251]],\n\n       [[0.90790039, 0.30655016, 0.02228959],\n        [0.34969458, 0.71397805, 0.9779997 ],\n        [0.13261487, 0.45477877, 0.50021876],\n        [0.71256557, 0.66536938, 0.789166  ],\n        [0.48786265, 0.07285372, 0.41137525],\n        [0.26253474, 0.81344564, 0.2669434 ],\n        [0.87953962, 0.98487119, 0.58886448],\n        [0.6628982 , 0.39219753, 0.2112249 ],\n        [0.10438938, 0.1329926 , 0.73328965]]])\n&gt;&gt;&gt; Faure(3,randomize=\"LMS\",seed=7,alpha=2,replications=2)(9,warn=False)\narray([[[0.        , 0.        , 0.        ],\n        [0.53148918, 0.85880934, 0.51685452],\n        [0.95184459, 0.60232457, 0.98293576],\n        [0.21282738, 0.35205188, 0.58389855],\n        [0.59616274, 0.20653524, 0.72441959],\n        [0.68272695, 0.94187019, 0.19189138],\n        [0.2733855 , 0.7034865 , 0.79264466],\n        [0.35996672, 0.5457835 , 0.27107157],\n        [0.85440183, 0.28913878, 0.43628397]],\n\n       [[0.        , 0.        , 0.        ],\n        [0.78765092, 0.85192948, 0.96805852],\n        [0.56277838, 0.59274149, 0.49028114],\n        [0.18069211, 0.36070495, 0.80396678],\n        [0.95171191, 0.21257791, 0.43852003],\n        [0.40600476, 0.9532375 , 0.29409298],\n        [0.31884304, 0.68248734, 0.57074159],\n        [0.76064476, 0.53425869, 0.20544726],\n        [0.53167411, 0.27502561, 0.72751996]]])\n&gt;&gt;&gt; Faure(3,randomize=\"NUS\",seed=7,alpha=2,replications=2)(9)\narray([[[0.25822173, 0.41712395, 0.27892432],\n        [0.88275133, 0.97811616, 0.46301445],\n        [0.3846734 , 0.21568889, 0.71276888],\n        [0.16342299, 0.03041244, 0.40224624],\n        [0.71060873, 0.62350799, 0.97150042],\n        [0.57999438, 0.78144529, 0.21321557],\n        [0.10439042, 0.71635743, 0.78194125],\n        [0.89714231, 0.26123842, 0.01970874],\n        [0.52510616, 0.46613858, 0.65895473]],\n\n       [[0.1950677 , 0.52270739, 0.50736994],\n        [0.95117968, 0.09639097, 0.23235875],\n        [0.37944055, 0.98868015, 0.6693519 ],\n        [0.27700817, 0.83696832, 0.072855  ],\n        [0.6981059 , 0.3763676 , 0.8630192 ],\n        [0.46377485, 0.28179682, 0.64253665],\n        [0.01958311, 0.13843641, 0.9588172 ],\n        [0.87752263, 0.7029967 , 0.35134227],\n        [0.64185819, 0.55907117, 0.19929854]]])\n</code></pre> Source code in <code>qmcpy/discrete_distribution/digital_net_any_bases/digital_net_any_bases.py</code> <pre><code>def __init__(self,\n             dimension = 1,\n             replications = None,\n             seed = None, \n             randomize = 'LMS DP',\n             bases_generating_matrices = None,\n             t = None,\n             alpha = 1,\n             n_lim = 2**32,\n             warn = True):\n    r\"\"\"\n    Args:\n        dimension (Union[int,np.ndarray]): Dimension of the generator.\n\n            - If an `int` is passed in, use generating vector components at indices 0,...,`dimension`-1.\n            - If an `np.ndarray` is passed in, use generating vector components at these indices.\n\n        replications (int): Number of independent randomizations of a pointset.\n        seed (Union[None,int,np.random.SeedSeq): Seed the random number generator for reproducibility.\n        randomize (str): Options are\n\n            - `'LMS DP'`: Linear matrix scramble with digital permutation.\n            - `'LMS DS'`: Linear matrix scramble with digital shift.\n            - `'LMS'`: Linear matrix scramble only.\n            - `'DP'`: Digital permutation scramble only.\n            - `'DS'`: Digital shift only.\n            - `'NUS'`: Nested uniform scrambling.\n            - `'QRNG'`: Deterministic permutation scramble and random digital shift from QRNG [1] (with `generalize=True`). Does *not* support replications&gt;1.\n            - `None`: No randomization. In this case the first point will be the origin. \n        bases_generating_matrices (Union[str, tuple]: Specify the bases and the generating matrices.\n\n            - `\"HALTON\"` will use Halton generating matrices.\n            - `\"FAURE\"` will use Faure generating matrices .\n            - `bases,generating_matrices` requires \n\n                - `bases` is an `np.ndarray` of integers with shape $(,d)$ or $(r,d)$ where $d$ is the number of dimensions and $r$ is the number of replications.\n                - `generating_matrices` is an `np.ndarray` of integers with shape $(d,m_\\mathrm{max},t_\\mathrm{max})$ or $(r,d,m_\\mathrm{max},t_\\mathrm{max})$ where $d$ is the number of dimensions, $r$ is the number of replications, and $2^{m_\\mathrm{max}}$ is the maximum number of supported points.\n\n        t (int): Number of digits *after* randomization. The number of digits in the generating matrices is inferred.\n        alpha (int): Interlacing factor for higher order nets.  \n            When `alpha`&gt;1, interlacing is performed regardless of the generating matrices,  \n            i.e., for `alpha`&gt;1 do *not* pass in generating matrices which are already interlaced.  \n            The Note for this class contains more info.  \n        n_lim (int): Maximum number of compatible points, determines the number of rows in the generating matrices. \n        warn (bool): If `False`, suppress warnings in construction \n    \"\"\"\n    self.parameters = ['randomize','t','n_limit']\n    self.all_primes = np.array([2, 3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, 47, 53, 59, 61, 67, 71, 73, 79, 83, 89, 97, 101, 103, 107, 109, 113, 127, 131, 137, 139, 149, 151, 157, 163, 167, 173, 179, 181, 191, 193, 197, 199, 211, 223, 227, 229, 233, 239, 241, 251, 257, 263, 269, 271, 277, 281, 283, 293, 307, 311, 313, 317, 331, 337, 347, 349, 353, 359, 367, 373, 379, 383, 389, 397, 401, 409, 419, 421, 431, 433, 439, 443, 449, 457, 461, 463, 467, 479, 487, 491, 499, 503, 509, 521, 523, 541, 547, 557, 563, 569, 571, 577, 587, 593, 599, 601, 607, 613, 617, 619, 631, 641, 643, 647, 653, 659, 661, 673, 677, 683, 691, 701, 709, 719, 727, 733, 739, 743, 751, 757, 761, 769, 773, 787, 797, 809, 811, 821, 823, 827, 829, 839, 853, 857, 859, 863, 877, 881, 883, 887, 907, 911, 919, 929, 937, 941, 947, 953, 967, 971, 977, 983, 991, 997, 1009, 1013, 1019, 1021, 1031, 1033, 1039, 1049, 1051, 1061, 1063, 1069, 1087, 1091, 1093, 1097, 1103, 1109, 1117, 1123, 1129, 1151, 1153, 1163, 1171, 1181, 1187, 1193, 1201, 1213, 1217, 1223, 1229, 1231, 1237, 1249, 1259, 1277, 1279, 1283, 1289, 1291, 1297, 1301, 1303, 1307, 1319, 1321, 1327, 1361, 1367, 1373, 1381, 1399, 1409, 1423, 1427, 1429, 1433, 1439, 1447, 1451, 1453, 1459, 1471, 1481, 1483, 1487, 1489, 1493, 1499, 1511, 1523, 1531, 1543, 1549, 1553, 1559, 1567, 1571, 1579, 1583, 1597, 1601, 1607, 1609, 1613, 1619, 1621, 1627, 1637, 1657, 1663, 1667, 1669, 1693, 1697, 1699, 1709, 1721, 1723, 1733, 1741, 1747, 1753, 1759, 1777, 1783, 1787, 1789, 1801, 1811, 1823, 1831, 1847, 1861, 1867, 1871, 1873, 1877, 1879, 1889, 1901, 1907, 1913, 1931, 1933, 1949, 1951, 1973, 1979, 1987, 1993, 1997, 1999, 2003, 2011, 2017, 2027, 2029, 2039, 2053, 2063, 2069, 2081, 2083, 2087, 2089, 2099, 2111, 2113, 2129, 2131, 2137, 2141, 2143, 2153, 2161, 2179, 2203, 2207, 2213, 2221, 2237, 2239, 2243, 2251, 2267, 2269, 2273, 2281, 2287, 2293, 2297, 2309, 2311, 2333, 2339, 2341, 2347, 2351, 2357, 2371, 2377, 2381, 2383, 2389, 2393, 2399, 2411, 2417, 2423, 2437, 2441, 2447, 2459, 2467, 2473, 2477, 2503, 2521, 2531, 2539, 2543, 2549, 2551, 2557, 2579, 2591, 2593, 2609, 2617, 2621, 2633, 2647, 2657, 2659, 2663, 2671, 2677, 2683, 2687, 2689, 2693, 2699, 2707, 2711, 2713, 2719, 2729, 2731, 2741, 2749, 2753, 2767, 2777, 2789, 2791, 2797, 2801, 2803, 2819, 2833, 2837, 2843, 2851, 2857, 2861, 2879, 2887, 2897, 2903, 2909, 2917, 2927, 2939, 2953, 2957, 2963, 2969, 2971, 2999, 3001, 3011, 3019, 3023, 3037, 3041, 3049, 3061, 3067, 3079, 3083, 3089, 3109, 3119, 3121, 3137, 3163, 3167, 3169, 3181, 3187, 3191, 3203, 3209, 3217, 3221, 3229, 3251, 3253, 3257, 3259, 3271, 3299, 3301, 3307, 3313, 3319, 3323, 3329, 3331, 3343, 3347, 3359, 3361, 3371, 3373, 3389, 3391, 3407, 3413, 3433, 3449, 3457, 3461, 3463, 3467, 3469, 3491, 3499, 3511, 3517, 3527, 3529, 3533, 3539, 3541, 3547, 3557, 3559, 3571, 3581, 3583, 3593, 3607, 3613, 3617, 3623, 3631, 3637, 3643, 3659, 3671, 3673, 3677, 3691, 3697, 3701, 3709, 3719, 3727, 3733, 3739, 3761, 3767, 3769, 3779, 3793, 3797, 3803, 3821, 3823, 3833, 3847, 3851, 3853, 3863, 3877, 3881, 3889, 3907, 3911, 3917, 3919, 3923, 3929, 3931, 3943, 3947, 3967, 3989, 4001, 4003, 4007, 4013, 4019, 4021, 4027, 4049, 4051, 4057, 4073, 4079, 4091, 4093, 4099, 4111, 4127, 4129, 4133, 4139, 4153, 4157, 4159, 4177, 4201, 4211, 4217, 4219, 4229, 4231, 4241, 4243, 4253, 4259, 4261, 4271, 4273, 4283, 4289, 4297, 4327, 4337, 4339, 4349, 4357, 4363, 4373, 4391, 4397, 4409, 4421, 4423, 4441, 4447, 4451, 4457, 4463, 4481, 4483, 4493, 4507, 4513, 4517, 4519, 4523, 4547, 4549, 4561, 4567, 4583, 4591, 4597, 4603, 4621, 4637, 4639, 4643, 4649, 4651, 4657, 4663, 4673, 4679, 4691, 4703, 4721, 4723, 4729, 4733, 4751, 4759, 4783, 4787, 4789, 4793, 4799, 4801, 4813, 4817, 4831, 4861, 4871, 4877, 4889, 4903, 4909, 4919, 4931, 4933, 4937, 4943, 4951, 4957, 4967, 4969, 4973, 4987, 4993, 4999, 5003, 5009, 5011, 5021, 5023, 5039, 5051, 5059, 5077, 5081, 5087, 5099, 5101, 5107, 5113, 5119, 5147, 5153, 5167, 5171, 5179, 5189, 5197, 5209, 5227, 5231, 5233, 5237, 5261, 5273, 5279, 5281, 5297, 5303, 5309, 5323, 5333, 5347, 5351, 5381, 5387, 5393, 5399, 5407, 5413, 5417, 5419, 5431, 5437, 5441, 5443, 5449, 5471, 5477, 5479, 5483, 5501, 5503, 5507, 5519, 5521, 5527, 5531, 5557, 5563, 5569, 5573, 5581, 5591, 5623, 5639, 5641, 5647, 5651, 5653, 5657, 5659, 5669, 5683, 5689, 5693, 5701, 5711, 5717, 5737, 5741, 5743, 5749, 5779, 5783, 5791, 5801, 5807, 5813, 5821, 5827, 5839, 5843, 5849, 5851, 5857, 5861, 5867, 5869, 5879, 5881, 5897, 5903, 5923, 5927, 5939, 5953, 5981, 5987, 6007, 6011, 6029, 6037, 6043, 6047, 6053, 6067, 6073, 6079, 6089, 6091, 6101, 6113, 6121, 6131, 6133, 6143, 6151, 6163, 6173, 6197, 6199, 6203, 6211, 6217, 6221, 6229, 6247, 6257, 6263, 6269, 6271, 6277, 6287, 6299, 6301, 6311, 6317, 6323, 6329, 6337, 6343, 6353, 6359, 6361, 6367, 6373, 6379, 6389, 6397, 6421, 6427, 6449, 6451, 6469, 6473, 6481, 6491, 6521, 6529, 6547, 6551, 6553, 6563, 6569, 6571, 6577, 6581, 6599, 6607, 6619, 6637, 6653, 6659, 6661, 6673, 6679, 6689, 6691, 6701, 6703, 6709, 6719, 6733, 6737, 6761, 6763, 6779, 6781, 6791, 6793, 6803, 6823, 6827, 6829, 6833, 6841, 6857, 6863, 6869, 6871, 6883, 6899, 6907, 6911, 6917, 6947, 6949, 6959, 6961, 6967, 6971, 6977, 6983, 6991, 6997, 7001, 7013, 7019, 7027, 7039, 7043, 7057, 7069, 7079, 7103, 7109, 7121, 7127, 7129, 7151, 7159, 7177, 7187, 7193, 7207, 7211, 7213, 7219, 7229, 7237, 7243, 7247, 7253, 7283, 7297, 7307, 7309, 7321, 7331, 7333, 7349, 7351, 7369, 7393, 7411, 7417, 7433, 7451, 7457, 7459, 7477, 7481, 7487, 7489, 7499, 7507, 7517, 7523, 7529, 7537, 7541, 7547, 7549, 7559, 7561, 7573, 7577, 7583, 7589, 7591, 7603, 7607, 7621, 7639, 7643, 7649, 7669, 7673, 7681, 7687, 7691, 7699, 7703, 7717, 7723, 7727, 7741, 7753, 7757, 7759, 7789, 7793, 7817, 7823, 7829, 7841, 7853, 7867, 7873, 7877, 7879, 7883, 7901, 7907, 7919],dtype=np.uint64)\n    if bases_generating_matrices is None:\n        if self.DEFAULT_GENERATING_MATRICES==\"HALTON\":\n            self.type_bases_generating_matrices = \"HALTON\"\n            d_limit = len(self.all_primes)\n        elif self.DEFAULT_GENERATING_MATRICES==\"FAURE\":\n            self.type_bases_generating_matrices = \"FAURE\"\n            d_limit = int(self.all_primes[-1])\n        else:\n            raise ParameterError(\"must supply bases_generating_matrices\")\n    else:\n        self.type_bases_generating_matrices = \"CUSTOM\"\n        assert len(bases_generating_matrices)==2\n        bases,generating_matrices = bases_generating_matrices\n        assert isinstance(generating_matrices,np.ndarray)\n        assert generating_matrices.ndim==3 or generating_matrices.ndim==4 \n        d_limit = generating_matrices.shape[1]\n        if np.isscalar(bases):\n            assert bases&gt;0\n            assert bases%1==0 \n            bases = int(bases)*np.ones(d_limit,dtype=int)\n        assert bases.ndim==1 or bases.ndim==2\n    self.input_t = deepcopy(t) \n    self.input_bases_generating_matrices = deepcopy(bases_generating_matrices)\n    super(DigitalNetAnyBases,self).__init__(dimension,replications,seed,d_limit,n_lim)\n    self.randomize = str(randomize).upper().strip().replace(\"_\",\" \")\n    if self.randomize==\"TRUE\": self.randomize = \"LMS DP\"\n    if self.randomize==\"LMS PERM\": self.randomize = \"LMS DP\"\n    if self.randomize==\"PERM\": self.randomize = \"DP\"\n    if self.randomize==\"OWEN\": self.randomize = \"NUS\"\n    if self.randomize==\"NONE\": self.randomize = \"FALSE\"\n    if self.randomize==\"NO\": self.randomize = \"FALSE\"\n    assert self.randomize in [\"LMS DP\",\"LMS DS\",\"LMS\",\"DP\",\"DS\",\"NUS\",\"QRNG\",\"FALSE\"]\n    if self.randomize==\"QRNG\":\n        assert self.type_bases_generating_matrices==\"HALTON\", \"QRNG randomization is only applicable for the Halton generator.\"\n        from .._c_lib import _load_c_lib\n        assert self.replications==1, \"QRNG requires replications=1\"\n        self.randu_d_32 = self.rng.uniform(size=(self.d,32))\n        _c_lib = _load_c_lib()\n        import ctypes\n        self.halton_cf_qrng = _c_lib.halton_qrng\n        self.halton_cf_qrng.argtypes = [\n            ctypes.c_int,  # n\n            ctypes.c_int,  # d\n            ctypes.c_int, # n0\n            ctypes.c_int, # generalized\n            np.ctypeslib.ndpointer(ctypes.c_double, flags='C_CONTIGUOUS'), # res\n            np.ctypeslib.ndpointer(ctypes.c_double, flags='C_CONTIGUOUS'), # randu_d_32\n            np.ctypeslib.ndpointer(ctypes.c_int, flags='C_CONTIGUOUS')]  # dvec\n        self.halton_cf_qrng.restype = None\n    self.alpha = alpha\n    assert self.alpha&gt;=1\n    assert self.alpha%1==0\n    if self.alpha&gt;1:\n        assert (self.dvec==np.arange(self.d)).all(), \"digital interlacing requires dimension is an int\"\n    self.dtalpha = self.alpha*self.d \n    if self.type_bases_generating_matrices==\"HALTON\":\n        self.bases = self.all_primes[self.dvec][None,:]\n        self.m_max = int(np.ceil(np.log(self.n_limit)/np.log(self.bases.min())))\n        self._t_curr = self.m_max\n        if t is None: t = int(np.ceil(-np.log(2**(-63))/np.log(self.bases.min())))\n        self.t = self.m_max if self.m_max&gt;t else t\n        self.C = qmctoolscl.gdn_get_halton_generating_matrix(np.uint64(1),np.uint64(self.d),np.uint64(self._t_curr))\n    elif self.type_bases_generating_matrices==\"FAURE\":\n        assert (self.dvec==np.arange(self.d)).all(), \"Faure requires dimension is an int\"\n        p = self.all_primes[np.argmax(self.all_primes&gt;=self.d)]\n        self.bases = p*np.ones((1,self.dtalpha),dtype=np.uint64)\n        self.m_max = int(np.ceil(np.log(self.n_limit)/np.log(p)))\n        if t is None: t = int(np.ceil(-np.log(2**(-63))/np.log(p)))\n        self._t_curr = self.m_max\n        self.t = self.m_max if self.m_max&gt;t else t\n        self.C = np.ones((self.dtalpha,1,1),dtype=np.uint64)*np.eye(self._t_curr,dtype=np.uint64)\n        if self.dtalpha&gt;1:\n            for a in range(self._t_curr):\n                for b in range(a+1):\n                    self.C[1,a,b] = comb(a,b)%p\n        if self.dtalpha&gt;2:\n            for k in range(2,self.dtalpha):\n                for a in range(self._t_curr):\n                    for b in range(a+1):\n                        self.C[k,a,b] = (int(self.C[1,a,b])*((k**(a-b))%p))%p\n        self.C = self.C[None,:,:,:]\n        self.faure_base = p\n    else:\n        self.bases = bases.astype(np.uint64)\n        if self.bases.ndim==1: self.bases = self.bases[None,:]\n        assert self.bases.shape[1]&gt;=self.dtalpha\n        if self.alpha==1:\n            self.bases = self.bases[:,self.dvec]\n        else:\n            self.bases = self.bases[:,:self.dtalpha]\n        self.C = generating_matrices.astype(np.uint64)\n        if self.C.ndim==3: self.C = self.C[None,:,:,:]\n        assert self.C.shape[1]&gt;=self.dtalpha\n        if self.alpha==1:\n            self.C = self.C[:,self.dvec,:,:]\n        else:\n            self.C = self.C[:,:self.dtalpha,:,:]\n        self.m_max,self._t_curr = self.C.shape[-2:]\n        if t is None: t = int(np.ceil(-np.log(2**(-63))/np.log(self.bases.min())))\n        self.t = self.m_max if self.m_max&gt;t else t\n        assert (0&lt;=self.C).all()\n        assert (self.C&lt;self.bases[:,:,None,None]).all()\n        self.C = np.ascontiguousarray(self.C) \n        self.bases = np.ascontiguousarray(self.bases)\n    if self.alpha&gt;1:\n        assert (self.bases==self.bases[0,0]).all(), \"alpha&gt;1 performs digital interlacing which requires the same base across dimensions and replications.\"\n        if warn and self.m_max!=self._t_curr:\n            warnings.warn(\"Digital interlacing is often performed on generating matrices with the number of columns (m_max = %d) equal to the number of rows (_t_curr = %d), but this is not the case. Ensure you are NOT setting alpha&gt;1 when generating matrices are already interlaced.\"%(self.m_max,self._t_curr),ParameterWarning)\n    assert self.bases.ndim==2\n    assert self.bases.shape[-1]==self.dtalpha\n    assert self.bases.shape[0]==1 or self.bases.shape[0]==self.replications\n    assert self.C.ndim==4\n    assert self.C.shape[-3:]==(self.dtalpha,self.m_max,self._t_curr)\n    assert self.C.shape[0]==1 or self.C.shape[0]==self.replications\n    r_b = self.bases.shape[0]\n    r_C = self.C.shape[0]\n    if self.randomize==\"FALSE\":\n        if self.alpha&gt;1:\n            t_alpha = min(self.t,self._t_curr*self.alpha)\n            C_ho = np.empty((self.replications,self.d,self.m_max,t_alpha),dtype=np.uint64)\n            qmctoolscl.gdn_interlace(np.uint64(self.replications),np.uint64(self.d),np.uint64(self.m_max),np.uint64(self.dtalpha),np.uint64(self._t_curr),np.uint64(t_alpha),np.uint64(self.alpha),self.C,C_ho)\n            self.C = C_ho\n            self._t_curr = t_alpha\n            self.t = t_alpha\n            self.bases = self.bases[:,:self.d]\n    elif self.randomize==\"DP\":\n        self.perms = qmctoolscl.gdn_get_digital_permutations(self.rng,np.uint64(self.replications),np.uint64(self.d),self.t,np.uint64(r_b),self.bases)\n    elif self.randomize==\"DS\":\n        self.rshift = qmctoolscl.gdn_get_digital_shifts(self.rng,np.uint64(self.replications),np.uint64(self.d),self.t,np.uint64(r_b),self.bases)\n    elif self.randomize in [\"LMS\",\"LMS DS\",\"LMS DP\"]:\n        if self.alpha==1:\n            S = qmctoolscl.gdn_get_linear_scramble_matrix(self.rng,np.uint64(self.replications),np.uint64(self.d),np.uint64(self._t_curr),np.uint64(self.t),np.uint64(r_b),self.bases)\n            C_lms = np.empty((self.replications,self.d,self.m_max,self.t),dtype=np.uint64)\n            qmctoolscl.gdn_linear_matrix_scramble(np.uint64(self.replications),np.uint64(self.d),np.uint64(self.m_max),np.uint64(r_C),np.uint64(r_b),np.uint64(self._t_curr),np.uint64(self.t),self.bases,S,self.C,C_lms,backend=\"c\")\n            self.C = C_lms\n            self._t_curr = self.t\n        else:\n            t_dig = np.ceil(max(self.t/self.alpha,self._t_curr))\n            S = qmctoolscl.gdn_get_linear_scramble_matrix(self.rng,np.uint64(self.replications),np.uint64(self.dtalpha),np.uint64(self._t_curr),np.uint64(t_dig),np.uint64(r_b),self.bases)\n            C_lms = np.empty((self.replications,self.dtalpha,self.m_max,self.t),dtype=np.uint64)\n            qmctoolscl.gdn_linear_matrix_scramble(np.uint64(self.replications),np.uint64(self.dtalpha),np.uint64(self.m_max),np.uint64(r_C),np.uint64(r_b),np.uint64(self._t_curr),np.uint64(t_dig),self.bases,S,self.C,C_lms,backend=\"c\")\n            C_lms_ho = np.empty((self.replications,self.d,self.m_max,self.t),dtype=np.uint64)\n            qmctoolscl.gdn_interlace(np.uint64(self.replications),np.uint64(self.d),np.uint64(self.m_max),np.uint64(self.dtalpha),np.uint64(t_dig),np.uint64(self.t),np.uint64(self.alpha),C_lms,C_lms_ho)\n            self.C = C_lms_ho\n            self._t_curr = self.t\n            self.t = self._t_curr\n            self.bases = self.bases[:,:self.d]\n        if self.randomize==\"LMS DP\":\n            self.perms = qmctoolscl.gdn_get_digital_permutations(self.rng,np.uint64(self.replications),np.uint64(self.d),self.t,np.uint64(r_b),self.bases)\n        elif self.randomize==\"LMS DS\":\n            self.rshift = qmctoolscl.gdn_get_digital_shifts(self.rng,np.uint64(self.replications),np.uint64(self.d),self.t,np.uint64(r_b),self.bases)\n    elif self.randomize==\"NUS\":\n        if self.alpha==1:\n            new_seeds = self._base_seed.spawn(self.replications*self.d)\n            self.rngs = np.array([np.random.Generator(np.random.SFC64(new_seeds[j])) for j in range(self.replications*self.d)]).reshape(self.replications,self.d)\n            self.root_nodes = np.array([qmctoolscl.NUSNode_gdn() for i in range(self.replications*self.d)]).reshape(self.replications,self.d)\n        else:\n            new_seeds = self._base_seed.spawn(self.replications*self.dtalpha)\n            self.rngs = np.array([np.random.Generator(np.random.SFC64(new_seeds[j])) for j in range(self.replications*self.dtalpha)]).reshape(self.replications,self.dtalpha)\n            self.root_nodes = np.array([qmctoolscl.NUSNode_gdn() for i in range(self.replications*self.dtalpha)]).reshape(self.replications,self.dtalpha)\n    assert self.C.ndim==4 and (self.C.shape[0]==1 or self.C.shape[0]==self.replications) and self.C.shape[1]==(self.dtalpha if self.randomize==\"NUS\" else self.d) and self.C.shape[2]==self.m_max and self.C.shape[3]==self._t_curr\n    assert self.bases.ndim==2 and (self.bases.shape[0]==1 or self.bases.shape[0]==self.replications) and self.bases.shape[1]==(self.dtalpha if self.randomize==\"NUS\" else self.d)\n    assert 0&lt;self._t_curr&lt;=self.t&lt;=64\n    if self.randomize==\"FALSE\": assert self.C.shape[0]==self.replications, \"randomize='FALSE' but replications = %d does not equal the number of sets of generating vectors %d\"%(self.replications,self.C.shape[0])\n    if warn and (self.bases==2).all():\n        warnings.warn(\"It is more efficient to use DigitalNetB2 instead of DigitalNetAnyBases when all bases are 2\")\n    self.warn = warn\n</code></pre>"},{"location":"api/discrete_distributions/#digitalnetanybases","title":"<code>DigitalNetAnyBases</code>","text":"<p>               Bases: <code>AbstractLDDiscreteDistribution</code></p> <p>Low discrepancy digital net with arbitrary bases for each dimension. </p> Note <ul> <li>Digital net samples sizes should be products of powers of bases,  i.e., a digital net with bases \\((b_1,\\dots,b_d)\\)  will prefer sample sizes \\(n = b_1^{p_1} \\cdots b_d^{p_d}\\) for some \\(p_1,\\dots,p_d \\in \\mathbb{N}_0\\).</li> <li>The first point of an unrandomized digital net is the origin. </li> <li> <p>The construction of higher order digital nets requires the same base for each dimension.  To construct higher order digital nets, either: </p> <ul> <li>Pass in <code>generating_matrices</code> without interlacing and supply <code>alpha&gt;1</code> to apply interlacing, or </li> <li>Pass in <code>generating_matrices</code> with interlacing and set <code>alpha=1</code> to avoid additional interlacing. </li> </ul> <p>i.e. do not pass in interlaced <code>generating_matrices</code> and set <code>alpha&gt;1</code>, this will apply additional interlacing. </p> </li> </ul> <p>A few examples below showcase how to pass in custom bases and generating matrices. Many other examples can be found in the Halton and Faure implementations</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; bases = 3 \n&gt;&gt;&gt; generating_matrices = np.array(\n...     [\n...         [[1, 0, 0],\n...          [0, 1, 0],\n...          [1, 2, 1]],\n...         [[2, 0, 0],\n...          [1, 2, 0],\n...          [0, 2, 2]]\n...     ],\n...     dtype=np.uint64)\n&gt;&gt;&gt; DigitalNetAnyBases(2,randomize=\"False\",bases_generating_matrices=(bases,generating_matrices))(9,warn=False)\narray([[0.        , 0.        ],\n       [0.33333333, 0.66666667],\n       [0.66666667, 0.33333333],\n       [0.11111111, 0.55555556],\n       [0.44444444, 0.22222222],\n       [0.77777778, 0.88888889],\n       [0.22222222, 0.77777778],\n       [0.55555556, 0.44444444],\n       [0.88888889, 0.11111111]])\n&gt;&gt;&gt; DigitalNetAnyBases(1,randomize=\"False\",alpha=2,bases_generating_matrices=(bases,generating_matrices))(9,warn=False)\narray([[0.        ],\n       [0.55555556],\n       [0.77777778],\n       [0.17283951],\n       [0.39506173],\n       [0.95061728],\n       [0.30864198],\n       [0.5308642 ],\n       [0.75308642]])\n</code></pre> <pre><code>&gt;&gt;&gt; rng = np.random.Generator(np.random.PCG64(7))\n&gt;&gt;&gt; bases = np.array(\n...     [[2,5,7,23],\n...      [3,11,13,17]],\n...     dtype=np.uint64)\n&gt;&gt;&gt; generating_matrices = np.stack(\n...     [\n...         np.stack(\n...             [   np.tril(rng.integers(0,2,(10,10))),\n...                 np.tril(rng.integers(0,5,(10,10))),\n...                 np.tril(rng.integers(0,7,(10,10))),\n...                 np.tril(rng.integers(0,23,(10,10))),],axis=0),\n...         np.stack(\n...             [   np.tril(rng.integers(0,3,(10,10))),\n...                 np.tril(rng.integers(0,11,(10,10))),\n...                 np.tril(rng.integers(0,13,(10,10))),\n...                 np.tril(rng.integers(0,17,(10,10))),],axis=0),\n...     ],axis=0).astype(np.uint64)\n&gt;&gt;&gt; discrete_distrib = DigitalNetAnyBases(\n...     dimension = 4,\n...     randomize = \"NUS\",\n...     seed = 7,\n...     replications = 2,\n...     bases_generating_matrices=(bases,generating_matrices),\n...     )\n&gt;&gt;&gt; discrete_distrib(10,warn=False)\narray([[[0.141964  , 0.50711584, 0.75766621, 0.43359373],\n        [0.65536579, 0.66327673, 0.21825902, 0.93901529],\n        [0.46955206, 0.21554445, 0.54265048, 0.17421341],\n        [0.78505432, 0.99432677, 0.08408121, 0.8317411 ],\n        [0.65536579, 0.05422203, 0.37171211, 0.05616883],\n        [0.141964  , 0.21554445, 0.94198662, 0.89162246],\n        [0.78505432, 0.99432677, 0.62395426, 0.70351649],\n        [0.46955206, 0.05422203, 0.51786701, 0.75215221],\n        [0.98479561, 0.50711584, 0.06267682, 0.59165814],\n        [0.31524867, 0.66327673, 0.29204202, 0.29200658]],\n\n       [[0.16158904, 0.00911626, 0.99631587, 0.7984386 ],\n        [0.33578478, 0.98734618, 0.48165571, 0.84189861],\n        [0.84596814, 0.67093277, 0.72532656, 0.49339621],\n        [0.33578478, 0.54281197, 0.26069214, 0.43832903],\n        [0.84596814, 0.58529648, 0.80521628, 0.15910404],\n        [0.16158904, 0.4486066 , 0.84973668, 0.6750655 ],\n        [0.84596814, 0.21294277, 0.40387763, 0.19059409],\n        [0.16158904, 0.74257456, 0.19604142, 0.98366484],\n        [0.33578478, 0.31746094, 0.35948446, 0.75911922],\n        [0.64593022, 0.11007165, 0.63174328, 0.55910368]]])\n</code></pre> <pre><code>&gt;&gt;&gt; bases = 2\n&gt;&gt;&gt; generating_matrices = np.array([\n...     [[1, 0, 0],\n...      [0, 1, 0],\n...      [1, 0, 1]],\n...     [[1, 0, 0],\n...      [1, 1, 0],\n...      [0, 1, 1]]],dtype=np.uint64)\n&gt;&gt;&gt; x = DigitalNetAnyBases(2,randomize=\"False\",bases_generating_matrices=(bases,generating_matrices),warn=False)(8,warn=False)\n&gt;&gt;&gt; x\narray([[0.   , 0.   ],\n       [0.5  , 0.5  ],\n       [0.25 , 0.75 ],\n       [0.75 , 0.25 ],\n       [0.625, 0.375],\n       [0.125, 0.875],\n       [0.875, 0.625],\n       [0.375, 0.125]])\n&gt;&gt;&gt; generating_matrices_b2 = (generating_matrices*2**np.array([2,1,0])).sum(-1).astype(np.uint64)\n&gt;&gt;&gt; x_b2 = DigitalNetB2(2,randomize=\"False\",order=\"radical inverse\",generating_matrices=generating_matrices_b2,t=3,msb=True)(8,warn=False)\n&gt;&gt;&gt; x_b2\narray([[0.   , 0.   ],\n       [0.5  , 0.5  ],\n       [0.25 , 0.75 ],\n       [0.75 , 0.25 ],\n       [0.625, 0.375],\n       [0.125, 0.875],\n       [0.875, 0.625],\n       [0.375, 0.125]])\n&gt;&gt;&gt; bool((x==x_b2).all())\nTrue\n</code></pre> <p>References:</p> <ol> <li> <p>Dick, Josef, and Friedrich Pillichshammer.     Digital nets and sequences: discrepancy theory and quasi\u2013Monte Carlo integration.     Cambridge University Press, 2010.</p> </li> <li> <p>Sorokin, Aleksei.     \"QMCPy: A Python Software for Randomized Low-Discrepancy Sequences, Quasi-Monte Carlo, and Fast Kernel Methods\"     arXiv preprint arXiv:2502.14256 (2025).</p> </li> </ol> <p>Parameters:</p> Name Type Description Default <code>dimension</code> <code>Union[int, ndarray]</code> <p>Dimension of the generator.</p> <ul> <li>If an <code>int</code> is passed in, use generating vector components at indices 0,...,<code>dimension</code>-1.</li> <li>If an <code>np.ndarray</code> is passed in, use generating vector components at these indices.</li> </ul> <code>1</code> <code>replications</code> <code>int</code> <p>Number of independent randomizations of a pointset.</p> <code>None</code> <code>seed</code> <code>Union[None,int,np.random.SeedSeq</code> <p>Seed the random number generator for reproducibility.</p> <code>None</code> <code>randomize</code> <code>str</code> <p>Options are</p> <ul> <li><code>'LMS DP'</code>: Linear matrix scramble with digital permutation.</li> <li><code>'LMS DS'</code>: Linear matrix scramble with digital shift.</li> <li><code>'LMS'</code>: Linear matrix scramble only.</li> <li><code>'DP'</code>: Digital permutation scramble only.</li> <li><code>'DS'</code>: Digital shift only.</li> <li><code>'NUS'</code>: Nested uniform scrambling.</li> <li><code>'QRNG'</code>: Deterministic permutation scramble and random digital shift from QRNG [1] (with <code>generalize=True</code>). Does not support replications&gt;1.</li> <li><code>None</code>: No randomization. In this case the first point will be the origin. </li> </ul> <code>'LMS DP'</code> <code>bases_generating_matrices (Union[str, tuple]</code> <p>Specify the bases and the generating matrices.</p> <ul> <li><code>\"HALTON\"</code> will use Halton generating matrices.</li> <li><code>\"FAURE\"</code> will use Faure generating matrices .</li> <li> <p><code>bases,generating_matrices</code> requires </p> <ul> <li><code>bases</code> is an <code>np.ndarray</code> of integers with shape \\((,d)\\) or \\((r,d)\\) where \\(d\\) is the number of dimensions and \\(r\\) is the number of replications.</li> <li><code>generating_matrices</code> is an <code>np.ndarray</code> of integers with shape \\((d,m_\\mathrm{max},t_\\mathrm{max})\\) or \\((r,d,m_\\mathrm{max},t_\\mathrm{max})\\) where \\(d\\) is the number of dimensions, \\(r\\) is the number of replications, and \\(2^{m_\\mathrm{max}}\\) is the maximum number of supported points.</li> </ul> </li> </ul> required <code>t</code> <code>int</code> <p>Number of digits after randomization. The number of digits in the generating matrices is inferred.</p> <code>None</code> <code>alpha</code> <code>int</code> <p>Interlacing factor for higher order nets. When <code>alpha</code>&gt;1, interlacing is performed regardless of the generating matrices, i.e., for <code>alpha</code>&gt;1 do not pass in generating matrices which are already interlaced. The Note for this class contains more info.  </p> <code>1</code> <code>n_lim</code> <code>int</code> <p>Maximum number of compatible points, determines the number of rows in the generating matrices. </p> <code>2 ** 32</code> <code>warn</code> <code>bool</code> <p>If <code>False</code>, suppress warnings in construction</p> <code>True</code> Source code in <code>qmcpy/discrete_distribution/digital_net_any_bases/digital_net_any_bases.py</code> <pre><code>def __init__(self,\n             dimension = 1,\n             replications = None,\n             seed = None, \n             randomize = 'LMS DP',\n             bases_generating_matrices = None,\n             t = None,\n             alpha = 1,\n             n_lim = 2**32,\n             warn = True):\n    r\"\"\"\n    Args:\n        dimension (Union[int,np.ndarray]): Dimension of the generator.\n\n            - If an `int` is passed in, use generating vector components at indices 0,...,`dimension`-1.\n            - If an `np.ndarray` is passed in, use generating vector components at these indices.\n\n        replications (int): Number of independent randomizations of a pointset.\n        seed (Union[None,int,np.random.SeedSeq): Seed the random number generator for reproducibility.\n        randomize (str): Options are\n\n            - `'LMS DP'`: Linear matrix scramble with digital permutation.\n            - `'LMS DS'`: Linear matrix scramble with digital shift.\n            - `'LMS'`: Linear matrix scramble only.\n            - `'DP'`: Digital permutation scramble only.\n            - `'DS'`: Digital shift only.\n            - `'NUS'`: Nested uniform scrambling.\n            - `'QRNG'`: Deterministic permutation scramble and random digital shift from QRNG [1] (with `generalize=True`). Does *not* support replications&gt;1.\n            - `None`: No randomization. In this case the first point will be the origin. \n        bases_generating_matrices (Union[str, tuple]: Specify the bases and the generating matrices.\n\n            - `\"HALTON\"` will use Halton generating matrices.\n            - `\"FAURE\"` will use Faure generating matrices .\n            - `bases,generating_matrices` requires \n\n                - `bases` is an `np.ndarray` of integers with shape $(,d)$ or $(r,d)$ where $d$ is the number of dimensions and $r$ is the number of replications.\n                - `generating_matrices` is an `np.ndarray` of integers with shape $(d,m_\\mathrm{max},t_\\mathrm{max})$ or $(r,d,m_\\mathrm{max},t_\\mathrm{max})$ where $d$ is the number of dimensions, $r$ is the number of replications, and $2^{m_\\mathrm{max}}$ is the maximum number of supported points.\n\n        t (int): Number of digits *after* randomization. The number of digits in the generating matrices is inferred.\n        alpha (int): Interlacing factor for higher order nets.  \n            When `alpha`&gt;1, interlacing is performed regardless of the generating matrices,  \n            i.e., for `alpha`&gt;1 do *not* pass in generating matrices which are already interlaced.  \n            The Note for this class contains more info.  \n        n_lim (int): Maximum number of compatible points, determines the number of rows in the generating matrices. \n        warn (bool): If `False`, suppress warnings in construction \n    \"\"\"\n    self.parameters = ['randomize','t','n_limit']\n    self.all_primes = np.array([2, 3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, 47, 53, 59, 61, 67, 71, 73, 79, 83, 89, 97, 101, 103, 107, 109, 113, 127, 131, 137, 139, 149, 151, 157, 163, 167, 173, 179, 181, 191, 193, 197, 199, 211, 223, 227, 229, 233, 239, 241, 251, 257, 263, 269, 271, 277, 281, 283, 293, 307, 311, 313, 317, 331, 337, 347, 349, 353, 359, 367, 373, 379, 383, 389, 397, 401, 409, 419, 421, 431, 433, 439, 443, 449, 457, 461, 463, 467, 479, 487, 491, 499, 503, 509, 521, 523, 541, 547, 557, 563, 569, 571, 577, 587, 593, 599, 601, 607, 613, 617, 619, 631, 641, 643, 647, 653, 659, 661, 673, 677, 683, 691, 701, 709, 719, 727, 733, 739, 743, 751, 757, 761, 769, 773, 787, 797, 809, 811, 821, 823, 827, 829, 839, 853, 857, 859, 863, 877, 881, 883, 887, 907, 911, 919, 929, 937, 941, 947, 953, 967, 971, 977, 983, 991, 997, 1009, 1013, 1019, 1021, 1031, 1033, 1039, 1049, 1051, 1061, 1063, 1069, 1087, 1091, 1093, 1097, 1103, 1109, 1117, 1123, 1129, 1151, 1153, 1163, 1171, 1181, 1187, 1193, 1201, 1213, 1217, 1223, 1229, 1231, 1237, 1249, 1259, 1277, 1279, 1283, 1289, 1291, 1297, 1301, 1303, 1307, 1319, 1321, 1327, 1361, 1367, 1373, 1381, 1399, 1409, 1423, 1427, 1429, 1433, 1439, 1447, 1451, 1453, 1459, 1471, 1481, 1483, 1487, 1489, 1493, 1499, 1511, 1523, 1531, 1543, 1549, 1553, 1559, 1567, 1571, 1579, 1583, 1597, 1601, 1607, 1609, 1613, 1619, 1621, 1627, 1637, 1657, 1663, 1667, 1669, 1693, 1697, 1699, 1709, 1721, 1723, 1733, 1741, 1747, 1753, 1759, 1777, 1783, 1787, 1789, 1801, 1811, 1823, 1831, 1847, 1861, 1867, 1871, 1873, 1877, 1879, 1889, 1901, 1907, 1913, 1931, 1933, 1949, 1951, 1973, 1979, 1987, 1993, 1997, 1999, 2003, 2011, 2017, 2027, 2029, 2039, 2053, 2063, 2069, 2081, 2083, 2087, 2089, 2099, 2111, 2113, 2129, 2131, 2137, 2141, 2143, 2153, 2161, 2179, 2203, 2207, 2213, 2221, 2237, 2239, 2243, 2251, 2267, 2269, 2273, 2281, 2287, 2293, 2297, 2309, 2311, 2333, 2339, 2341, 2347, 2351, 2357, 2371, 2377, 2381, 2383, 2389, 2393, 2399, 2411, 2417, 2423, 2437, 2441, 2447, 2459, 2467, 2473, 2477, 2503, 2521, 2531, 2539, 2543, 2549, 2551, 2557, 2579, 2591, 2593, 2609, 2617, 2621, 2633, 2647, 2657, 2659, 2663, 2671, 2677, 2683, 2687, 2689, 2693, 2699, 2707, 2711, 2713, 2719, 2729, 2731, 2741, 2749, 2753, 2767, 2777, 2789, 2791, 2797, 2801, 2803, 2819, 2833, 2837, 2843, 2851, 2857, 2861, 2879, 2887, 2897, 2903, 2909, 2917, 2927, 2939, 2953, 2957, 2963, 2969, 2971, 2999, 3001, 3011, 3019, 3023, 3037, 3041, 3049, 3061, 3067, 3079, 3083, 3089, 3109, 3119, 3121, 3137, 3163, 3167, 3169, 3181, 3187, 3191, 3203, 3209, 3217, 3221, 3229, 3251, 3253, 3257, 3259, 3271, 3299, 3301, 3307, 3313, 3319, 3323, 3329, 3331, 3343, 3347, 3359, 3361, 3371, 3373, 3389, 3391, 3407, 3413, 3433, 3449, 3457, 3461, 3463, 3467, 3469, 3491, 3499, 3511, 3517, 3527, 3529, 3533, 3539, 3541, 3547, 3557, 3559, 3571, 3581, 3583, 3593, 3607, 3613, 3617, 3623, 3631, 3637, 3643, 3659, 3671, 3673, 3677, 3691, 3697, 3701, 3709, 3719, 3727, 3733, 3739, 3761, 3767, 3769, 3779, 3793, 3797, 3803, 3821, 3823, 3833, 3847, 3851, 3853, 3863, 3877, 3881, 3889, 3907, 3911, 3917, 3919, 3923, 3929, 3931, 3943, 3947, 3967, 3989, 4001, 4003, 4007, 4013, 4019, 4021, 4027, 4049, 4051, 4057, 4073, 4079, 4091, 4093, 4099, 4111, 4127, 4129, 4133, 4139, 4153, 4157, 4159, 4177, 4201, 4211, 4217, 4219, 4229, 4231, 4241, 4243, 4253, 4259, 4261, 4271, 4273, 4283, 4289, 4297, 4327, 4337, 4339, 4349, 4357, 4363, 4373, 4391, 4397, 4409, 4421, 4423, 4441, 4447, 4451, 4457, 4463, 4481, 4483, 4493, 4507, 4513, 4517, 4519, 4523, 4547, 4549, 4561, 4567, 4583, 4591, 4597, 4603, 4621, 4637, 4639, 4643, 4649, 4651, 4657, 4663, 4673, 4679, 4691, 4703, 4721, 4723, 4729, 4733, 4751, 4759, 4783, 4787, 4789, 4793, 4799, 4801, 4813, 4817, 4831, 4861, 4871, 4877, 4889, 4903, 4909, 4919, 4931, 4933, 4937, 4943, 4951, 4957, 4967, 4969, 4973, 4987, 4993, 4999, 5003, 5009, 5011, 5021, 5023, 5039, 5051, 5059, 5077, 5081, 5087, 5099, 5101, 5107, 5113, 5119, 5147, 5153, 5167, 5171, 5179, 5189, 5197, 5209, 5227, 5231, 5233, 5237, 5261, 5273, 5279, 5281, 5297, 5303, 5309, 5323, 5333, 5347, 5351, 5381, 5387, 5393, 5399, 5407, 5413, 5417, 5419, 5431, 5437, 5441, 5443, 5449, 5471, 5477, 5479, 5483, 5501, 5503, 5507, 5519, 5521, 5527, 5531, 5557, 5563, 5569, 5573, 5581, 5591, 5623, 5639, 5641, 5647, 5651, 5653, 5657, 5659, 5669, 5683, 5689, 5693, 5701, 5711, 5717, 5737, 5741, 5743, 5749, 5779, 5783, 5791, 5801, 5807, 5813, 5821, 5827, 5839, 5843, 5849, 5851, 5857, 5861, 5867, 5869, 5879, 5881, 5897, 5903, 5923, 5927, 5939, 5953, 5981, 5987, 6007, 6011, 6029, 6037, 6043, 6047, 6053, 6067, 6073, 6079, 6089, 6091, 6101, 6113, 6121, 6131, 6133, 6143, 6151, 6163, 6173, 6197, 6199, 6203, 6211, 6217, 6221, 6229, 6247, 6257, 6263, 6269, 6271, 6277, 6287, 6299, 6301, 6311, 6317, 6323, 6329, 6337, 6343, 6353, 6359, 6361, 6367, 6373, 6379, 6389, 6397, 6421, 6427, 6449, 6451, 6469, 6473, 6481, 6491, 6521, 6529, 6547, 6551, 6553, 6563, 6569, 6571, 6577, 6581, 6599, 6607, 6619, 6637, 6653, 6659, 6661, 6673, 6679, 6689, 6691, 6701, 6703, 6709, 6719, 6733, 6737, 6761, 6763, 6779, 6781, 6791, 6793, 6803, 6823, 6827, 6829, 6833, 6841, 6857, 6863, 6869, 6871, 6883, 6899, 6907, 6911, 6917, 6947, 6949, 6959, 6961, 6967, 6971, 6977, 6983, 6991, 6997, 7001, 7013, 7019, 7027, 7039, 7043, 7057, 7069, 7079, 7103, 7109, 7121, 7127, 7129, 7151, 7159, 7177, 7187, 7193, 7207, 7211, 7213, 7219, 7229, 7237, 7243, 7247, 7253, 7283, 7297, 7307, 7309, 7321, 7331, 7333, 7349, 7351, 7369, 7393, 7411, 7417, 7433, 7451, 7457, 7459, 7477, 7481, 7487, 7489, 7499, 7507, 7517, 7523, 7529, 7537, 7541, 7547, 7549, 7559, 7561, 7573, 7577, 7583, 7589, 7591, 7603, 7607, 7621, 7639, 7643, 7649, 7669, 7673, 7681, 7687, 7691, 7699, 7703, 7717, 7723, 7727, 7741, 7753, 7757, 7759, 7789, 7793, 7817, 7823, 7829, 7841, 7853, 7867, 7873, 7877, 7879, 7883, 7901, 7907, 7919],dtype=np.uint64)\n    if bases_generating_matrices is None:\n        if self.DEFAULT_GENERATING_MATRICES==\"HALTON\":\n            self.type_bases_generating_matrices = \"HALTON\"\n            d_limit = len(self.all_primes)\n        elif self.DEFAULT_GENERATING_MATRICES==\"FAURE\":\n            self.type_bases_generating_matrices = \"FAURE\"\n            d_limit = int(self.all_primes[-1])\n        else:\n            raise ParameterError(\"must supply bases_generating_matrices\")\n    else:\n        self.type_bases_generating_matrices = \"CUSTOM\"\n        assert len(bases_generating_matrices)==2\n        bases,generating_matrices = bases_generating_matrices\n        assert isinstance(generating_matrices,np.ndarray)\n        assert generating_matrices.ndim==3 or generating_matrices.ndim==4 \n        d_limit = generating_matrices.shape[1]\n        if np.isscalar(bases):\n            assert bases&gt;0\n            assert bases%1==0 \n            bases = int(bases)*np.ones(d_limit,dtype=int)\n        assert bases.ndim==1 or bases.ndim==2\n    self.input_t = deepcopy(t) \n    self.input_bases_generating_matrices = deepcopy(bases_generating_matrices)\n    super(DigitalNetAnyBases,self).__init__(dimension,replications,seed,d_limit,n_lim)\n    self.randomize = str(randomize).upper().strip().replace(\"_\",\" \")\n    if self.randomize==\"TRUE\": self.randomize = \"LMS DP\"\n    if self.randomize==\"LMS PERM\": self.randomize = \"LMS DP\"\n    if self.randomize==\"PERM\": self.randomize = \"DP\"\n    if self.randomize==\"OWEN\": self.randomize = \"NUS\"\n    if self.randomize==\"NONE\": self.randomize = \"FALSE\"\n    if self.randomize==\"NO\": self.randomize = \"FALSE\"\n    assert self.randomize in [\"LMS DP\",\"LMS DS\",\"LMS\",\"DP\",\"DS\",\"NUS\",\"QRNG\",\"FALSE\"]\n    if self.randomize==\"QRNG\":\n        assert self.type_bases_generating_matrices==\"HALTON\", \"QRNG randomization is only applicable for the Halton generator.\"\n        from .._c_lib import _load_c_lib\n        assert self.replications==1, \"QRNG requires replications=1\"\n        self.randu_d_32 = self.rng.uniform(size=(self.d,32))\n        _c_lib = _load_c_lib()\n        import ctypes\n        self.halton_cf_qrng = _c_lib.halton_qrng\n        self.halton_cf_qrng.argtypes = [\n            ctypes.c_int,  # n\n            ctypes.c_int,  # d\n            ctypes.c_int, # n0\n            ctypes.c_int, # generalized\n            np.ctypeslib.ndpointer(ctypes.c_double, flags='C_CONTIGUOUS'), # res\n            np.ctypeslib.ndpointer(ctypes.c_double, flags='C_CONTIGUOUS'), # randu_d_32\n            np.ctypeslib.ndpointer(ctypes.c_int, flags='C_CONTIGUOUS')]  # dvec\n        self.halton_cf_qrng.restype = None\n    self.alpha = alpha\n    assert self.alpha&gt;=1\n    assert self.alpha%1==0\n    if self.alpha&gt;1:\n        assert (self.dvec==np.arange(self.d)).all(), \"digital interlacing requires dimension is an int\"\n    self.dtalpha = self.alpha*self.d \n    if self.type_bases_generating_matrices==\"HALTON\":\n        self.bases = self.all_primes[self.dvec][None,:]\n        self.m_max = int(np.ceil(np.log(self.n_limit)/np.log(self.bases.min())))\n        self._t_curr = self.m_max\n        if t is None: t = int(np.ceil(-np.log(2**(-63))/np.log(self.bases.min())))\n        self.t = self.m_max if self.m_max&gt;t else t\n        self.C = qmctoolscl.gdn_get_halton_generating_matrix(np.uint64(1),np.uint64(self.d),np.uint64(self._t_curr))\n    elif self.type_bases_generating_matrices==\"FAURE\":\n        assert (self.dvec==np.arange(self.d)).all(), \"Faure requires dimension is an int\"\n        p = self.all_primes[np.argmax(self.all_primes&gt;=self.d)]\n        self.bases = p*np.ones((1,self.dtalpha),dtype=np.uint64)\n        self.m_max = int(np.ceil(np.log(self.n_limit)/np.log(p)))\n        if t is None: t = int(np.ceil(-np.log(2**(-63))/np.log(p)))\n        self._t_curr = self.m_max\n        self.t = self.m_max if self.m_max&gt;t else t\n        self.C = np.ones((self.dtalpha,1,1),dtype=np.uint64)*np.eye(self._t_curr,dtype=np.uint64)\n        if self.dtalpha&gt;1:\n            for a in range(self._t_curr):\n                for b in range(a+1):\n                    self.C[1,a,b] = comb(a,b)%p\n        if self.dtalpha&gt;2:\n            for k in range(2,self.dtalpha):\n                for a in range(self._t_curr):\n                    for b in range(a+1):\n                        self.C[k,a,b] = (int(self.C[1,a,b])*((k**(a-b))%p))%p\n        self.C = self.C[None,:,:,:]\n        self.faure_base = p\n    else:\n        self.bases = bases.astype(np.uint64)\n        if self.bases.ndim==1: self.bases = self.bases[None,:]\n        assert self.bases.shape[1]&gt;=self.dtalpha\n        if self.alpha==1:\n            self.bases = self.bases[:,self.dvec]\n        else:\n            self.bases = self.bases[:,:self.dtalpha]\n        self.C = generating_matrices.astype(np.uint64)\n        if self.C.ndim==3: self.C = self.C[None,:,:,:]\n        assert self.C.shape[1]&gt;=self.dtalpha\n        if self.alpha==1:\n            self.C = self.C[:,self.dvec,:,:]\n        else:\n            self.C = self.C[:,:self.dtalpha,:,:]\n        self.m_max,self._t_curr = self.C.shape[-2:]\n        if t is None: t = int(np.ceil(-np.log(2**(-63))/np.log(self.bases.min())))\n        self.t = self.m_max if self.m_max&gt;t else t\n        assert (0&lt;=self.C).all()\n        assert (self.C&lt;self.bases[:,:,None,None]).all()\n        self.C = np.ascontiguousarray(self.C) \n        self.bases = np.ascontiguousarray(self.bases)\n    if self.alpha&gt;1:\n        assert (self.bases==self.bases[0,0]).all(), \"alpha&gt;1 performs digital interlacing which requires the same base across dimensions and replications.\"\n        if warn and self.m_max!=self._t_curr:\n            warnings.warn(\"Digital interlacing is often performed on generating matrices with the number of columns (m_max = %d) equal to the number of rows (_t_curr = %d), but this is not the case. Ensure you are NOT setting alpha&gt;1 when generating matrices are already interlaced.\"%(self.m_max,self._t_curr),ParameterWarning)\n    assert self.bases.ndim==2\n    assert self.bases.shape[-1]==self.dtalpha\n    assert self.bases.shape[0]==1 or self.bases.shape[0]==self.replications\n    assert self.C.ndim==4\n    assert self.C.shape[-3:]==(self.dtalpha,self.m_max,self._t_curr)\n    assert self.C.shape[0]==1 or self.C.shape[0]==self.replications\n    r_b = self.bases.shape[0]\n    r_C = self.C.shape[0]\n    if self.randomize==\"FALSE\":\n        if self.alpha&gt;1:\n            t_alpha = min(self.t,self._t_curr*self.alpha)\n            C_ho = np.empty((self.replications,self.d,self.m_max,t_alpha),dtype=np.uint64)\n            qmctoolscl.gdn_interlace(np.uint64(self.replications),np.uint64(self.d),np.uint64(self.m_max),np.uint64(self.dtalpha),np.uint64(self._t_curr),np.uint64(t_alpha),np.uint64(self.alpha),self.C,C_ho)\n            self.C = C_ho\n            self._t_curr = t_alpha\n            self.t = t_alpha\n            self.bases = self.bases[:,:self.d]\n    elif self.randomize==\"DP\":\n        self.perms = qmctoolscl.gdn_get_digital_permutations(self.rng,np.uint64(self.replications),np.uint64(self.d),self.t,np.uint64(r_b),self.bases)\n    elif self.randomize==\"DS\":\n        self.rshift = qmctoolscl.gdn_get_digital_shifts(self.rng,np.uint64(self.replications),np.uint64(self.d),self.t,np.uint64(r_b),self.bases)\n    elif self.randomize in [\"LMS\",\"LMS DS\",\"LMS DP\"]:\n        if self.alpha==1:\n            S = qmctoolscl.gdn_get_linear_scramble_matrix(self.rng,np.uint64(self.replications),np.uint64(self.d),np.uint64(self._t_curr),np.uint64(self.t),np.uint64(r_b),self.bases)\n            C_lms = np.empty((self.replications,self.d,self.m_max,self.t),dtype=np.uint64)\n            qmctoolscl.gdn_linear_matrix_scramble(np.uint64(self.replications),np.uint64(self.d),np.uint64(self.m_max),np.uint64(r_C),np.uint64(r_b),np.uint64(self._t_curr),np.uint64(self.t),self.bases,S,self.C,C_lms,backend=\"c\")\n            self.C = C_lms\n            self._t_curr = self.t\n        else:\n            t_dig = np.ceil(max(self.t/self.alpha,self._t_curr))\n            S = qmctoolscl.gdn_get_linear_scramble_matrix(self.rng,np.uint64(self.replications),np.uint64(self.dtalpha),np.uint64(self._t_curr),np.uint64(t_dig),np.uint64(r_b),self.bases)\n            C_lms = np.empty((self.replications,self.dtalpha,self.m_max,self.t),dtype=np.uint64)\n            qmctoolscl.gdn_linear_matrix_scramble(np.uint64(self.replications),np.uint64(self.dtalpha),np.uint64(self.m_max),np.uint64(r_C),np.uint64(r_b),np.uint64(self._t_curr),np.uint64(t_dig),self.bases,S,self.C,C_lms,backend=\"c\")\n            C_lms_ho = np.empty((self.replications,self.d,self.m_max,self.t),dtype=np.uint64)\n            qmctoolscl.gdn_interlace(np.uint64(self.replications),np.uint64(self.d),np.uint64(self.m_max),np.uint64(self.dtalpha),np.uint64(t_dig),np.uint64(self.t),np.uint64(self.alpha),C_lms,C_lms_ho)\n            self.C = C_lms_ho\n            self._t_curr = self.t\n            self.t = self._t_curr\n            self.bases = self.bases[:,:self.d]\n        if self.randomize==\"LMS DP\":\n            self.perms = qmctoolscl.gdn_get_digital_permutations(self.rng,np.uint64(self.replications),np.uint64(self.d),self.t,np.uint64(r_b),self.bases)\n        elif self.randomize==\"LMS DS\":\n            self.rshift = qmctoolscl.gdn_get_digital_shifts(self.rng,np.uint64(self.replications),np.uint64(self.d),self.t,np.uint64(r_b),self.bases)\n    elif self.randomize==\"NUS\":\n        if self.alpha==1:\n            new_seeds = self._base_seed.spawn(self.replications*self.d)\n            self.rngs = np.array([np.random.Generator(np.random.SFC64(new_seeds[j])) for j in range(self.replications*self.d)]).reshape(self.replications,self.d)\n            self.root_nodes = np.array([qmctoolscl.NUSNode_gdn() for i in range(self.replications*self.d)]).reshape(self.replications,self.d)\n        else:\n            new_seeds = self._base_seed.spawn(self.replications*self.dtalpha)\n            self.rngs = np.array([np.random.Generator(np.random.SFC64(new_seeds[j])) for j in range(self.replications*self.dtalpha)]).reshape(self.replications,self.dtalpha)\n            self.root_nodes = np.array([qmctoolscl.NUSNode_gdn() for i in range(self.replications*self.dtalpha)]).reshape(self.replications,self.dtalpha)\n    assert self.C.ndim==4 and (self.C.shape[0]==1 or self.C.shape[0]==self.replications) and self.C.shape[1]==(self.dtalpha if self.randomize==\"NUS\" else self.d) and self.C.shape[2]==self.m_max and self.C.shape[3]==self._t_curr\n    assert self.bases.ndim==2 and (self.bases.shape[0]==1 or self.bases.shape[0]==self.replications) and self.bases.shape[1]==(self.dtalpha if self.randomize==\"NUS\" else self.d)\n    assert 0&lt;self._t_curr&lt;=self.t&lt;=64\n    if self.randomize==\"FALSE\": assert self.C.shape[0]==self.replications, \"randomize='FALSE' but replications = %d does not equal the number of sets of generating vectors %d\"%(self.replications,self.C.shape[0])\n    if warn and (self.bases==2).all():\n        warnings.warn(\"It is more efficient to use DigitalNetB2 instead of DigitalNetAnyBases when all bases are 2\")\n    self.warn = warn\n</code></pre>"},{"location":"api/discrete_distributions/#iidstduniform","title":"<code>IIDStdUniform</code>","text":"<p>               Bases: <code>AbstractIIDDiscreteDistribution</code></p> <p>IID standard uniform points, a wrapper around <code>numpy.random.rand</code>.</p> Note <ul> <li>Unlike low discrepancy sequence, calling an <code>IIDStdUniform</code> instance gives new samples every time,     e.g., running the first doctest below with <code>dd = Lattice(dimension=2)</code> would give the same 4 points in both calls,     but since we are using an <code>IIDStdUniform</code> instance it gives different points every call.</li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; discrete_distrib = IIDStdUniform(dimension=2,seed=7)\n&gt;&gt;&gt; discrete_distrib(4)\narray([[0.04386058, 0.58727432],\n       [0.3691824 , 0.65212985],\n       [0.69669968, 0.10605352],\n       [0.63025643, 0.13630282]])\n&gt;&gt;&gt; discrete_distrib(4) # gives new samples every time\narray([[0.5968363 , 0.0576251 ],\n       [0.2028797 , 0.22909681],\n       [0.1366783 , 0.75220658],\n       [0.84501765, 0.56269008]])\n&gt;&gt;&gt; discrete_distrib\nIIDStdUniform (AbstractIIDDiscreteDistribution)\n    d               2^(1)\n    replications    1\n    entropy         7\n</code></pre> <p>Replications (implemented for API consistency)</p> <pre><code>&gt;&gt;&gt; x = IIDStdUniform(dimension=3,replications=2,seed=7)(4)\n&gt;&gt;&gt; x.shape\n(2, 4, 3)\n&gt;&gt;&gt; x\narray([[[0.04386058, 0.58727432, 0.3691824 ],\n        [0.65212985, 0.69669968, 0.10605352],\n        [0.63025643, 0.13630282, 0.5968363 ],\n        [0.0576251 , 0.2028797 , 0.22909681]],\n\n       [[0.1366783 , 0.75220658, 0.84501765],\n        [0.56269008, 0.04826852, 0.71308655],\n        [0.80983568, 0.85383675, 0.80475135],\n        [0.6171181 , 0.1239209 , 0.16809479]]])\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>dimension</code> <code>int</code> <p>Dimension of the samples.</p> <code>1</code> <code>replications</code> <code>Union[None, int]</code> <p>Number of randomizations. This is implemented only for API consistency. Equivalent to reshaping samples.</p> <code>None</code> <code>seed</code> <code>Union[None, int, np.random.SeedSeq</code> <p>Seed the random number generator for reproducibility.</p> <code>None</code> Source code in <code>qmcpy/discrete_distribution/iid_std_uniform.py</code> <pre><code>def __init__(self, dimension=1, replications=None, seed=None):\n    r\"\"\"\n    Args:\n        dimension (int): Dimension of the samples.\n        replications (Union[None, int]): Number of randomizations. This is implemented only for API consistency. Equivalent to reshaping samples.\n        seed (Union[None, int, np.random.SeedSeq): Seed the random number generator for reproducibility.\n    \"\"\"\n    super(IIDStdUniform, self).__init__(\n        int(dimension), replications, seed, d_limit=np.inf, n_limit=np.inf\n    )\n    if not (self.dvec == np.arange(self.d)).all():\n        warnings.warn(\"IIDStdUniform does not accomodate dvec\", ParameterWarning)\n</code></pre>"},{"location":"api/discrete_distributions/#uml-specific","title":"UML Specific","text":""},{"location":"api/fast_transforms/","title":"Fast Transforms","text":""},{"location":"api/fast_transforms/#fast-transforms","title":"Fast Transforms","text":""},{"location":"api/fast_transforms/#numpy-compatible","title":"<code>numpy</code> compatible","text":""},{"location":"api/fast_transforms/#fwht","title":"<code>fwht</code>","text":"<p>1 dimensional Fast Walsh Hadamard Transform (FWHT) along the last dimension. Requires the size of the last dimension is a power of 2.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; rng = np.random.Generator(np.random.SFC64(11))\n&gt;&gt;&gt; with np.printoptions(precision=2):\n...     fwht(rng.random(8))\narray([ 1.07, -0.29,  0.12,  0.08,  0.1 , -0.45,  0.1 , -0.03])\n&gt;&gt;&gt; fwht(rng.random((2,3,4,5,8))).shape\n(2, 3, 4, 5, 8)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ndarray</code> <p>Array of samples at which to run FWHT.</p> required <p>Returns:</p> Name Type Description <code>y</code> <code>ndarray</code> <p>FWHT values.</p> Source code in <code>qmcpy/fast_transform/ft.py</code> <pre><code>def fwht(x):\n    r\"\"\"\n    1 dimensional Fast Walsh Hadamard Transform (FWHT) along the last dimension.\n    Requires the size of the last dimension is a power of 2.\n\n    Examples:\n        &gt;&gt;&gt; rng = np.random.Generator(np.random.SFC64(11))\n        &gt;&gt;&gt; with np.printoptions(precision=2):\n        ...     fwht(rng.random(8))\n        array([ 1.07, -0.29,  0.12,  0.08,  0.1 , -0.45,  0.1 , -0.03])\n        &gt;&gt;&gt; fwht(rng.random((2,3,4,5,8))).shape\n        (2, 3, 4, 5, 8)\n\n    Args:\n        x (np.ndarray): Array of samples at which to run FWHT.\n\n    Returns:\n        y (np.ndarray): FWHT values.\n    \"\"\"\n    y = x.copy() + 0.0\n    n = x.shape[-1]\n    if n &lt;= 1:\n        return y\n    assert n &amp; (n - 1) == 0  # require n is a power of 2\n    m = int(np.log2(n))\n    it = np.arange(n, dtype=np.int64).reshape(\n        [2] * m\n    )  # 2 x 2 x ... x 2 array (size 2^m)\n    idx0 = [slice(None)] * (m - 1) + [0]\n    idx1 = [slice(None)] * (m - 1) + [1]\n    for k in range(m):\n        eps0 = it[tuple(idx0[-(k + 1) :])].flatten()\n        eps1 = it[tuple(idx1[-(k + 1) :])].flatten()\n        y0, y1 = y[..., eps0], y[..., eps1]\n        y[..., eps0], y[..., eps1] = (y0 + y1) / np.sqrt(2), (y0 - y1) / np.sqrt(2)\n    return y\n</code></pre>"},{"location":"api/fast_transforms/#fftbr","title":"<code>fftbr</code>","text":"<p>1 dimensional Bit-Reversed-Order (BRO) Fast Fourier Transform (FFT) along the last dimension. Requires the last dimension of x is already in BRO, so we can skip the first step of the decimation-in-time FFT. Requires the size of the last dimension is a power of 2.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; rng = np.random.Generator(np.random.SFC64(11))\n&gt;&gt;&gt; x = rng.random(8)+1j*rng.random(8)\n&gt;&gt;&gt; with np.printoptions(precision=2):\n...     fftbr(x)\narray([ 1.07+1.14j, -0.32+0.26j, -0.27+0.22j, -0.27-0.34j,  0.1 +0.16j,\n       -0.15+0.17j,  0.5 +0.24j,  0.06-0.02j])\n&gt;&gt;&gt; fftbr(rng.random((2,3,4,5,8))+1j*rng.random((2,3,4,5,8))).shape\n(2, 3, 4, 5, 8)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ndarray</code> <p>Array of samples at which to run BRO-FFT.</p> required <p>Returns:</p> Name Type Description <code>y</code> <code>ndarray</code> <p>BRO-FFT values.</p> Source code in <code>qmcpy/fast_transform/ft.py</code> <pre><code>def fftbr(x):\n    r\"\"\"\n    1 dimensional Bit-Reversed-Order (BRO) Fast Fourier Transform (FFT) along the last dimension.\n    Requires the last dimension of x is already in BRO, so we can skip the first step of the decimation-in-time FFT.\n    Requires the size of the last dimension is a power of 2.\n\n    Examples:\n        &gt;&gt;&gt; rng = np.random.Generator(np.random.SFC64(11))\n        &gt;&gt;&gt; x = rng.random(8)+1j*rng.random(8)\n        &gt;&gt;&gt; with np.printoptions(precision=2):\n        ...     fftbr(x)\n        array([ 1.07+1.14j, -0.32+0.26j, -0.27+0.22j, -0.27-0.34j,  0.1 +0.16j,\n               -0.15+0.17j,  0.5 +0.24j,  0.06-0.02j])\n        &gt;&gt;&gt; fftbr(rng.random((2,3,4,5,8))+1j*rng.random((2,3,4,5,8))).shape\n        (2, 3, 4, 5, 8)\n\n    Args:\n        x (np.ndarray): Array of samples at which to run BRO-FFT.\n\n    Returns:\n        y (np.ndarray): BRO-FFT values.\n    \"\"\"\n    n = x.shape[-1]\n    assert n &amp; (n - 1) == 0  # require n is a power of 2\n    m = int(np.log2(n))\n    shape = list(x.shape)\n    ndim = x.ndim\n    twos = [2] * m\n    xs = x.reshape(shape[:-1] + twos)\n    pdims = tuple(\n        itertools.chain(range(ndim - 1), range(m + ndim - 2, ndim - 2, -1))\n    )  # [i for i in range(ndim-1)]+[i+ndim-1 for i in range(m-1,-1,-1)]\n    xrf = np.moveaxis(xs, np.arange(len(pdims)), pdims)\n    xr = np.ascontiguousarray(xrf).reshape(shape)\n    return scipy.fft.fft(xr, norm=\"ortho\")\n</code></pre>"},{"location":"api/fast_transforms/#ifftbr","title":"<code>ifftbr</code>","text":"<p>1 dimensional Bit-Reversed-Order (BRO) Inverse Fast Fourier Transform (IFFT) along the last dimension. Outputs an array in bit-reversed order, so we can skip the last step of the decimation-in-time IFFT. Requires the size of the last dimension is a power of 2.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; rng = np.random.Generator(np.random.SFC64(11))\n&gt;&gt;&gt; x = rng.random(8)+1j*rng.random(8)\n&gt;&gt;&gt; with np.printoptions(precision=2):\n...     ifftbr(x)\narray([ 1.07+1.14j, -0.29-0.22j,  0.3 +0.06j, -0.09+0.02j,  0.03+0.54j,\n       -0.04-0.33j, -0.19+0.26j, -0.08+0.36j])\n&gt;&gt;&gt; ifftbr(rng.random((2,3,4,5,8))+1j*rng.random((2,3,4,5,8))).shape\n(2, 3, 4, 5, 8)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ndarray</code> <p>Array of samples at which to run BRO-IFFT.</p> required <p>Returns:</p> Name Type Description <code>y</code> <code>ndarray</code> <p>BRO-IFFT values.</p> Source code in <code>qmcpy/fast_transform/ft.py</code> <pre><code>def ifftbr(x):\n    r\"\"\"\n    1 dimensional Bit-Reversed-Order (BRO) Inverse Fast Fourier Transform (IFFT) along the last dimension.\n    Outputs an array in bit-reversed order, so we can skip the last step of the decimation-in-time IFFT.\n    Requires the size of the last dimension is a power of 2.\n\n    Examples:\n        &gt;&gt;&gt; rng = np.random.Generator(np.random.SFC64(11))\n        &gt;&gt;&gt; x = rng.random(8)+1j*rng.random(8)\n        &gt;&gt;&gt; with np.printoptions(precision=2):\n        ...     ifftbr(x)\n        array([ 1.07+1.14j, -0.29-0.22j,  0.3 +0.06j, -0.09+0.02j,  0.03+0.54j,\n               -0.04-0.33j, -0.19+0.26j, -0.08+0.36j])\n        &gt;&gt;&gt; ifftbr(rng.random((2,3,4,5,8))+1j*rng.random((2,3,4,5,8))).shape\n        (2, 3, 4, 5, 8)\n\n    Args:\n        x (np.ndarray): Array of samples at which to run BRO-IFFT.\n\n    Returns:\n        y (np.ndarray): BRO-IFFT values.\n    \"\"\"\n    n = x.shape[-1]\n    assert n &amp; (n - 1) == 0  # require n is a power of 2\n    m = int(np.log2(n))\n    shape = list(x.shape)\n    ndim = x.ndim\n    twos = [2] * m\n    x = scipy.fft.ifft(x, norm=\"ortho\")\n    xs = x.reshape(shape[:-1] + twos)\n    pdims = tuple(itertools.chain(range(ndim - 1), range(m + ndim - 2, ndim - 2, -1)))\n    xrf = np.moveaxis(xs, np.arange(len(pdims)), pdims)\n    xr = np.ascontiguousarray(xrf).reshape(shape)\n    return xr\n</code></pre>"},{"location":"api/fast_transforms/#omega_fwht","title":"<code>omega_fwht</code>","text":"<p>A useful when efficiently updating FWHT values after doubling the sample size.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; rng = np.random.Generator(np.random.SFC64(11))\n&gt;&gt;&gt; m = 3\n&gt;&gt;&gt; x1 = rng.random((3,5,7,2**m))\n&gt;&gt;&gt; x2 = rng.random((3,5,7,2**m))\n&gt;&gt;&gt; x = np.concatenate([x1,x2],axis=-1)\n&gt;&gt;&gt; ytrue = fwht(x)\n&gt;&gt;&gt; omega = omega_fwht(m)\n&gt;&gt;&gt; y1 = fwht(x1)\n&gt;&gt;&gt; y2 = fwht(x2)\n&gt;&gt;&gt; y = np.concatenate([y1+omega*y2,y1-omega*y2],axis=-1)/np.sqrt(2)\n&gt;&gt;&gt; np.allclose(y,ytrue)\nTrue\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>m</code> <code>int</code> <p>Size \\(2^m\\) output.</p> required <p>Returns:</p> Name Type Description <code>y</code> <code>ndarray</code> <p>\\(\\left(1\\right)_{k=0}^{2^m}\\).</p> Source code in <code>qmcpy/fast_transform/ft.py</code> <pre><code>def omega_fwht(m):\n    r\"\"\"\n    A useful when efficiently updating FWHT values after doubling the sample size.\n\n    Examples:\n        &gt;&gt;&gt; rng = np.random.Generator(np.random.SFC64(11))\n        &gt;&gt;&gt; m = 3\n        &gt;&gt;&gt; x1 = rng.random((3,5,7,2**m))\n        &gt;&gt;&gt; x2 = rng.random((3,5,7,2**m))\n        &gt;&gt;&gt; x = np.concatenate([x1,x2],axis=-1)\n        &gt;&gt;&gt; ytrue = fwht(x)\n        &gt;&gt;&gt; omega = omega_fwht(m)\n        &gt;&gt;&gt; y1 = fwht(x1)\n        &gt;&gt;&gt; y2 = fwht(x2)\n        &gt;&gt;&gt; y = np.concatenate([y1+omega*y2,y1-omega*y2],axis=-1)/np.sqrt(2)\n        &gt;&gt;&gt; np.allclose(y,ytrue)\n        True\n\n    Args:\n        m (int): Size $2^m$ output.\n\n    Returns:\n        y (np.ndarray): $\\left(1\\right)_{k=0}^{2^m}$.\n    \"\"\"\n    return np.ones(2**m)\n</code></pre>"},{"location":"api/fast_transforms/#omega_fftbr","title":"<code>omega_fftbr</code>","text":"<p>A useful when efficiently updating FFT values after doubling the sample size.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; rng = np.random.Generator(np.random.SFC64(11))\n&gt;&gt;&gt; m = 3\n&gt;&gt;&gt; x1 = rng.random((3,5,7,2**m))+1j*rng.random((3,5,7,2**m))\n&gt;&gt;&gt; x2 = rng.random((3,5,7,2**m))+1j*rng.random((3,5,7,2**m))\n&gt;&gt;&gt; x = np.concatenate([x1,x2],axis=-1)\n&gt;&gt;&gt; ytrue = fftbr(x)\n&gt;&gt;&gt; omega = omega_fftbr(m)\n&gt;&gt;&gt; y1 = fftbr(x1)\n&gt;&gt;&gt; y2 = fftbr(x2)\n&gt;&gt;&gt; y = np.concatenate([y1+omega*y2,y1-omega*y2],axis=-1)/np.sqrt(2)\n&gt;&gt;&gt; np.allclose(y,ytrue)\nTrue\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>m</code> <code>int</code> <p>Size \\(2^m\\) output.</p> required <p>Returns:</p> Name Type Description <code>y</code> <code>ndarray</code> <p>\\(\\left(e^{- \\pi \\mathrm{i} k / 2^m}\\right)_{k=0}^{2^m}\\).</p> Source code in <code>qmcpy/fast_transform/ft.py</code> <pre><code>def omega_fftbr(m):\n    r\"\"\"\n    A useful when efficiently updating FFT values after doubling the sample size.\n\n    Examples:\n        &gt;&gt;&gt; rng = np.random.Generator(np.random.SFC64(11))\n        &gt;&gt;&gt; m = 3\n        &gt;&gt;&gt; x1 = rng.random((3,5,7,2**m))+1j*rng.random((3,5,7,2**m))\n        &gt;&gt;&gt; x2 = rng.random((3,5,7,2**m))+1j*rng.random((3,5,7,2**m))\n        &gt;&gt;&gt; x = np.concatenate([x1,x2],axis=-1)\n        &gt;&gt;&gt; ytrue = fftbr(x)\n        &gt;&gt;&gt; omega = omega_fftbr(m)\n        &gt;&gt;&gt; y1 = fftbr(x1)\n        &gt;&gt;&gt; y2 = fftbr(x2)\n        &gt;&gt;&gt; y = np.concatenate([y1+omega*y2,y1-omega*y2],axis=-1)/np.sqrt(2)\n        &gt;&gt;&gt; np.allclose(y,ytrue)\n        True\n\n    Args:\n        m (int): Size $2^m$ output.\n\n    Returns:\n        y (np.ndarray): $\\left(e^{- \\pi \\mathrm{i} k / 2^m}\\right)_{k=0}^{2^m}$.\n    \"\"\"\n    return np.exp(-np.pi * 1j * np.arange(2**m) / 2**m)\n</code></pre>"},{"location":"api/fast_transforms/#torch-compatible","title":"<code>torch</code> compatible","text":""},{"location":"api/fast_transforms/#fwht_torch","title":"<code>fwht_torch</code>","text":"<p>Torch implementation of the 1 dimensional Fast Walsh Hadamard Transform (FWHT) along the last dimension. Requires the size of the last dimension is a power of 2.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; rng = np.random.Generator(np.random.SFC64(11))\n&gt;&gt;&gt; x = torch.from_numpy(rng.random(8)).float().requires_grad_()\n&gt;&gt;&gt; y = fwht_torch(x)\n&gt;&gt;&gt; with torch._tensor_str.printoptions(precision=2):\n...     y.detach()\ntensor([ 1.07, -0.29,  0.12,  0.08,  0.10, -0.45,  0.10, -0.03])\n&gt;&gt;&gt; v = torch.sum(y**2)\n&gt;&gt;&gt; dvdx = torch.autograd.grad(v,x)[0]\n&gt;&gt;&gt; with torch._tensor_str.printoptions(precision=2):\n...     x.detach()\ntensor([0.25, 0.73, 0.06, 0.61, 0.45, 0.26, 0.35, 0.32])\n&gt;&gt;&gt; print(\"%.4f\"%v.detach())\n1.4694\n&gt;&gt;&gt; with torch._tensor_str.printoptions(precision=2):\n...     dvdx.detach()\ntensor([0.50, 1.47, 0.11, 1.23, 0.90, 0.51, 0.70, 0.64])\n&gt;&gt;&gt; fwht_torch(torch.rand((2,3,4,5,8))).shape\ntorch.Size([2, 3, 4, 5, 8])\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Array of samples at which to run FWHT.</p> required <p>Returns:</p> Name Type Description <code>y</code> <code>Tensor</code> <p>FWHT values.</p> Source code in <code>qmcpy/fast_transform/ft_pytorch.py</code> <pre><code>def fwht_torch(x):\n    r\"\"\"\n    Torch implementation of the 1 dimensional Fast Walsh Hadamard Transform (FWHT) along the last dimension.\n    Requires the size of the last dimension is a power of 2.\n\n    Examples:\n        &gt;&gt;&gt; rng = np.random.Generator(np.random.SFC64(11))\n        &gt;&gt;&gt; x = torch.from_numpy(rng.random(8)).float().requires_grad_()\n        &gt;&gt;&gt; y = fwht_torch(x)\n        &gt;&gt;&gt; with torch._tensor_str.printoptions(precision=2):\n        ...     y.detach()\n        tensor([ 1.07, -0.29,  0.12,  0.08,  0.10, -0.45,  0.10, -0.03])\n        &gt;&gt;&gt; v = torch.sum(y**2)\n        &gt;&gt;&gt; dvdx = torch.autograd.grad(v,x)[0]\n        &gt;&gt;&gt; with torch._tensor_str.printoptions(precision=2):\n        ...     x.detach()\n        tensor([0.25, 0.73, 0.06, 0.61, 0.45, 0.26, 0.35, 0.32])\n        &gt;&gt;&gt; print(\"%.4f\"%v.detach())\n        1.4694\n        &gt;&gt;&gt; with torch._tensor_str.printoptions(precision=2):\n        ...     dvdx.detach()\n        tensor([0.50, 1.47, 0.11, 1.23, 0.90, 0.51, 0.70, 0.64])\n        &gt;&gt;&gt; fwht_torch(torch.rand((2,3,4,5,8))).shape\n        torch.Size([2, 3, 4, 5, 8])\n\n    Args:\n        x (torch.Tensor): Array of samples at which to run FWHT.\n\n    Returns:\n        y (torch.Tensor): FWHT values.\n    \"\"\"\n    return _FWHTB2Ortho.apply(x)\n</code></pre>"},{"location":"api/fast_transforms/#fftbr_torch","title":"<code>fftbr_torch</code>","text":"<p>Torch implementation of the 1 dimensional Bit-Reversed-Order (BRO) Fast Fourier Transform (FFT) along the last dimension. Requires the last dimension of x is already in BRO, so we can skip the first step of the decimation-in-time FFT. Requires the size of the last dimension is a power of 2.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; rng = np.random.Generator(np.random.SFC64(11))\n&gt;&gt;&gt; x = torch.from_numpy(rng.random(8)+1j*rng.random(8)).to(torch.complex64).requires_grad_()\n&gt;&gt;&gt; y = fftbr_torch(x)\n&gt;&gt;&gt; with torch._tensor_str.printoptions(precision=2):\n...     y.detach()\ntensor([ 1.07+1.14j, -0.32+0.26j, -0.27+0.22j, -0.27-0.34j,  0.10+0.16j,\n        -0.15+0.17j,  0.50+0.24j,  0.06-0.02j])\n&gt;&gt;&gt; v = torch.abs(torch.sum(y**2))\n&gt;&gt;&gt; dvdx = torch.autograd.grad(v,x)[0]\n&gt;&gt;&gt; with torch._tensor_str.printoptions(precision=2):\n...     x.detach()\ntensor([0.25+0.65j, 0.73+0.60j, 0.06+0.20j, 0.61+0.39j, 0.45+0.06j, 0.26+0.09j,\n        0.35+0.39j, 0.32+0.85j])\n&gt;&gt;&gt; print(\"%.4f\"%v.detach())\n2.5584\n&gt;&gt;&gt; with torch._tensor_str.printoptions(precision=2):\n...     dvdx.detach()\ntensor([1.30+0.49j, 1.21+1.45j, 0.79+1.22j, 0.41+0.11j, 1.71+0.61j, 0.79+0.69j,\n        0.19+0.51j, 0.12+0.90j])\n&gt;&gt;&gt; fftbr_torch(torch.rand((2,3,4,5,8))+1j*torch.rand((2,3,4,5,8))).shape\ntorch.Size([2, 3, 4, 5, 8])\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Array of samples at which to run BRO-FFT.</p> required <p>Returns:</p> Name Type Description <code>y</code> <code>Tensor</code> <p>BRO-FFT values.</p> Source code in <code>qmcpy/fast_transform/ft_pytorch.py</code> <pre><code>def fftbr_torch(x):\n    r\"\"\"\n    Torch implementation of the 1 dimensional Bit-Reversed-Order (BRO) Fast Fourier Transform (FFT) along the last dimension.\n    Requires the last dimension of x is already in BRO, so we can skip the first step of the decimation-in-time FFT.\n    Requires the size of the last dimension is a power of 2.\n\n    Examples:\n        &gt;&gt;&gt; rng = np.random.Generator(np.random.SFC64(11))\n        &gt;&gt;&gt; x = torch.from_numpy(rng.random(8)+1j*rng.random(8)).to(torch.complex64).requires_grad_()\n        &gt;&gt;&gt; y = fftbr_torch(x)\n        &gt;&gt;&gt; with torch._tensor_str.printoptions(precision=2):\n        ...     y.detach()\n        tensor([ 1.07+1.14j, -0.32+0.26j, -0.27+0.22j, -0.27-0.34j,  0.10+0.16j,\n                -0.15+0.17j,  0.50+0.24j,  0.06-0.02j])\n        &gt;&gt;&gt; v = torch.abs(torch.sum(y**2))\n        &gt;&gt;&gt; dvdx = torch.autograd.grad(v,x)[0]\n        &gt;&gt;&gt; with torch._tensor_str.printoptions(precision=2):\n        ...     x.detach()\n        tensor([0.25+0.65j, 0.73+0.60j, 0.06+0.20j, 0.61+0.39j, 0.45+0.06j, 0.26+0.09j,\n                0.35+0.39j, 0.32+0.85j])\n        &gt;&gt;&gt; print(\"%.4f\"%v.detach())\n        2.5584\n        &gt;&gt;&gt; with torch._tensor_str.printoptions(precision=2):\n        ...     dvdx.detach()\n        tensor([1.30+0.49j, 1.21+1.45j, 0.79+1.22j, 0.41+0.11j, 1.71+0.61j, 0.79+0.69j,\n                0.19+0.51j, 0.12+0.90j])\n        &gt;&gt;&gt; fftbr_torch(torch.rand((2,3,4,5,8))+1j*torch.rand((2,3,4,5,8))).shape\n        torch.Size([2, 3, 4, 5, 8])\n\n    Args:\n        x (torch.Tensor): Array of samples at which to run BRO-FFT.\n\n    Returns:\n        y (torch.Tensor): BRO-FFT values.\n    \"\"\"\n    n = x.size(-1)\n    assert n &amp; (n - 1) == 0  # require n is a power of 2\n    m = int(np.log2(n))\n    shape = list(x.shape)\n    ndim = x.ndim\n    twos = [2] * m\n    xs = x.reshape(shape[:-1] + twos)\n    pdims = tuple(\n        itertools.chain(range(ndim - 1), range(m + ndim - 2, ndim - 2, -1))\n    )  # [i for i in range(ndim-1)]+[i+ndim-1 for i in range(m-1,-1,-1)]\n    xrf = torch.permute(xs, pdims)\n    xr = xrf.contiguous().view(shape)\n    return torch.fft.fft(xr, norm=\"ortho\")\n</code></pre>"},{"location":"api/fast_transforms/#ifftbr_torch","title":"<code>ifftbr_torch</code>","text":"<p>Torch implementation of the 1 dimensional Bit-Reversed-Order (BRO) Inverse Fast Fourier Transform (IFFT) along the last dimension. Outputs an array in bit-reversed order, so we can skip the last step of the decimation-in-time IFFT. Requires the size of the last dimension is a power of 2.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; rng = np.random.Generator(np.random.SFC64(11))\n&gt;&gt;&gt; x = torch.from_numpy(rng.random(8)+1j*rng.random(8)).to(torch.complex64).requires_grad_()\n&gt;&gt;&gt; y = ifftbr_torch(x)\n&gt;&gt;&gt; with torch._tensor_str.printoptions(precision=2):\n...     y.detach()\ntensor([ 1.07+1.14j, -0.29-0.22j,  0.30+0.06j, -0.09+0.02j,  0.03+0.54j,\n        -0.04-0.33j, -0.19+0.26j, -0.08+0.36j])\n&gt;&gt;&gt; v = torch.abs(torch.sum(y**2))\n&gt;&gt;&gt; dvdx = torch.autograd.grad(v,x)[0]\n&gt;&gt;&gt; with torch._tensor_str.printoptions(precision=2):\n...     x.detach()\ntensor([0.25+0.65j, 0.73+0.60j, 0.06+0.20j, 0.61+0.39j, 0.45+0.06j, 0.26+0.09j,\n        0.35+0.39j, 0.32+0.85j])\n&gt;&gt;&gt; print(\"%.4f\"%v.detach())\n2.5656\n&gt;&gt;&gt; with torch._tensor_str.printoptions(precision=2):\n...     dvdx.detach()\ntensor([ 1.15+0.79j,  1.51+1.01j,  0.60+0.86j,  0.06+0.54j, -0.10+0.90j,  0.47+1.37j,\n         0.37+0.20j,  0.83+1.70j])\n&gt;&gt;&gt; ifftbr_torch(torch.rand((2,3,4,5,8))+1j*torch.rand((2,3,4,5,8))).shape\ntorch.Size([2, 3, 4, 5, 8])\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Array of samples at which to run BRO-IFFT.</p> required <p>Returns:</p> Name Type Description <code>y</code> <code>Tensor</code> <p>BRO-IFFT values.</p> Source code in <code>qmcpy/fast_transform/ft_pytorch.py</code> <pre><code>def ifftbr_torch(x):\n    r\"\"\"\n    Torch implementation of the 1 dimensional Bit-Reversed-Order (BRO) Inverse Fast Fourier Transform (IFFT) along the last dimension.\n    Outputs an array in bit-reversed order, so we can skip the last step of the decimation-in-time IFFT.\n    Requires the size of the last dimension is a power of 2.\n\n    Examples:\n        &gt;&gt;&gt; rng = np.random.Generator(np.random.SFC64(11))\n        &gt;&gt;&gt; x = torch.from_numpy(rng.random(8)+1j*rng.random(8)).to(torch.complex64).requires_grad_()\n        &gt;&gt;&gt; y = ifftbr_torch(x)\n        &gt;&gt;&gt; with torch._tensor_str.printoptions(precision=2):\n        ...     y.detach()\n        tensor([ 1.07+1.14j, -0.29-0.22j,  0.30+0.06j, -0.09+0.02j,  0.03+0.54j,\n                -0.04-0.33j, -0.19+0.26j, -0.08+0.36j])\n        &gt;&gt;&gt; v = torch.abs(torch.sum(y**2))\n        &gt;&gt;&gt; dvdx = torch.autograd.grad(v,x)[0]\n        &gt;&gt;&gt; with torch._tensor_str.printoptions(precision=2):\n        ...     x.detach()\n        tensor([0.25+0.65j, 0.73+0.60j, 0.06+0.20j, 0.61+0.39j, 0.45+0.06j, 0.26+0.09j,\n                0.35+0.39j, 0.32+0.85j])\n        &gt;&gt;&gt; print(\"%.4f\"%v.detach())\n        2.5656\n        &gt;&gt;&gt; with torch._tensor_str.printoptions(precision=2):\n        ...     dvdx.detach()\n        tensor([ 1.15+0.79j,  1.51+1.01j,  0.60+0.86j,  0.06+0.54j, -0.10+0.90j,  0.47+1.37j,\n                 0.37+0.20j,  0.83+1.70j])\n        &gt;&gt;&gt; ifftbr_torch(torch.rand((2,3,4,5,8))+1j*torch.rand((2,3,4,5,8))).shape\n        torch.Size([2, 3, 4, 5, 8])\n\n    Args:\n        x (torch.Tensor): Array of samples at which to run BRO-IFFT.\n\n    Returns:\n        y (torch.Tensor): BRO-IFFT values.\n    \"\"\"\n    n = x.size(-1)\n    assert n &amp; (n - 1) == 0  # require n is a power of 2\n    m = int(np.log2(n))\n    shape = list(x.shape)\n    ndim = x.ndim\n    twos = [2] * m\n    x = torch.fft.ifft(x, norm=\"ortho\")\n    xs = x.reshape(shape[:-1] + twos)\n    pdims = tuple(itertools.chain(range(ndim - 1), range(m + ndim - 2, ndim - 2, -1)))\n    xrf = torch.permute(xs, pdims)\n    xr = xrf.contiguous().view(shape)\n    return xr\n</code></pre>"},{"location":"api/fast_transforms/#omega_fwht_1","title":"<code>omega_fwht</code>","text":"<p>Torch implementation useful when efficiently updating FWHT values after doubling the sample size.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; rng = np.random.Generator(np.random.SFC64(11))\n&gt;&gt;&gt; m = 3\n&gt;&gt;&gt; x1 = torch.from_numpy(rng.random((3,5,7,2**m)))\n&gt;&gt;&gt; x2 = torch.from_numpy(rng.random((3,5,7,2**m)))\n&gt;&gt;&gt; x = torch.cat([x1,x2],axis=-1)\n&gt;&gt;&gt; ytrue = fwht_torch(x)\n&gt;&gt;&gt; omega = omega_fwht_torch(m)\n&gt;&gt;&gt; y1 = fwht_torch(x1)\n&gt;&gt;&gt; y2 = fwht_torch(x2)\n&gt;&gt;&gt; y = torch.cat([y1+omega*y2,y1-omega*y2],axis=-1)/np.sqrt(2)\n&gt;&gt;&gt; np.allclose(y,ytrue)\nTrue\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>m</code> <code>int</code> <p>Size \\(2^m\\) output.</p> required <p>Returns:</p> Name Type Description <code>y</code> <code>ndarray</code> <p>\\(\\left(1\\right)_{k=0}^{2^m}\\).</p> Source code in <code>qmcpy/fast_transform/ft_pytorch.py</code> <pre><code>def omega_fwht_torch(m, device=None):\n    r\"\"\"\n    Torch implementation useful when efficiently updating FWHT values after doubling the sample size.\n\n    Examples:\n        &gt;&gt;&gt; rng = np.random.Generator(np.random.SFC64(11))\n        &gt;&gt;&gt; m = 3\n        &gt;&gt;&gt; x1 = torch.from_numpy(rng.random((3,5,7,2**m)))\n        &gt;&gt;&gt; x2 = torch.from_numpy(rng.random((3,5,7,2**m)))\n        &gt;&gt;&gt; x = torch.cat([x1,x2],axis=-1)\n        &gt;&gt;&gt; ytrue = fwht_torch(x)\n        &gt;&gt;&gt; omega = omega_fwht_torch(m)\n        &gt;&gt;&gt; y1 = fwht_torch(x1)\n        &gt;&gt;&gt; y2 = fwht_torch(x2)\n        &gt;&gt;&gt; y = torch.cat([y1+omega*y2,y1-omega*y2],axis=-1)/np.sqrt(2)\n        &gt;&gt;&gt; np.allclose(y,ytrue)\n        True\n\n    Args:\n        m (int): Size $2^m$ output.\n\n    Returns:\n        y (np.ndarray): $\\left(1\\right)_{k=0}^{2^m}$.\n    \"\"\"\n    if device is None:\n        device = \"cpu\"\n    return torch.ones(2**m, device=device)\n</code></pre>"},{"location":"api/fast_transforms/#omega_fftbr_torch","title":"<code>omega_fftbr_torch</code>","text":"<p>Torch implementation useful when efficiently updating FFT values after doubling the sample size.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; rng = np.random.Generator(np.random.SFC64(11))\n&gt;&gt;&gt; m = 3\n&gt;&gt;&gt; x1 = torch.from_numpy(rng.random((3,5,7,2**m))+1j*rng.random((3,5,7,2**m)))\n&gt;&gt;&gt; x2 = torch.from_numpy(rng.random((3,5,7,2**m))+1j*rng.random((3,5,7,2**m)))\n&gt;&gt;&gt; x = torch.cat([x1,x2],axis=-1)\n&gt;&gt;&gt; ytrue = fftbr_torch(x)\n&gt;&gt;&gt; omega = omega_fftbr_torch(m)\n&gt;&gt;&gt; y1 = fftbr_torch(x1)\n&gt;&gt;&gt; y2 = fftbr_torch(x2)\n&gt;&gt;&gt; y = torch.cat([y1+omega*y2,y1-omega*y2],axis=-1)/np.sqrt(2)\n&gt;&gt;&gt; np.allclose(y,ytrue)\nTrue\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>m</code> <code>int</code> <p>Size \\(2^m\\) output.</p> required <p>Returns:</p> Name Type Description <code>y</code> <code>ndarray</code> <p>\\(\\left(e^{- \\pi \\mathrm{i} k / 2^m}\\right)_{k=0}^{2^m}\\).</p> Source code in <code>qmcpy/fast_transform/ft_pytorch.py</code> <pre><code>def omega_fftbr_torch(m, device=None):\n    r\"\"\"\n    Torch implementation useful when efficiently updating FFT values after doubling the sample size.\n\n    Examples:\n        &gt;&gt;&gt; rng = np.random.Generator(np.random.SFC64(11))\n        &gt;&gt;&gt; m = 3\n        &gt;&gt;&gt; x1 = torch.from_numpy(rng.random((3,5,7,2**m))+1j*rng.random((3,5,7,2**m)))\n        &gt;&gt;&gt; x2 = torch.from_numpy(rng.random((3,5,7,2**m))+1j*rng.random((3,5,7,2**m)))\n        &gt;&gt;&gt; x = torch.cat([x1,x2],axis=-1)\n        &gt;&gt;&gt; ytrue = fftbr_torch(x)\n        &gt;&gt;&gt; omega = omega_fftbr_torch(m)\n        &gt;&gt;&gt; y1 = fftbr_torch(x1)\n        &gt;&gt;&gt; y2 = fftbr_torch(x2)\n        &gt;&gt;&gt; y = torch.cat([y1+omega*y2,y1-omega*y2],axis=-1)/np.sqrt(2)\n        &gt;&gt;&gt; np.allclose(y,ytrue)\n        True\n\n    Args:\n        m (int): Size $2^m$ output.\n\n    Returns:\n        y (np.ndarray): $\\left(e^{- \\pi \\mathrm{i} k / 2^m}\\right)_{k=0}^{2^m}$.\n    \"\"\"\n    if device is None:\n        device = \"cpu\"\n    return torch.exp(-torch.pi * 1j * torch.arange(2**m, device=device) / 2**m)\n</code></pre>"},{"location":"api/integrands/","title":"Integrands","text":""},{"location":"api/integrands/#integrands","title":"Integrands","text":""},{"location":"api/integrands/#uml-overview","title":"UML Overview","text":""},{"location":"api/integrands/#abstractintegrand","title":"<code>AbstractIntegrand</code>","text":"<p>               Bases: <code>object</code></p> <p>Parameters:</p> Name Type Description Default <code>dimension_indv</code> <code>tuple</code> <p>Individual solution shape.</p> required <code>dimension_comb</code> <code>tuple</code> <p>Combined solution shape.</p> required <code>parallel</code> <code>int</code> <p>Parallelization flag.</p> <ul> <li>When <code>parallel = 0</code> or <code>parallel = 1</code> then function evaluation is done in serial fashion.</li> <li><code>parallel &gt; 1</code> specifies the number of processes used by <code>multiprocessing.Pool</code> or <code>multiprocessing.pool.ThreadPool</code>.</li> </ul> <p>Setting <code>parallel=True</code> is equivalent to <code>parallel = os.cpu_count()</code>.</p> required <code>threadpool</code> <code>bool</code> <p>When <code>parallel &gt; 1</code>:</p> <ul> <li>Setting <code>threadpool = True</code> will use <code>multiprocessing.pool.ThreadPool</code>.</li> <li>Setting <code>threadpool = False</code> will use <code>setting multiprocessing.Pool</code>.</li> </ul> <code>False</code> Source code in <code>qmcpy/integrand/abstract_integrand.py</code> <pre><code>def __init__(self, dimension_indv, dimension_comb, parallel, threadpool=False):\n    r\"\"\"\n    Args:\n        dimension_indv (tuple): Individual solution shape.\n        dimension_comb (tuple): Combined solution shape.\n        parallel (int): Parallelization flag.\n\n            - When `parallel = 0` or `parallel = 1` then function evaluation is done in serial fashion.\n            - `parallel &gt; 1` specifies the number of processes used by `multiprocessing.Pool` or `multiprocessing.pool.ThreadPool`.\n\n            Setting `parallel=True` is equivalent to `parallel = os.cpu_count()`.\n        threadpool (bool): When `parallel &gt; 1`:\n\n            - Setting `threadpool = True` will use `multiprocessing.pool.ThreadPool`.\n            - Setting `threadpool = False` will use `setting multiprocessing.Pool`.\n    \"\"\"\n    prefix = \"A concrete implementation of AbstractIntegrand must have \"\n    self.d = self.true_measure.d\n    self.d_indv = (\n        (dimension_indv,)\n        if isinstance(dimension_indv, int)\n        else tuple(dimension_indv)\n    )\n    self.d_comb = (\n        (dimension_comb,)\n        if isinstance(dimension_comb, int)\n        else tuple(dimension_comb)\n    )\n    cpus = os.cpu_count()\n    self.parallel = cpus if parallel is True else int(parallel)\n    self.parallel = 0 if self.parallel == 1 else self.parallel\n    self.threadpool = threadpool\n    if not (\n        hasattr(self, \"sampler\")\n        and isinstance(\n            self.sampler, (AbstractTrueMeasure, AbstractDiscreteDistribution)\n        )\n    ):\n        raise ParameterError(\n            prefix\n            + \"self.sampler, a AbstractTrueMeasure or AbstractDiscreteDistribution instance\"\n        )\n    if not (\n        hasattr(self, \"true_measure\")\n        and isinstance(self.true_measure, AbstractTrueMeasure)\n    ):\n        raise ParameterError(\n            prefix + \"self.true_measure, a AbstractTrueMeasure instance\"\n        )\n    if not hasattr(self, \"parameters\"):\n        self.parameters = []\n    if not hasattr(self, \"multilevel\"):\n        self.multilevel = False\n    assert isinstance(self.multilevel, bool)\n    if not hasattr(self, \"max_level\"):\n        self.max_level = np.inf\n    if not hasattr(self, \"discrete_distrib\"):\n        self.discrete_distrib = self.true_measure.discrete_distrib\n    if (\n        self.true_measure.transform != self.true_measure\n        and not (self.true_measure.range == self.true_measure.transform.range).all()\n    ):\n        raise ParameterError(\n            \"The range of the composed transform is not compatible with this true measure\"\n        )\n    self.EPS = np.finfo(float).eps\n</code></pre>"},{"location":"api/integrands/#qmcpy.integrand.abstract_integrand.AbstractIntegrand.__call__","title":"__call__","text":"<pre><code>__call__(n=None, n_min=None, n_max=None, warn=True)\n</code></pre> <ul> <li>If just <code>n</code> is supplied, generate samples from the sequence at indices 0,...,<code>n</code>-1.</li> <li>If <code>n_min</code> and <code>n_max</code> are supplied, generate samples from the sequence at indices <code>n_min</code>,...,<code>n_max</code>-1.</li> <li>If <code>n</code> and <code>n_min</code> are supplied, then generate samples from the sequence at indices <code>n</code>,...,<code>n_min</code>-1.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>Union[None, int]</code> <p>Number of points to generate.</p> <code>None</code> <code>n_min</code> <code>Union[None, int]</code> <p>Starting index of sequence.</p> <code>None</code> <code>n_max</code> <code>Union[None, int]</code> <p>Final index of sequence.</p> <code>None</code> <code>warn</code> <code>bool</code> <p>If <code>False</code>, disable warnings when generating samples.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>t</code> <code>ndarray</code> <p>Samples from the sequence.</p> <ul> <li>If <code>replications</code> is <code>None</code> then this will be of size (<code>n_max</code>-<code>n_min</code>) \\(\\times\\) <code>dimension</code></li> <li>If <code>replications</code> is a positive int, then <code>t</code> will be of size <code>replications</code> \\(\\times\\) (<code>n_max</code>-<code>n_min</code>) \\(\\times\\) <code>dimension</code></li> </ul> <code>weights</code> <code>ndarray</code> <p>Only returned when <code>return_weights=True</code>. The Jacobian weights for the transformation</p> Source code in <code>qmcpy/integrand/abstract_integrand.py</code> <pre><code>def __call__(self, n=None, n_min=None, n_max=None, warn=True):\n    r\"\"\"\n    - If just `n` is supplied, generate samples from the sequence at indices 0,...,`n`-1.\n    - If `n_min` and `n_max` are supplied, generate samples from the sequence at indices `n_min`,...,`n_max`-1.\n    - If `n` and `n_min` are supplied, then generate samples from the sequence at indices `n`,...,`n_min`-1.\n\n    Args:\n        n (Union[None, int]): Number of points to generate.\n        n_min (Union[None, int]): Starting index of sequence.\n        n_max (Union[None, int]): Final index of sequence.\n        warn (bool): If `False`, disable warnings when generating samples.\n\n    Returns:\n        t (np.ndarray): Samples from the sequence.\n\n            - If `replications` is `None` then this will be of size (`n_max`-`n_min`) $\\times$ `dimension`\n            - If `replications` is a positive int, then `t` will be of size `replications` $\\times$ (`n_max`-`n_min`) $\\times$ `dimension`\n        weights (np.ndarray): Only returned when `return_weights=True`. The Jacobian weights for the transformation\n    \"\"\"\n    return self.gen_samples(n=n, n_min=n_min, n_max=n_max, warn=warn)\n</code></pre>"},{"location":"api/integrands/#qmcpy.integrand.abstract_integrand.AbstractIntegrand.g","title":"g","text":"<pre><code>g(t, *args, **kwargs)\n</code></pre> <p>Abstract method implementing the integrand as a function of the true measure.</p> <p>Parameters:</p> Name Type Description Default <code>t</code> <code>ndarray</code> <p>Inputs with shape <code>(*batch_shape, d)</code>.</p> required <code>args</code> <code>tuple</code> <p>positional arguments to <code>g</code>.</p> <code>()</code> <code>kwargs</code> <code>dict</code> <p>keyword arguments to <code>g</code>.</p> <p>Some algorithms will additionally try to pass in a <code>compute_flags</code> keyword argument. This <code>np.ndarray</code> are flags indicating which outputs require evaluation. For example, if the vector function has 3 outputs and <code>compute_flags = [False, True, False]</code>, then the function is only required to evaluate the second output and may leave the remaining outputs as <code>np.nan</code> values, i.e., the outputs corresponding to <code>compute_flags</code> which are <code>False</code> will not be used in the computation.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>y</code> <code>ndarray</code> <p>function evaluations with shape <code>(*batch_shape, *dimension_indv)</code> where <code>dimension_indv</code> is the shape of the function outputs.</p> Source code in <code>qmcpy/integrand/abstract_integrand.py</code> <pre><code>def g(self, t, *args, **kwargs):\n    r\"\"\"\n    *Abstract method* implementing the integrand as a function of the true measure.\n\n    Args:\n        t (np.ndarray): Inputs with shape `(*batch_shape, d)`.\n        args (tuple): positional arguments to `g`.\n        kwargs (dict): keyword arguments to `g`.\n\n            Some algorithms will additionally try to pass in a `compute_flags` keyword argument.\n            This `np.ndarray` are flags indicating which outputs require evaluation.\n            For example, if the vector function has 3 outputs and `compute_flags = [False, True, False]`,\n            then the function is only required to evaluate the second output and may leave the remaining outputs as `np.nan` values,\n            i.e., the outputs corresponding to `compute_flags` which are `False` will not be used in the computation.\n\n    Returns:\n        y (np.ndarray): function evaluations with shape `(*batch_shape, *dimension_indv)` where `dimension_indv` is the shape of the function outputs.\n    \"\"\"\n    raise MethodImplementationError(self, \"g\")\n</code></pre>"},{"location":"api/integrands/#qmcpy.integrand.abstract_integrand.AbstractIntegrand.f","title":"f","text":"<pre><code>f(x, *args, **kwargs)\n</code></pre> <p>Function to evaluate the transformed integrand as a function of the discrete distribution. Automatically applies the transformation determined by the true measure.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ndarray</code> <p>Inputs with shape <code>(*batch_shape, d)</code>.</p> required <code>args</code> <code>tuple</code> <p>positional arguments to <code>g</code>.</p> <code>()</code> <code>kwargs</code> <code>dict</code> <p>keyword arguments to <code>g</code>.</p> <p>Some algorithms will additionally try to pass in a <code>compute_flags</code> keyword argument. This <code>np.ndarray</code> are flags indicating which outputs require evaluation. For example, if the vector function has 3 outputs and <code>compute_flags = [False, True, False]</code>, then the function is only required to evaluate the second output and may leave the remaining outputs as <code>np.nan</code> values, i.e., the outputs corresponding to <code>compute_flags</code> which are <code>False</code> will not be used in the computation.</p> <p>The keyword argument <code>periodization_transform</code>, a string, specifies a periodization transform. Options are:</p> <ul> <li><code>False</code>: No periodizing transform, \\(\\psi(x) = x\\).</li> <li><code>'BAKER'</code>: Baker tansform \\(\\psi(x) = 1-2\\lvert x-1/2 \\rvert\\).</li> <li><code>'C0'</code>: \\(C^0\\) transform \\(\\psi(x) = 3x^2-2x^3\\).</li> <li><code>'C1'</code>: \\(C^1\\) transform \\(\\psi(x) = x^3(10-15x+6x^2)\\).</li> <li><code>'C1SIN'</code>: Sidi \\(C^1\\) transform \\(\\psi(x) = x-\\sin(2 \\pi x)/(2 \\pi)\\).</li> <li><code>'C2SIN'</code>: Sidi \\(C^2\\) transform \\(\\psi(x) = (8-9 \\cos(\\pi x)+\\cos(3 \\pi x))/16\\).</li> <li><code>'C3SIN'</code>: Sidi \\(C^3\\) transform \\(\\psi(x) = (12\\pi x-8\\sin(2 \\pi x) + \\sin(4 \\pi x))/(12 \\pi)\\).</li> </ul> <code>{}</code> <p>Returns:</p> Name Type Description <code>y</code> <code>ndarray</code> <p>function evaluations with shape <code>(*batch_shape, *dimension_indv)</code> where <code>dimension_indv</code> is the shape of the function outputs.</p> Source code in <code>qmcpy/integrand/abstract_integrand.py</code> <pre><code>def f(self, x, *args, **kwargs):\n    r\"\"\"\n    Function to evaluate the transformed integrand as a function of the discrete distribution.\n    Automatically applies the transformation determined by the true measure.\n\n    Args:\n        x (np.ndarray): Inputs with shape `(*batch_shape, d)`.\n        args (tuple): positional arguments to `g`.\n        kwargs (dict): keyword arguments to `g`.\n\n            Some algorithms will additionally try to pass in a `compute_flags` keyword argument.\n            This `np.ndarray` are flags indicating which outputs require evaluation.\n            For example, if the vector function has 3 outputs and `compute_flags = [False, True, False]`,\n            then the function is only required to evaluate the second output and may leave the remaining outputs as `np.nan` values,\n            i.e., the outputs corresponding to `compute_flags` which are `False` will not be used in the computation.\n\n            The keyword argument `periodization_transform`, a string, specifies a periodization transform.\n            Options are:\n\n            - `False`: No periodizing transform, $\\psi(x) = x$.\n            - `'BAKER'`: Baker tansform $\\psi(x) = 1-2\\lvert x-1/2 \\rvert$.\n            - `'C0'`: $C^0$ transform $\\psi(x) = 3x^2-2x^3$.\n            - `'C1'`: $C^1$ transform $\\psi(x) = x^3(10-15x+6x^2)$.\n            - `'C1SIN'`: Sidi $C^1$ transform $\\psi(x) = x-\\sin(2 \\pi x)/(2 \\pi)$.\n            - `'C2SIN'`: Sidi $C^2$ transform $\\psi(x) = (8-9 \\cos(\\pi x)+\\cos(3 \\pi x))/16$.\n            - `'C3SIN'`: Sidi $C^3$ transform $\\psi(x) = (12\\pi x-8\\sin(2 \\pi x) + \\sin(4 \\pi x))/(12 \\pi)$.\n\n    Returns:\n        y (np.ndarray): function evaluations with shape `(*batch_shape, *dimension_indv)` where `dimension_indv` is the shape of the function outputs.\n    \"\"\"\n    if \"periodization_transform\" in kwargs:\n        periodization_transform = kwargs[\"periodization_transform\"]\n        del kwargs[\"periodization_transform\"]\n    else:\n        periodization_transform = \"None\"\n    periodization_transform = str(periodization_transform).upper()\n    batch_shape = tuple(x.shape[:-1])\n    d_indv_ndim = len(self.d_indv)\n    if periodization_transform == \"NONE\":\n        periodization_transform = \"FALSE\"\n    if (\n        self.discrete_distrib.mimics != \"StdUniform\"\n        and periodization_transform != \"NONE\"\n    ):\n        raise ParameterError(\n            \"\"\"\n            Applying a periodization transform currently requires a discrete distribution \n            that mimics a standard uniform measure.\"\"\"\n        )\n    if periodization_transform == \"FALSE\":\n        xp = x\n        wp = np.ones(batch_shape, dtype=float)\n    elif periodization_transform == \"BAKER\":\n        xp = 1 - 2 * abs(x - 1 / 2)\n        wp = np.ones(batch_shape, dtype=float)\n    elif periodization_transform == \"C0\":\n        xp = 3 * x**2 - 2 * x**3\n        wp = np.prod(6 * x * (1 - x), -1)\n    elif periodization_transform == \"C1\":\n        xp = x**3 * (10 - 15 * x + 6 * x**2)\n        wp = np.prod(30 * x**2 * (1 - x) ** 2, -1)\n    elif periodization_transform == \"C1SIN\":\n        xp = x - np.sin(2 * np.pi * x) / (2 * np.pi)\n        wp = np.prod(2 * np.sin(np.pi * x) ** 2, -1)\n    elif periodization_transform == \"C2SIN\":\n        xp = (8 - 9 * np.cos(np.pi * x) + np.cos(3 * np.pi * x)) / 16\n        wp = np.prod(\n            (9 * np.sin(np.pi * x) * np.pi - np.sin(3 * np.pi * x) * 3 * np.pi)\n            / 16,\n            -1,\n        )\n    elif periodization_transform == \"C3SIN\":\n        xp = (\n            12 * np.pi * x - 8 * np.sin(2 * np.pi * x) + np.sin(4 * np.pi * x)\n        ) / (12 * np.pi)\n        wp = np.prod(\n            (\n                12 * np.pi\n                - 8 * np.cos(2 * np.pi * x) * 2 * np.pi\n                + np.sin(4 * np.pi * x) * 4 * np.pi\n            )\n            / (12 * np.pi),\n            -1,\n        )\n    else:\n        raise ParameterError(\n            \"The %s periodization transform is not implemented\"\n            % periodization_transform\n        )\n    if periodization_transform in [\"C1\", \"C1SIN\", \"C2SIN\", \"C3SIN\"]:\n        xp[xp &lt;= 0] = self.EPS\n        xp[xp &gt;= 1] = 1 - self.EPS\n    assert wp.shape == batch_shape\n    assert xp.shape == x.shape\n    # function evaluation with chain rule\n    i = (None,) * d_indv_ndim + (...,)\n    if self.true_measure == self.true_measure.transform:\n        # jacobian*weight/pdf will cancel so f(x) = g(\\Psi(x))\n        xtf = self.true_measure._jacobian_transform_r(\n            xp, return_weights=False\n        )  # get transformed samples, equivalent to self.true_measure._transform_r(x)\n        assert xtf.shape == xp.shape\n        y = self._g(xtf, *args, **kwargs)\n    else:  # using importance sampling --&gt; need to compute pdf, jacobian(s), and weight explicitly\n        pdf = self.discrete_distrib.pdf(xp)  # pdf of samples\n        assert pdf.shape == batch_shape\n        xtf, jacobians = self.true_measure.transform._jacobian_transform_r(\n            xp, return_weights=True\n        )  # compute recursive transform+jacobian\n        assert xtf.shape == xp.shape\n        assert jacobians.shape == batch_shape\n        weight = self.true_measure._weight(xtf)  # weight based on the true measure\n        assert weight.shape == batch_shape\n        gvals = self._g(xtf, *args, **kwargs)\n        assert gvals.shape == (self.d_indv + batch_shape)\n        y = gvals * weight[i] / pdf[i] * jacobians[i]\n    assert y.shape == (self.d_indv + batch_shape)\n    # account for periodization weight\n    y = y * wp[i]\n    assert y.shape == (self.d_indv + batch_shape)\n    return y\n</code></pre>"},{"location":"api/integrands/#qmcpy.integrand.abstract_integrand.AbstractIntegrand.bound_fun","title":"bound_fun","text":"<pre><code>bound_fun(bound_low, bound_high)\n</code></pre> <p>Compute the bounds on the combined function based on bounds for the individual functions.</p> <p>Defaults to the identity where we essentially do not combine integrands, but instead integrate each function individually.</p> <p>Parameters:</p> Name Type Description Default <code>bound_low</code> <code>ndarray</code> <p>Lower bounds on individual estimates with shape <code>integrand.d_indv</code>.</p> required <code>bound_high</code> <code>ndarray</code> <p>Upper bounds on individual estimates with shape <code>integrand.d_indv</code>.</p> required <p>Returns:</p> Name Type Description <code>comb_bound_low</code> <code>ndarray</code> <p>Lower bounds on combined estimates with shape <code>integrand.d_comb</code>.</p> <code>comb_bound_high</code> <code>ndarray</code> <p>Upper bounds on combined estimates with shape <code>integrand.d_comb</code>.</p> Source code in <code>qmcpy/integrand/abstract_integrand.py</code> <pre><code>def bound_fun(self, bound_low, bound_high):\n    \"\"\"\n    Compute the bounds on the combined function based on bounds for the\n    individual functions.\n\n    Defaults to the identity where we essentially\n    do not combine integrands, but instead integrate each function\n    individually.\n\n    Args:\n        bound_low (np.ndarray): Lower bounds on individual estimates with shape `integrand.d_indv`.\n        bound_high (np.ndarray): Upper bounds on individual estimates with shape `integrand.d_indv`.\n\n    Returns:\n        comb_bound_low (np.ndarray): Lower bounds on combined estimates with shape `integrand.d_comb`.\n        comb_bound_high (np.ndarray): Upper bounds on combined estimates with shape `integrand.d_comb`.\n    \"\"\"\n    if self.d_indv != self.d_comb:\n        raise ParameterError(\n            \"\"\"\n            Set bound_fun explicitly. \n            The default bound_fun is the identity map. \n            Since the individual solution dimensions d_indv = %s does not equal the combined solution dimensions d_comb = %d, \n            QMCPy cannot infer a reasonable bound function.\"\"\"\n            % (str(self.d_indv), str(self.d_comb))\n        )\n    return bound_low, bound_high\n</code></pre>"},{"location":"api/integrands/#qmcpy.integrand.abstract_integrand.AbstractIntegrand.dependency","title":"dependency","text":"<pre><code>dependency(comb_flags)\n</code></pre> <p>Takes a vector of indicators of weather of not the error bound is satisfied for combined integrands and returns flags for individual integrands.</p> <p>For example, if we are taking the ratio of 2 individual integrands, then getting <code>comb_flags=True</code> means the ratio has not been approximated to within the tolerance, so the dependency function should return <code>indv_flags=[True,True]</code> indicating that both the numerator and denominator integrands need to be better approximated.</p> <p>Parameters:</p> Name Type Description Default <code>comb_flags</code> <code>ndarray</code> <p>Flags of shape <code>integrand.d_comb</code> indicating whether the combined outputs are insufficiently approximated.</p> required <p>Returns:</p> Name Type Description <code>indv_flags</code> <code>ndarray</code> <p>Flags of shape <code>integrand.d_indv</code> indicating whether the individual integrands require additional sampling.</p> Source code in <code>qmcpy/integrand/abstract_integrand.py</code> <pre><code>def dependency(self, comb_flags):\n    \"\"\"\n    Takes a vector of indicators of weather of not the error bound is satisfied for combined integrands and returns flags for individual integrands.\n\n    For example, if we are taking the ratio of 2 individual integrands, then getting `comb_flags=True` means the ratio\n    has not been approximated to within the tolerance, so the dependency function should return `indv_flags=[True,True]`\n    indicating that both the numerator and denominator integrands need to be better approximated.\n\n    Args:\n        comb_flags (np.ndarray): Flags of shape `integrand.d_comb` indicating whether the combined outputs are insufficiently approximated.\n\n    Returns:\n        indv_flags (np.ndarray): Flags of shape `integrand.d_indv` indicating whether the individual integrands require additional sampling.\n    \"\"\"\n    return (\n        comb_flags\n        if self.d_indv == self.d_comb\n        else np.tile((comb_flags == False).any(), self.d_indv)\n    )\n</code></pre>"},{"location":"api/integrands/#qmcpy.integrand.abstract_integrand.AbstractIntegrand.spawn","title":"spawn","text":"<pre><code>spawn(levels)\n</code></pre> <p>Spawn new instances of the current integrand at different levels with new seeds. Used by multi-level QMC algorithms which require integrands at multiple levels.</p> Note <p>Use <code>replications</code> instead of using <code>spawn</code> when possible, e.g., when spawning copies which all have the same level.</p> <p>Parameters:</p> Name Type Description Default <code>levels</code> <code>ndarray</code> <p>Levels at which to spawn new integrands.</p> required <p>Returns:</p> Name Type Description <code>spawned_integrand</code> <code>list</code> <p>Integrands with new true measures and discrete distributions.</p> Source code in <code>qmcpy/integrand/abstract_integrand.py</code> <pre><code>def spawn(self, levels):\n    r\"\"\"\n    Spawn new instances of the current integrand at different levels with new seeds.\n    Used by multi-level QMC algorithms which require integrands at multiple levels.\n\n    Note:\n        Use `replications` instead of using `spawn` when possible, e.g., when spawning copies which all have the same level.\n\n    Args:\n        levels (np.ndarray): Levels at which to spawn new integrands.\n\n    Returns:\n        spawned_integrand (list): Integrands with new true measures and discrete distributions.\n    \"\"\"\n    levels = np.array([levels]) if np.isscalar(levels) else np.array(levels)\n    if (levels &gt; self.max_level).any():\n        raise ParameterError(\"requested spawn level exceeds max level\")\n    n_levels = len(levels)\n    new_dims = np.array([self.dimension_at_level(level) for level in levels])\n    tm_spawns = self.sampler.spawn(s=n_levels, dimensions=new_dims)\n    spawned_integrand = [None] * n_levels\n    for l, level in enumerate(levels):\n        spawned_integrand[l] = self._spawn(level, tm_spawns[l])\n    return spawned_integrand\n</code></pre>"},{"location":"api/integrands/#qmcpy.integrand.abstract_integrand.AbstractIntegrand.dimension_at_level","title":"dimension_at_level","text":"<pre><code>dimension_at_level(level)\n</code></pre> <p>Abstract method which returns the dimension of the generator required for a given level.</p> Note <p>Only used for multilevel problems.</p> <p>Parameters:</p> Name Type Description Default <code>level</code> <code>int</code> <p>Level at which to return the dimension.</p> required <p>Returns:</p> Name Type Description <code>d</code> <code>int</code> <p>Dimension at the given input level.</p> Source code in <code>qmcpy/integrand/abstract_integrand.py</code> <pre><code>def dimension_at_level(self, level):\n    \"\"\"\n    *Abstract method* which returns the dimension of the generator required for a given level.\n\n    Note:\n        Only used for multilevel problems.\n\n    Args:\n        level (int): Level at which to return the dimension.\n\n    Returns:\n        d (int): Dimension at the given input level.\n    \"\"\"\n    return self.d\n</code></pre>"},{"location":"api/integrands/#financialoption","title":"<code>FinancialOption</code>","text":"<p>               Bases: <code>AbstractIntegrand</code></p> <p>Financial options.</p> <ul> <li>Start price \\(S_0\\)</li> <li>Strike price \\(K\\)</li> <li>Interest rate \\(r\\)</li> <li>Volatility \\(\\sigma\\)</li> <li>Drift \\(\\gamma\\)</li> <li>Equidistant monitoring times \\(\\boldsymbol{\\tau} = (\\tau_1,\\dots,\\tau_d)^T\\) with \\(\\tau_d\\) the final (exercise) time and \\(\\tau_j = \\tau_d j/d\\).</li> </ul> <p>Define the geometric brownian motion as</p> \\[\\boldsymbol{S}(\\boldsymbol{t}) = S_0 e^{(\\gamma-\\sigma^2/2)\\boldsymbol{\\tau}+\\sigma\\boldsymbol{t}}, \\qquad \\boldsymbol{T} \\sim \\mathcal{N}(\\boldsymbol{0},\\mathsf{\\Sigma})\\] <p>where \\(\\boldsymbol{T}\\) is a standard Brownian motion so \\(\\mathsf{\\Sigma} = \\left(\\min\\{\\tau_j,\\tau_{j'}\\}\\right)_{j,j'=1}^d\\).</p> <p>The discounted payoff is</p> \\[g(\\boldsymbol{t}) = P(\\boldsymbol{S}(\\boldsymbol{t}))e^{-r \\tau_d}\\] <p>where the payoff function \\(P\\) will be defined depending on the option.</p> <p>Below we will use \\(S_{-1}\\) to denote the final element of \\(\\boldsymbol{S}\\), the value of the path at exercise time.</p>"},{"location":"api/integrands/#qmcpy.integrand.financial_option.FinancialOption--european-options","title":"European Options","text":"<p>European Call and Put Options have respective payoffs</p> \\[P(\\boldsymbol{S}) = \\max\\{S_{-1}-K,0\\}, \\qquad P(\\boldsymbol{S}) = \\max\\{K-S_{-1},0\\}.\\]"},{"location":"api/integrands/#qmcpy.integrand.financial_option.FinancialOption--asian-options","title":"Asian Options","text":"<p>An asian option considers the average value of an asset path across time. We use the trapezoidal rule to approximate either the arithmetic mean by</p> \\[A(\\boldsymbol{S}) = \\frac{1}{d}\\left[\\frac{1}{2} S_0 + \\sum_{j=1}^{d-1} S_j + \\frac{1}{2} S_{-1}\\right]\\] <p>or the geometric mean by</p> \\[A(\\boldsymbol{S}) = \\left[\\sqrt{S_0} \\prod_{j=1}^{d-1} S_j \\sqrt{S_{-1}}\\right]^{1/d}.\\] <p>Asian Call and Put Option have respective payoffs</p> \\[P(\\boldsymbol{S}) = \\max\\{A(\\boldsymbol{S})-K,0\\}, \\qquad P(\\boldsymbol{S}) = \\max\\{K-A(\\boldsymbol{S}),0\\}.\\]"},{"location":"api/integrands/#qmcpy.integrand.financial_option.FinancialOption--barrier-options","title":"Barrier Options","text":"<ul> <li>Barrier \\(B\\).</li> </ul> <p>In options are activate when the path crosses the barrier \\(B\\), while out options are activated only if the path never crosses the barrier \\(B\\). An up option satisfies \\(S_0&lt;B\\) while a down option satisfies \\(S_0&gt;B\\), both indicating the direction of the barrier from the start price.</p> <p>Barrier Up-In Call and Put Options have respective payoffs</p> \\[P(\\boldsymbol{S}) = \\begin{cases} \\max\\{S_{-1})-K,0\\}, &amp; \\text{any } \\boldsymbol{S} \\geq B \\\\ 0, &amp; \\mathrm{otherwise} \\end{cases}, \\qquad P(\\boldsymbol{S}) = \\begin{cases} \\max\\{K-S_{-1}),0\\}, &amp; \\text{any } \\boldsymbol{S} \\geq B \\\\ 0, &amp; \\mathrm{otherwise} \\end{cases}.\\] <p>Barrier Up-Out Call and Put Options have respective payoffs</p> \\[P(\\boldsymbol{S}) = \\begin{cases} \\max\\{S_{-1})-K,0\\}, &amp; \\text{all } \\boldsymbol{S} &lt; B \\\\ 0, &amp; \\mathrm{otherwise} \\end{cases}, \\qquad P(\\boldsymbol{S}) = \\begin{cases} \\max\\{K-S_{-1}),0\\}, &amp; \\text{all } \\boldsymbol{S} &lt; B \\\\ 0, &amp; \\mathrm{otherwise} \\end{cases}.\\] <p>Barrier Down-In Call and Put Options have respective payoffs</p> \\[P(\\boldsymbol{S}) = \\begin{cases} \\max\\{S_{-1})-K,0\\}, &amp; \\text{any } \\boldsymbol{S} \\leq B \\\\ 0, &amp; \\mathrm{otherwise} \\end{cases}, \\qquad P(\\boldsymbol{S}) = \\begin{cases} \\max\\{K-S_{-1}),0\\}, &amp; \\text{any } \\boldsymbol{S} \\leq B \\\\ 0, &amp; \\mathrm{otherwise} \\end{cases}.\\] <p>Barrier Down-Out Call and Put Options have respective payoffs</p> \\[P(\\boldsymbol{S}) = \\begin{cases} \\max\\{S_{-1})-K,0\\}, &amp; \\text{all } \\boldsymbol{S} &gt; B \\\\ 0, &amp; \\mathrm{otherwise} \\end{cases}, \\qquad P(\\boldsymbol{S}) = \\begin{cases} \\max\\{K-S_{-1}),0\\}, &amp; \\text{all } \\boldsymbol{S} &gt; B \\\\ 0, &amp; \\mathrm{otherwise} \\end{cases}.\\]"},{"location":"api/integrands/#qmcpy.integrand.financial_option.FinancialOption--lookback-options","title":"Lookback Options","text":"<p>Lookback Call and Put Options have respective payoffs</p> \\[P(\\boldsymbol{S}) = S_{-1}-\\min(S_0, \\ldots S_{-1}), \\qquad P(\\boldsymbol{S}) = \\max(S_0, \\ldots S_{-1})-S_{-1}.\\]"},{"location":"api/integrands/#qmcpy.integrand.financial_option.FinancialOption--digital-option","title":"Digital Option","text":"<ul> <li>Payout \\(\\rho\\).</li> </ul> <p>Digital Call and Put Options have respective payoffs</p> \\[P(\\boldsymbol{S}) = \\begin{cases} \\rho, &amp; S_{-1} \\geq K \\\\ 0, &amp; \\mathrm{otherwise} \\end{cases}, \\qquad P(\\boldsymbol{S}) =  \\begin{cases} \\rho, &amp; S_{-1} \\leq K \\\\ 0, &amp; \\mathrm{otherwise} \\end{cases}.\\]"},{"location":"api/integrands/#qmcpy.integrand.financial_option.FinancialOption--multilevel-options","title":"Multilevel Options","text":"<ul> <li>Initial level \\(\\ell_0 \\geq 0\\).</li> <li>Level \\(\\ell \\geq \\ell_0\\).</li> </ul> <p>Let \\(\\boldsymbol{S}_\\mathrm{fine}=\\boldsymbol{S}\\) be the fine full path. For \\(\\ell&gt;\\ell_0\\) write the coarse path as \\(\\boldsymbol{S}_\\mathrm{coarse} = (S_j)_{j \\text{ even}}\\) which only considers every other element of \\(\\boldsymbol{S}\\). In this multilevel setting the payoff is</p> \\[P_\\ell(\\boldsymbol{S}) = \\begin{cases} P(\\boldsymbol{S}_\\mathrm{fine}), &amp; \\ell = \\ell_0, \\\\ P(\\boldsymbol{S}_\\mathrm{fine})-P(\\boldsymbol{S}_\\mathrm{coarse}), &amp; \\ell &gt; \\ell_0 \\end{cases}.\\] <p>Cancellations from the telescoping sum allow us to write</p> \\[\\lim_{\\ell \\to \\infty} P_\\ell = P_{\\ell_0} + \\sum_{\\ell=\\ell_0+1}^\\infty P_\\ell.\\] <p>Examples:</p> <pre><code>&gt;&gt;&gt; integrand = FinancialOption(DigitalNetB2(dimension=3,seed=7),option=\"EUROPEAN\")\n&gt;&gt;&gt; y = integrand(2**10)\n&gt;&gt;&gt; y.shape\n(1024,)\n&gt;&gt;&gt; print(\"%.4f\"%y.mean())\n4.2126\n&gt;&gt;&gt; print(\"%.4f\"%integrand.get_exact_value())\n4.2115\n&gt;&gt;&gt; integrand\nFinancialOption (AbstractIntegrand)\n    option          EUROPEAN\n    call_put        CALL\n    volatility      2^(-1)\n    start_price     30\n    strike_price    35\n    interest_rate   0\n    t_final         1\n&gt;&gt;&gt; integrand.true_measure\nGeometricBrownianMotion (AbstractTrueMeasure)\n    time_vec        [0.333 0.667 1.   ]\n    drift           0\n    diffusion       2^(-2)\n    mean_gbm        [30. 30. 30.]\n    covariance_gbm  [[ 78.214  78.214  78.214]\n                     [ 78.214 163.224 163.224]\n                     [ 78.214 163.224 255.623]]\n    decomp_type     PCA\n</code></pre> <pre><code>&gt;&gt;&gt; integrand = FinancialOption(DigitalNetB2(dimension=64,seed=7),option=\"ASIAN\")\n&gt;&gt;&gt; y = integrand(2**10)\n&gt;&gt;&gt; y.shape\n(1024,)\n&gt;&gt;&gt; print(\"%.4f\"%y.mean())\n1.7782\n</code></pre> <p>With independent replications</p> <pre><code>&gt;&gt;&gt; integrand = FinancialOption(DigitalNetB2(dimension=64,seed=7,replications=2**4),option=\"ASIAN\")\n&gt;&gt;&gt; y = integrand(2**6)\n&gt;&gt;&gt; y.shape\n(16, 64)\n&gt;&gt;&gt; muhats = y.mean(-1)\n&gt;&gt;&gt; muhats.shape\n(16,)\n&gt;&gt;&gt; print(\"%.4f\"%muhats.mean())\n1.7765\n</code></pre> <p>Multi-level options</p> <pre><code>&gt;&gt;&gt; seed_seq = np.random.SeedSequence(7)\n&gt;&gt;&gt; d_coarsest = 8\n&gt;&gt;&gt; num_levels = 4\n&gt;&gt;&gt; ns = [2**11,2**10,2**9,2**8]\n&gt;&gt;&gt; integrands = [FinancialOption(DigitalNetB2(dimension=2**l*d_coarsest,seed=seed_seq.spawn(1)[0]),option=\"ASIAN\",level=l,d_coarsest=d_coarsest) for l in range(num_levels)]\n&gt;&gt;&gt; ys = [integrands[l](ns[l]) for l in range(num_levels)]\n&gt;&gt;&gt; for l in range(num_levels):\n...     print(\"ys[%d].shape = %s\"%(l,ys[l].shape))\nys[0].shape = (2, 2048)\nys[1].shape = (2, 1024)\nys[2].shape = (2, 512)\nys[3].shape = (2, 256)\n&gt;&gt;&gt; ymeans = np.stack([(ys[l][1]-ys[l][0]).mean(-1) for l in range(num_levels)])\n&gt;&gt;&gt; with np.printoptions(formatter={\"float\": lambda x: \"%.2e\"%x}):\n...     ymeans\narray([1.78e+00, 2.61e-03, 5.57e-04, 1.10e-03])\n&gt;&gt;&gt; print(\"%.4f\"%ymeans.sum())\n1.7850\n</code></pre> <p>Multi-level options with independent replications</p> <pre><code>&gt;&gt;&gt; seed_seq = np.random.SeedSequence(7)\n&gt;&gt;&gt; d_coarsest = 8\n&gt;&gt;&gt; num_levels = 4\n&gt;&gt;&gt; ns = [2**7,2**6,2**5,2**4]\n&gt;&gt;&gt; integrands = [FinancialOption(DigitalNetB2(dimension=2**l*d_coarsest,seed=seed_seq.spawn(1)[0],replications=2**4),option=\"ASIAN\",level=l,d_coarsest=d_coarsest) for l in range(num_levels)]\n&gt;&gt;&gt; ys = [integrands[l](ns[l]) for l in range(num_levels)]\n&gt;&gt;&gt; for l in range(num_levels):\n...     print(\"ys[%d].shape = %s\"%(l,ys[l].shape))\nys[0].shape = (2, 16, 128)\nys[1].shape = (2, 16, 64)\nys[2].shape = (2, 16, 32)\nys[3].shape = (2, 16, 16)\n&gt;&gt;&gt; muhats = np.stack([(ys[l][1]-ys[l][0]).mean(-1) for l in range(num_levels)])\n&gt;&gt;&gt; muhats.shape\n(4, 16)\n&gt;&gt;&gt; muhathat = muhats.mean(-1)\n&gt;&gt;&gt; with np.printoptions(formatter={\"float\": lambda x: \"%.2e\"%x}):\n...     muhathat\narray([1.80e+00, -3.08e-03, 2.64e-03, 1.14e-03])\n&gt;&gt;&gt; print(\"%.4f\"%muhathat.sum())\n1.7982\n</code></pre> <p>References:</p> <ol> <li>M.B. Giles.     Improved multilevel Monte Carlo convergence using the Milstein scheme.     343-358, in Monte Carlo and Quasi-Monte Carlo Methods 2006, Springer, 2008.     http://people.maths.ox.ac.uk/~gilesm/files/mcqmc06.pdf.</li> </ol> <p>Parameters:</p> Name Type Description Default <code>sampler</code> <code>Union[AbstractDiscreteDistribution, AbstractTrueMeasure]</code> <p>Either</p> <ul> <li>a discrete distribution from which to transform samples, or</li> <li>a true measure by which to compose a transform.</li> </ul> required <code>option</code> <code>str</code> <p>Option type in <code>['ASIAN', 'EUROPEAN', 'BARRIER', 'LOOKBACK', 'DIGITAL']</code></p> <code>'ASIAN'</code> <code>call_put</code> <code>str</code> <p>Either <code>'CALL'</code> or <code>'PUT'</code>.</p> <code>'CALL'</code> <code>volatility</code> <code>float</code> <p>\\(\\sigma\\).</p> <code>0.5</code> <code>start_price</code> <code>float</code> <p>\\(S_0\\).</p> <code>30</code> <code>strike_price</code> <code>float</code> <p>\\(K\\).</p> <code>35</code> <code>interest_rate</code> <code>float</code> <p>\\(r\\).</p> <code>0</code> <code>t_final</code> <code>float</code> <p>\\(\\tau_d\\).</p> <code>1</code> <code>decomp_type</code> <code>str</code> <p>Method for decomposition for covariance matrix. Options include</p> <ul> <li><code>'PCA'</code> for principal component analysis, or</li> <li><code>'Cholesky'</code> for cholesky decomposition.</li> </ul> <code>'PCA'</code> <code>level</code> <code>Union[None, int]</code> <p>Level for multilevel problems</p> <code>None</code> <code>d_coarsest</code> <code>Union[None, int]</code> <p>Dimension of the problem on the coarsest level.</p> <code>2</code> <code>asian_mean</code> <code>str</code> <p>Either <code>'ARITHMETIC'</code> or <code>'GEOMETRIC'</code>.</p> <code>'ARITHMETIC'</code> <code>asian_mean_quadrature_rule</code> <code>str</code> <p>Either 'TRAPEZOIDAL' or 'RIGHT'.</p> <code>'TRAPEZOIDAL'</code> <code>barrier_in_out</code> <code>str</code> <p>Either <code>'IN'</code> or <code>'OUT'</code>.</p> <code>'IN'</code> <code>barrier_price</code> <code>float</code> <p>\\(B\\).</p> <code>38</code> <code>digital_payout</code> <code>float</code> <p>\\(\\rho\\).</p> <code>10</code> Source code in <code>qmcpy/integrand/financial_option.py</code> <pre><code>def __init__(\n    self,\n    sampler,\n    option=\"ASIAN\",\n    call_put=\"CALL\",\n    volatility=0.5,\n    start_price=30,\n    strike_price=35,\n    interest_rate=0,\n    t_final=1,\n    decomp_type=\"PCA\",\n    level=None,\n    d_coarsest=2,\n    asian_mean=\"ARITHMETIC\",\n    asian_mean_quadrature_rule=\"TRAPEZOIDAL\",\n    barrier_in_out=\"IN\",\n    barrier_price=38,\n    digital_payout=10,\n):\n    r\"\"\"\n    Args:\n        sampler (Union[AbstractDiscreteDistribution, AbstractTrueMeasure]): Either\n\n            - a discrete distribution from which to transform samples, or\n            - a true measure by which to compose a transform.\n        option (str): Option type in `['ASIAN', 'EUROPEAN', 'BARRIER', 'LOOKBACK', 'DIGITAL']`\n        call_put (str): Either `'CALL'` or `'PUT'`.\n        volatility (float): $\\sigma$.\n        start_price (float): $S_0$.\n        strike_price (float): $K$.\n        interest_rate (float): $r$.\n        t_final (float): $\\tau_d$.\n        decomp_type (str): Method for decomposition for covariance matrix. Options include\n\n            - `'PCA'` for principal component analysis, or\n            - `'Cholesky'` for cholesky decomposition.\n        level (Union[None, int]): Level for multilevel problems\n        d_coarsest (Union[None, int]): Dimension of the problem on the coarsest level.\n        asian_mean (str): Either `'ARITHMETIC'` or `'GEOMETRIC'`.\n        asian_mean_quadrature_rule (str): Either 'TRAPEZOIDAL' or 'RIGHT'.\n        barrier_in_out (str): Either `'IN'` or `'OUT'`.\n        barrier_price (float): $B$.\n        digital_payout (float): $\\rho$.\n    \"\"\"\n    self.parameters = [\n        \"option\",\n        \"call_put\",\n        \"volatility\",\n        \"start_price\",\n        \"strike_price\",\n        \"interest_rate\",\n        \"t_final\",\n    ]\n    self.t_final = t_final\n    self.sampler = sampler\n    self.decomp_type = decomp_type\n    self.volatility = float(volatility)\n    self.start_price = float(start_price)\n    self.strike_price = float(strike_price)\n    self.interest_rate = float(interest_rate)\n    self.true_measure = GeometricBrownianMotion(\n        self.sampler,\n        t_final=self.t_final,\n        initial_value=self.start_price,\n        drift=self.interest_rate,\n        diffusion=self.volatility**2,\n        decomp_type=self.decomp_type,\n    )\n    self.discount_factor = np.exp(-self.interest_rate * self.t_final)\n    self.level = level\n    self.d_coarsest = d_coarsest\n    if self.level is not None:\n        self.multilevel = True\n        self.parameters += [\"level\", \"d_coarsest\"]\n        assert np.isscalar(self.level) and self.level % 1 == 0\n        assert (\n            np.isscalar(self.d_coarsest)\n            and self.d_coarsest % 1 == 0\n            and d_coarsest &gt; 0\n            and np.log2(d_coarsest) % 1 == 0\n        ), \"d_coarsest must be an integer power of 2\"\n        self.level = int(self.level)\n        self.d_coarsest = int(self.d_coarsest)\n        assert (\n            self.sampler.d == self.d_coarsest * 2**self.level\n        ), \"the dimension of the sampler must equal d_coarsest*2^level = %d\" % (\n            d_coarsest * 2**self.level\n        )\n        self.cost = self.d_coarsest * 2**self.level\n        dim_shape = (2,)\n    else:\n        self.multilevel = False\n        dim_shape = ()\n    self.call_put = str(call_put).upper()\n    assert self.call_put in [\"CALL\", \"PUT\"], \"invalid call_put = %s\" % self.call_put\n    self.option = str(option).upper()\n    self.asian_mean = str(asian_mean).upper()\n    self.asian_mean_quadrature_rule = str(asian_mean_quadrature_rule).upper()\n    self.barrier_in_out = str(barrier_in_out).upper()\n    assert np.isscalar(barrier_price)\n    self.barrier_price = float(barrier_price)\n    assert np.isscalar(digital_payout) and digital_payout &gt; 0\n    self.digital_payout = float(digital_payout)\n    if self.option == \"EUROPEAN\":\n        self.payoff = (\n            self.payoff_european_call\n            if self.call_put == \"CALL\"\n            else self.payoff_european_put\n        )\n    elif self.option == \"ASIAN\":\n        self.parameters += [\"asian_mean\"]\n        assert self.asian_mean in [\"ARITHMETIC\", \"GEOMETRIC\"], (\n            \"invalid asian_mean = %s\" % self.asian_mean\n        )\n        assert self.asian_mean_quadrature_rule in [\"TRAPEZOIDAL\", \"RIGHT\"], (\n            \"invalid asian_mean_quadrature_rule = %s\"\n            % self.asian_mean_quadrature_rule\n        )\n        if self.asian_mean == \"ARITHMETIC\":\n            if self.asian_mean_quadrature_rule == \"TRAPEZOIDAL\":\n                self.payoff = (\n                    self.payoff_asian_arithmetic_trap_call\n                    if self.call_put == \"CALL\"\n                    else self.payoff_asian_arithmetic_trap_put\n                )\n            elif self.asian_mean_quadrature_rule == \"RIGHT\":\n                self.payoff = (\n                    self.payoff_asian_arithmetic_right_call\n                    if self.call_put == \"CALL\"\n                    else self.payoff_asian_arithmetic_right_put\n                )\n        elif self.asian_mean == \"GEOMETRIC\":\n            if self.asian_mean_quadrature_rule == \"TRAPEZOIDAL\":\n                self.payoff = (\n                    self.payoff_asian_geometric_trap_call\n                    if self.call_put == \"CALL\"\n                    else self.payoff_asian_geometric_trap_put\n                )\n            elif self.asian_mean_quadrature_rule == \"RIGHT\":\n                self.payoff = (\n                    self.payoff_asian_geometric_right_call\n                    if self.call_put == \"CALL\"\n                    else self.payoff_asian_geometric_right_put\n                )\n    elif self.option == \"BARRIER\":\n        self.parameters += [\"barrier_in_out\", \"barrier_up_down\", \"barrier_price\"]\n        self.barrier_up_down = (\n            \"UP\" if self.start_price &lt; self.barrier_price else \"DOWN\"\n        )\n        if self.barrier_in_out == \"IN\":\n            if self.barrier_up_down == \"UP\":\n                self.payoff = (\n                    self.payoff_barrier_in_up_call\n                    if self.call_put == \"CALL\"\n                    else self.payoff_barrier_in_up_put\n                )\n            else:\n                self.payoff = (\n                    self.payoff_barrier_in_down_call\n                    if self.call_put == \"CALL\"\n                    else self.payoff_barrier_in_down_put\n                )\n        elif self.barrier_in_out == \"OUT\":\n            if self.barrier_up_down == \"UP\":\n                self.payoff = (\n                    self.payoff_barrier_out_up_call\n                    if self.call_put == \"CALL\"\n                    else self.payoff_barrier_out_up_put\n                )\n            else:\n                self.payoff = (\n                    self.payoff_barrier_out_down_call\n                    if self.call_put == \"CALL\"\n                    else self.payoff_barrier_out_down_put\n                )\n        else:\n            raise ParameterError(\n                \"invalid barrier_in_out = %s\" % self.barrier_in_out\n            )\n    elif self.option == \"LOOKBACK\":\n        self.payoff = (\n            self.payoff_lookback_call\n            if self.call_put == \"CALL\"\n            else self.payoff_lookback_put\n        )\n    elif self.option == \"DIGITAL\":\n        self.payoff = (\n            self.payoff_digital_call\n            if self.call_put == \"CALL\"\n            else self.payoff_digital_put\n        )\n    else:\n        raise ParameterError(\"invalid option type %s\" % self.option)\n    super(FinancialOption, self).__init__(\n        dimension_indv=dim_shape, dimension_comb=dim_shape, parallel=False\n    )\n</code></pre>"},{"location":"api/integrands/#qmcpy.integrand.financial_option.FinancialOption.get_exact_value","title":"get_exact_value","text":"<pre><code>get_exact_value()\n</code></pre> <p>Compute the exact analytic fair price of the option in finite dimensions. Supports</p> <ul> <li><code>option='EUROPEAN'</code></li> <li><code>option='ASIAN'</code> with <code>asian_mean='GEOMETRIC'</code> and <code>asian_mean_quadrature_rule='RIGHT'</code></li> </ul> <p>Returns:</p> Name Type Description <code>mean</code> <code>float</code> <p>Exact value of the integral.</p> Source code in <code>qmcpy/integrand/financial_option.py</code> <pre><code>def get_exact_value(self):\n    \"\"\"\n    Compute the exact analytic fair price of the option in finite dimensions. Supports\n\n    - `option='EUROPEAN'`\n    - `option='ASIAN'` with `asian_mean='GEOMETRIC'` and `asian_mean_quadrature_rule='RIGHT'`\n\n    Returns:\n        mean (float): Exact value of the integral.\n    \"\"\"\n    if self.option == \"EUROPEAN\":\n        denom = self.volatility * np.sqrt(self.t_final)\n        decay = self.strike_price * self.discount_factor\n        if self.call_put == \"CALL\":\n            term1 = (\n                np.log(self.start_price / self.strike_price)\n                + (self.interest_rate + self.volatility**2 / 2) * self.t_final\n            )\n            term2 = (\n                np.log(self.start_price / self.strike_price)\n                + (self.interest_rate - self.volatility**2 / 2) * self.t_final\n            )\n            fp = self.start_price * norm.cdf(term1 / denom) - decay * norm.cdf(\n                term2 / denom\n            )\n        elif self.call_put == \"PUT\":\n            term1 = (\n                np.log(self.strike_price / self.start_price)\n                - (self.interest_rate - self.volatility**2 / 2) * self.t_final\n            )\n            term2 = (\n                np.log(self.strike_price / self.start_price)\n                - (self.interest_rate + self.volatility**2 / 2) * self.t_final\n            )\n            fp = decay * norm.cdf(term1 / denom) - self.start_price * norm.cdf(\n                term2 / denom\n            )\n    elif self.option == \"ASIAN\":\n        assert (\n            self.asian_mean == \"GEOMETRIC\"\n            and self.asian_mean_quadrature_rule == \"RIGHT\"\n        ), \"exact value for Asian options only implemented for self.asian_mean=='GEOMETRIC' and self.asian_mean_quadrature_rule=='RIGHT'\"\n        Tbar = (1 + 1 / self.d) * self.t_final / 2\n        sigmabar = self.volatility * np.sqrt((2 + 1 / self.d) / 3)\n        rbar = self.interest_rate + (sigmabar**2 - self.volatility**2) / 2\n        gmeancall, gmeanput = _eurogbmprice(\n            self.start_price, rbar, Tbar, sigmabar, self.strike_price\n        )\n        if self.call_put == \"CALL\":\n            fp = gmeancall * np.exp(rbar * Tbar - self.interest_rate * self.t_final)\n        elif self.call_put == \"PUT\":\n            fp = gmeanput * np.exp(rbar * Tbar - self.interest_rate * self.t_final)\n        return fp\n    else:\n        raise ParameterError(\n            \"exact value not supported for option = %s\" % self.option\n        )\n    return fp\n</code></pre>"},{"location":"api/integrands/#qmcpy.integrand.financial_option.FinancialOption.get_exact_value_inf_dim","title":"get_exact_value_inf_dim","text":"<pre><code>get_exact_value_inf_dim()\n</code></pre> <p>Get the exact analytic fair price of the option in infinite dimensions. Supports</p> <ul> <li><code>option='ASIAN'</code> with <code>asian_mean='GEOMETRIC'</code></li> </ul> <p>Returns:</p> Name Type Description <code>mean</code> <code>float</code> <p>Exact value of the integral.</p> Source code in <code>qmcpy/integrand/financial_option.py</code> <pre><code>def get_exact_value_inf_dim(self):\n    r\"\"\"\n    Get the exact analytic fair price of the option in infinite dimensions. Supports\n\n    - `option='ASIAN'` with `asian_mean='GEOMETRIC'`\n\n    Returns:\n        mean (float): Exact value of the integral.\n    \"\"\"\n    if self.option == \"ASIAN\":\n        assert (\n            self.asian_mean == \"GEOMETRIC\"\n        ), \"get_exact_value_inf_dim for the Asian option only available for self.asian_mean=='GEOMETRIC'\"\n        sigma_g = self.volatility / np.sqrt(3)\n        b = 1 / 2 * (self.interest_rate - 1 / 2 * sigma_g**2)\n        d1 = (\n            np.log(self.start_price / self.strike_price)\n            + (b + 1 / 2 * sigma_g**2) * self.t_final\n        ) / (sigma_g * np.sqrt(self.t_final))\n        d2 = d1 - sigma_g * np.sqrt(self.t_final)\n        f1 = self.start_price * np.exp((b - self.interest_rate) * self.t_final)\n        f2 = self.strike_price * np.exp(-self.interest_rate * self.t_final)\n        if self.call_put == \"CALL\":\n            val = f1 * norm.cdf(d1) - f2 * norm.cdf(d2)\n        elif self.call_put == \"PUT\":\n            val = f2 * norm.cdf(-d2) - f1 * norm.cdf(-d1)\n    else:\n        raise Exception(\n            \"get_exact_value_inf_dim not implemented for option = %s\" % self.option\n        )\n    return val\n</code></pre>"},{"location":"api/integrands/#customfun","title":"<code>CustomFun</code>","text":"<p>               Bases: <code>AbstractIntegrand</code></p> <p>User supplied integrand \\(g\\). In the following example we implement</p> <p>Examples:</p> <p>First we will implement</p> \\[g(\\boldsymbol{t}) = t_1^2t_2, \\qquad \\boldsymbol{T}=(T_1,T_2) \\sim \\mathcal{N}((1,2)^T,\\mathsf{I}).\\] <pre><code>&gt;&gt;&gt; integrand = CustomFun(\n...     true_measure = Gaussian(DigitalNetB2(2,seed=7),mean=[1,2]),\n...     g = lambda t: t[...,0]**2*t[...,1])\n&gt;&gt;&gt; y = integrand(2**10)\n&gt;&gt;&gt; print(\"%.4f\"%y.mean())\n3.9991\n</code></pre> <p>With independent replications</p> <pre><code>&gt;&gt;&gt; integrand = CustomFun(\n...     true_measure = Gaussian(DigitalNetB2(2,seed=7,replications=2**4),mean=[1,2]),\n...     g = lambda t: t[...,0]**2*t[...,1])\n&gt;&gt;&gt; y = integrand(2**6)\n&gt;&gt;&gt; y.shape\n(16, 64)\n&gt;&gt;&gt; muhats = y.mean(1)\n&gt;&gt;&gt; muhats.shape\n(16,)\n&gt;&gt;&gt; print(\"%.4f\"%muhats.mean())\n3.9330\n</code></pre> <p>Next we will implement the multi-output function</p> \\[g(\\boldsymbol{t}) = \\begin{pmatrix} \\sin(t_1)\\cos(t_2) \\\\ \\cos(t_1)\\sin(t_2) \\\\ \\sin(t_1)+\\cos(t_2) \\\\ \\cos(t_1)+\\sin(t_2) \\end{pmatrix} \\qquad \\boldsymbol{T}=(T_1,T_2) \\sim \\mathcal{U}[0,2\\pi]^2.\\] <pre><code>&gt;&gt;&gt; def g(t):\n...     t1,t2 = t[...,0],t[...,1]\n...     sint1,cost1,sint2,cost2 = np.sin(t1),np.cos(t1),np.sin(t2),np.cos(t2)\n...     y1 = sint1*cost2\n...     y2 = cost1*sint2\n...     y3 = sint1+cost2\n...     y4 = cost1+sint2\n...     y = np.stack([y1,y2,y3,y4])\n...     return y\n&gt;&gt;&gt; integrand = CustomFun(\n...     true_measure = Uniform(DigitalNetB2(2,seed=7),lower_bound=0,upper_bound=2*np.pi),\n...     g = g,\n...     dimension_indv = (4,))\n&gt;&gt;&gt; x = integrand.discrete_distrib(2**10)\n&gt;&gt;&gt; y = integrand.f(x)\n&gt;&gt;&gt; y.shape\n(4, 1024)\n&gt;&gt;&gt; with np.printoptions(formatter={\"float\": lambda x: \"%.2e\"%x}):\n...     y.mean(-1)\narray([8.18e-04, 1.92e-06, -2.26e-10, 5.05e-07])\n</code></pre> <p>Stopping criterion which supporting vectorized outputs may pass in Boolean <code>compute_flags</code> with <code>dimension_indv</code> shape indicating which output need to evaluated,     i.e. where <code>compute_flags</code> is <code>False</code> we do not need to evaluate the integrand. We have not used this in inexpensive example above.</p> <p>With independent replications</p> <pre><code>&gt;&gt;&gt; integrand = CustomFun(\n...     true_measure = Uniform(DigitalNetB2(2,seed=7,replications=2**4),lower_bound=0,upper_bound=2*np.pi),\n...     g = g,\n...     dimension_indv = (4,))\n&gt;&gt;&gt; x = integrand.discrete_distrib(2**6)\n&gt;&gt;&gt; x.shape\n(16, 64, 2)\n&gt;&gt;&gt; y = integrand.f(x)\n&gt;&gt;&gt; y.shape\n(4, 16, 64)\n&gt;&gt;&gt; muhats = y.mean(-1)\n&gt;&gt;&gt; muhats.shape\n(4, 16)\n&gt;&gt;&gt; with np.printoptions(formatter={\"float\": lambda x: \"%.2e\"%x}):\n...     muhats.mean(-1)\narray([3.83e-03, -6.78e-03, -1.56e-03, -5.65e-04])\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>true_measure</code> <code>AbstractTrueMeasure</code> <p>The true measure.</p> required <code>g</code> <code>callable</code> <p>A function handle.</p> required <code>dimension_indv</code> <code>tuple</code> <p>Shape of individual solution outputs from <code>g</code>.</p> <code>()</code> <code>parallel</code> <code>int</code> <p>Parallelization flag.</p> <ul> <li>When <code>parallel = 0</code> or <code>parallel = 1</code> then function evaluation is done in serial fashion.</li> <li><code>parallel &gt; 1</code> specifies the number of processes used by <code>multiprocessing.Pool</code> or <code>multiprocessing.pool.ThreadPool</code>.</li> </ul> <p>Setting <code>parallel=True</code> is equivalent to <code>parallel = os.cpu_count()</code>.</p> <code>False</code> Note <p>For <code>parallel &gt; 1</code> do not set <code>g</code> to be anonymous function (i.e. a <code>lambda</code> function)</p> Source code in <code>qmcpy/integrand/custom_fun.py</code> <pre><code>def __init__(self, true_measure, g, dimension_indv=(), parallel=False):\n    \"\"\"\n    Args:\n        true_measure (AbstractTrueMeasure): The true measure.\n        g (callable): A function handle.\n        dimension_indv (tuple): Shape of individual solution outputs from `g`.\n        parallel (int): Parallelization flag.\n\n            - When `parallel = 0` or `parallel = 1` then function evaluation is done in serial fashion.\n            - `parallel &gt; 1` specifies the number of processes used by `multiprocessing.Pool` or `multiprocessing.pool.ThreadPool`.\n\n            Setting `parallel=True` is equivalent to `parallel = os.cpu_count()`.\n\n    Note:\n        For `parallel &gt; 1` do *not* set `g` to be anonymous function (i.e. a `lambda` function)\n    \"\"\"\n    self.parameters = []\n    self.true_measure = true_measure\n    self.sampler = self.true_measure\n    self.__g = g\n    super(CustomFun, self).__init__(\n        dimension_indv=dimension_indv,\n        dimension_comb=dimension_indv,\n        parallel=parallel,\n    )\n</code></pre>"},{"location":"api/integrands/#umbridgewrapper","title":"<code>UMBridgeWrapper</code>","text":"<p>               Bases: <code>AbstractIntegrand</code></p> <p>Wrapper around a <code>UM-Bridge</code> model. See also the <code>UM-Bridge</code> documentation for the QMCPy client. Requires Docker is installed.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; _ = os.system('docker run --name muqbppytest -dit -p 4243:4243 linusseelinger/benchmark-muq-beam-propagation:latest &gt; /dev/null')\n&gt;&gt;&gt; import umbridge\n&gt;&gt;&gt; dnb2 = DigitalNetB2(dimension=3,seed=7)\n&gt;&gt;&gt; true_measure = Uniform(dnb2,lower_bound=1,upper_bound=1.05)\n&gt;&gt;&gt; um_bridge_model = umbridge.HTTPModel('http://localhost:4243','forward')\n&gt;&gt;&gt; um_bridge_config = {\"d\": dnb2.d}\n&gt;&gt;&gt; integrand = UMBridgeWrapper(true_measure,um_bridge_model,um_bridge_config,parallel=False)\n&gt;&gt;&gt; y = integrand(2**10)\n&gt;&gt;&gt; with np.printoptions(formatter={\"float\":lambda x: \"%.1e\"%x}):\n...     y.mean(-1)\narray([0.0e+00, 3.9e+00, 1.5e+01, 3.2e+01, 5.5e+01, 8.3e+01, 1.2e+02,\n       1.5e+02, 2.0e+02, 2.4e+02, 2.9e+02, 3.4e+02, 3.9e+02, 4.3e+02,\n       4.7e+02, 5.0e+02, 5.3e+02, 5.6e+02, 5.9e+02, 6.2e+02, 6.4e+02,\n       6.6e+02, 6.9e+02, 7.2e+02, 7.6e+02, 7.9e+02, 8.3e+02, 8.6e+02,\n       9.0e+02, 9.4e+02, 9.7e+02])\n&gt;&gt;&gt; _ = os.system('docker rm -f muqbppytest &gt; /dev/null')\n</code></pre> <p>Custom model with independent replications</p> <pre><code>&gt;&gt;&gt; class TestModel(umbridge.Model):\n...     def __init__(self):\n...         super().__init__(\"forward\")\n...     def get_input_sizes(self, config):\n...         return [1,2,3]\n...     def get_output_sizes(self, config):\n...         return [3,2,1]\n...     def __call__(self, parameters, config):\n...         out0 = [parameters[2][0],sum(parameters[2][:2]),sum(parameters[2])]\n...         out1 = [parameters[1][0],sum(parameters[1])]\n...         out2 = [parameters[0]]\n...         return [out0,out1,out2]\n...     def supports_evaluate(self):\n...         return True\n&gt;&gt;&gt; um_bridge_model = TestModel()\n&gt;&gt;&gt; um_bridge_config = {}\n&gt;&gt;&gt; d = sum(um_bridge_model.get_input_sizes(config=um_bridge_config))\n&gt;&gt;&gt; true_measure = Uniform(DigitalNetB2(dimension=d,seed=7,replications=15),lower_bound=-1,upper_bound=1)\n&gt;&gt;&gt; integrand = UMBridgeWrapper(true_measure,um_bridge_model)\n&gt;&gt;&gt; y = integrand(2**6)\n&gt;&gt;&gt; y.shape\n(6, 15, 64)\n&gt;&gt;&gt; muhats = y.mean(-1)\n&gt;&gt;&gt; muhats.shape\n(6, 15)\n&gt;&gt;&gt; muhats_aggregate = muhats.mean(-1)\n&gt;&gt;&gt; muhats_aggregate.shape\n(6,)\n&gt;&gt;&gt; muhats_agg_list_of_lists = integrand.to_umbridge_out_sizes(muhats_aggregate)\n&gt;&gt;&gt; [[\"%.2e\"%ii for ii in i] for i in muhats_agg_list_of_lists]\n[['-1.59e-08', '1.49e-04', '1.49e-04'], ['8.20e-06', '-1.38e-04'], ['-8.14e-06']]\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>true_measure</code> <code>AbstractTrueMeasure</code> <p>The true measure.</p> required <code>model</code> <code>HTTPModel</code> <p>A <code>UM-Bridge</code> model.</p> required <code>config</code> <code>dict</code> <p>Configuration keyword argument to <code>umbridge.HTTPModel(url,name).__call__</code>.</p> <code>{}</code> <code>parallel</code> <code>int</code> <p>Parallelization flag.</p> <ul> <li>When <code>parallel = 0</code> or <code>parallel = 1</code> then function evaluation is done in serial fashion.</li> <li><code>parallel &gt; 1</code> specifies the number of processes used by <code>multiprocessing.Pool</code> or <code>multiprocessing.pool.ThreadPool</code>.</li> </ul> <p>Setting <code>parallel=True</code> is equivalent to <code>parallel = os.cpu_count()</code>.</p> <code>False</code> Source code in <code>qmcpy/integrand/umbridge_wrapper.py</code> <pre><code>def __init__(self, true_measure, model, config={}, parallel=False):\n    \"\"\"\n    Args:\n        true_measure (AbstractTrueMeasure): The true measure.\n        model (umbridge.HTTPModel): A `UM-Bridge` model.\n        config (dict): Configuration keyword argument to `umbridge.HTTPModel(url,name).__call__`.\n        parallel (int): Parallelization flag.\n\n            - When `parallel = 0` or `parallel = 1` then function evaluation is done in serial fashion.\n            - `parallel &gt; 1` specifies the number of processes used by `multiprocessing.Pool` or `multiprocessing.pool.ThreadPool`.\n\n            Setting `parallel=True` is equivalent to `parallel = os.cpu_count()`.\n    \"\"\"\n    import umbridge\n\n    self.parameters = []\n    self.true_measure = true_measure\n    self.sampler = self.true_measure\n    self.model = model\n    if not self.model.supports_evaluate():\n        raise ParameterError(\"UMBridgeWrapper requires model supports evaluation.\")\n    self.config = config\n    self.parallel = parallel\n    self.d_in_umbridge = np.append(\n        0, np.cumsum(self.model.get_input_sizes(self.config))\n    )\n    self.n_d_in_umbridge = len(self.d_in_umbridge) - 1\n    if self.true_measure.d != self.d_in_umbridge[-1]:\n        raise ParameterError(\n            \"sampler dimension (%d) must equal the sum of UMBridgeWrapper input sizes (%d).\"\n            % (self.true_measure.d, self.d_in_umbridge[-1])\n        )\n    self.d_out_umbridge = np.append(\n        0, np.cumsum(self.model.get_output_sizes(self.config))\n    )\n    self.n_d_out_umbridge = len(self.d_out_umbridge) - 1\n    self.total_out_elements = int(self.d_out_umbridge[-1])\n    super(UMBridgeWrapper, self).__init__(\n        dimension_indv=(\n            () if self.total_out_elements == 1 else (self.total_out_elements,)\n        ),\n        dimension_comb=(\n            () if self.total_out_elements == 1 else (self.total_out_elements,)\n        ),\n        parallel=self.parallel,\n        threadpool=True,\n    )\n</code></pre>"},{"location":"api/integrands/#qmcpy.integrand.umbridge_wrapper.UMBridgeWrapper.to_umbridge_out_sizes","title":"to_umbridge_out_sizes","text":"<pre><code>to_umbridge_out_sizes(x)\n</code></pre> <p>Convert a data attribute to <code>UM-Bridge</code> output sized list of lists.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ndarray</code> <p>Array of length <code>sum(model.get_output_sizes(self.config))</code> where <code>model</code> is a <code>umbridge.HTTPModel</code>.</p> required <p>Returns:</p> Name Type Description <code>x_list_list</code> <code>list</code> <p>List of lists with sub-list lengths specified by <code>model.get_output_sizes(self.config)</code>.</p> Source code in <code>qmcpy/integrand/umbridge_wrapper.py</code> <pre><code>def to_umbridge_out_sizes(self, x):\n    \"\"\"\n    Convert a data attribute to `UM-Bridge` output sized list of lists.\n\n    Args:\n        x (np.ndarray): Array of length `sum(model.get_output_sizes(self.config))` where `model` is a `umbridge.HTTPModel`.\n\n    Returns:\n        x_list_list (list): List of lists with sub-list lengths specified by `model.get_output_sizes(self.config)`.\n    \"\"\"\n    return [\n        x[..., self.d_out_umbridge[j] : self.d_out_umbridge[j + 1]].tolist()\n        for j in range(self.n_d_out_umbridge)\n    ]\n</code></pre>"},{"location":"api/integrands/#sensitivityindices","title":"<code>SensitivityIndices</code>","text":"<p>               Bases: <code>AbstractIntegrand</code></p> <p>Sensitivity indices i.e. normalized Sobol' Indices.</p> <p>Examples:</p> <p>Singleton indices</p> <pre><code>&gt;&gt;&gt; function = Keister(DigitalNetB2(dimension=4,seed=7))\n&gt;&gt;&gt; integrand = SensitivityIndices(function,indices='singletons')\n&gt;&gt;&gt; integrand.indices\narray([[ True, False, False, False],\n       [False,  True, False, False],\n       [False, False,  True, False],\n       [False, False, False,  True]])\n&gt;&gt;&gt; y = integrand(2**10)\n&gt;&gt;&gt; y.shape\n(2, 3, 4, 1024)\n&gt;&gt;&gt; ymean = y.mean(-1)\n&gt;&gt;&gt; ymean.shape\n(2, 3, 4)\n&gt;&gt;&gt; sigma_hat = ymean[:,2,:]-ymean[:,1,:]**2\n&gt;&gt;&gt; sigma_hat.shape\n(2, 4)\n&gt;&gt;&gt; sigma_hat\narray([[17.81421941, 17.81421941, 17.81421941, 17.81421941],\n       [17.81421941, 17.81421941, 17.81421941, 17.81421941]])\n&gt;&gt;&gt; closed_total_approx = ymean[:,0]/sigma_hat\n&gt;&gt;&gt; closed_total_approx.shape\n(2, 4)\n&gt;&gt;&gt; closed_total_approx\narray([[0.23756849, 0.2423556 , 0.24844127, 0.24771739],\n       [0.25090825, 0.25344588, 0.24531633, 0.26244379]])\n</code></pre> <p>Check what all indices look like for \\(d=3\\)</p> <pre><code>&gt;&gt;&gt; integrand = SensitivityIndices(Keister(DigitalNetB2(dimension=3,seed=7)),indices='all')\n&gt;&gt;&gt; integrand.indices\narray([[ True, False, False],\n       [False,  True, False],\n       [False, False,  True],\n       [ True,  True, False],\n       [ True, False,  True],\n       [False,  True,  True]])\n</code></pre> <p>Vectorized function for all singletons and pairs of dimensions</p> <pre><code>&gt;&gt;&gt; function = BoxIntegral(DigitalNetB2(dimension=4,seed=7,replications=2**4),s=np.arange(1,31).reshape((5,6)))\n&gt;&gt;&gt; indices = np.zeros((function.d,function.d,function.d),dtype=bool)\n&gt;&gt;&gt; r = np.arange(function.d)\n&gt;&gt;&gt; indices[r,:,r] = True\n&gt;&gt;&gt; indices[:,r,r] = True\n&gt;&gt;&gt; integrand = SensitivityIndices(function,indices=indices)\n&gt;&gt;&gt; integrand.indices.shape\n(4, 4, 4)\n&gt;&gt;&gt; integrand.indices\narray([[[ True, False, False, False],\n        [ True,  True, False, False],\n        [ True, False,  True, False],\n        [ True, False, False,  True]],\n\n       [[ True,  True, False, False],\n        [False,  True, False, False],\n        [False,  True,  True, False],\n        [False,  True, False,  True]],\n\n       [[ True, False,  True, False],\n        [False,  True,  True, False],\n        [False, False,  True, False],\n        [False, False,  True,  True]],\n\n       [[ True, False, False,  True],\n        [False,  True, False,  True],\n        [False, False,  True,  True],\n        [False, False, False,  True]]])\n&gt;&gt;&gt; y = integrand(2**10)\n&gt;&gt;&gt; y.shape\n(2, 3, 4, 4, 5, 6, 16, 1024)\n&gt;&gt;&gt; muhats = y.mean(-1)\n&gt;&gt;&gt; muhats.shape\n(2, 3, 4, 4, 5, 6, 16)\n&gt;&gt;&gt; muhathat = muhats.mean(-1)\n&gt;&gt;&gt; muhathat.shape\n(2, 3, 4, 4, 5, 6)\n&gt;&gt;&gt; sigma_hat = muhathat[:,2,:]-muhathat[:,1,:]**2\n&gt;&gt;&gt; sigma_hat.shape\n(2, 4, 4, 5, 6)\n&gt;&gt;&gt; closed_total_approx = muhathat[:,0]/sigma_hat\n&gt;&gt;&gt; closed_total_approx.shape\n(2, 4, 4, 5, 6)\n</code></pre> <p>References:</p> <ol> <li> <p>Aleksei G. Sorokin and Jagadeeswaran Rathinavel.     On Bounding and Approximating Functions of Multiple Expectations Using Quasi-Monte Carlo.     International Conference on Monte Carlo and Quasi-Monte Carlo Methods in Scientific Computing.     Cham: Springer International Publishing, 2022.     https://link.springer.com/chapter/10.1007/978-3-031-59762-6_29.</p> </li> <li> <p>Art B. Owen.     Monte Carlo theory, methods and examples.     Appendix A. Equations (A.16) and (A.18). 2013.     https://artowen.su.domains/mc/A-anova.pdf.</p> </li> </ol> <p>Parameters:</p> Name Type Description Default <code>integrand</code> <code>AbstractIntegrand</code> <p>Integrand to find sensitivity indices of.</p> required <code>indices</code> <code>ndarray</code> <p>Bool array with shape \\((\\dots,d)\\) where each length \\(d\\) vector item indicates which dimensions are active in the subset.</p> <ul> <li>The default <code>indices='singletons'</code> sets <code>indices=np.eye(d,dtype=bool)</code>.</li> <li>Setting <code>incides='all'</code> sets <code>indices = np.array([[bool(int(b)) for b in np.binary_repr(i,width=d)] for i in range(1,2**d-1)],dtype=bool)</code></li> </ul> <code>'singletons'</code> Source code in <code>qmcpy/integrand/sensitivity_indices.py</code> <pre><code>def __init__(self, integrand, indices=\"singletons\"):\n    r\"\"\"\n    Args:\n        integrand (AbstractIntegrand): Integrand to find sensitivity indices of.\n        indices (np.ndarray): Bool array with shape $(\\dots,d)$ where each length $d$ vector item indicates which dimensions are active in the subset.\n\n            - The default `indices='singletons'` sets `indices=np.eye(d,dtype=bool)`.\n            - Setting `incides='all'` sets `indices = np.array([[bool(int(b)) for b in np.binary_repr(i,width=d)] for i in range(1,2**d-1)],dtype=bool)`\n    \"\"\"\n    self.parameters = [\"indices\"]\n    self.integrand = integrand\n    self.dtilde = self.integrand.d\n    assert self.dtilde &gt; 1, \"SensitivityIndices does not make sense for d=1\"\n    self.indices = indices\n    if isinstance(self.indices, str) and self.indices == \"singletons\":\n        self.indices = np.eye(self.dtilde, dtype=bool)\n    elif isinstance(self.indices, str) and self.indices == \"all\":\n        self.indices = np.zeros((0, self.dtilde), dtype=bool)\n        for r in range(1, self.dtilde):\n            idxs_r = np.zeros(\n                (int(scipy.special.comb(self.dtilde, r)), self.dtilde), dtype=bool\n            )\n            for i, comb in enumerate(combinations(range(self.dtilde), r)):\n                idxs_r[i, comb] = True\n            self.indices = np.vstack([self.indices, idxs_r])\n    self.indices = np.atleast_1d(self.indices)\n    assert (\n        self.indices.dtype == bool\n        and self.indices.ndim &gt;= 1\n        and self.indices.shape[-1] == self.dtilde\n    )\n    assert (\n        not (self.indices == self.indices[..., 0, None]).all(-1).any()\n    ), \"indices cannot include the emptyset or the set of all dimensions\"\n    self.not_indices = ~self.indices\n    # sensitivity_index\n    self.true_measure = self.integrand.true_measure\n    self.discrete_distrib = self.integrand.discrete_distrib.spawn(\n        s=1, dimensions=[2 * self.dtilde]\n    )[0]\n    self.sampler = self.integrand.sampler\n    self.i_slice = (slice(None),) * len(self.integrand.d_indv)\n    super(SensitivityIndices, self).__init__(\n        dimension_indv=(2, 3) + self.indices.shape[:-1] + self.integrand.d_indv,\n        dimension_comb=(2,) + self.indices.shape[:-1] + self.integrand.d_indv,\n        parallel=False,\n    )\n    self.d = 2 * self.dtilde\n</code></pre>"},{"location":"api/integrands/#keister","title":"<code>Keister</code>","text":"<p>               Bases: <code>AbstractIntegrand</code></p> <p>Keister function from [1].</p> \\[f(\\boldsymbol{t}) = \\pi^{d/2} \\cos(\\lVert \\boldsymbol{t} \\rVert_2) \\qquad \\boldsymbol{T} \\sim \\mathcal{N}(\\boldsymbol{0},\\mathsf{I}/2).\\] <p>Examples:</p> <pre><code>&gt;&gt;&gt; integrand = Keister(DigitalNetB2(2,seed=7))\n&gt;&gt;&gt; y = integrand(2**10)\n&gt;&gt;&gt; print(\"%.4f\"%y.mean())\n1.8080\n&gt;&gt;&gt; integrand.true_measure\nGaussian (AbstractTrueMeasure)\n    mean            0\n    covariance      2^(-1)\n    decomp_type     PCA\n</code></pre> <p>With independent replications</p> <pre><code>&gt;&gt;&gt; integrand = Keister(DigitalNetB2(2,seed=7,replications=2**4))\n&gt;&gt;&gt; y = integrand(2**6)\n&gt;&gt;&gt; y.shape\n(16, 64)\n&gt;&gt;&gt; muhats = y.mean(-1)\n&gt;&gt;&gt; muhats.shape\n(16,)\n&gt;&gt;&gt; print(\"%.4f\"%muhats.mean())\n1.8024\n</code></pre> <p>References:</p> <ol> <li>B. D. Keister.     Multidimensional Quadrature Algorithms.     Computers in Physics, 10, pp. 119-122, 1996.</li> </ol> <p>Parameters:</p> Name Type Description Default <code>sampler</code> <code>Union[AbstractDiscreteDistribution, AbstractTrueMeasure]</code> <p>Either</p> <ul> <li>a discrete distribution from which to transform samples, or</li> <li>a true measure by which to compose a transform.</li> </ul> required Source code in <code>qmcpy/integrand/keister.py</code> <pre><code>def __init__(self, sampler):\n    r\"\"\"\n    Args:\n        sampler (Union[AbstractDiscreteDistribution, AbstractTrueMeasure]): Either\n\n            - a discrete distribution from which to transform samples, or\n            - a true measure by which to compose a transform.\n    \"\"\"\n    self.sampler = sampler\n    self.true_measure = Gaussian(self.sampler, mean=0, covariance=1 / 2)\n    super(Keister, self).__init__(\n        dimension_indv=(), dimension_comb=(), parallel=False\n    )\n</code></pre>"},{"location":"api/integrands/#qmcpy.integrand.keister.Keister.get_exact_value","title":"get_exact_value  <code>classmethod</code>","text":"<pre><code>get_exact_value(d)\n</code></pre> <p>Compute the exact analytic value of the Keister integral with dimension \\(d\\).</p> <p>Parameters:</p> Name Type Description Default <code>d</code> <code>int</code> <p>Dimension.</p> required <p>Returns:</p> Name Type Description <code>mean</code> <code>float</code> <p>Exact value of the integral.</p> Source code in <code>qmcpy/integrand/keister.py</code> <pre><code>@classmethod\ndef get_exact_value(self, d):\n    \"\"\"\n    Compute the exact analytic value of the Keister integral with dimension $d$.\n\n    Args:\n        d (int): Dimension.\n\n    Returns:\n        mean (float): Exact value of the integral.\n    \"\"\"\n    cosinteg = np.zeros(shape=(d))\n    cosinteg[0] = np.sqrt(np.pi) / (2 * np.exp(1 / 4))\n    sininteg = np.zeros(shape=(d))\n    sininteg[0] = 4.244363835020225e-01\n    cosinteg[1] = (1 - sininteg[0]) / 2\n    sininteg[1] = cosinteg[0] / 2\n    for j in range(2, d):\n        cosinteg[j] = ((j - 1) * cosinteg[j - 2] - sininteg[j - 1]) / 2\n        sininteg[j] = ((j - 1) * sininteg[j - 2] + cosinteg[j - 1]) / 2\n    I = (2 * (np.pi ** (d / 2)) / gamma(d / 2)) * cosinteg[d - 1]\n    return I\n</code></pre>"},{"location":"api/integrands/#genz","title":"<code>Genz</code>","text":"<p>               Bases: <code>AbstractIntegrand</code></p> <p>Genz function following the <code>DAKOTA</code> implementation.</p> \\[g_\\mathrm{oscillatory}(\\boldsymbol{t}) = \\cos\\left(-\\sum_{j=1}^d c_j t_j\\right)\\] <p>or</p> \\[g_\\mathrm{corner-peak}(\\boldsymbol{t}) = \\left(1+\\sum_{j=1}^d c_j t_j\\right)^{-(d+1)}\\] <p>where</p> \\[\\boldsymbol{T} \\sim \\mathcal{U}[0,1]^d\\] <p>and the coefficients \\(\\boldsymbol{c}\\) are have three kinds</p> \\[c_k^{(1)} = \\frac{k-1/2}{d}, \\qquad c_k^{(2)} = \\frac{1}{k^2}, \\qquad c_k^{(3)} = \\exp\\left(\\frac{k \\log(10^{-8})}{d}\\right), \\qquad k=1,\\dots,d.\\] <p>Examples:</p> <pre><code>&gt;&gt;&gt; for kind_func in ['OSCILLATORY','CORNER PEAK']:\n...     for kind_coeff in [1,2,3]:\n...         integrand = Genz(DigitalNetB2(2,seed=7),kind_func=kind_func,kind_coeff=kind_coeff)\n...         y = integrand(2**14)\n...         mu_hat = y.mean()\n...         print('%-15s %-3d %.3f'%(kind_func,kind_coeff,mu_hat))\nOSCILLATORY     1   -0.351\nOSCILLATORY     2   -0.329\nOSCILLATORY     3   -0.217\nCORNER PEAK     1   0.713\nCORNER PEAK     2   0.714\nCORNER PEAK     3   0.720\n</code></pre> <p>With independent replications</p> <pre><code>&gt;&gt;&gt; integrand = Genz(DigitalNetB2(2,seed=7,replications=2**4),kind_func=\"CORNER PEAK\",kind_coeff=3)\n&gt;&gt;&gt; y = integrand(2**6)\n&gt;&gt;&gt; y.shape\n(16, 64)\n&gt;&gt;&gt; muhats = y.mean(-1)\n&gt;&gt;&gt; muhats.shape\n(16,)\n&gt;&gt;&gt; print(\"%.4f\"%muhats.mean())\n0.7200\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>sampler</code> <code>Union[AbstractDiscreteDistribution, AbstractTrueMeasure]</code> <p>Either</p> <ul> <li>a discrete distribution from which to transform samples, or</li> <li>a true measure by which to compose a transform.</li> </ul> required <code>kind_func</code> <code>str</code> <p>Either <code>'OSCILLATORY'</code> or <code>'CORNER PEAK'</code></p> <code>'OSCILLATORY'</code> <code>kind_coeff</code> <code>int</code> <p>1, 2, or 3 for choice of coefficients</p> <code>1</code> Source code in <code>qmcpy/integrand/genz.py</code> <pre><code>def __init__(self, sampler, kind_func=\"OSCILLATORY\", kind_coeff=1):\n    \"\"\"\n    Args:\n        sampler (Union[AbstractDiscreteDistribution, AbstractTrueMeasure]): Either\n\n            - a discrete distribution from which to transform samples, or\n            - a true measure by which to compose a transform.\n        kind_func (str): Either `'OSCILLATORY'` or `'CORNER PEAK'`\n        kind_coeff (int): 1, 2, or 3 for choice of coefficients\n    \"\"\"\n    self.kind_func = (\n        str(kind_func).upper().strip().replace(\"_\", \" \").replace(\"-\", \" \")\n    )\n    self.kind_coeff = kind_coeff\n    if (self.kind_func not in [\"OSCILLATORY\", \"CORNER PEAK\"]) or (\n        self.kind_coeff not in [1, 2, 3]\n    ):\n        raise ParameterError(\n            \"\"\"\n            Genz expects \n                kind_func in ['OSCILLATORY','CORNER PEAK'] and \n                kind_coeffs in [1,2,3]\"\"\"\n        )\n    self.sampler = sampler\n    self.true_measure = Uniform(self.sampler)\n    self.d = self.true_measure.d\n    if self.kind_coeff == 1:\n        self.c = (np.arange(1, self.d + 1) - 0.5) / self.d\n    elif self.kind_coeff == 2:\n        self.c = 1 / np.arange(1, self.d + 1) ** 2\n    elif self.kind_coeff == 3:\n        self.c = np.exp(np.arange(1, self.d + 1) * np.log(10 ** (-8)) / self.d)\n    if self.kind_func == \"OSCILLATORY\":\n        self.g = self.g_oscillatory\n        self.c = 4.5 * self.c / self.c.sum()\n    elif self.kind_func == \"CORNER PEAK\":\n        self.g = self.g_corner_peak\n        self.c = 0.25 * self.c / self.c.sum()\n    self.c = self.c[None, :]\n    self.parameters = [\"kind_func\", \"kind_coeff\"]\n    super(Genz, self).__init__(dimension_indv=(), dimension_comb=(), parallel=False)\n</code></pre>"},{"location":"api/integrands/#boxintegral","title":"<code>BoxIntegral</code>","text":"<p>               Bases: <code>AbstractIntegrand</code></p> <p>Box integral from [1], see also</p> \\[B_s(\\boldsymbol{t}) = \\left(\\sum_{j=1}^d t_j^2 \\right)^{s/2}, \\qquad \\boldsymbol{T} \\sim \\mathcal{U}[0,1]^d.\\] <p>Examples:</p> <p>Scalar <code>s</code></p> <pre><code>&gt;&gt;&gt; integrand = BoxIntegral(DigitalNetB2(2,seed=7),s=7)\n&gt;&gt;&gt; y = integrand(2**10)\n&gt;&gt;&gt; y.shape\n(1024,)\n&gt;&gt;&gt; print(\"%.4f\"%y.mean(0))\n0.7519\n</code></pre> <p>With independent replications</p> <pre><code>&gt;&gt;&gt; integrand = BoxIntegral(DigitalNetB2(2,seed=7,replications=2**4),s=7)\n&gt;&gt;&gt; y = integrand(2**10)\n&gt;&gt;&gt; y.shape\n(16, 1024)\n&gt;&gt;&gt; muhats = y.mean(1)\n&gt;&gt;&gt; muhats.shape\n(16,)\n&gt;&gt;&gt; print(\"%.4f\"%muhats.mean(0))\n0.7518\n</code></pre> <p>Array <code>s</code></p> <pre><code>&gt;&gt;&gt; integrand = BoxIntegral(DigitalNetB2(5,seed=7),s=np.arange(6).reshape((2,3)))\n&gt;&gt;&gt; y = integrand(2**10)\n&gt;&gt;&gt; y.shape\n(2, 3, 1024)\n&gt;&gt;&gt; y.mean(-1)\narray([[1.        , 1.26234461, 1.66666661],\n       [2.28201516, 3.22195096, 4.67188113]])\n</code></pre> <p>With independent replications</p> <pre><code>&gt;&gt;&gt; integrand = BoxIntegral(DigitalNetB2(2,seed=7,replications=2**4),s=np.arange(6).reshape((2,3)))\n&gt;&gt;&gt; y = integrand(2**10)\n&gt;&gt;&gt; y.shape\n(2, 3, 16, 1024)\n&gt;&gt;&gt; muhats = y.mean(-1)\n&gt;&gt;&gt; muhats.shape\n(2, 3, 16)\n&gt;&gt;&gt; muhats.mean(-1)\narray([[1.        , 0.76519118, 0.66666666],\n       [0.62718785, 0.62224086, 0.64273341]])\n</code></pre> <p>References:</p> <ol> <li>D.H. Bailey, J.M. Borwein, R.E. Crandall, Box integrals.     Journal of Computational and Applied Mathematics, Volume 206, Issue 1, 2007, Pages 196-208, ISSN 0377-0427.     https://doi.org/10.1016/j.cam.2006.06.010.     https://www.sciencedirect.com/science/article/pii/S0377042706004250.     https://www.davidhbailey.com/dhbpapers/boxintegrals.pdf</li> </ol> <p>Parameters:</p> Name Type Description Default <code>sampler</code> <code>Union[AbstractDiscreteDistribution, AbstractTrueMeasure]</code> <p>Either</p> <ul> <li>a discrete distribution from which to transform samples, or</li> <li>a true measure by which to compose a transform.</li> </ul> required <code>s</code> <code>Union[float, ndarray]</code> <p><code>s</code> parameter or parameters. The output shape of <code>g</code> is the shape of <code>s</code>.</p> <code>1</code> Source code in <code>qmcpy/integrand/box_integral.py</code> <pre><code>def __init__(self, sampler, s=1):\n    r\"\"\"\n    Args:\n        sampler (Union[AbstractDiscreteDistribution, AbstractTrueMeasure]): Either\n\n            - a discrete distribution from which to transform samples, or\n            - a true measure by which to compose a transform.\n        s (Union[float, np.ndarray]): `s` parameter or parameters. The output shape of `g` is the shape of `s`.\n    \"\"\"\n    self.parameters = [\"s\"]\n    self.s = np.array(s)\n    assert self.s.size &gt; 0\n    self.sampler = sampler\n    self.true_measure = Uniform(self.sampler)\n    self.s_over_2 = self.s / 2\n    super(BoxIntegral, self).__init__(\n        dimension_indv=self.s.shape, dimension_comb=self.s.shape, parallel=False\n    )\n</code></pre>"},{"location":"api/integrands/#ishigami","title":"<code>Ishigami</code>","text":"<p>               Bases: <code>AbstractIntegrand</code></p> <p>Ishigami function in \\(d=3\\) dimensions from [1] and https://www.sfu.ca/~ssurjano/ishigami.html.</p> \\[g(\\boldsymbol{t}) = (1+bt_2^4)\\sin(t_0)+a\\sin^2(t_1), \\qquad \\boldsymbol{T} = (T_0,T_1,T_2) \\sim \\mathcal{U}(-\\pi,\\pi)^3.\\] <p>Examples:</p> <pre><code>&gt;&gt;&gt; integrand = Ishigami(DigitalNetB2(3,seed=7))\n&gt;&gt;&gt; y = integrand(2**10)\n&gt;&gt;&gt; y.shape\n(1024,)\n&gt;&gt;&gt; print(\"%.4f\"%y.mean())\n3.5000\n&gt;&gt;&gt; integrand.true_measure\nUniform (AbstractTrueMeasure)\n    lower_bound     -3.142\n    upper_bound     3.142\n</code></pre> <p>With independent replications</p> <pre><code>&gt;&gt;&gt; integrand = Ishigami(DigitalNetB2(3,seed=7,replications=2**4))\n&gt;&gt;&gt; y = integrand(2**6)\n&gt;&gt;&gt; y.shape\n(16, 64)\n&gt;&gt;&gt; muhats = y.mean(-1)\n&gt;&gt;&gt; muhats.shape\n(16,)\n&gt;&gt;&gt; print(\"%.4f\"%muhats.mean())\n3.4646\n</code></pre> <p>References:</p> <ol> <li>Ishigami, T., &amp; Homma, T.     An importance quantification technique in uncertainty analysis for computer models.     In Uncertainty Modeling and Analysis, 1990.     Proceedings, First International Symposium on (pp. 398-403). IEEE.</li> </ol> <p>Parameters:</p> Name Type Description Default <code>sampler</code> <code>Union[AbstractDiscreteDistribution, AbstractTrueMeasure]</code> <p>Either</p> <ul> <li>a discrete distribution from which to transform samples, or</li> <li>a true measure by which to compose a transform.</li> </ul> required <code>a</code> <code>float</code> <p>First parameter \\(a\\).</p> <code>7</code> <code>b</code> <code>float</code> <p>Second parameter \\(b\\).</p> <code>0.1</code> Source code in <code>qmcpy/integrand/ishigami.py</code> <pre><code>def __init__(self, sampler, a=7, b=0.1):\n    r\"\"\"\n    Args:\n        sampler (Union[AbstractDiscreteDistribution, AbstractTrueMeasure]): Either\n\n            - a discrete distribution from which to transform samples, or\n            - a true measure by which to compose a transform.\n        a (float): First parameter $a$.\n        b (float): Second parameter $b$.\n    \"\"\"\n    self.sampler = sampler\n    if self.sampler.d != 3:\n        raise ParameterError(\"Ishigami integrand requires 3 dimensional sampler\")\n    self.a = a\n    self.b = b\n    self.true_measure = Uniform(self.sampler, lower_bound=-np.pi, upper_bound=np.pi)\n    super(Ishigami, self).__init__(\n        dimension_indv=(), dimension_comb=(), parallel=False\n    )\n</code></pre>"},{"location":"api/integrands/#multimodal2d","title":"<code>Multimodal2d</code>","text":"<p>               Bases: <code>AbstractIntegrand</code></p> <p>Multimodal function in \\(d=2\\) dimensions.</p> \\[g(\\boldsymbol{t}) = (t_0^2+4)(t_1-1)/20-\\sin(5t_0/2)-2 \\qquad \\boldsymbol{T} = (T_0,T_1) \\sim \\mathcal{U}([-4,7] \\times [-3,8]).\\] <p>Examples:</p> <pre><code>&gt;&gt;&gt; integrand = Multimodal2d(DigitalNetB2(2,seed=7))\n&gt;&gt;&gt; y = integrand(2**10)\n&gt;&gt;&gt; print(\"%.4f\"%y.mean())\n-0.7365\n&gt;&gt;&gt; integrand.true_measure\nUniform (AbstractTrueMeasure)\n    lower_bound     [-4 -3]\n    upper_bound     [7 8]\n</code></pre> <p>With independent replications</p> <pre><code>&gt;&gt;&gt; integrand = Multimodal2d(DigitalNetB2(2,seed=7,replications=2**4))\n&gt;&gt;&gt; y = integrand(2**6)\n&gt;&gt;&gt; y.shape\n(16, 64)\n&gt;&gt;&gt; muhats = y.mean(-1)\n&gt;&gt;&gt; muhats.shape\n(16,)\n&gt;&gt;&gt; print(\"%.4f\"%muhats.mean())\n-0.7366\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>sampler</code> <code>Union[AbstractDiscreteDistribution, AbstractTrueMeasure]</code> <p>Either</p> <ul> <li>a discrete distribution from which to transform samples, or</li> <li>a true measure by which to compose a transform.</li> </ul> required Source code in <code>qmcpy/integrand/multimodal2d.py</code> <pre><code>def __init__(self, sampler):\n    r\"\"\"\n    Args:\n        sampler (Union[AbstractDiscreteDistribution, AbstractTrueMeasure]): Either\n\n            - a discrete distribution from which to transform samples, or\n            - a true measure by which to compose a transform.\n    \"\"\"\n    self.sampler = sampler\n    assert self.sampler.d == 2\n    self.true_measure = Uniform(\n        self.sampler, lower_bound=[-4, -3], upper_bound=[7, 8]\n    )\n    super(Multimodal2d, self).__init__(\n        dimension_indv=(), dimension_comb=(), parallel=False\n    )\n</code></pre>"},{"location":"api/integrands/#hartmann6d","title":"<code>Hartmann6d</code>","text":"<p>               Bases: <code>AbstractIntegrand</code></p> <p>Wrapper around <code>BoTorch</code>'s implementation of the Augmented Hartmann function in dimension \\(d=6\\).</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; integrand = Hartmann6d(DigitalNetB2(6,seed=7))\n&gt;&gt;&gt; y = integrand(2**10)\n&gt;&gt;&gt; print(\"%.4f\"%y.mean())\n-0.2644\n&gt;&gt;&gt; integrand.true_measure\nUniform (AbstractTrueMeasure)\n    lower_bound     0\n    upper_bound     1\n</code></pre> <p>With independent replications</p> <pre><code>&gt;&gt;&gt; integrand = Hartmann6d(DigitalNetB2(6,seed=7,replications=2**4))\n&gt;&gt;&gt; y = integrand(2**6)\n&gt;&gt;&gt; y.shape\n(16, 64)\n&gt;&gt;&gt; muhats = y.mean(-1)\n&gt;&gt;&gt; muhats.shape\n(16,)\n&gt;&gt;&gt; print(\"%.4f\"%muhats.mean())\n-0.2599\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>sampler</code> <code>Union[AbstractDiscreteDistribution, AbstractTrueMeasure]</code> <p>Either</p> <ul> <li>a discrete distribution from which to transform samples, or</li> <li>a true measure by which to compose a transform.</li> </ul> required Source code in <code>qmcpy/integrand/hartmann6d.py</code> <pre><code>def __init__(self, sampler):\n    r\"\"\"\n    Args:\n        sampler (Union[AbstractDiscreteDistribution, AbstractTrueMeasure]): Either\n\n            - a discrete distribution from which to transform samples, or\n            - a true measure by which to compose a transform.\n    \"\"\"\n    self.sampler = sampler\n    assert self.sampler.d == 6\n    self.true_measure = Uniform(self.sampler, lower_bound=0, upper_bound=1)\n    super(Hartmann6d, self).__init__(\n        dimension_indv=(), dimension_comb=(), parallel=False\n    )\n    from botorch.test_functions.multi_fidelity import AugmentedHartmann\n\n    self.ah = AugmentedHartmann(negate=False)\n</code></pre>"},{"location":"api/integrands/#fourbranch2d","title":"<code>FourBranch2d</code>","text":"<p>               Bases: <code>AbstractIntegrand</code></p> <p>Four Branch function in \\(d=2\\).</p> \\[g(\\boldsymbol{t}) = \\min \\begin{cases} 3+0.1(t_0-t_1)^2-\\frac{t_0-t_1}{\\sqrt{2}} \\\\ 3+0.1(t_0-t_1)^2+\\frac{t_0-t_1}{\\sqrt{2}} \\\\ t_0-t_1 + 7/\\sqrt{2} \\\\ t_1-t_0 + 7/\\sqrt{2}\\end{cases}, \\qquad \\boldsymbol{T}=(T_0,T_1) \\sim \\mathcal{U}[-8,8]^2.\\] <p>Examples:</p> <pre><code>&gt;&gt;&gt; integrand = FourBranch2d(DigitalNetB2(2,seed=7))\n&gt;&gt;&gt; y = integrand(2**10)\n&gt;&gt;&gt; print(\"%.4f\"%y.mean())\n-2.4995\n&gt;&gt;&gt; integrand.true_measure\nUniform (AbstractTrueMeasure)\n    lower_bound     -8\n    upper_bound     2^(3)\n</code></pre> <p>With independent replications</p> <pre><code>&gt;&gt;&gt; integrand = FourBranch2d(DigitalNetB2(2,seed=7,replications=2**4))\n&gt;&gt;&gt; y = integrand(2**6)\n&gt;&gt;&gt; y.shape\n(16, 64)\n&gt;&gt;&gt; muhats = y.mean(-1)\n&gt;&gt;&gt; muhats.shape\n(16,)\n&gt;&gt;&gt; print(\"%.4f\"%muhats.mean())\n-2.5042\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>sampler</code> <code>Union[AbstractDiscreteDistribution, AbstractTrueMeasure]</code> <p>Either</p> <ul> <li>a discrete distribution from which to transform samples, or</li> <li>a true measure by which to compose a transform.</li> </ul> required Source code in <code>qmcpy/integrand/fourbranch2d.py</code> <pre><code>def __init__(self, sampler):\n    r\"\"\"\n    Args:\n        sampler (Union[AbstractDiscreteDistribution, AbstractTrueMeasure]): Either\n\n            - a discrete distribution from which to transform samples, or\n            - a true measure by which to compose a transform.\n    \"\"\"\n    self.sampler = sampler\n    assert self.sampler.d == 2\n    self.true_measure = Uniform(self.sampler, lower_bound=-8, upper_bound=8)\n    super(FourBranch2d, self).__init__(\n        dimension_indv=(), dimension_comb=(), parallel=False\n    )\n</code></pre>"},{"location":"api/integrands/#bayesianlrcoeffs","title":"<code>BayesianLRCoeffs</code>","text":"<p>               Bases: <code>AbstractIntegrand</code></p> <p>Logistic Regression Coefficients computed as the posterior mean in a Bayesian framework.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; integrand = BayesianLRCoeffs(DigitalNetB2(3,seed=7),feature_array=np.arange(8).reshape((4,2)),response_vector=[0,0,1,1])\n&gt;&gt;&gt; y = integrand(2**10)\n&gt;&gt;&gt; y.shape\n(2, 3, 1024)\n&gt;&gt;&gt; y.mean(-1)\narray([[ 0.04517466, -0.01103669, -0.06614381],\n       [ 0.02162049,  0.02162049,  0.02162049]])\n</code></pre> <p>With independent replications</p> <pre><code>&gt;&gt;&gt; integrand = BayesianLRCoeffs(DigitalNetB2(3,seed=7,replications=2**4),feature_array=np.arange(8).reshape((4,2)),response_vector=[0,0,1,1])\n&gt;&gt;&gt; y = integrand(2**6)\n&gt;&gt;&gt; y.shape\n(2, 3, 16, 64)\n&gt;&gt;&gt; muhats = y.mean(-1)\n&gt;&gt;&gt; muhats.shape\n(2, 3, 16)\n&gt;&gt;&gt; muhats.mean(-1)\narray([[ 0.0587368 , -0.01718134, -0.07203021],\n       [ 0.02498059,  0.02498059,  0.02498059]])\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>sampler</code> <code>Union[AbstractDiscreteDistribution, AbstractTrueMeasure]</code> <p>Either</p> <ul> <li>a discrete distribution from which to transform samples, or</li> <li>a true measure by which to compose a transform.</li> </ul> required <code>feature_array</code> <code>ndarray</code> <p>Array of features with shape \\((N,d-1)\\) where \\(N\\) is the number of observations and \\(d\\) is the dimension.</p> required <code>response_vector</code> <code>ndarray</code> <p>Binary responses vector of length \\(N\\).</p> required <code>prior_mean</code> <code>ndarray</code> <p>Length \\(d\\) vector of prior means, one for each coefficient.</p> <ul> <li>The first \\(d-1\\) inputs correspond to the \\(d-1\\) features.</li> <li>The last input corresponds to the intercept coefficient.</li> </ul> <code>0</code> <code>prior_covariance</code> <code>ndarray</code> <p>Prior covariance array with shape \\((d,d)\\) d x d where indexing is consistent with the prior mean.</p> <code>10</code> Source code in <code>qmcpy/integrand/bayesian_lr_coeffs.py</code> <pre><code>def __init__(\n    self, sampler, feature_array, response_vector, prior_mean=0, prior_covariance=10\n):\n    r\"\"\"\n    Args:\n        sampler (Union[AbstractDiscreteDistribution, AbstractTrueMeasure]): Either\n\n            - a discrete distribution from which to transform samples, or\n            - a true measure by which to compose a transform.\n        feature_array (np.ndarray): Array of features with shape $(N,d-1)$ where $N$ is the number of observations and $d$ is the dimension.\n        response_vector (np.ndarray): Binary responses vector of length $N$.\n        prior_mean (np.ndarray): Length $d$ vector of prior means, one for each coefficient.\n\n            - The first $d-1$ inputs correspond to the $d-1$ features.\n            - The last input corresponds to the intercept coefficient.\n        prior_covariance (np.ndarray): Prior covariance array with shape $(d,d)$ d x d where indexing is consistent with the prior mean.\n    \"\"\"\n    self.prior_mean = prior_mean\n    self.prior_covariance = prior_covariance\n    self.sampler = sampler\n    self.true_measure = Gaussian(\n        self.sampler, mean=self.prior_mean, covariance=self.prior_covariance\n    )\n    self.feature_array = np.array(feature_array, dtype=float)\n    self.response_vector = np.array(response_vector, dtype=float)\n    obs, dm1 = self.feature_array.shape\n    self.num_coeffs = dm1 + 1\n    if self.num_coeffs != self.true_measure.d:\n        ParameterError(\n            \"sampler must have dimension one more than the number of features in the feature_array.\"\n        )\n    if (\n        self.response_vector.shape != (obs,)\n        or ((self.response_vector != 0) &amp; (self.response_vector != 1)).any()\n    ):\n        ParameterError(\n            \"response_vector must have the same length as feature_array and contain only 0 or 1 entries.\"\n        )\n    self.feature_array = np.column_stack((self.feature_array, np.ones((obs, 1))))\n    super(BayesianLRCoeffs, self).__init__(\n        dimension_indv=(2, self.num_coeffs),\n        dimension_comb=self.num_coeffs,\n        parallel=False,\n    )\n</code></pre>"},{"location":"api/integrands/#sin1d","title":"<code>Sin1d</code>","text":"<p>               Bases: <code>AbstractIntegrand</code></p> <p>Sine function in \\(d=1\\) dimension.</p> \\[g(t) = \\sin(t), \\qquad t \\sim \\mathcal{U}[0,2\\pi k]\\] <p>Examples:</p> <pre><code>&gt;&gt;&gt; integrand = Sin1d(DigitalNetB2(1,seed=7),k=1)\n&gt;&gt;&gt; y = integrand(2**10)\n&gt;&gt;&gt; print(\"%.4e\"%y.mean())\n-1.3582e-10\n&gt;&gt;&gt; integrand.true_measure\nUniform (AbstractTrueMeasure)\n    lower_bound     0\n    upper_bound     6.283\n</code></pre> <p>With independent replications</p> <pre><code>&gt;&gt;&gt; integrand = Sin1d(DigitalNetB2(1,seed=7,replications=2**4),k=1)\n&gt;&gt;&gt; y = integrand(2**6)\n&gt;&gt;&gt; y.shape\n(16, 64)\n&gt;&gt;&gt; muhats = y.mean(-1)\n&gt;&gt;&gt; muhats.shape\n(16,)\n&gt;&gt;&gt; print(\"%.4e\"%muhats.mean())\n7.0800e-04\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>sampler</code> <code>Union[AbstractDiscreteDistribution, AbstractTrueMeasure]</code> <p>Either</p> <ul> <li>a discrete distribution from which to transform samples, or</li> <li>a true measure by which to compose a transform.</li> </ul> required <code>k</code> <code>float</code> <p>The true measure will be uniform between \\(0\\) and \\(2 \\pi k\\).</p> <code>1</code> Source code in <code>qmcpy/integrand/sin1d.py</code> <pre><code>def __init__(self, sampler, k=1):\n    r\"\"\"\n    Args:\n        sampler (Union[AbstractDiscreteDistribution, AbstractTrueMeasure]): Either\n\n            - a discrete distribution from which to transform samples, or\n            - a true measure by which to compose a transform.\n        k (float): The true measure will be uniform between $0$ and $2 \\pi k$.\n    \"\"\"\n    self.sampler = sampler\n    self.k = k\n    assert self.sampler.d == 1\n    self.true_measure = Uniform(\n        self.sampler, lower_bound=0, upper_bound=2 * self.k * np.pi\n    )\n    super(Sin1d, self).__init__(\n        dimension_indv=(), dimension_comb=(), parallel=False\n    )\n</code></pre>"},{"location":"api/integrands/#linear0","title":"<code>Linear0</code>","text":"<p>               Bases: <code>AbstractIntegrand</code></p> <p>Linear Function with analytic mean \\(0\\).</p> \\[g(\\boldsymbol{t}) = \\sum_{j=1}^d t_j \\qquad \\boldsymbol{T} \\sim \\mathcal{U}[0,1]^d.\\] <p>Examples:</p> <pre><code>&gt;&gt;&gt; integrand = Linear0(DigitalNetB2(100,seed=7))\n&gt;&gt;&gt; y = integrand(2**10)\n&gt;&gt;&gt; print(\"%.4e\"%y.mean())\n6.0560e-05\n</code></pre> <p>With independent replications</p> <pre><code>&gt;&gt;&gt; integrand = Linear0(DigitalNetB2(100,seed=7,replications=2**4))\n&gt;&gt;&gt; y = integrand(2**6)\n&gt;&gt;&gt; y.shape\n(16, 64)\n&gt;&gt;&gt; muhats = y.mean(-1)\n&gt;&gt;&gt; muhats.shape\n(16,)\n&gt;&gt;&gt; print(\"%.4e\"%muhats.mean())\n-9.8203e-05\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>sampler</code> <code>Union[AbstractDiscreteDistribution, AbstractTrueMeasure]</code> <p>Either</p> <ul> <li>a discrete distribution from which to transform samples, or</li> <li>a true measure by which to compose a transform.</li> </ul> required Source code in <code>qmcpy/integrand/linear0.py</code> <pre><code>def __init__(self, sampler):\n    r\"\"\"\n    Args:\n        sampler (Union[AbstractDiscreteDistribution, AbstractTrueMeasure]): Either\n\n            - a discrete distribution from which to transform samples, or\n            - a true measure by which to compose a transform.\n    \"\"\"\n    self.sampler = sampler\n    self.true_measure = Uniform(self.sampler, lower_bound=-0.5, upper_bound=0.5)\n    super(Linear0, self).__init__(\n        dimension_indv=(), dimension_comb=(), parallel=False\n    )\n</code></pre>"},{"location":"api/integrands/#uml-specific","title":"UML Specific","text":""},{"location":"api/kernels/","title":"Kernels","text":""},{"location":"api/kernels/#kernels","title":"Kernels","text":""},{"location":"api/kernels/#uml-overview","title":"UML Overview","text":""},{"location":"api/kernels/#abstractkernel","title":"<code>AbstractKernel</code>","text":"<p>               Bases: <code>object</code></p> Source code in <code>qmcpy/kernel/abstract_kernel.py</code> <pre><code>def __init__(self, d, torchify, device, compile_call, comiple_call_kwargs):\n    super().__init__()\n    # dimension\n    assert d % 1 == 0 and d &gt; 0, \"dimension d must be a positive int\"\n    self.d = d\n    # torchify\n    self.torchify = torchify\n    if self.torchify:\n        import torch\n\n        self.npt = torch\n        self.nptarray = torch.tensor\n        self.nptarraytype = torch.Tensor\n        self.device = torch.device(device)\n        self.nptkwargs = {\"device\": device}\n    else:\n        self.npt = np\n        self.nptarray = np.array\n        self.nptarraytype = np.ndarray\n        self.device = None\n        self.nptkwargs = {}\n    self.batch_param_names = []\n    if compile_call:\n        assert self.torchify, \"compile_call requires torchify is True\"\n        import torch\n\n        self.compiled_parsed___call__ = torch.compile(\n            self.parsed___call__, **comiple_call_kwargs\n        )\n    else:\n        self.compiled_parsed___call__ = self.parsed___call__\n</code></pre>"},{"location":"api/kernels/#qmcpy.kernel.abstract_kernel.AbstractKernel.__call__","title":"__call__","text":"<pre><code>__call__(x0, x1, beta0=None, beta1=None, c=None, **kwargs)\n</code></pre> <p>Evaluate the kernel with (optional) partial derivatives</p> \\[\\sum_{\\ell=1}^p c_{\\ell} \\partial_{\\boldsymbol{x}_0}^{\\boldsymbol{\\beta}_{\\ell 0}} \\partial_{\\boldsymbol{x}_1}^{\\boldsymbol{\\beta}_{\\ell 1}} K(\\boldsymbol{x}_0,\\boldsymbol{x}_1).\\] <p>Parameters:</p> Name Type Description Default <code>x0</code> <code>Union[ndarray, Tensor]</code> <p>Shape <code>x0.shape=(...,d)</code> first input to kernel with</p> required <code>x1</code> <code>Union[ndarray, Tensor]</code> <p>Shape <code>x1.shape=(...,d)</code> second input to kernel with</p> required <code>beta0</code> <code>Union[ndarray, Tensor]</code> <p>Shape <code>beta0.shape=(p,d)</code> derivative orders with respect to first inputs, \\(\\boldsymbol{\\beta}_0\\).</p> <code>None</code> <code>beta1</code> <code>Union[ndarray, Tensor]</code> <p>Shape <code>beta1.shape=(p,d)</code> derivative orders with respect to first inputs, \\(\\boldsymbol{\\beta}_1\\).</p> <code>None</code> <code>c</code> <code>Union[ndarray, Tensor]</code> <p>Shape <code>c.shape=(p,)</code> coefficients of derivatives.</p> <code>None</code> <code>kwargs</code> <code>dict</code> <p>keyword arguments to parsed call</p> <code>{}</code> <p>Returns:     k (Union[np.ndarray, torch.Tensor]): Shape <code>y.shape=(x0+x1).shape[:-1]</code> kernel evaluations.</p> Source code in <code>qmcpy/kernel/abstract_kernel.py</code> <pre><code>def __call__(self, x0, x1, beta0=None, beta1=None, c=None, **kwargs):\n    r\"\"\"\n    Evaluate the kernel with (optional) partial derivatives\n\n    $$\\sum_{\\ell=1}^p c_{\\ell} \\partial_{\\boldsymbol{x}_0}^{\\boldsymbol{\\beta}_{\\ell 0}} \\partial_{\\boldsymbol{x}_1}^{\\boldsymbol{\\beta}_{\\ell 1}} K(\\boldsymbol{x}_0,\\boldsymbol{x}_1).$$\n\n    Args:\n        x0 (Union[np.ndarray, torch.Tensor]): Shape `x0.shape=(...,d)` first input to kernel with\n        x1 (Union[np.ndarray, torch.Tensor]): Shape `x1.shape=(...,d)` second input to kernel with\n        beta0 (Union[np.ndarray, torch.Tensor]): Shape `beta0.shape=(p,d)` derivative orders with respect to first inputs, $\\boldsymbol{\\beta}_0$.\n        beta1 (Union[np.ndarray, torch.Tensor]): Shape `beta1.shape=(p,d)` derivative orders with respect to first inputs, $\\boldsymbol{\\beta}_1$.\n        c (Union[np.ndarray, torch.Tensor]): Shape `c.shape=(p,)` coefficients of derivatives.\n        kwargs (dict): keyword arguments to parsed call\n    Returns:\n        k (Union[np.ndarray, torch.Tensor]): Shape `y.shape=(x0+x1).shape[:-1]` kernel evaluations.\n    \"\"\"\n    assert isinstance(x0, self.nptarraytype)\n    assert isinstance(x0, self.nptarraytype)\n    assert (\n        x0.shape[-1] == self.d\n    ), \"the size of the last dimension of x0 must equal d=%d, got x0.shape=%s\" % (\n        self.d,\n        str(tuple(x0.shape)),\n    )\n    assert (\n        x1.shape[-1] == self.d\n    ), \"the size of the last dimension of x1 must equal d=%d, got x1.shape=%s\" % (\n        self.d,\n        str(tuple(x1.shape)),\n    )\n    if beta0 is None:\n        beta0 = self.npt.zeros((1, self.d), dtype=int, **self.nptkwargs)\n    if beta1 is None:\n        beta1 = self.npt.zeros((1, self.d), dtype=int, **self.nptkwargs)\n    if not isinstance(beta0, self.nptarraytype):\n        beta0 = self.nptarray(beta0)\n    if not isinstance(beta1, self.nptarraytype):\n        beta1 = self.nptarray(beta1)\n    beta0 = self.npt.atleast_2d(beta0)\n    beta1 = self.npt.atleast_2d(beta1)\n    assert (\n        beta0.ndim == 2 and beta1.ndim == 2\n    ), \"beta0 and beta1 must both be 2 dimensional\"\n    p = beta0.shape[0]\n    assert beta0.shape == (\n        p,\n        self.d,\n    ), \"expected beta0.shape=(%d,%d) but got beta0.shape=%s\" % (\n        p,\n        self.d,\n        str(tuple(beta0.shape)),\n    )\n    assert beta1.shape == (\n        p,\n        self.d,\n    ), \"expected beta1.shape=(%d,%d) but got beta1.shape=%s\" % (\n        p,\n        self.d,\n        str(tuple(beta1.shape)),\n    )\n    assert (beta0 % 1 == 0).all() and (beta0 &gt;= 0).all(), \"require int beta0 &gt;= 0\"\n    assert (beta1 % 1 == 0).all() and (beta1 &gt;= 0).all(), \"require int beta1 &gt;= 0\"\n    if c is None:\n        c = self.npt.ones(p, **self.nptkwargs)\n    if not isinstance(c, self.nptarraytype):\n        c = self.nptarray(c)\n    c = self.npt.atleast_1d(c)\n    assert c.shape == (p,), \"expected c.shape=(%d,) but got c.shape=%s\" % (\n        p,\n        str(tuple(c.shape)),\n    )\n    if not self.AUTOGRADKERNEL:\n        batch_params = self.get_batch_params(max(x0.ndim - 1, x1.ndim - 1))\n        k = self.compiled_parsed___call__(\n            x0, x1, beta0, beta1, c, batch_params, **kwargs\n        )\n    else:\n        if (beta0 == 0).all() and (beta1 == 0).all():\n            batch_params = self.get_batch_params(max(x0.ndim - 1, x1.ndim - 1))\n            k = c.sum() * self.compiled_parsed___call__(\n                x0, x1, batch_params, **kwargs\n            )\n        else:  # requires autograd, so self.npt=torch\n            assert self.torchify, \"autograd requires torchify=True\"\n            import torch\n\n            incoming_grad_enabled = torch.is_grad_enabled()\n            torch.set_grad_enabled(True)\n            incoming_grad_enabled_params = {\n                pname: param.requires_grad\n                for pname, param in self.named_parameters()\n            }\n            if not incoming_grad_enabled:\n                for pname, param in self.named_parameters():\n                    param.requires_grad_(False)\n            if (beta0 &gt; 0).any():\n                tileshapex0 = tuple(\n                    self.npt.ceil(\n                        self.npt.tensor(x1.shape[:-1])\n                        / self.npt.tensor(x0.shape[:-1])\n                    ).to(int)\n                )\n                x0gs = [\n                    self.npt.tile(\n                        x0[..., j].clone().requires_grad_(True), tileshapex0\n                    )\n                    for j in range(self.d)\n                ]\n                [x0gj.requires_grad_(True) for x0gj in x0gs]\n                x0g = self.npt.stack(x0gs, dim=-1)\n            else:\n                x0g = x0\n            if (beta1 &gt; 0).any():\n                tileshapex1 = tuple(\n                    self.npt.ceil(\n                        self.npt.tensor(x0.shape[:-1])\n                        / self.npt.tensor(x1.shape[:-1])\n                    ).to(int)\n                )\n                x1gs = [\n                    self.npt.tile(\n                        x1[..., j].clone().requires_grad_(True), tileshapex1\n                    )\n                    for j in range(self.d)\n                ]\n                [x1gj.requires_grad_(True) for x1gj in x1gs]\n                x1g = self.npt.stack(x1gs, dim=-1)\n            else:\n                x1g = x1\n            batch_params = self.get_batch_params(max(x0.ndim - 1, x1.ndim - 1))\n            k = 0.0\n            k_base = self.compiled_parsed___call__(x0g, x1g, batch_params, **kwargs)\n            for l in range(p):\n                if (beta0[l] &gt; 0).any() or (beta1[l] &gt; 0).any():\n                    k_part = k_base.clone()\n                    for j0 in range(self.d):\n                        for _ in range(beta0[l, j0]):\n                            k_part = torch.autograd.grad(\n                                k_part,\n                                x0gs[j0],\n                                grad_outputs=torch.ones_like(\n                                    k_part, requires_grad=True\n                                ),\n                                create_graph=True,\n                            )[0]\n                    for j1 in range(self.d):\n                        for _ in range(beta1[l, j1]):\n                            k_part = torch.autograd.grad(\n                                k_part,\n                                x1gs[j1],\n                                grad_outputs=torch.ones_like(\n                                    k_part, requires_grad=True\n                                ),\n                                create_graph=True,\n                            )[0]\n                else:\n                    k_part = k_base\n                k += c[l] * k_part\n            if not incoming_grad_enabled:\n                for pname, param in self.named_parameters():\n                    param.requires_grad_(incoming_grad_enabled_params[pname])\n            if (not incoming_grad_enabled) or (\n                (not any(incoming_grad_enabled_params.values()))\n                and (not x0.requires_grad)\n                and (not x1.requires_grad)\n            ):\n                k = k.detach()\n            torch.set_grad_enabled(incoming_grad_enabled)\n    return k\n</code></pre>"},{"location":"api/kernels/#qmcpy.kernel.abstract_kernel.AbstractKernel.single_integral_01d","title":"single_integral_01d","text":"<pre><code>single_integral_01d(x)\n</code></pre> <p>Evaluate the integral of the kernel over the unit cube</p> \\[\\tilde{K}(\\boldsymbol{x}) = \\int_{[0,1]^d} K(\\boldsymbol{x},\\boldsymbol{z}) \\; \\mathrm{d} \\boldsymbol{z}.\\] <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Union[ndarray, Tensor]</code> <p>Shape <code>x0.shape=(...,d)</code> first input to kernel with</p> required <p>Returns:</p> Name Type Description <code>tildek</code> <code>Union[ndarray, Tensor]</code> <p>Shape <code>y.shape=x.shape[:-1]</code> integral kernel evaluations.</p> Source code in <code>qmcpy/kernel/abstract_kernel.py</code> <pre><code>def single_integral_01d(self, x):\n    r\"\"\"\n    Evaluate the integral of the kernel over the unit cube\n\n    $$\\tilde{K}(\\boldsymbol{x}) = \\int_{[0,1]^d} K(\\boldsymbol{x},\\boldsymbol{z}) \\; \\mathrm{d} \\boldsymbol{z}.$$\n\n    Args:\n        x (Union[np.ndarray, torch.Tensor]): Shape `x0.shape=(...,d)` first input to kernel with\n\n    Returns:\n        tildek (Union[np.ndarray, torch.Tensor]): Shape `y.shape=x.shape[:-1]` integral kernel evaluations.\n    \"\"\"\n    if self.npt == np:\n        assert isinstance(x, np.ndarray)\n    else:  # self.npt==torch\n        assert isinstance(x, self.npt.Tensor)\n    assert (\n        x.shape[-1] == self.d\n    ), \"the size of the last dimension of x must equal d=%d, got x.shape=%s\" % (\n        self.d,\n        str(tuple(x.shape)),\n    )\n    batch_params = self.get_batch_params(x.ndim - 1)\n    return self.parsed_single_integral_01d(x, batch_params)\n</code></pre>"},{"location":"api/kernels/#qmcpy.kernel.abstract_kernel.AbstractKernel.double_integral_01d","title":"double_integral_01d","text":"<pre><code>double_integral_01d()\n</code></pre> <p>Evaluate the integral of the kernel over the unit cube</p> \\[\\tilde{K} = \\int_{[0,1]^d} \\int_{[0,1]^d} K(\\boldsymbol{x},\\boldsymbol{z}) \\; \\mathrm{d} \\boldsymbol{x} \\; \\mathrm{d} \\boldsymbol{z}.\\] <p>Returns:</p> Name Type Description <code>tildek</code> <code>Union[ndarray, Tensor]</code> <p>Double integral kernel evaluations.</p> Source code in <code>qmcpy/kernel/abstract_kernel.py</code> <pre><code>def double_integral_01d(self):\n    r\"\"\"\n    Evaluate the integral of the kernel over the unit cube\n\n    $$\\tilde{K} = \\int_{[0,1]^d} \\int_{[0,1]^d} K(\\boldsymbol{x},\\boldsymbol{z}) \\; \\mathrm{d} \\boldsymbol{x} \\; \\mathrm{d} \\boldsymbol{z}.$$\n\n    Returns:\n        tildek (Union[np.ndarray, torch.Tensor]): Double integral kernel evaluations.\n    \"\"\"\n    raise MethodImplementationError(self, \"double_integral_01d\")\n</code></pre>"},{"location":"api/kernels/#abstractkernelscalelengthscales","title":"<code>AbstractKernelScaleLengthscales</code>","text":"<p>               Bases: <code>AbstractKernel</code></p> <p>Parameters:</p> Name Type Description Default <code>d</code> <code>int</code> <p>Dimension.</p> required <code>scale</code> <code>Union[ndarray, Tensor]</code> <p>Scaling factor \\(S\\).</p> <code>1.0</code> <code>lengthscales</code> <code>Union[ndarray, Tensor]</code> <p>Lengthscales \\(\\boldsymbol{\\gamma}\\).</p> <code>1.0</code> <code>shape_scale</code> <code>list</code> <p>Shape of <code>scale</code> when <code>np.isscalar(scale)</code>.</p> <code>[1]</code> <code>shape_lengthscales</code> <code>list</code> <p>Shape of <code>lengthscales</code> when <code>np.isscalar(lengthscales)</code></p> <code>None</code> <code>tfs_scale</code> <code>Tuple[callable, callable]</code> <p>The first argument transforms to the raw value to be optimized; the second applies the inverse transform.</p> <code>(tf_exp_eps_inv, tf_exp_eps)</code> <code>tfs_lengthscales</code> <code>Tuple[callable, callable]</code> <p>The first argument transforms to the raw value to be optimized; the second applies the inverse transform.</p> <code>(tf_exp_eps_inv, tf_exp_eps)</code> <code>torchify</code> <code>bool</code> <p>If <code>True</code>, use the <code>torch</code> backend. Set to <code>True</code> if computing gradients with respect to inputs and/or hyperparameters.</p> <code>False</code> <code>requires_grad_scale</code> <code>bool</code> <p>If <code>True</code> and <code>torchify</code>, set <code>requires_grad=True</code> for <code>scale</code>.</p> <code>True</code> <code>requires_grad_lengthscales</code> <code>bool</code> <p>If <code>True</code> and <code>torchify</code>, set <code>requires_grad=True</code> for <code>lengthscales</code>.</p> <code>True</code> <code>device</code> <code>device</code> <p>If <code>torchify</code>, put things onto this device.</p> <code>'cpu'</code> <code>compile_call</code> <code>bool</code> <p>If <code>True</code>, <code>torch.compile</code> the <code>parsed___call__</code> method.</p> <code>False</code> <code>comiple_call_kwargs</code> <code>dict</code> <p>When <code>compile_call</code> is <code>True</code>, pass these keyword arguments to <code>torch.compile</code>.</p> <code>{}</code> Source code in <code>qmcpy/kernel/abstract_kernel.py</code> <pre><code>def __init__(\n    self,\n    d,\n    scale=1.0,\n    lengthscales=1.0,\n    shape_scale=[1],\n    shape_lengthscales=None,\n    tfs_scale=(tf_exp_eps_inv, tf_exp_eps),\n    tfs_lengthscales=(tf_exp_eps_inv, tf_exp_eps),\n    torchify=False,\n    requires_grad_scale=True,\n    requires_grad_lengthscales=True,\n    device=\"cpu\",\n    compile_call=False,\n    comiple_call_kwargs={},\n):\n    r\"\"\"\n    Args:\n        d (int): Dimension.\n        scale (Union[np.ndarray, torch.Tensor]): Scaling factor $S$.\n        lengthscales (Union[np.ndarray, torch.Tensor]): Lengthscales $\\boldsymbol{\\gamma}$.\n        shape_scale (list): Shape of `scale` when `np.isscalar(scale)`.\n        shape_lengthscales (list): Shape of `lengthscales` when `np.isscalar(lengthscales)`\n        tfs_scale (Tuple[callable,callable]): The first argument transforms to the raw value to be optimized; the second applies the inverse transform.\n        tfs_lengthscales (Tuple[callable,callable]): The first argument transforms to the raw value to be optimized; the second applies the inverse transform.\n        torchify (bool): If `True`, use the `torch` backend. Set to `True` if computing gradients with respect to inputs and/or hyperparameters.\n        requires_grad_scale (bool): If `True` and `torchify`, set `requires_grad=True` for `scale`.\n        requires_grad_lengthscales (bool): If `True` and `torchify`, set `requires_grad=True` for `lengthscales`.\n        device (torch.device): If `torchify`, put things onto this device.\n        compile_call (bool): If `True`, `torch.compile` the `parsed___call__` method.\n        comiple_call_kwargs (dict): When `compile_call` is `True`, pass these keyword arguments to `torch.compile`.\n    \"\"\"\n    super().__init__(\n        d=d,\n        torchify=torchify,\n        device=device,\n        compile_call=compile_call,\n        comiple_call_kwargs=comiple_call_kwargs,\n    )\n    self.raw_scale = self.parse_assign_param(\n        pname=\"scale\",\n        param=scale,\n        shape_param=shape_scale,\n        requires_grad_param=requires_grad_scale,\n        tfs_param=tfs_scale,\n        endsize_ops=[1],\n        constraints=[\"POSITIVE\"],\n    )\n    self.tfs_scale = tfs_scale\n    self.batch_param_names.append(\"scale\")\n    self.raw_lengthscales = self.parse_assign_param(\n        pname=\"lengthscales\",\n        param=lengthscales,\n        shape_param=[self.d] if shape_lengthscales is None else shape_lengthscales,\n        requires_grad_param=requires_grad_lengthscales,\n        tfs_param=tfs_lengthscales,\n        endsize_ops=[1, self.d],\n        constraints=[\"POSITIVE\"],\n    )\n    self.tfs_lengthscales = tfs_lengthscales\n    self.batch_param_names.append(\"lengthscales\")\n</code></pre>"},{"location":"api/kernels/#kernelshiftinvar","title":"<code>KernelShiftInvar</code>","text":"<p>               Bases: <code>AbstractSIDSIKernel</code></p> <p>Shift invariant kernel with  smoothness \\(\\boldsymbol{\\alpha}\\), product weights (lengthscales) \\(\\boldsymbol{\\gamma}\\), and scale \\(S\\):</p> \\[\\begin{aligned}     K(\\boldsymbol{x},\\boldsymbol{z}) &amp;= S \\prod_{j=1}^d \\left(1+ \\gamma_j \\tilde{K}_{\\alpha_j}((x_j - z_j) \\mod 1))\\right), \\\\      \\tilde{K}_\\alpha(x) &amp;= (-1)^{\\alpha+1}\\frac{(2 \\pi)^{2 \\alpha}}{(2\\alpha)!} B_{2\\alpha}(x) \\end{aligned}\\] <p>where \\(B_n\\) is the \\(n^\\text{th}\\) Bernoulli polynomial.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from qmcpy import Lattice, fftbr, ifftbr\n&gt;&gt;&gt; n = 8\n&gt;&gt;&gt; d = 4\n&gt;&gt;&gt; lat = Lattice(d,seed=11)\n&gt;&gt;&gt; x = lat(n)\n&gt;&gt;&gt; x.shape\n(8, 4)\n&gt;&gt;&gt; x.dtype\ndtype('float64')\n&gt;&gt;&gt; kernel = KernelShiftInvar(\n...     d = d, \n...     alpha = list(range(1,d+1)),\n...     scale = 10,\n...     lengthscales = [1/j**2 for j in range(1,d+1)])\n&gt;&gt;&gt; k00 = kernel(x[0],x[0])\n&gt;&gt;&gt; k00.item()\n91.23444453396341\n&gt;&gt;&gt; k0 = kernel(x,x[0])\n&gt;&gt;&gt; with np.printoptions(precision=2):\n...     print(k0)\n[91.23 -2.32  5.69  5.69 12.7  -4.78 -4.78 12.7 ]\n&gt;&gt;&gt; assert k0[0]==k00\n&gt;&gt;&gt; kmat = kernel(x[:,None,:],x[None,:,:])\n&gt;&gt;&gt; with np.printoptions(precision=2):\n...     print(kmat)\n[[91.23 -2.32  5.69  5.69 12.7  -4.78 -4.78 12.7 ]\n [-2.32 91.23  5.69  5.69 -4.78 12.7  12.7  -4.78]\n [ 5.69  5.69 91.23 -2.32 12.7  -4.78 12.7  -4.78]\n [ 5.69  5.69 -2.32 91.23 -4.78 12.7  -4.78 12.7 ]\n [12.7  -4.78 12.7  -4.78 91.23 -2.32  5.69  5.69]\n [-4.78 12.7  -4.78 12.7  -2.32 91.23  5.69  5.69]\n [-4.78 12.7  12.7  -4.78  5.69  5.69 91.23 -2.32]\n [12.7  -4.78 -4.78 12.7   5.69  5.69 -2.32 91.23]]\n&gt;&gt;&gt; assert (kmat[:,0]==k0).all()\n&gt;&gt;&gt; lam = np.sqrt(n)*fftbr(k0)\n&gt;&gt;&gt; y = np.random.Generator(np.random.PCG64(7)).uniform(low=0,high=1,size=(n))\n&gt;&gt;&gt; np.allclose(ifftbr(fftbr(y)*lam),kmat@y)\nTrue\n&gt;&gt;&gt; np.allclose(ifftbr(fftbr(y)/lam),np.linalg.solve(kmat,y))\nTrue\n&gt;&gt;&gt; import torch \n&gt;&gt;&gt; xtorch = torch.from_numpy(x)\n&gt;&gt;&gt; kernel_torch = KernelShiftInvar(\n...     d = d, \n...     alpha = list(range(1,d+1)),\n...     scale = 10,\n...     lengthscales = [1/j**2 for j in range(1,d+1)],\n...     torchify = True)\n&gt;&gt;&gt; kmat_torch = kernel_torch(xtorch[:,None,:],xtorch[None,:,:])\n&gt;&gt;&gt; np.allclose(kmat_torch.detach().numpy(),kmat)\nTrue\n&gt;&gt;&gt; kernel.single_integral_01d(x)\narray([10., 10., 10., 10., 10., 10., 10., 10.])\n&gt;&gt;&gt; kernel_torch.single_integral_01d(xtorch)\ntensor([10., 10., 10., 10., 10., 10., 10., 10.], dtype=torch.float64,\n       grad_fn=&lt;AddBackward0&gt;)\n</code></pre> <p>Batch Params </p> <pre><code>&gt;&gt;&gt; rng = np.random.Generator(np.random.PCG64(7))\n&gt;&gt;&gt; kernel = KernelShiftInvar(\n...     d = 2, \n...     shape_scale = [4,3,1],\n...     shape_lengthscales = [3,2])\n&gt;&gt;&gt; x = rng.uniform(low=0,high=1,size=(6,5,2))\n&gt;&gt;&gt; kernel(x,x).shape \n(4, 3, 6, 5)\n&gt;&gt;&gt; kernel(x[:,:,None,:],x[:,None,:,:]).shape\n(4, 3, 6, 5, 5)\n&gt;&gt;&gt; kfast = kernel(x[:,None,:,None,:],x[None,:,None,:,:])\n&gt;&gt;&gt; kfast.shape\n(4, 3, 6, 6, 5, 5)\n&gt;&gt;&gt; kstable = kernel(x[:,None,:,None,:],x[None,:,None,:,:],stable=True)\n&gt;&gt;&gt; np.abs(kfast-kstable).max()\nnp.float64(4.440892098500626e-16)\n</code></pre> <p>Derivatives </p> <pre><code>&gt;&gt;&gt; rng = np.random.Generator(np.random.PCG64(7))\n&gt;&gt;&gt; scale = rng.uniform(low=0,high=1,size=(1,))\n&gt;&gt;&gt; lengthscales = rng.uniform(low=0,high=1,size=(3,))\n&gt;&gt;&gt; kernel = KernelShiftInvar(\n...     d = 3,\n...     alpha = 3,\n...     torchify = True,\n...     scale = torch.from_numpy(scale),\n...     lengthscales = torch.from_numpy(lengthscales))\n&gt;&gt;&gt; x0 = torch.from_numpy(rng.uniform(low=0,high=1,size=(4,))).requires_grad_(True)\n&gt;&gt;&gt; x1 = torch.from_numpy(rng.uniform(low=0,high=1,size=(4,))).requires_grad_(True)\n&gt;&gt;&gt; x2 = torch.from_numpy(rng.uniform(low=0,high=1,size=(4,))).requires_grad_(True)\n&gt;&gt;&gt; x = torch.stack([x0,x1,x2],axis=-1)\n&gt;&gt;&gt; z0 = torch.from_numpy(rng.uniform(low=0,high=1,size=(4,))).requires_grad_(True)\n&gt;&gt;&gt; z1 = torch.from_numpy(rng.uniform(low=0,high=1,size=(4,))).requires_grad_(True)\n&gt;&gt;&gt; z2 = torch.from_numpy(rng.uniform(low=0,high=1,size=(4,))).requires_grad_(True)\n&gt;&gt;&gt; z = torch.stack([z0,z1,z2],axis=-1)\n&gt;&gt;&gt; c = torch.from_numpy(rng.uniform(low=0,high=1,size=(2,)))\n&gt;&gt;&gt; beta0 = torch.tensor([\n...     [1,0,0],\n...     [0,2,0]])\n&gt;&gt;&gt; beta1 = torch.tensor([\n...     [0,0,2],\n...     [2,1,0]])\n&gt;&gt;&gt; with torch.no_grad():\n...     y = kernel(x,z,beta0,beta1,c)\n&gt;&gt;&gt; with np.printoptions(formatter={\"float\": lambda x: \"%.2f\"%x}):\n...     y.numpy()\narray([1455.14, 9475.57, 7807.08, 2785.47])\n&gt;&gt;&gt; y_no_deriv = kernel(x,z)\n&gt;&gt;&gt; y_first = y_no_deriv.clone()\n&gt;&gt;&gt; y_first = torch.autograd.grad(y_first,x0,grad_outputs=torch.ones_like(y_first,requires_grad=True),create_graph=True)[0]\n&gt;&gt;&gt; y_first = torch.autograd.grad(y_first,z2,grad_outputs=torch.ones_like(y_first,requires_grad=True),create_graph=True)[0]\n&gt;&gt;&gt; y_first = torch.autograd.grad(y_first,z2,grad_outputs=torch.ones_like(y_first,requires_grad=True),create_graph=True)[0]\n&gt;&gt;&gt; y_second = y_no_deriv.clone()\n&gt;&gt;&gt; y_second = torch.autograd.grad(y_second,x1,grad_outputs=torch.ones_like(y_second,requires_grad=True),create_graph=True)[0]\n&gt;&gt;&gt; y_second = torch.autograd.grad(y_second,x1,grad_outputs=torch.ones_like(y_second,requires_grad=True),create_graph=True)[0]\n&gt;&gt;&gt; y_second = torch.autograd.grad(y_second,z0,grad_outputs=torch.ones_like(y_second,requires_grad=True),create_graph=True)[0]\n&gt;&gt;&gt; y_second = torch.autograd.grad(y_second,z0,grad_outputs=torch.ones_like(y_second,requires_grad=True),create_graph=True)[0]\n&gt;&gt;&gt; y_second = torch.autograd.grad(y_second,z1,grad_outputs=torch.ones_like(y_second,requires_grad=True),create_graph=True)[0]\n&gt;&gt;&gt; yhat = (y_first*c[0]+y_second*c[1]).detach()\n&gt;&gt;&gt; with np.printoptions(formatter={\"float\": lambda x: \"%.3f\"%x}):\n...     yhat.numpy()\narray([1455.140, 9475.570, 7807.076, 2785.473])\n&gt;&gt;&gt; torch.allclose(y,yhat)\nTrue\n&gt;&gt;&gt; kernel = KernelShiftInvar(\n...     d = 3,\n...     alpha = 3,\n...     scale = scale,\n...     lengthscales = lengthscales)\n&gt;&gt;&gt; ynp = kernel(x.detach().numpy(),z.detach().numpy(),beta0.numpy(),beta1.numpy(),c.numpy())\n&gt;&gt;&gt; with np.printoptions(formatter={\"float\": lambda x: \"%.2f\"%x}):\n...     ynp\narray([1455.14, 9475.59, 7807.09, 2785.48])\n&gt;&gt;&gt; np.allclose(ynp,y.numpy())\nTrue\n</code></pre> <p>References: </p> <ol> <li>Kaarnioja, Vesa, Frances Y. Kuo, and Ian H. Sloan.     \"Lattice-based kernel approximation and serendipitous weights for parametric PDEs in very high dimensions.\"     International Conference on Monte Carlo and Quasi-Monte Carlo Methods in Scientific Computing. Cham: Springer International Publishing, 2022.</li> </ol> <p>Parameters:</p> Name Type Description Default <code>d</code> <code>int</code> <p>Dimension.</p> required <code>scale</code> <code>Union[ndarray, Tensor]</code> <p>Scaling factor \\(S\\).</p> <code>1.0</code> <code>lengthscales</code> <code>Union[ndarray, Tensor]</code> <p>Product weights \\((\\gamma_1,\\dots,\\gamma_d)\\).</p> <code>None</code> <code>alpha</code> <code>Union[ndarray, Tensor]</code> <p>Smoothness parameters \\((\\alpha_1,\\dots,\\alpha_d)\\) where \\(\\alpha_j \\geq 1\\) for \\(j=1,\\dots,d\\).</p> <code>2</code> <code>shape_scale</code> <code>list</code> <p>Shape of <code>scale</code> when <code>np.isscalar(scale)</code>.</p> <code>[1]</code> <code>shape_lengthscales</code> <code>list</code> <p>Shape of <code>lengthscales</code> when <code>np.isscalar(lengthscales)</code></p> <code>None</code> <code>tfs_scale</code> <code>Tuple[callable, callable]</code> <p>The first argument transforms to the raw value to be optimized; the second applies the inverse transform.</p> <code>(tf_exp_eps_inv, tf_exp_eps)</code> <code>tfs_lengthscales</code> <code>Tuple[callable, callable]</code> <p>The first argument transforms to the raw value to be optimized; the second applies the inverse transform.</p> <code>(tf_exp_eps_inv, tf_exp_eps)</code> <code>torchify</code> <code>bool</code> <p>If <code>True</code>, use the <code>torch</code> backend. Set to <code>True</code> if computing gradients with respect to inputs and/or hyperparameters.</p> <code>False</code> <code>requires_grad_scale</code> <code>bool</code> <p>If <code>True</code> and <code>torchify</code>, set <code>requires_grad=True</code> for <code>scale</code>.</p> <code>True</code> <code>requires_grad_lengthscales</code> <code>bool</code> <p>If <code>True</code> and <code>torchify</code>, set <code>requires_grad=True</code> for <code>lengthscales</code>.</p> <code>True</code> <code>device</code> <code>device</code> <p>If <code>torchify</code>, put things onto this device.</p> <code>'cpu'</code> <code>compile_call</code> <code>bool</code> <p>If <code>True</code>, <code>torch.compile</code> the <code>parsed___call__</code> method.</p> <code>False</code> <code>comiple_call_kwargs</code> <code>dict</code> <p>When <code>compile_call</code> is <code>True</code>, pass these keyword arguments to <code>torch.compile</code>.</p> <code>{}</code> Source code in <code>qmcpy/kernel/si_dsi_kernels.py</code> <pre><code>def __init__(\n    self,\n    d,\n    scale=1.0,\n    lengthscales=None,\n    alpha=2,\n    shape_scale=[1],\n    shape_lengthscales=None,\n    tfs_scale=(tf_exp_eps_inv, tf_exp_eps),\n    tfs_lengthscales=(tf_exp_eps_inv, tf_exp_eps),\n    torchify=False,\n    requires_grad_scale=True,\n    requires_grad_lengthscales=True,\n    device=\"cpu\",\n    compile_call=False,\n    comiple_call_kwargs={},\n):\n    r\"\"\"\n    Args:\n        d (int): Dimension.\n        scale (Union[np.ndarray, torch.Tensor]): Scaling factor $S$.\n        lengthscales (Union[np.ndarray, torch.Tensor]): Product weights $(\\gamma_1,\\dots,\\gamma_d)$.\n        alpha (Union[np.ndarray, torch.Tensor]): Smoothness parameters $(\\alpha_1,\\dots,\\alpha_d)$ where $\\alpha_j \\geq 1$ for $j=1,\\dots,d$.\n        shape_scale (list): Shape of `scale` when `np.isscalar(scale)`.\n        shape_lengthscales (list): Shape of `lengthscales` when `np.isscalar(lengthscales)`\n        tfs_scale (Tuple[callable,callable]): The first argument transforms to the raw value to be optimized; the second applies the inverse transform.\n        tfs_lengthscales (Tuple[callable,callable]): The first argument transforms to the raw value to be optimized; the second applies the inverse transform.\n        torchify (bool): If `True`, use the `torch` backend. Set to `True` if computing gradients with respect to inputs and/or hyperparameters.\n        requires_grad_scale (bool): If `True` and `torchify`, set `requires_grad=True` for `scale`.\n        requires_grad_lengthscales (bool): If `True` and `torchify`, set `requires_grad=True` for `lengthscales`.\n        device (torch.device): If `torchify`, put things onto this device.\n        compile_call (bool): If `True`, `torch.compile` the `parsed___call__` method.\n        comiple_call_kwargs (dict): When `compile_call` is `True`, pass these keyword arguments to `torch.compile`.\n    \"\"\"\n    super().__init__(\n        d=d,\n        scale=scale,\n        lengthscales=lengthscales,\n        alpha=alpha,\n        shape_alpha=[d],\n        alpha_endsize_ops=[d],\n        shape_scale=shape_scale,\n        shape_lengthscales=shape_lengthscales,\n        tfs_alpha=(tf_identity, tf_identity),\n        tfs_scale=tfs_scale,\n        requires_grad_alpha=False,\n        tfs_lengthscales=tfs_lengthscales,\n        torchify=torchify,\n        requires_grad_scale=requires_grad_scale,\n        requires_grad_lengthscales=requires_grad_lengthscales,\n        device=device,\n        compile_call=compile_call,\n        comiple_call_kwargs=comiple_call_kwargs,\n    )\n    assert self.alpha.shape == (self.d,)\n    assert all(int(alphaj) in BERNOULLIPOLYSDICT for alphaj in self.alpha)\n    if self.torchify:\n        import torch\n\n        self.lgamma = torch.lgamma\n    else:\n        self.lgamma = scipy.special.loggamma\n</code></pre>"},{"location":"api/kernels/#kernelshiftinvarcombined","title":"<code>KernelShiftInvarCombined</code>","text":"<p>               Bases: <code>AbstractSIDSIKernel</code></p> <p>Shift invariant kernel with combination weights \\(\\boldsymbol{\\alpha}_1,\\dots,\\boldsymbol{\\alpha}_d \\in \\mathbb{R}_{&gt;0}^4\\), product weights (lengthscales) \\(\\boldsymbol{\\gamma}\\), and scale \\(S\\):</p> \\[\\begin{aligned}     K(\\boldsymbol{x},\\boldsymbol{z}) &amp;= S \\prod_{j=1}^d \\left(1+ \\gamma_j \\left(\\sum_{p=1}^4 \\alpha_{jp} \\tilde{K}_p(x_j \\mod 1 z_j)\\right)\\right) \\end{aligned}\\] <p>where, \\(\\tilde{K}_p\\) are defined in <code>KernelShiftInvar</code> for \\(p \\in \\{1,2,3,4\\}\\)</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from qmcpy import Lattice, fftbr, ifftbr\n&gt;&gt;&gt; n = 8\n&gt;&gt;&gt; d = 4\n&gt;&gt;&gt; lat = Lattice(d,seed=11)\n&gt;&gt;&gt; x = lat(n)\n&gt;&gt;&gt; x.shape\n(8, 4)\n&gt;&gt;&gt; x.dtype\ndtype('float64')\n&gt;&gt;&gt; kernel = KernelShiftInvarCombined(\n...     d = d,\n...     scale = 10,\n...     lengthscales = [1/j**2 for j in range(1,d+1)])\n&gt;&gt;&gt; k00 = kernel(x[0],x[0])\n&gt;&gt;&gt; k00.item()\n117.22427096475315\n&gt;&gt;&gt; k0 = kernel(x,x[0])\n&gt;&gt;&gt; with np.printoptions(precision=2):\n...     print(k0)\n[ 117.22  153.84   28.59   36.1  -139.83   77.1    -8.51   35.22]\n&gt;&gt;&gt; assert k0[0]==k00\n&gt;&gt;&gt; kmat = kernel(x[:,None,:],x[None,:,:])\n&gt;&gt;&gt; with np.printoptions(precision=2):\n...     print(kmat)\n[[ 117.22  153.84   36.1    28.59   35.22   -8.51   77.1  -139.83]\n [ 153.84  117.22   28.59   36.1    -8.51   35.22 -139.83   77.1 ]\n [  28.59   36.1   117.22  153.84 -139.83   77.1    35.22   -8.51]\n [  36.1    28.59  153.84  117.22   77.1  -139.83   -8.51   35.22]\n [-139.83   77.1    35.22   -8.51  117.22  153.84   36.1    28.59]\n [  77.1  -139.83   -8.51   35.22  153.84  117.22   28.59   36.1 ]\n [  -8.51   35.22 -139.83   77.1    28.59   36.1   117.22  153.84]\n [  35.22   -8.51   77.1  -139.83   36.1    28.59  153.84  117.22]]\n&gt;&gt;&gt; assert (kmat[:,0]==k0).all()\n&gt;&gt;&gt; lam = np.sqrt(n)*fftbr(k0)\n&gt;&gt;&gt; y = np.random.Generator(np.random.PCG64(7)).uniform(low=0,high=1,size=(n))\n&gt;&gt;&gt; np.allclose(ifftbr(fftbr(y)*lam),kmat@y)\nTrue\n&gt;&gt;&gt; np.allclose(ifftbr(fftbr(y)/lam),np.linalg.solve(kmat,y))\nTrue\n&gt;&gt;&gt; import torch\n&gt;&gt;&gt; xtorch = torch.from_numpy(x)\n&gt;&gt;&gt; kernel_torch = KernelShiftInvarCombined(\n...     d = d,\n...     scale = 10,\n...     lengthscales = [1/j**2 for j in range(1,d+1)],\n...     torchify = True)\n&gt;&gt;&gt; kmat_torch = kernel_torch(xtorch[:,None,:],xtorch[None,:,:])\n&gt;&gt;&gt; np.allclose(kmat_torch.detach().numpy(),kmat)\nTrue\n&gt;&gt;&gt; kernel.single_integral_01d(x)\narray([10., 10., 10., 10., 10., 10., 10., 10.])\n&gt;&gt;&gt; kernel_torch.single_integral_01d(xtorch)\ntensor([10., 10., 10., 10., 10., 10., 10., 10.], dtype=torch.float64,\n       grad_fn=&lt;AddBackward0&gt;)\n</code></pre> <p>Batch Params</p> <pre><code>&gt;&gt;&gt; rng = np.random.Generator(np.random.PCG64(7))\n&gt;&gt;&gt; kernel = KernelShiftInvarCombined(\n...     d = 2,\n...     shape_scale = [4,3,1],\n...     shape_lengthscales = [3,2])\n&gt;&gt;&gt; x = rng.uniform(low=0,high=1,size=(6,5,2))\n&gt;&gt;&gt; kernel(x,x).shape\n(4, 3, 6, 5)\n&gt;&gt;&gt; kernel(x[:,:,None,:],x[:,None,:,:]).shape\n(4, 3, 6, 5, 5)\n&gt;&gt;&gt; kfast = kernel(x[:,None,:,None,:],x[None,:,None,:,:])\n&gt;&gt;&gt; kfast.shape\n(4, 3, 6, 6, 5, 5)\n&gt;&gt;&gt; kstable = kernel(x[:,None,:,None,:],x[None,:,None,:,:],stable=True)\n&gt;&gt;&gt; np.abs(kfast-kstable).max()\nnp.float64(3.552713678800501e-15)\n</code></pre> <p>References:</p> <ol> <li>Kaarnioja, Vesa, Frances Y. Kuo, and Ian H. Sloan.     \"Lattice-based kernel approximation and serendipitous weights for parametric PDEs in very high dimensions.\"     International Conference on Monte Carlo and Quasi-Monte Carlo Methods in Scientific Computing. Cham: Springer International Publishing, 2022.</li> </ol> <p>Parameters:</p> Name Type Description Default <code>d</code> <code>int</code> <p>Dimension.</p> required <code>scale</code> <code>Union[ndarray, Tensor]</code> <p>Scaling factor \\(S\\).</p> <code>1.0</code> <code>lengthscales</code> <code>Union[ndarray, Tensor]</code> <p>Product weights \\((\\gamma_1,\\dots,\\gamma_d)\\).</p> <code>None</code> <code>alpha</code> <code>Union[ndarray, Tensor]</code> <p>Weights \\(\\boldsymbol{\\alpha}_1,\\dots,\\boldsymbol{\\alpha}_d \\in \\mathbb{R}_{&gt;0}^4\\).</p> <code>1</code> <code>shape_scale</code> <code>list</code> <p>Shape of <code>scale</code> when <code>np.isscalar(scale)</code>.</p> <code>[1]</code> <code>shape_lengthscales</code> <code>list</code> <p>Shape of <code>lengthscales</code> when <code>np.isscalar(lengthscales)</code></p> <code>None</code> <code>shape_alpha</code> <code>list</code> <p>Shape of <code>alpha</code> when <code>np.isscalar(alpha)</code></p> <code>None</code> <code>tfs_scale</code> <code>Tuple[callable, callable]</code> <p>The first argument transforms to the raw value to be optimized; the second applies the inverse transform.</p> <code>(tf_exp_eps_inv, tf_exp_eps)</code> <code>tfs_lengthscales</code> <code>Tuple[callable, callable]</code> <p>The first argument transforms to the raw value to be optimized; the second applies the inverse transform.</p> <code>(tf_exp_eps_inv, tf_exp_eps)</code> <code>tfs_alpha</code> <code>Tuple[callable, callable]</code> <p>The first argument transforms to the raw value to be optimized; the second applies the inverse transform.</p> <code>(tf_exp_eps_inv, tf_exp_eps)</code> <code>torchify</code> <code>bool</code> <p>If <code>True</code>, use the <code>torch</code> backend. Set to <code>True</code> if computing gradients with respect to inputs and/or hyperparameters.</p> <code>False</code> <code>requires_grad_scale</code> <code>bool</code> <p>If <code>True</code> and <code>torchify</code>, set <code>requires_grad=True</code> for <code>scale</code>.</p> <code>True</code> <code>requires_grad_lengthscales</code> <code>bool</code> <p>If <code>True</code> and <code>torchify</code>, set <code>requires_grad=True</code> for <code>lengthscales</code>.</p> <code>True</code> <code>requires_grad_alpha</code> <code>bool</code> <p>If <code>True</code> and <code>torchify</code>, set <code>requires_grad=True</code> for <code>alpha</code>.</p> <code>True</code> <code>device</code> <code>device</code> <p>If <code>torchify</code>, put things onto this device.</p> <code>'cpu'</code> <code>compile_call</code> <code>bool</code> <p>If <code>True</code>, <code>torch.compile</code> the <code>parsed___call__</code> method.</p> <code>False</code> <code>comiple_call_kwargs</code> <code>dict</code> <p>When <code>compile_call</code> is <code>True</code>, pass these keyword arguments to <code>torch.compile</code>.</p> <code>{}</code> Source code in <code>qmcpy/kernel/si_dsi_kernels.py</code> <pre><code>def __init__(\n    self,\n    d,\n    scale=1.0,\n    lengthscales=None,\n    alpha=1,\n    shape_scale=[1],\n    shape_lengthscales=None,\n    shape_alpha=None,\n    tfs_scale=(tf_exp_eps_inv, tf_exp_eps),\n    tfs_lengthscales=(tf_exp_eps_inv, tf_exp_eps),\n    tfs_alpha=(tf_exp_eps_inv, tf_exp_eps),\n    torchify=False,\n    requires_grad_scale=True,\n    requires_grad_lengthscales=True,\n    requires_grad_alpha=True,\n    device=\"cpu\",\n    compile_call=False,\n    comiple_call_kwargs={},\n):\n    r\"\"\"\n    Args:\n        d (int): Dimension.\n        scale (Union[np.ndarray, torch.Tensor]): Scaling factor $S$.\n        lengthscales (Union[np.ndarray, torch.Tensor]): Product weights $(\\gamma_1,\\dots,\\gamma_d)$.\n        alpha (Union[np.ndarray, torch.Tensor]): Weights $\\boldsymbol{\\alpha}_1,\\dots,\\boldsymbol{\\alpha}_d \\in \\mathbb{R}_{&gt;0}^4$.\n        shape_scale (list): Shape of `scale` when `np.isscalar(scale)`.\n        shape_lengthscales (list): Shape of `lengthscales` when `np.isscalar(lengthscales)`\n        shape_alpha (list): Shape of `alpha` when `np.isscalar(alpha)`\n        tfs_scale (Tuple[callable,callable]): The first argument transforms to the raw value to be optimized; the second applies the inverse transform.\n        tfs_lengthscales (Tuple[callable,callable]): The first argument transforms to the raw value to be optimized; the second applies the inverse transform.\n        tfs_alpha (Tuple[callable,callable]): The first argument transforms to the raw value to be optimized; the second applies the inverse transform.\n        torchify (bool): If `True`, use the `torch` backend. Set to `True` if computing gradients with respect to inputs and/or hyperparameters.\n        requires_grad_scale (bool): If `True` and `torchify`, set `requires_grad=True` for `scale`.\n        requires_grad_lengthscales (bool): If `True` and `torchify`, set `requires_grad=True` for `lengthscales`.\n        requires_grad_alpha (bool): If `True` and `torchify`, set `requires_grad=True` for `alpha`.\n        device (torch.device): If `torchify`, put things onto this device.\n        compile_call (bool): If `True`, `torch.compile` the `parsed___call__` method.\n        comiple_call_kwargs (dict): When `compile_call` is `True`, pass these keyword arguments to `torch.compile`.\n    \"\"\"\n    super().__init__(\n        d=d,\n        scale=scale,\n        lengthscales=lengthscales,\n        alpha=alpha,\n        alpha_endsize_ops=[d],\n        shape_scale=shape_scale,\n        shape_lengthscales=shape_lengthscales,\n        shape_alpha=[4, d] if shape_alpha is None else shape_alpha,\n        tfs_scale=tfs_scale,\n        tfs_alpha=tfs_alpha,\n        tfs_lengthscales=tfs_lengthscales,\n        torchify=torchify,\n        requires_grad_scale=requires_grad_scale,\n        requires_grad_lengthscales=requires_grad_lengthscales,\n        requires_grad_alpha=requires_grad_alpha,\n        device=device,\n        compile_call=compile_call,\n        comiple_call_kwargs=comiple_call_kwargs,\n    )\n    assert self.alpha.shape[-2:] == (4, d)\n    if self.torchify:\n        import torch\n\n        self.lgamma = torch.lgamma\n    else:\n        self.lgamma = scipy.special.loggamma\n    a = self.npt.arange(1, 5, **self.nptkwargs)\n    self.coeffs = (-1) ** (a + 1) * self.npt.exp(\n        2 * a * np.log(2 * np.pi) - self.lgamma(2 * a + 1)\n    )\n</code></pre>"},{"location":"api/kernels/#kerneldigshiftinvar","title":"<code>KernelDigShiftInvar</code>","text":"<p>               Bases: <code>AbstractSIDSIKernel</code></p> <p>Digitally shift invariant kernel in base \\(b=2\\) with  smoothness \\(\\boldsymbol{\\alpha}\\), product weights \\(\\boldsymbol{\\gamma}\\), and scale \\(S\\): </p> \\[\\begin{aligned}     K(\\boldsymbol{x},\\boldsymbol{z}) &amp;= S \\prod_{j=1}^d \\left(1+ \\gamma_j \\tilde{K}_{\\alpha_j}(x_j \\oplus z_j)\\right), \\qquad\\mathrm{where} \\\\     \\tilde{K}_1(x) &amp;= 6 \\left(\\frac{1}{6} - 2^{\\lfloor \\log_2(x) \\rfloor -1}\\right), \\\\     \\tilde{K}_2(x) &amp;= \\sum_{k \\in \\mathbb{N}} \\frac{\\mathrm{wal}_k(x)}{2^{\\mu_2(k)}} = -\\beta(x) x + \\frac{5}{2}\\left[1-t_1(x)\\right]-1, \\\\     \\tilde{K}_3(x) &amp;= \\sum_{k \\in \\mathbb{N}} \\frac{\\mathrm{wal}_k(x)}{2^{\\mu_3(k)}} = \\beta(x)x^2-5\\left[1-t_1(x)\\right]x+\\frac{43}{18}\\left[1-t_2(x)\\right]-1, \\\\     \\tilde{K}_4(x) &amp;= \\sum_{k \\in \\mathbb{N}} \\frac{\\mathrm{wal}_k(x)}{2^{\\mu_4(k)}} = - \\frac{2}{3}\\beta(x)x^3+5\\left[1-t_1(x)\\right]x^2 - \\frac{43}{9}\\left[1-t_2(x)\\right]x +\\frac{701}{294}\\left[1-t_3(x)\\right]+\\beta(x)\\left[\\frac{1}{48}\\sum_{a=0}^\\infty \\frac{\\mathrm{wal}_{2^a}(x)}{2^{3a}} - \\frac{1}{42}\\right] - 1. \\end{aligned}\\] <p>where </p> <ul> <li>\\(x \\oplus z\\) is XOR between bits, </li> <li>\\(\\mathrm{wal}_k\\) is the \\(k^\\text{th}\\) Walsh function, </li> <li>\\(\\beta(x) = - \\lfloor \\log_2(x) \\rfloor\\) and \\(t_\\nu(x) = 2^{-\\nu \\beta(x)}\\) where \\(\\beta(0)=t_\\nu(0) = 0\\), and </li> <li>and \\(\\mu_\\alpha\\) is the Dick weight function which sums the first \\(\\alpha\\) largest indices of \\(1\\) bits in the binary expansion of \\(k\\)  e.g. \\(k=13=1101_2\\) has 1-bit indexes \\((4,3,1)\\) so </li> </ul> \\[\\mu_1(k) = 4, \\mu_2(k) = 4+3, \\mu_3(k) = 4+3+1 = \\mu_4(k) = \\mu_5(k) = \\dots.\\] <p>Examples:</p> <pre><code>&gt;&gt;&gt; from qmcpy import DigitalNetB2, fwht\n&gt;&gt;&gt; n = 8\n&gt;&gt;&gt; d = 4\n&gt;&gt;&gt; dnb2 = DigitalNetB2(d,seed=11)\n&gt;&gt;&gt; x = dnb2(n,return_binary=True)\n&gt;&gt;&gt; x.shape\n(8, 4)\n&gt;&gt;&gt; x.dtype\ndtype('uint64')\n&gt;&gt;&gt; kernel = KernelDigShiftInvar(\n...     d = d, \n...     t = dnb2.t,\n...     alpha = list(range(1,d+1)),\n...     scale = 10,\n...     lengthscales = [1/j**2 for j in range(1,d+1)])\n&gt;&gt;&gt; k00 = kernel(x[0],x[0])\n&gt;&gt;&gt; k00.item()\n34.490370029184525\n&gt;&gt;&gt; k0 = kernel(x,x[0])\n&gt;&gt;&gt; with np.printoptions(precision=2):\n...     print(k0)\n[34.49  4.15  9.59  4.98 15.42  5.45 11.99  4.51]\n&gt;&gt;&gt; assert k0[0]==k00\n&gt;&gt;&gt; kmat = kernel(x[:,None,:],x[None,:,:])\n&gt;&gt;&gt; with np.printoptions(precision=2):\n...     print(kmat)\n[[34.49  4.15  9.59  4.98 15.42  5.45 11.99  4.51]\n [ 4.15 34.49  4.98  9.59  5.45 15.42  4.51 11.99]\n [ 9.59  4.98 34.49  4.15 11.99  4.51 15.42  5.45]\n [ 4.98  9.59  4.15 34.49  4.51 11.99  5.45 15.42]\n [15.42  5.45 11.99  4.51 34.49  4.15  9.59  4.98]\n [ 5.45 15.42  4.51 11.99  4.15 34.49  4.98  9.59]\n [11.99  4.51 15.42  5.45  9.59  4.98 34.49  4.15]\n [ 4.51 11.99  5.45 15.42  4.98  9.59  4.15 34.49]]\n&gt;&gt;&gt; assert (kmat[:,0]==k0).all()\n&gt;&gt;&gt; lam = np.sqrt(n)*fwht(k0)\n&gt;&gt;&gt; y = np.random.Generator(np.random.PCG64(7)).uniform(low=0,high=1,size=(n))\n&gt;&gt;&gt; np.allclose(fwht(fwht(y)*lam),kmat@y)\nTrue\n&gt;&gt;&gt; np.allclose(fwht(fwht(y)/lam),np.linalg.solve(kmat,y))\nTrue\n&gt;&gt;&gt; import torch \n&gt;&gt;&gt; xtorch = bin_from_numpy_to_torch(x)\n&gt;&gt;&gt; kernel_torch = KernelDigShiftInvar(\n...     d = d, \n...     t = dnb2.t,\n...     alpha = list(range(1,d+1)),\n...     scale = 10,\n...     lengthscales = [1/j**2 for j in range(1,d+1)],\n...     torchify = True)\n&gt;&gt;&gt; kmat_torch = kernel_torch(xtorch[:,None,:],xtorch[None,:,:])\n&gt;&gt;&gt; np.allclose(kmat_torch.detach().numpy(),kmat)\nTrue\n&gt;&gt;&gt; xf = to_float(x,dnb2.t)\n&gt;&gt;&gt; kmat_from_floats = kernel(xf[:,None,:],xf[None,:,:])\n&gt;&gt;&gt; np.allclose(kmat,kmat_from_floats)\nTrue\n&gt;&gt;&gt; xftorch = to_float(xtorch,dnb2.t)\n&gt;&gt;&gt; xftorch.dtype\ntorch.float32\n&gt;&gt;&gt; kmat_torch_from_floats = kernel_torch(xftorch[:,None,:],xftorch[None,:,:])\n&gt;&gt;&gt; torch.allclose(kmat_torch_from_floats,kmat_torch)\nTrue\n&gt;&gt;&gt; kernel.single_integral_01d(x)\narray([10., 10., 10., 10., 10., 10., 10., 10.])\n&gt;&gt;&gt; kernel_torch.single_integral_01d(xtorch)\ntensor([10., 10., 10., 10., 10., 10., 10., 10.], grad_fn=&lt;AddBackward0&gt;)\n</code></pre> <p>Batch Params </p> <pre><code>&gt;&gt;&gt; rng = np.random.Generator(np.random.PCG64(7))\n&gt;&gt;&gt; kernel = KernelDigShiftInvar(\n...     d = 2, \n...     t = 10,\n...     shape_scale = [4,3,1],\n...     shape_lengthscales = [3,2])\n&gt;&gt;&gt; x = rng.uniform(low=0,high=1,size=(6,5,2))\n&gt;&gt;&gt; kernel(x,x).shape \n(4, 3, 6, 5)\n&gt;&gt;&gt; kernel(x[:,:,None,:],x[:,None,:,:]).shape\n(4, 3, 6, 5, 5)\n&gt;&gt;&gt; kfast = kernel(x[:,None,:,None,:],x[None,:,None,:,:])\n&gt;&gt;&gt; kfast.shape\n(4, 3, 6, 6, 5, 5)\n&gt;&gt;&gt; kstable = kernel(x[:,None,:,None,:],x[None,:,None,:,:],stable=True)\n&gt;&gt;&gt; np.abs(kfast-kstable).max()\nnp.float64(4.440892098500626e-16)\n</code></pre> <p>References:</p> <ol> <li> <p>Dick, Josef.     \"Walsh spaces containing smooth functions and quasi-Monte Carlo rules of arbitrary high order.\"     SIAM Journal on Numerical Analysis 46.3 (2008): 1519-1553.</p> </li> <li> <p>Dick, Josef.     \"The decay of the Walsh coefficients of smooth functions.\"     Bulletin of the Australian Mathematical Society 80.3 (2009): 430-453.  </p> </li> <li> <p>Jagadeeswaran, Rathinavel, and Fred J. Hickernell.     \"Fast automatic Bayesian cubature using Sobol' sampling.\"     Advances in Modeling and Simulation: Festschrift for Pierre L'Ecuyer. Cham: Springer International Publishing, 2022. 301-318.</p> </li> <li> <p>Rathinavel, Jagadeeswaran.     Fast automatic Bayesian cubature using matching kernels and designs.     Illinois Institute of Technology, 2019.</p> </li> <li> <p>Sorokin, Aleksei.     \"A Unified Implementation of Quasi-Monte Carlo Generators, Randomization Routines, and Fast Kernel Methods.\"     arXiv preprint arXiv:2502.14256 (2025).</p> </li> </ol> <p>Parameters:</p> Name Type Description Default <code>d</code> <code>int</code> <p>Dimension.</p> required <code>t</code> <code>int</code> <p>number of bits in binary represtnations. Typically <code>dnb2.t</code> where <code>isinstance(dnb2,DigitalNetB2)</code>.</p> <code>None</code> <code>scale</code> <code>Union[ndarray, Tensor]</code> <p>Scaling factor \\(S\\).</p> <code>1.0</code> <code>lengthscales</code> <code>Union[ndarray, Tensor]</code> <p>Product weights \\((\\gamma_1,\\dots,\\gamma_d)\\).</p> <code>None</code> <code>alpha</code> <code>Union[ndarray, Tensor]</code> <p>Smoothness parameters \\((\\alpha_1,\\dots,\\alpha_d)\\) where \\(\\alpha_j \\geq 1\\) for \\(j=1,\\dots,d\\).</p> <code>2</code> <code>shape_scale</code> <code>list</code> <p>Shape of <code>scale</code> when <code>np.isscalar(scale)</code>.</p> <code>[1]</code> <code>shape_lengthscales</code> <code>list</code> <p>Shape of <code>lengthscales</code> when <code>np.isscalar(lengthscales)</code></p> <code>None</code> <code>tfs_scale</code> <code>Tuple[callable, callable]</code> <p>The first argument transforms to the raw value to be optimized; the second applies the inverse transform.</p> <code>(tf_exp_eps_inv, tf_exp_eps)</code> <code>tfs_lengthscales</code> <code>Tuple[callable, callable]</code> <p>The first argument transforms to the raw value to be optimized; the second applies the inverse transform.</p> <code>(tf_exp_eps_inv, tf_exp_eps)</code> <code>torchify</code> <code>bool</code> <p>If <code>True</code>, use the <code>torch</code> backend. Set to <code>True</code> if computing gradients with respect to inputs and/or hyperparameters.</p> <code>False</code> <code>requires_grad_scale</code> <code>bool</code> <p>If <code>True</code> and <code>torchify</code>, set <code>requires_grad=True</code> for <code>scale</code>.</p> <code>True</code> <code>requires_grad_lengthscales</code> <code>bool</code> <p>If <code>True</code> and <code>torchify</code>, set <code>requires_grad=True</code> for <code>lengthscales</code>.</p> <code>True</code> <code>device</code> <code>device</code> <p>If <code>torchify</code>, put things onto this device.</p> <code>'cpu'</code> <code>compile_call</code> <code>bool</code> <p>If <code>True</code>, <code>torch.compile</code> the <code>parsed___call__</code> method.</p> <code>False</code> <code>comiple_call_kwargs</code> <code>dict</code> <p>When <code>compile_call</code> is <code>True</code>, pass these keyword arguments to <code>torch.compile</code>.</p> <code>{}</code> Source code in <code>qmcpy/kernel/si_dsi_kernels.py</code> <pre><code>def __init__(\n    self,\n    d,\n    t=None,\n    scale=1.0,\n    lengthscales=None,\n    alpha=2,\n    shape_scale=[1],\n    shape_lengthscales=None,\n    tfs_scale=(tf_exp_eps_inv, tf_exp_eps),\n    tfs_lengthscales=(tf_exp_eps_inv, tf_exp_eps),\n    torchify=False,\n    requires_grad_scale=True,\n    requires_grad_lengthscales=True,\n    device=\"cpu\",\n    compile_call=False,\n    comiple_call_kwargs={},\n):\n    r\"\"\"\n    Args:\n        d (int): Dimension.\n        t (int): number of bits in binary represtnations. Typically `dnb2.t` where `isinstance(dnb2,DigitalNetB2)`.\n        scale (Union[np.ndarray, torch.Tensor]): Scaling factor $S$.\n        lengthscales (Union[np.ndarray, torch.Tensor]): Product weights $(\\gamma_1,\\dots,\\gamma_d)$.\n        alpha (Union[np.ndarray, torch.Tensor]): Smoothness parameters $(\\alpha_1,\\dots,\\alpha_d)$ where $\\alpha_j \\geq 1$ for $j=1,\\dots,d$.\n        shape_scale (list): Shape of `scale` when `np.isscalar(scale)`.\n        shape_lengthscales (list): Shape of `lengthscales` when `np.isscalar(lengthscales)`\n        tfs_scale (Tuple[callable,callable]): The first argument transforms to the raw value to be optimized; the second applies the inverse transform.\n        tfs_lengthscales (Tuple[callable,callable]): The first argument transforms to the raw value to be optimized; the second applies the inverse transform.\n        torchify (bool): If `True`, use the `torch` backend. Set to `True` if computing gradients with respect to inputs and/or hyperparameters.\n        requires_grad_scale (bool): If `True` and `torchify`, set `requires_grad=True` for `scale`.\n        requires_grad_lengthscales (bool): If `True` and `torchify`, set `requires_grad=True` for `lengthscales`.\n        device (torch.device): If `torchify`, put things onto this device.\n        compile_call (bool): If `True`, `torch.compile` the `parsed___call__` method.\n        comiple_call_kwargs (dict): When `compile_call` is `True`, pass these keyword arguments to `torch.compile`.\n    \"\"\"\n    super().__init__(\n        d=d,\n        scale=scale,\n        lengthscales=lengthscales,\n        alpha=alpha,\n        shape_alpha=[d],\n        alpha_endsize_ops=[d],\n        shape_scale=shape_scale,\n        shape_lengthscales=shape_lengthscales,\n        tfs_alpha=(tf_identity, tf_identity),\n        tfs_scale=tfs_scale,\n        tfs_lengthscales=tfs_lengthscales,\n        torchify=torchify,\n        requires_grad_alpha=False,\n        requires_grad_scale=requires_grad_scale,\n        requires_grad_lengthscales=requires_grad_lengthscales,\n        device=device,\n        compile_call=compile_call,\n        comiple_call_kwargs=comiple_call_kwargs,\n    )\n    assert self.alpha.shape == (self.d,)\n    self.set_t(t)\n    assert all(1 &lt;= int(alphaj) &lt;= 4 for alphaj in self.alpha)\n</code></pre>"},{"location":"api/kernels/#kerneldigshiftinvaradaptivealpha","title":"<code>KernelDigShiftInvarAdaptiveAlpha</code>","text":"<p>               Bases: <code>AbstractSIDSIKernel</code></p> <p>Digitally shift invariant kernel in base \\(b=2\\) with  smoothness \\(\\boldsymbol{\\alpha} \\geq \\boldsymbol{0}\\), product weights \\(\\boldsymbol{\\gamma}\\), and scale \\(S\\): </p> \\[\\begin{aligned}     K(\\boldsymbol{x},\\boldsymbol{z}) &amp;= S \\prod_{j=1}^d \\left(1+ \\gamma_j \\tilde{K}_{\\alpha_j}(x_j \\oplus z_j)\\right), \\qquad\\mathrm{where} \\\\     \\tilde{K}_\\alpha(x) &amp;= \\sum_{k \\in \\mathbb{N}} \\frac{\\mathrm{wal}_k(x)}{2^{{\\alpha+1} (\\mu_1(k)-1)}} = \\frac{2^{\\alpha+1}}{2^{\\alpha+1}-2} - \\left(\\frac{2^{\\alpha+1}}{2^{\\alpha+1}-2}+1\\right) 2^{\\alpha(\\lfloor \\log_2(x) \\rfloor+1)}, \\\\ \\end{aligned}\\] <p>where </p> <ul> <li>\\(x \\oplus z\\) is XOR between bits, </li> <li>\\(\\mathrm{wal}_k\\) is the \\(k^\\text{th}\\) Walsh function, </li> <li>\\(\\beta(x) = - \\lfloor \\log_2(x) \\rfloor\\) and \\(t_\\nu(x) = 2^{-\\nu \\beta(x)}\\) where \\(\\beta(0)=t_\\nu(0) = 0\\), and </li> <li>and \\(\\mu_\\alpha\\) is the Dick weight function which sums the first \\(\\alpha\\) largest indices of \\(1\\) bits in the binary expansion of \\(k\\)  e.g. \\(k=13=1101_2\\) has 1-bit indexes \\((4,3,1)\\) so </li> </ul> \\[\\mu_1(k) = 4, \\mu_2(k) = 4+3, \\mu_3(k) = 4+3+1 = \\mu_4(k) = \\mu_5(k) = \\dots.\\] <p>Examples:</p> <pre><code>&gt;&gt;&gt; from qmcpy import DigitalNetB2, fwht\n&gt;&gt;&gt; n = 8\n&gt;&gt;&gt; d = 4\n&gt;&gt;&gt; dnb2 = DigitalNetB2(d,seed=11)\n&gt;&gt;&gt; x = dnb2(n,return_binary=True)\n&gt;&gt;&gt; x.shape\n(8, 4)\n&gt;&gt;&gt; x.dtype\ndtype('uint64')\n&gt;&gt;&gt; kernel = KernelDigShiftInvarAdaptiveAlpha(\n...     d = d, \n...     t = dnb2.t,\n...     alpha = list(range(1,d+1)),\n...     scale = 10,\n...     lengthscales = [1/j**2 for j in range(1,d+1)])\n&gt;&gt;&gt; k00 = kernel(x[0],x[0])\n&gt;&gt;&gt; k00.item()\n48.084656084656096\n&gt;&gt;&gt; k0 = kernel(x,x[0])\n&gt;&gt;&gt; with np.printoptions(precision=2):\n...     print(k0)\n[48.08  0.    9.38  0.   19.74  0.   14.84  0.  ]\n&gt;&gt;&gt; assert k0[0]==k00\n&gt;&gt;&gt; kmat = kernel(x[:,None,:],x[None,:,:])\n&gt;&gt;&gt; with np.printoptions(precision=2):\n...     print(kmat)\n[[48.08  0.    9.38  0.   19.74  0.   14.84  0.  ]\n [ 0.   48.08  0.    9.38  0.   19.74  0.   14.84]\n [ 9.38  0.   48.08  0.   14.84  0.   19.74  0.  ]\n [ 0.    9.38  0.   48.08  0.   14.84  0.   19.74]\n [19.74  0.   14.84  0.   48.08  0.    9.38  0.  ]\n [ 0.   19.74  0.   14.84  0.   48.08  0.    9.38]\n [14.84  0.   19.74  0.    9.38  0.   48.08  0.  ]\n [ 0.   14.84  0.   19.74  0.    9.38  0.   48.08]]\n&gt;&gt;&gt; assert (kmat[:,0]==k0).all()\n&gt;&gt;&gt; lam = np.sqrt(n)*fwht(k0)\n&gt;&gt;&gt; y = np.random.Generator(np.random.PCG64(7)).uniform(low=0,high=1,size=(n))\n&gt;&gt;&gt; np.allclose(fwht(fwht(y)*lam),kmat@y)\nTrue\n&gt;&gt;&gt; np.allclose(fwht(fwht(y)/lam),np.linalg.solve(kmat,y))\nTrue\n&gt;&gt;&gt; import torch \n&gt;&gt;&gt; xtorch = bin_from_numpy_to_torch(x)\n&gt;&gt;&gt; kernel_torch = KernelDigShiftInvarAdaptiveAlpha(\n...     d = d, \n...     t = dnb2.t,\n...     alpha = list(range(1,d+1)),\n...     scale = 10,\n...     lengthscales = [1/j**2 for j in range(1,d+1)],\n...     torchify = True)\n&gt;&gt;&gt; kmat_torch = kernel_torch(xtorch[:,None,:],xtorch[None,:,:])\n&gt;&gt;&gt; np.allclose(kmat_torch.detach().numpy(),kmat,atol=1e-5)\nTrue\n&gt;&gt;&gt; xf = to_float(x,dnb2.t)\n&gt;&gt;&gt; kmat_from_floats = kernel(xf[:,None,:],xf[None,:,:])\n&gt;&gt;&gt; np.allclose(kmat,kmat_from_floats)\nTrue\n&gt;&gt;&gt; xftorch = to_float(xtorch,dnb2.t)\n&gt;&gt;&gt; xftorch.dtype\ntorch.float32\n&gt;&gt;&gt; kmat_torch_from_floats = kernel_torch(xftorch[:,None,:],xftorch[None,:,:])\n&gt;&gt;&gt; torch.allclose(kmat_torch_from_floats,kmat_torch)\nTrue\n&gt;&gt;&gt; kernel.single_integral_01d(x)\narray([10., 10., 10., 10., 10., 10., 10., 10.])\n&gt;&gt;&gt; kernel_torch.single_integral_01d(xtorch)\ntensor([10., 10., 10., 10., 10., 10., 10., 10.], grad_fn=&lt;AddBackward0&gt;)\n</code></pre> <p>Batch Params </p> <pre><code>&gt;&gt;&gt; rng = np.random.Generator(np.random.PCG64(7))\n&gt;&gt;&gt; kernel = KernelDigShiftInvarAdaptiveAlpha(\n...     d = 2, \n...     t = 10,\n...     shape_scale = [4,3,1],\n...     shape_lengthscales = [3,2])\n&gt;&gt;&gt; x = rng.uniform(low=0,high=1,size=(6,5,2))\n&gt;&gt;&gt; kernel(x,x).shape \n(4, 3, 6, 5)\n&gt;&gt;&gt; kernel(x[:,:,None,:],x[:,None,:,:]).shape\n(4, 3, 6, 5, 5)\n&gt;&gt;&gt; kfast = kernel(x[:,None,:,None,:],x[None,:,None,:,:])\n&gt;&gt;&gt; kfast.shape\n(4, 3, 6, 6, 5, 5)\n&gt;&gt;&gt; kstable = kernel(x[:,None,:,None,:],x[None,:,None,:,:],stable=True)\n&gt;&gt;&gt; np.abs(kfast-kstable).max()\nnp.float64(4.440892098500626e-16)\n</code></pre> <p>References:</p> <ol> <li>Dick, Josef, and Friedrich Pillichshammer.     \"Multivariate integration in weighted Hilbert spaces based on Walsh functions and weighted Sobolev spaces.\"     Journal of Complexity 21.2 (2005): 149-195.</li> </ol> <p>Parameters:</p> Name Type Description Default <code>d</code> <code>int</code> <p>Dimension.</p> required <code>t</code> <code>int</code> <p>number of bits in binary represtnations. Typically <code>dnb2.t</code> where <code>isinstance(dnb2,DigitalNetB2)</code>.</p> <code>None</code> <code>scale</code> <code>Union[ndarray, Tensor]</code> <p>Scaling factor \\(S\\).</p> <code>1.0</code> <code>lengthscales</code> <code>Union[ndarray, Tensor]</code> <p>Product weights \\((\\gamma_1,\\dots,\\gamma_d)\\).</p> <code>None</code> <code>alpha</code> <code>Union[ndarray, Tensor]</code> <p>Smoothness parameters \\((\\alpha_1,\\dots,\\alpha_d)\\) where \\(\\alpha_j \\geq 1\\) for \\(j=1,\\dots,d\\).</p> <code>1</code> <code>shape_alpha</code> <code>list</code> <p>Shape of <code>alpha</code> when <code>np.isscalar(alpha)</code></p> <code>None</code> <code>shape_scale</code> <code>list</code> <p>Shape of <code>scale</code> when <code>np.isscalar(scale)</code>.</p> <code>[1]</code> <code>shape_lengthscales</code> <code>list</code> <p>Shape of <code>lengthscales</code> when <code>np.isscalar(lengthscales)</code></p> <code>None</code> <code>tfs_scale</code> <code>Tuple[callable, callable]</code> <p>The first argument transforms to the raw value to be optimized; the second applies the inverse transform.</p> <code>(tf_exp_eps_inv, tf_exp_eps)</code> <code>tfs_lengthscales</code> <code>Tuple[callable, callable]</code> <p>The first argument transforms to the raw value to be optimized; the second applies the inverse transform.</p> <code>(tf_exp_eps_inv, tf_exp_eps)</code> <code>tfs_alpha</code> <code>Tuple[callable, callable]</code> <p>The first argument transforms to the raw value to be optimized; the second applies the inverse transform.</p> <code>(tf_exp_eps_inv, tf_exp_eps)</code> <code>torchify</code> <code>bool</code> <p>If <code>True</code>, use the <code>torch</code> backend. Set to <code>True</code> if computing gradients with respect to inputs and/or hyperparameters.</p> <code>False</code> <code>requires_grad_scale</code> <code>bool</code> <p>If <code>True</code> and <code>torchify</code>, set <code>requires_grad=True</code> for <code>scale</code>.</p> <code>True</code> <code>requires_grad_lengthscales</code> <code>bool</code> <p>If <code>True</code> and <code>torchify</code>, set <code>requires_grad=True</code> for <code>lengthscales</code>.</p> <code>True</code> <code>requires_grad_alpha</code> <code>bool</code> <p>If <code>True</code> and <code>torchify</code>, set <code>requires_grad=True</code> for <code>alpha</code>.</p> <code>True</code> <code>device</code> <code>device</code> <p>If <code>torchify</code>, put things onto this device.</p> <code>'cpu'</code> <code>compile_call</code> <code>bool</code> <p>If <code>True</code>, <code>torch.compile</code> the <code>parsed___call__</code> method.</p> <code>False</code> <code>comiple_call_kwargs</code> <code>dict</code> <p>When <code>compile_call</code> is <code>True</code>, pass these keyword arguments to <code>torch.compile</code>.</p> <code>{}</code> Source code in <code>qmcpy/kernel/si_dsi_kernels.py</code> <pre><code>def __init__(\n    self,\n    d,\n    t=None,\n    scale=1.0,\n    lengthscales=None,\n    alpha=1,\n    shape_scale=[1],\n    shape_lengthscales=None,\n    shape_alpha=None,\n    tfs_scale=(tf_exp_eps_inv, tf_exp_eps),\n    tfs_lengthscales=(tf_exp_eps_inv, tf_exp_eps),\n    tfs_alpha=(tf_exp_eps_inv, tf_exp_eps),\n    torchify=False,\n    requires_grad_scale=True,\n    requires_grad_lengthscales=True,\n    requires_grad_alpha=True,\n    device=\"cpu\",\n    compile_call=False,\n    comiple_call_kwargs={},\n):\n    r\"\"\"\n    Args:\n        d (int): Dimension.\n        t (int): number of bits in binary represtnations. Typically `dnb2.t` where `isinstance(dnb2,DigitalNetB2)`.\n        scale (Union[np.ndarray, torch.Tensor]): Scaling factor $S$.\n        lengthscales (Union[np.ndarray, torch.Tensor]): Product weights $(\\gamma_1,\\dots,\\gamma_d)$.\n        alpha (Union[np.ndarray, torch.Tensor]): Smoothness parameters $(\\alpha_1,\\dots,\\alpha_d)$ where $\\alpha_j \\geq 1$ for $j=1,\\dots,d$.\n        shape_alpha (list): Shape of `alpha` when `np.isscalar(alpha)`\n        shape_scale (list): Shape of `scale` when `np.isscalar(scale)`.\n        shape_lengthscales (list): Shape of `lengthscales` when `np.isscalar(lengthscales)`\n        tfs_scale (Tuple[callable,callable]): The first argument transforms to the raw value to be optimized; the second applies the inverse transform.\n        tfs_lengthscales (Tuple[callable,callable]): The first argument transforms to the raw value to be optimized; the second applies the inverse transform.\n        tfs_alpha (Tuple[callable,callable]): The first argument transforms to the raw value to be optimized; the second applies the inverse transform.\n        torchify (bool): If `True`, use the `torch` backend. Set to `True` if computing gradients with respect to inputs and/or hyperparameters.\n        requires_grad_scale (bool): If `True` and `torchify`, set `requires_grad=True` for `scale`.\n        requires_grad_lengthscales (bool): If `True` and `torchify`, set `requires_grad=True` for `lengthscales`.\n        requires_grad_alpha (bool): If `True` and `torchify`, set `requires_grad=True` for `alpha`.\n        device (torch.device): If `torchify`, put things onto this device.\n        compile_call (bool): If `True`, `torch.compile` the `parsed___call__` method.\n        comiple_call_kwargs (dict): When `compile_call` is `True`, pass these keyword arguments to `torch.compile`.\n    \"\"\"\n    super().__init__(\n        d=d,\n        scale=scale,\n        lengthscales=lengthscales,\n        alpha=alpha,\n        shape_alpha=[d] if shape_alpha is None else shape_alpha,\n        alpha_endsize_ops=[d],\n        shape_scale=shape_scale,\n        shape_lengthscales=shape_lengthscales,\n        tfs_alpha=tfs_alpha,\n        tfs_scale=tfs_scale,\n        tfs_lengthscales=tfs_lengthscales,\n        torchify=torchify,\n        requires_grad_alpha=requires_grad_alpha,\n        requires_grad_scale=requires_grad_scale,\n        requires_grad_lengthscales=requires_grad_lengthscales,\n        device=device,\n        compile_call=compile_call,\n        comiple_call_kwargs=comiple_call_kwargs,\n    )\n    self.set_t(t)\n    self.batch_param_names.append(\"alpha\")\n</code></pre>"},{"location":"api/kernels/#kerneldigshiftinvarcombined","title":"<code>KernelDigShiftInvarCombined</code>","text":"<p>               Bases: <code>AbstractSIDSIKernel</code></p> <p>Digitally shift invariant kernel in base \\(b=2\\) with combination weights \\(\\boldsymbol{\\alpha}_1,\\dots,\\boldsymbol{\\alpha}_d \\in \\mathbb{R}_{&gt;0}^4\\), smoothness \\(\\boldsymbol{\\alpha}\\), product weights \\(\\boldsymbol{\\gamma}\\), and scale \\(S\\):</p> \\[\\begin{aligned}     K(\\boldsymbol{x},\\boldsymbol{z}) &amp;= S \\prod_{j=1}^d \\left(1+ \\gamma_j \\left(\\sum_{p=1}^4 \\alpha_{jp} \\tilde{K}_p(x_j \\oplus z_j)\\right)\\right) \\end{aligned}\\] <p>where, \\(\\oplus\\) is defined in the docs for <code>KernelDigShiftInvar</code> and so are \\(\\tilde{K}_p\\) for \\(p \\in \\{1,2,3,4\\}\\)</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from qmcpy import DigitalNetB2, fwht\n&gt;&gt;&gt; n = 8\n&gt;&gt;&gt; d = 4\n&gt;&gt;&gt; dnb2 = DigitalNetB2(d,seed=11)\n&gt;&gt;&gt; x = dnb2(n,return_binary=True)\n&gt;&gt;&gt; x.shape\n(8, 4)\n&gt;&gt;&gt; x.dtype\ndtype('uint64')\n&gt;&gt;&gt; kernel = KernelDigShiftInvarCombined(\n...     d = d,\n...     t = dnb2.t,\n...     scale = 10,\n...     lengthscales = [1/j**2 for j in range(1,d+1)])\n&gt;&gt;&gt; k00 = kernel(x[0],x[0])\n&gt;&gt;&gt; k00.item()\n306.66030731930863\n&gt;&gt;&gt; k0 = kernel(x,x[0])\n&gt;&gt;&gt; with np.printoptions(precision=2):\n...     print(k0)\n[306.66  -1.65   6.88 -13.5   22.57  -9.34   7.99  -7.54]\n&gt;&gt;&gt; assert k0[0]==k00\n&gt;&gt;&gt; kmat = kernel(x[:,None,:],x[None,:,:])\n&gt;&gt;&gt; with np.printoptions(precision=2):\n...     print(kmat)\n[[306.66  -1.65   6.88 -13.5   22.57  -9.34   7.99  -7.54]\n [ -1.65 306.66 -13.5    6.88  -9.34  22.57  -7.54   7.99]\n [  6.88 -13.5  306.66  -1.65   7.99  -7.54  22.57  -9.34]\n [-13.5    6.88  -1.65 306.66  -7.54   7.99  -9.34  22.57]\n [ 22.57  -9.34   7.99  -7.54 306.66  -1.65   6.88 -13.5 ]\n [ -9.34  22.57  -7.54   7.99  -1.65 306.66 -13.5    6.88]\n [  7.99  -7.54  22.57  -9.34   6.88 -13.5  306.66  -1.65]\n [ -7.54   7.99  -9.34  22.57 -13.5    6.88  -1.65 306.66]]\n&gt;&gt;&gt; assert (kmat[:,0]==k0).all()\n&gt;&gt;&gt; lam = np.sqrt(n)*fwht(k0)\n&gt;&gt;&gt; y = np.random.Generator(np.random.PCG64(7)).uniform(low=0,high=1,size=(n))\n&gt;&gt;&gt; np.allclose(fwht(fwht(y)*lam),kmat@y)\nTrue\n&gt;&gt;&gt; np.allclose(fwht(fwht(y)/lam),np.linalg.solve(kmat,y))\nTrue\n&gt;&gt;&gt; import torch\n&gt;&gt;&gt; xtorch = bin_from_numpy_to_torch(x)\n&gt;&gt;&gt; kernel_torch = KernelDigShiftInvarCombined(\n...     d = d,\n...     t = dnb2.t,\n...     scale = 10,\n...     lengthscales = [1/j**2 for j in range(1,d+1)],\n...     torchify = True)\n&gt;&gt;&gt; kmat_torch = kernel_torch(xtorch[:,None,:],xtorch[None,:,:])\n&gt;&gt;&gt; np.allclose(kmat_torch.detach().numpy(),kmat)\nTrue\n&gt;&gt;&gt; xf = to_float(x,dnb2.t)\n&gt;&gt;&gt; kmat_from_floats = kernel(xf[:,None,:],xf[None,:,:])\n&gt;&gt;&gt; np.allclose(kmat,kmat_from_floats)\nTrue\n&gt;&gt;&gt; xftorch = to_float(xtorch,dnb2.t)\n&gt;&gt;&gt; xftorch.dtype\ntorch.float32\n&gt;&gt;&gt; kmat_torch_from_floats = kernel_torch(xftorch[:,None,:],xftorch[None,:,:])\n&gt;&gt;&gt; torch.allclose(kmat_torch_from_floats,kmat_torch)\nTrue\n&gt;&gt;&gt; kernel.single_integral_01d(x)\narray([10., 10., 10., 10., 10., 10., 10., 10.])\n&gt;&gt;&gt; kernel_torch.single_integral_01d(xtorch)\ntensor([10., 10., 10., 10., 10., 10., 10., 10.], grad_fn=&lt;AddBackward0&gt;)\n</code></pre> <p>Batch Params</p> <pre><code>&gt;&gt;&gt; rng = np.random.Generator(np.random.PCG64(7))\n&gt;&gt;&gt; kernel = KernelDigShiftInvarCombined(\n...     d = 2,\n...     t = 10,\n...     shape_scale = [4,3,1],\n...     shape_lengthscales = [3,2])\n&gt;&gt;&gt; x = rng.uniform(low=0,high=1,size=(6,5,2))\n&gt;&gt;&gt; kernel(x,x).shape\n(4, 3, 6, 5)\n&gt;&gt;&gt; kernel(x[:,:,None,:],x[:,None,:,:]).shape\n(4, 3, 6, 5, 5)\n&gt;&gt;&gt; kfast = kernel(x[:,None,:,None,:],x[None,:,None,:,:])\n&gt;&gt;&gt; kfast.shape\n(4, 3, 6, 6, 5, 5)\n&gt;&gt;&gt; kstable = kernel(x[:,None,:,None,:],x[None,:,None,:,:],stable=True)\n&gt;&gt;&gt; np.abs(kfast-kstable).max()\nnp.float64(8.881784197001252e-16)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>d</code> <code>int</code> <p>Dimension.</p> required <code>t</code> <code>int</code> <p>number of bits in binary represtnations. Typically <code>dnb2.t</code> where <code>isinstance(dnb2,DigitalNetB2)</code>.</p> <code>None</code> <code>scale</code> <code>Union[ndarray, Tensor]</code> <p>Scaling factor \\(S\\).</p> <code>1.0</code> <code>lengthscales</code> <code>Union[ndarray, Tensor]</code> <p>Product weights \\((\\gamma_1,\\dots,\\gamma_d)\\).</p> <code>None</code> <code>alpha</code> <code>Union[ndarray, Tensor]</code> <p>Weights \\(\\boldsymbol{\\alpha}_1,\\dots,\\boldsymbol{\\alpha}_d \\in \\mathbb{R}_{&gt;0}^4\\).</p> <code>1.0</code> <code>shape_scale</code> <code>list</code> <p>Shape of <code>scale</code> when <code>np.isscalar(scale)</code>.</p> <code>[1]</code> <code>shape_lengthscales</code> <code>list</code> <p>Shape of <code>lengthscales</code> when <code>np.isscalar(lengthscales)</code></p> <code>None</code> <code>shape_alpha</code> <code>list</code> <p>Shape of <code>alpha</code> when <code>np.isscalar(alpha)</code></p> <code>None</code> <code>tfs_scale</code> <code>Tuple[callable, callable]</code> <p>The first argument transforms to the raw value to be optimized; the second applies the inverse transform.</p> <code>(tf_exp_eps_inv, tf_exp_eps)</code> <code>tfs_lengthscales</code> <code>Tuple[callable, callable]</code> <p>The first argument transforms to the raw value to be optimized; the second applies the inverse transform.</p> <code>(tf_exp_eps_inv, tf_exp_eps)</code> <code>torchify</code> <code>bool</code> <p>If <code>True</code>, use the <code>torch</code> backend. Set to <code>True</code> if computing gradients with respect to inputs and/or hyperparameters.</p> <code>False</code> <code>requires_grad_scale</code> <code>bool</code> <p>If <code>True</code> and <code>torchify</code>, set <code>requires_grad=True</code> for <code>scale</code>.</p> <code>True</code> <code>requires_grad_lengthscales</code> <code>bool</code> <p>If <code>True</code> and <code>torchify</code>, set <code>requires_grad=True</code> for <code>lengthscales</code>.</p> <code>True</code> <code>requires_grad_alpha</code> <code>bool</code> <p>If <code>True</code> and <code>torchify</code>, set <code>requires_grad=True</code> for <code>alpha</code>.</p> <code>True</code> <code>device</code> <code>device</code> <p>If <code>torchify</code>, put things onto this device.</p> <code>'cpu'</code> <code>compile_call</code> <code>bool</code> <p>If <code>True</code>, <code>torch.compile</code> the <code>parsed___call__</code> method.</p> <code>False</code> <code>comiple_call_kwargs</code> <code>dict</code> <p>When <code>compile_call</code> is <code>True</code>, pass these keyword arguments to <code>torch.compile</code>.</p> <code>{}</code> Source code in <code>qmcpy/kernel/si_dsi_kernels.py</code> <pre><code>def __init__(\n    self,\n    d,\n    t=None,\n    scale=1.0,\n    lengthscales=None,\n    alpha=1.0,\n    shape_scale=[1],\n    shape_lengthscales=None,\n    shape_alpha=None,\n    tfs_scale=(tf_exp_eps_inv, tf_exp_eps),\n    tfs_lengthscales=(tf_exp_eps_inv, tf_exp_eps),\n    tfs_alpha=(tf_exp_eps_inv, tf_exp_eps),\n    torchify=False,\n    requires_grad_scale=True,\n    requires_grad_lengthscales=True,\n    requires_grad_alpha=True,\n    device=\"cpu\",\n    compile_call=False,\n    comiple_call_kwargs={},\n):\n    r\"\"\"\n    Args:\n        d (int): Dimension.\n        t (int): number of bits in binary represtnations. Typically `dnb2.t` where `isinstance(dnb2,DigitalNetB2)`.\n        scale (Union[np.ndarray, torch.Tensor]): Scaling factor $S$.\n        lengthscales (Union[np.ndarray, torch.Tensor]): Product weights $(\\gamma_1,\\dots,\\gamma_d)$.\n        alpha (Union[np.ndarray, torch.Tensor]): Weights $\\boldsymbol{\\alpha}_1,\\dots,\\boldsymbol{\\alpha}_d \\in \\mathbb{R}_{&gt;0}^4$.\n        shape_scale (list): Shape of `scale` when `np.isscalar(scale)`.\n        shape_lengthscales (list): Shape of `lengthscales` when `np.isscalar(lengthscales)`\n        shape_alpha (list): Shape of `alpha` when `np.isscalar(alpha)`\n        tfs_scale (Tuple[callable,callable]): The first argument transforms to the raw value to be optimized; the second applies the inverse transform.\n        tfs_lengthscales (Tuple[callable,callable]): The first argument transforms to the raw value to be optimized; the second applies the inverse transform.\n        torchify (bool): If `True`, use the `torch` backend. Set to `True` if computing gradients with respect to inputs and/or hyperparameters.\n        requires_grad_scale (bool): If `True` and `torchify`, set `requires_grad=True` for `scale`.\n        requires_grad_lengthscales (bool): If `True` and `torchify`, set `requires_grad=True` for `lengthscales`.\n        requires_grad_alpha (bool): If `True` and `torchify`, set `requires_grad=True` for `alpha`.\n        device (torch.device): If `torchify`, put things onto this device.\n        compile_call (bool): If `True`, `torch.compile` the `parsed___call__` method.\n        comiple_call_kwargs (dict): When `compile_call` is `True`, pass these keyword arguments to `torch.compile`.\n    \"\"\"\n    super().__init__(\n        d=d,\n        scale=scale,\n        lengthscales=lengthscales,\n        alpha=alpha,\n        shape_alpha=[4, d] if shape_alpha is None else shape_alpha,\n        alpha_endsize_ops=[d],\n        shape_scale=shape_scale,\n        shape_lengthscales=shape_lengthscales,\n        tfs_alpha=tfs_alpha,\n        tfs_scale=tfs_scale,\n        tfs_lengthscales=tfs_lengthscales,\n        torchify=torchify,\n        requires_grad_alpha=requires_grad_alpha,\n        requires_grad_scale=requires_grad_scale,\n        requires_grad_lengthscales=requires_grad_lengthscales,\n        device=device,\n        compile_call=compile_call,\n        comiple_call_kwargs=comiple_call_kwargs,\n    )\n    self.set_t(t)\n    assert self.alpha.shape[-2:] == (4, d)\n</code></pre>"},{"location":"api/kernels/#kernelgaussian","title":"<code>KernelGaussian</code>","text":"<p>               Bases: <code>AbstractKernelGaussianSE</code></p> <p>Gaussian / Squared Exponential kernel implemented using the product of exponentials.</p> \\[K(\\boldsymbol{x},\\boldsymbol{z}) = S \\prod_{j=1}^d \\exp\\left(-\\left(\\frac{x_j-z_j}{\\sqrt{2} \\gamma_j}\\right)^2\\right)\\] <p>Examples:</p> <pre><code>&gt;&gt;&gt; rng = np.random.Generator(np.random.PCG64(7))\n&gt;&gt;&gt; kernel = KernelGaussian(d=2)\n&gt;&gt;&gt; x = rng.uniform(low=0,high=1,size=(4,2))\n&gt;&gt;&gt; kernel(x[0],x[0]).item()\n1.0\n&gt;&gt;&gt; kernel(x,x)\narray([1., 1., 1., 1.])\n&gt;&gt;&gt; kernel(x[:,None,:],x[None,:,:])\narray([[1.        , 0.78888466, 0.9483142 , 0.8228498 ],\n       [0.78888466, 1.        , 0.72380317, 0.62226176],\n       [0.9483142 , 0.72380317, 1.        , 0.95613874],\n       [0.8228498 , 0.62226176, 0.95613874, 1.        ]])\n</code></pre> <p>Multiple randomizations</p> <pre><code>&gt;&gt;&gt; x = rng.uniform(low=0,high=1,size=(6,5,2))\n&gt;&gt;&gt; kernel(x,x).shape\n(6, 5)\n&gt;&gt;&gt; kernel(x[:,:,None,:],x[:,None,:,:]).shape\n(6, 5, 5)\n&gt;&gt;&gt; kernel(x[:,None,:,None,:],x[None,:,None,:,:]).shape\n(6, 6, 5, 5)\n</code></pre> <p>Batch hyperparameters</p> <pre><code>&gt;&gt;&gt; kernel = KernelGaussian(\n...     d = 2,\n...     shape_scale = [4,3,1],\n...     shape_lengthscales = [3,1])\n&gt;&gt;&gt; kernel(x,x).shape\n(4, 3, 6, 5)\n&gt;&gt;&gt; kernel(x[:,:,None,:],x[:,None,:,:]).shape\n(4, 3, 6, 5, 5)\n&gt;&gt;&gt; kernel(x[:,None,:,None,:],x[None,:,None,:,:]).shape\n(4, 3, 6, 6, 5, 5)\n</code></pre> <p>Integrals</p> <pre><code>&gt;&gt;&gt; kernel = KernelGaussian(\n...     d = 2,\n...     scale = rng.uniform(low=0,high=1,size=(3,1,)),\n...     lengthscales = rng.uniform(low=0,high=1,size=(1,2)))\n&gt;&gt;&gt; kintint = kernel.double_integral_01d()\n&gt;&gt;&gt; kintint\narray([0.50079567, 0.11125229, 0.34760005])\n&gt;&gt;&gt; x_qmc_4d = DigitalNetB2(4,seed=7)(2**16)\n&gt;&gt;&gt; kintint_qmc = kernel(x_qmc_4d[:,:2],x_qmc_4d[:,2:]).mean(1)\n&gt;&gt;&gt; kintint_qmc\narray([0.5007959 , 0.11125234, 0.34760021])\n&gt;&gt;&gt; with np.printoptions(formatter={\"float\":lambda x: \"%.1e\"%x}):\n...     np.abs(kintint-kintint_qmc)\narray([2.3e-07, 5.1e-08, 1.6e-07])\n&gt;&gt;&gt; x = rng.uniform(low=0,high=1,size=(4,2))\n&gt;&gt;&gt; kint = kernel.single_integral_01d(x)\n&gt;&gt;&gt; kint\narray([[0.54610372, 0.4272801 , 0.43001936, 0.44688778],\n       [0.12131753, 0.09492073, 0.09552926, 0.0992766 ],\n       [0.37904817, 0.29657323, 0.29847453, 0.31018283]])\n&gt;&gt;&gt; x_qmc_2d = DigitalNetB2(2,seed=7)(2**16)\n&gt;&gt;&gt; kint_qmc = kernel(x[:,None,:],x_qmc_2d).mean(-1)\n&gt;&gt;&gt; kint_qmc\narray([[0.54610372, 0.4272801 , 0.43001936, 0.44688778],\n       [0.12131753, 0.09492073, 0.09552926, 0.0992766 ],\n       [0.37904817, 0.29657323, 0.29847453, 0.31018283]])\n&gt;&gt;&gt; with np.printoptions(formatter={\"float\":lambda x: \"%.1e\"%x}):\n...     np.abs(kint-kint_qmc)\narray([[2.2e-13, 4.0e-13, 4.2e-12, 3.9e-12],\n       [4.8e-14, 8.9e-14, 9.4e-13, 8.8e-13],\n       [1.5e-13, 2.8e-13, 2.9e-12, 2.7e-12]])\n&gt;&gt;&gt; k_1l = KernelGaussian(d=2,lengthscales=[.5])\n&gt;&gt;&gt; k_2l = KernelGaussian(d=2,lengthscales=[.5,.5])\n&gt;&gt;&gt; print(\"%.5f\"%k_2l.double_integral_01d())\n0.58363\n&gt;&gt;&gt; print(\"%.5f\"%k_1l.double_integral_01d())\n0.58363\n&gt;&gt;&gt; k_1l.single_integral_01d(x)\narray([0.58119655, 0.46559577, 0.57603494, 0.53494393])\n&gt;&gt;&gt; k_2l.single_integral_01d(x)\narray([0.58119655, 0.46559577, 0.57603494, 0.53494393])\n</code></pre> <p>PyTorch</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; rng = np.random.Generator(np.random.PCG64(7))\n&gt;&gt;&gt; kernel = KernelGaussian(d=2,torchify=True)\n&gt;&gt;&gt; x = torch.from_numpy(rng.uniform(low=0,high=1,size=(4,2)))\n&gt;&gt;&gt; kernel(x[0],x[0]).item()\n1.0\n&gt;&gt;&gt; kernel(x,x)\ntensor([1., 1., 1., 1.], dtype=torch.float64, grad_fn=&lt;MulBackward0&gt;)\n&gt;&gt;&gt; kernel(x[:,None,:],x[None,:,:])\ntensor([[1.0000, 0.7889, 0.9483, 0.8228],\n        [0.7889, 1.0000, 0.7238, 0.6223],\n        [0.9483, 0.7238, 1.0000, 0.9561],\n        [0.8228, 0.6223, 0.9561, 1.0000]], dtype=torch.float64,\n       grad_fn=&lt;MulBackward0&gt;)\n</code></pre> <p>Multiple randomizations</p> <pre><code>&gt;&gt;&gt; x = torch.from_numpy(rng.uniform(low=0,high=1,size=(6,5,2)))\n&gt;&gt;&gt; kernel(x,x).shape\ntorch.Size([6, 5])\n&gt;&gt;&gt; kernel(x[:,:,None,:],x[:,None,:,:]).shape\ntorch.Size([6, 5, 5])\n&gt;&gt;&gt; kernel(x[:,None,:,None,:],x[None,:,None,:,:]).shape\ntorch.Size([6, 6, 5, 5])\n</code></pre> <p>Batch hyperparameters</p> <pre><code>&gt;&gt;&gt; kernel = KernelGaussian(\n...     d = 2,\n...     shape_scale = [4,3,1],\n...     shape_lengthscales = [3,1],\n...     torchify = True)\n&gt;&gt;&gt; kernel(x,x).shape\ntorch.Size([4, 3, 6, 5])\n&gt;&gt;&gt; kernel(x[:,:,None,:],x[:,None,:,:]).shape\ntorch.Size([4, 3, 6, 5, 5])\n&gt;&gt;&gt; kernel(x[:,None,:,None,:],x[None,:,None,:,:]).shape\ntorch.Size([4, 3, 6, 6, 5, 5])\n</code></pre> <p>Integrals</p> <pre><code>&gt;&gt;&gt; kernel = KernelGaussian(\n...     d = 2,\n...     torchify = True,\n...     scale = torch.from_numpy(rng.uniform(low=0,high=1,size=(3,1,))),\n...     lengthscales = torch.from_numpy(rng.uniform(low=0,high=1,size=(1,2))),\n...     requires_grad_scale = False,\n...     requires_grad_lengthscales = False)\n&gt;&gt;&gt; kintint = kernel.double_integral_01d()\n&gt;&gt;&gt; kintint\ntensor([0.5008, 0.1113, 0.3476], dtype=torch.float64)\n&gt;&gt;&gt; x_qmc_4d = torch.from_numpy(DigitalNetB2(4,seed=7)(2**16))\n&gt;&gt;&gt; kintint_qmc = kernel(x_qmc_4d[:,:2],x_qmc_4d[:,2:]).mean(1)\n&gt;&gt;&gt; kintint_qmc\ntensor([0.5008, 0.1113, 0.3476], dtype=torch.float64)\n&gt;&gt;&gt; with np.printoptions(formatter={\"float\":lambda x: \"%.1e\"%x}):\n...     torch.abs(kintint-kintint_qmc).numpy()\narray([2.3e-07, 5.1e-08, 1.6e-07])\n&gt;&gt;&gt; x = torch.from_numpy(rng.uniform(low=0,high=1,size=(4,2)))\n&gt;&gt;&gt; kint = kernel.single_integral_01d(x)\n&gt;&gt;&gt; kint\ntensor([[0.5461, 0.4273, 0.4300, 0.4469],\n        [0.1213, 0.0949, 0.0955, 0.0993],\n        [0.3790, 0.2966, 0.2985, 0.3102]], dtype=torch.float64)\n&gt;&gt;&gt; x_qmc_2d = torch.from_numpy(DigitalNetB2(2,seed=7)(2**16))\n&gt;&gt;&gt; kint_qmc = kernel(x[:,None,:],x_qmc_2d).mean(-1)\n&gt;&gt;&gt; kint_qmc\ntensor([[0.5461, 0.4273, 0.4300, 0.4469],\n        [0.1213, 0.0949, 0.0955, 0.0993],\n        [0.3790, 0.2966, 0.2985, 0.3102]], dtype=torch.float64)\n&gt;&gt;&gt; with np.printoptions(formatter={\"float\":lambda x: \"%.1e\"%x}):\n...     torch.abs(kint-kint_qmc).numpy()\narray([[2.2e-13, 4.0e-13, 4.2e-12, 3.9e-12],\n       [4.8e-14, 8.9e-14, 9.4e-13, 8.8e-13],\n       [1.5e-13, 2.8e-13, 2.9e-12, 2.7e-12]])\n&gt;&gt;&gt; k_1l = KernelGaussian(d=2,lengthscales=[.5],torchify=True)\n&gt;&gt;&gt; k_2l = KernelGaussian(d=2,lengthscales=[.5,.5],torchify=True)\n&gt;&gt;&gt; k_2l.double_integral_01d()\ntensor(0.5836, grad_fn=&lt;MulBackward0&gt;)\n&gt;&gt;&gt; k_1l.double_integral_01d()\ntensor(0.5836, grad_fn=&lt;MulBackward0&gt;)\n&gt;&gt;&gt; k_1l.single_integral_01d(x)\ntensor([0.5812, 0.4656, 0.5760, 0.5349], dtype=torch.float64,\n       grad_fn=&lt;MulBackward0&gt;)\n&gt;&gt;&gt; k_2l.single_integral_01d(x)\ntensor([0.5812, 0.4656, 0.5760, 0.5349], dtype=torch.float64,\n       grad_fn=&lt;MulBackward0&gt;)\n</code></pre> <p>Derivatives</p> <pre><code>&gt;&gt;&gt; kernel = KernelGaussian(\n...     d = 3,\n...     torchify = True,\n...     scale = torch.from_numpy(rng.uniform(low=0,high=1,size=(1,))),\n...     lengthscales = torch.from_numpy(rng.uniform(low=0,high=1,size=(3,))))\n&gt;&gt;&gt; x0 = torch.from_numpy(rng.uniform(low=0,high=1,size=(4,))).requires_grad_(True)\n&gt;&gt;&gt; x1 = torch.from_numpy(rng.uniform(low=0,high=1,size=(4,))).requires_grad_(True)\n&gt;&gt;&gt; x2 = torch.from_numpy(rng.uniform(low=0,high=1,size=(4,))).requires_grad_(True)\n&gt;&gt;&gt; x = torch.stack([x0,x1,x2],axis=-1)\n&gt;&gt;&gt; z0 = torch.from_numpy(rng.uniform(low=0,high=1,size=(4,))).requires_grad_(True)\n&gt;&gt;&gt; z1 = torch.from_numpy(rng.uniform(low=0,high=1,size=(4,))).requires_grad_(True)\n&gt;&gt;&gt; z2 = torch.from_numpy(rng.uniform(low=0,high=1,size=(4,))).requires_grad_(True)\n&gt;&gt;&gt; z = torch.stack([z0,z1,z2],axis=-1)\n&gt;&gt;&gt; c = torch.from_numpy(rng.uniform(low=0,high=1,size=(2,)))\n&gt;&gt;&gt; beta0 = torch.tensor([\n...     [1,0,0],\n...     [0,2,0]])\n&gt;&gt;&gt; beta1 = torch.tensor([\n...     [0,0,2],\n...     [2,1,0]])\n&gt;&gt;&gt; with torch.no_grad():\n...     y = kernel(x,z,beta0,beta1,c)\n&gt;&gt;&gt; y\ntensor([ 97.6657,   3.8621, -65.9329,  -1.1932], dtype=torch.float64)\n&gt;&gt;&gt; y_no_deriv = kernel(x,z)\n&gt;&gt;&gt; y_first = y_no_deriv.clone()\n&gt;&gt;&gt; y_first = torch.autograd.grad(y_first,x0,grad_outputs=torch.ones_like(y_first,requires_grad=True),create_graph=True)[0]\n&gt;&gt;&gt; y_first = torch.autograd.grad(y_first,z2,grad_outputs=torch.ones_like(y_first,requires_grad=True),create_graph=True)[0]\n&gt;&gt;&gt; y_first = torch.autograd.grad(y_first,z2,grad_outputs=torch.ones_like(y_first,requires_grad=True),create_graph=True)[0]\n&gt;&gt;&gt; y_second = y_no_deriv.clone()\n&gt;&gt;&gt; y_second = torch.autograd.grad(y_second,x1,grad_outputs=torch.ones_like(y_second,requires_grad=True),create_graph=True)[0]\n&gt;&gt;&gt; y_second = torch.autograd.grad(y_second,x1,grad_outputs=torch.ones_like(y_second,requires_grad=True),create_graph=True)[0]\n&gt;&gt;&gt; y_second = torch.autograd.grad(y_second,z0,grad_outputs=torch.ones_like(y_second,requires_grad=True),create_graph=True)[0]\n&gt;&gt;&gt; y_second = torch.autograd.grad(y_second,z0,grad_outputs=torch.ones_like(y_second,requires_grad=True),create_graph=True)[0]\n&gt;&gt;&gt; y_second = torch.autograd.grad(y_second,z1,grad_outputs=torch.ones_like(y_second,requires_grad=True),create_graph=True)[0]\n&gt;&gt;&gt; yhat = (y_first*c[0]+y_second*c[1]).detach()\n&gt;&gt;&gt; yhat\ntensor([ 97.6657,   3.8621, -65.9329,  -1.1932], dtype=torch.float64)\n&gt;&gt;&gt; torch.allclose(y,yhat)\nTrue\n</code></pre> Source code in <code>qmcpy/kernel/abstract_kernel.py</code> <pre><code>def __init__(\n    self,\n    d,\n    scale=1.0,\n    lengthscales=1.0,\n    shape_scale=[1],\n    shape_lengthscales=None,\n    tfs_scale=(tf_exp_eps_inv, tf_exp_eps),\n    tfs_lengthscales=(tf_exp_eps_inv, tf_exp_eps),\n    torchify=False,\n    requires_grad_scale=True,\n    requires_grad_lengthscales=True,\n    device=\"cpu\",\n    compile_call=False,\n    comiple_call_kwargs={},\n):\n    r\"\"\"\n    Args:\n        d (int): Dimension.\n        scale (Union[np.ndarray, torch.Tensor]): Scaling factor $S$.\n        lengthscales (Union[np.ndarray, torch.Tensor]): Lengthscales $\\boldsymbol{\\gamma}$.\n        shape_scale (list): Shape of `scale` when `np.isscalar(scale)`.\n        shape_lengthscales (list): Shape of `lengthscales` when `np.isscalar(lengthscales)`\n        tfs_scale (Tuple[callable,callable]): The first argument transforms to the raw value to be optimized; the second applies the inverse transform.\n        tfs_lengthscales (Tuple[callable,callable]): The first argument transforms to the raw value to be optimized; the second applies the inverse transform.\n        torchify (bool): If `True`, use the `torch` backend. Set to `True` if computing gradients with respect to inputs and/or hyperparameters.\n        requires_grad_scale (bool): If `True` and `torchify`, set `requires_grad=True` for `scale`.\n        requires_grad_lengthscales (bool): If `True` and `torchify`, set `requires_grad=True` for `lengthscales`.\n        device (torch.device): If `torchify`, put things onto this device.\n        compile_call (bool): If `True`, `torch.compile` the `parsed___call__` method.\n        comiple_call_kwargs (dict): When `compile_call` is `True`, pass these keyword arguments to `torch.compile`.\n    \"\"\"\n    super().__init__(\n        d=d,\n        torchify=torchify,\n        device=device,\n        compile_call=compile_call,\n        comiple_call_kwargs=comiple_call_kwargs,\n    )\n    self.raw_scale = self.parse_assign_param(\n        pname=\"scale\",\n        param=scale,\n        shape_param=shape_scale,\n        requires_grad_param=requires_grad_scale,\n        tfs_param=tfs_scale,\n        endsize_ops=[1],\n        constraints=[\"POSITIVE\"],\n    )\n    self.tfs_scale = tfs_scale\n    self.batch_param_names.append(\"scale\")\n    self.raw_lengthscales = self.parse_assign_param(\n        pname=\"lengthscales\",\n        param=lengthscales,\n        shape_param=[self.d] if shape_lengthscales is None else shape_lengthscales,\n        requires_grad_param=requires_grad_lengthscales,\n        tfs_param=tfs_lengthscales,\n        endsize_ops=[1, self.d],\n        constraints=[\"POSITIVE\"],\n    )\n    self.tfs_lengthscales = tfs_lengthscales\n    self.batch_param_names.append(\"lengthscales\")\n</code></pre>"},{"location":"api/kernels/#kernelsquaredexponential","title":"<code>KernelSquaredExponential</code>","text":"<p>               Bases: <code>AbstractKernelGaussianSE</code></p> <p>Gaussian / Squared Exponential kernel implemented using the pairwise distance function. Please use <code>KernelGaussian</code> when using derivative information.</p> \\[K(\\boldsymbol{x},\\boldsymbol{z}) = S \\exp\\left(-d_{\\boldsymbol{\\gamma}}^2(\\boldsymbol{x},\\boldsymbol{z})\\right), \\qquad d_{\\boldsymbol{\\gamma}}(\\boldsymbol{x},\\boldsymbol{z}) = \\left\\lVert\\frac{\\boldsymbol{x}-\\boldsymbol{z}}{\\sqrt{2}\\boldsymbol{\\gamma}}\\right\\rVert_2.\\] <p>Examples:</p> <pre><code>&gt;&gt;&gt; rng = np.random.Generator(np.random.PCG64(7))\n&gt;&gt;&gt; kernel = KernelSquaredExponential(d=2)\n&gt;&gt;&gt; x = rng.uniform(low=0,high=1,size=(4,2))\n&gt;&gt;&gt; kernel(x[0],x[0]).item()\n1.0\n&gt;&gt;&gt; kernel(x,x)\narray([1., 1., 1., 1.])\n&gt;&gt;&gt; kernel(x[:,None,:],x[None,:,:])\narray([[1.        , 0.78888466, 0.9483142 , 0.8228498 ],\n       [0.78888466, 1.        , 0.72380317, 0.62226176],\n       [0.9483142 , 0.72380317, 1.        , 0.95613874],\n       [0.8228498 , 0.62226176, 0.95613874, 1.        ]])\n</code></pre> <p>Multiple randomizations</p> <pre><code>&gt;&gt;&gt; x = rng.uniform(low=0,high=1,size=(6,5,2))\n&gt;&gt;&gt; kernel(x,x).shape\n(6, 5)\n&gt;&gt;&gt; kernel(x[:,:,None,:],x[:,None,:,:]).shape\n(6, 5, 5)\n&gt;&gt;&gt; kernel(x[:,None,:,None,:],x[None,:,None,:,:]).shape\n(6, 6, 5, 5)\n</code></pre> <p>Batch hyperparameters</p> <pre><code>&gt;&gt;&gt; kernel = KernelSquaredExponential(\n...     d = 2,\n...     shape_scale = [4,3,1],\n...     shape_lengthscales = [3,1])\n&gt;&gt;&gt; kernel(x,x).shape\n(4, 3, 6, 5)\n&gt;&gt;&gt; kernel(x[:,:,None,:],x[:,None,:,:]).shape\n(4, 3, 6, 5, 5)\n&gt;&gt;&gt; kernel(x[:,None,:,None,:],x[None,:,None,:,:]).shape\n(4, 3, 6, 6, 5, 5)\n</code></pre> <p>PyTorch</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; rng = np.random.Generator(np.random.PCG64(7))\n&gt;&gt;&gt; kernel = KernelSquaredExponential(d=2,torchify=True)\n&gt;&gt;&gt; x = torch.from_numpy(rng.uniform(low=0,high=1,size=(4,2)))\n&gt;&gt;&gt; kernel(x[0],x[0]).item()\n1.0\n&gt;&gt;&gt; kernel(x,x)\ntensor([1., 1., 1., 1.], dtype=torch.float64, grad_fn=&lt;MulBackward0&gt;)\n&gt;&gt;&gt; kernel(x[:,None,:],x[None,:,:])\ntensor([[1.0000, 0.7889, 0.9483, 0.8228],\n        [0.7889, 1.0000, 0.7238, 0.6223],\n        [0.9483, 0.7238, 1.0000, 0.9561],\n        [0.8228, 0.6223, 0.9561, 1.0000]], dtype=torch.float64,\n       grad_fn=&lt;MulBackward0&gt;)\n</code></pre> <p>Multiple randomizations</p> <pre><code>&gt;&gt;&gt; x = torch.from_numpy(rng.uniform(low=0,high=1,size=(6,5,2)))\n&gt;&gt;&gt; kernel(x,x).shape\ntorch.Size([6, 5])\n&gt;&gt;&gt; kernel(x[:,:,None,:],x[:,None,:,:]).shape\ntorch.Size([6, 5, 5])\n&gt;&gt;&gt; kernel(x[:,None,:,None,:],x[None,:,None,:,:]).shape\ntorch.Size([6, 6, 5, 5])\n</code></pre> <p>Batch hyperparameters</p> <pre><code>&gt;&gt;&gt; kernel = KernelSquaredExponential(\n...     d = 2,\n...     shape_scale = [4,3,1],\n...     shape_lengthscales = [3,1],\n...     torchify = True)\n&gt;&gt;&gt; kernel(x,x).shape\ntorch.Size([4, 3, 6, 5])\n&gt;&gt;&gt; kernel(x[:,:,None,:],x[:,None,:,:]).shape\ntorch.Size([4, 3, 6, 5, 5])\n&gt;&gt;&gt; kernel(x[:,None,:,None,:],x[None,:,None,:,:]).shape\ntorch.Size([4, 3, 6, 6, 5, 5])\n</code></pre> Source code in <code>qmcpy/kernel/abstract_kernel.py</code> <pre><code>def __init__(\n    self,\n    d,\n    scale=1.0,\n    lengthscales=1.0,\n    shape_scale=[1],\n    shape_lengthscales=None,\n    tfs_scale=(tf_exp_eps_inv, tf_exp_eps),\n    tfs_lengthscales=(tf_exp_eps_inv, tf_exp_eps),\n    torchify=False,\n    requires_grad_scale=True,\n    requires_grad_lengthscales=True,\n    device=\"cpu\",\n    compile_call=False,\n    comiple_call_kwargs={},\n):\n    r\"\"\"\n    Args:\n        d (int): Dimension.\n        scale (Union[np.ndarray, torch.Tensor]): Scaling factor $S$.\n        lengthscales (Union[np.ndarray, torch.Tensor]): Lengthscales $\\boldsymbol{\\gamma}$.\n        shape_scale (list): Shape of `scale` when `np.isscalar(scale)`.\n        shape_lengthscales (list): Shape of `lengthscales` when `np.isscalar(lengthscales)`\n        tfs_scale (Tuple[callable,callable]): The first argument transforms to the raw value to be optimized; the second applies the inverse transform.\n        tfs_lengthscales (Tuple[callable,callable]): The first argument transforms to the raw value to be optimized; the second applies the inverse transform.\n        torchify (bool): If `True`, use the `torch` backend. Set to `True` if computing gradients with respect to inputs and/or hyperparameters.\n        requires_grad_scale (bool): If `True` and `torchify`, set `requires_grad=True` for `scale`.\n        requires_grad_lengthscales (bool): If `True` and `torchify`, set `requires_grad=True` for `lengthscales`.\n        device (torch.device): If `torchify`, put things onto this device.\n        compile_call (bool): If `True`, `torch.compile` the `parsed___call__` method.\n        comiple_call_kwargs (dict): When `compile_call` is `True`, pass these keyword arguments to `torch.compile`.\n    \"\"\"\n    super().__init__(\n        d=d,\n        torchify=torchify,\n        device=device,\n        compile_call=compile_call,\n        comiple_call_kwargs=comiple_call_kwargs,\n    )\n    self.raw_scale = self.parse_assign_param(\n        pname=\"scale\",\n        param=scale,\n        shape_param=shape_scale,\n        requires_grad_param=requires_grad_scale,\n        tfs_param=tfs_scale,\n        endsize_ops=[1],\n        constraints=[\"POSITIVE\"],\n    )\n    self.tfs_scale = tfs_scale\n    self.batch_param_names.append(\"scale\")\n    self.raw_lengthscales = self.parse_assign_param(\n        pname=\"lengthscales\",\n        param=lengthscales,\n        shape_param=[self.d] if shape_lengthscales is None else shape_lengthscales,\n        requires_grad_param=requires_grad_lengthscales,\n        tfs_param=tfs_lengthscales,\n        endsize_ops=[1, self.d],\n        constraints=[\"POSITIVE\"],\n    )\n    self.tfs_lengthscales = tfs_lengthscales\n    self.batch_param_names.append(\"lengthscales\")\n</code></pre>"},{"location":"api/kernels/#kernelrationalquadratic","title":"<code>KernelRationalQuadratic</code>","text":"<p>               Bases: <code>AbstractKernelScaleLengthscales</code></p> <p>Rational Quadratic kernel</p> \\[K(\\boldsymbol{x},\\boldsymbol{z}) = S \\left(1+\\frac{d_{\\boldsymbol{\\gamma}}^2(\\boldsymbol{x},\\boldsymbol{z})}{\\alpha}\\right)^{-\\alpha}, \\qquad d_{\\boldsymbol{\\gamma}}(\\boldsymbol{x},\\boldsymbol{z}) = \\left\\lVert\\frac{\\boldsymbol{x}-\\boldsymbol{z}}{\\sqrt{2}\\boldsymbol{\\gamma}}\\right\\rVert_2.\\] <p>Examples:</p> <pre><code>&gt;&gt;&gt; rng = np.random.Generator(np.random.PCG64(7))\n&gt;&gt;&gt; kernel = KernelRationalQuadratic(d=2)\n&gt;&gt;&gt; x = rng.uniform(low=0,high=1,size=(4,2))\n&gt;&gt;&gt; kernel(x[0],x[0]).item()\n1.0\n&gt;&gt;&gt; kernel(x,x)\narray([1., 1., 1., 1.])\n&gt;&gt;&gt; kernel(x[:,None,:],x[None,:,:])\narray([[1.        , 0.80831912, 0.94960504, 0.83683297],\n       [0.80831912, 1.        , 0.75572321, 0.67824456],\n       [0.94960504, 0.75572321, 1.        , 0.95707312],\n       [0.83683297, 0.67824456, 0.95707312, 1.        ]])\n</code></pre> <p>Multiple randomizations</p> <pre><code>&gt;&gt;&gt; x = rng.uniform(low=0,high=1,size=(6,5,2))\n&gt;&gt;&gt; kernel(x,x).shape\n(6, 5)\n&gt;&gt;&gt; kernel(x[:,:,None,:],x[:,None,:,:]).shape\n(6, 5, 5)\n&gt;&gt;&gt; kernel(x[:,None,:,None,:],x[None,:,None,:,:]).shape\n(6, 6, 5, 5)\n</code></pre> <p>Batch hyperparameters</p> <pre><code>&gt;&gt;&gt; kernel = KernelRationalQuadratic(\n...     d = 2,\n...     shape_scale = [4,3,1],\n...     shape_lengthscales = [3,1])\n&gt;&gt;&gt; kernel(x,x).shape\n(4, 3, 6, 5)\n&gt;&gt;&gt; kernel(x[:,:,None,:],x[:,None,:,:]).shape\n(4, 3, 6, 5, 5)\n&gt;&gt;&gt; kernel(x[:,None,:,None,:],x[None,:,None,:,:]).shape\n(4, 3, 6, 6, 5, 5)\n</code></pre> <p>PyTorch</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; rng = np.random.Generator(np.random.PCG64(7))\n&gt;&gt;&gt; kernel = KernelRationalQuadratic(d=2,torchify=True)\n&gt;&gt;&gt; x = torch.from_numpy(rng.uniform(low=0,high=1,size=(4,2)))\n&gt;&gt;&gt; kernel(x[0],x[0]).item()\n1.0\n&gt;&gt;&gt; kernel(x,x)\ntensor([1., 1., 1., 1.], dtype=torch.float64, grad_fn=&lt;MulBackward0&gt;)\n&gt;&gt;&gt; kernel(x[:,None,:],x[None,:,:])\ntensor([[1.0000, 0.8083, 0.9496, 0.8368],\n        [0.8083, 1.0000, 0.7557, 0.6782],\n        [0.9496, 0.7557, 1.0000, 0.9571],\n        [0.8368, 0.6782, 0.9571, 1.0000]], dtype=torch.float64,\n       grad_fn=&lt;MulBackward0&gt;)\n</code></pre> <p>Multiple randomizations</p> <pre><code>&gt;&gt;&gt; x = torch.from_numpy(rng.uniform(low=0,high=1,size=(6,5,2)))\n&gt;&gt;&gt; kernel(x,x).shape\ntorch.Size([6, 5])\n&gt;&gt;&gt; kernel(x[:,:,None,:],x[:,None,:,:]).shape\ntorch.Size([6, 5, 5])\n&gt;&gt;&gt; kernel(x[:,None,:,None,:],x[None,:,None,:,:]).shape\ntorch.Size([6, 6, 5, 5])\n</code></pre> <p>Batch hyperparameters</p> <pre><code>&gt;&gt;&gt; kernel = KernelRationalQuadratic(\n...     d = 2,\n...     shape_alpha = [4,3,1],\n...     shape_lengthscales = [3,1],\n...     torchify = True)\n&gt;&gt;&gt; kernel(x,x).shape\ntorch.Size([4, 3, 6, 5])\n&gt;&gt;&gt; kernel(x[:,:,None,:],x[:,None,:,:]).shape\ntorch.Size([4, 3, 6, 5, 5])\n&gt;&gt;&gt; kernel(x[:,None,:,None,:],x[None,:,None,:,:]).shape\ntorch.Size([4, 3, 6, 6, 5, 5])\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>d</code> <code>int</code> <p>Dimension.</p> required <code>scale</code> <code>Union[ndarray, Tensor]</code> <p>Scaling factor \\(S\\).</p> <code>1.0</code> <code>lengthscales</code> <code>Union[ndarray, Tensor]</code> <p>Lengthscales \\(\\boldsymbol{\\gamma}\\).</p> <code>1.0</code> <code>alpha</code> <code>Union[ndarray, Tensor]</code> <p>Scale mixture parameter \\(\\alpha\\).</p> <code>1.0</code> <code>shape_scale</code> <code>list</code> <p>Shape of <code>scale</code> when <code>np.isscalar(scale)</code>.</p> <code>[1]</code> <code>shape_lengthscales</code> <code>list</code> <p>Shape of <code>lengthscales</code> when <code>np.isscalar(lengthscales)</code></p> <code>None</code> <code>shape_alpha</code> <code>list</code> <p>Shape of <code>alpha</code> when <code>np.isscalar(alpha)</code></p> <code>[1]</code> <code>tfs_scale</code> <code>Tuple[callable, callable]</code> <p>The first argument transforms to the raw value to be optimized; the second applies the inverse transform.</p> <code>(tf_exp_eps_inv, tf_exp_eps)</code> <code>tfs_lengthscales</code> <code>Tuple[callable, callable]</code> <p>The first argument transforms to the raw value to be optimized; the second applies the inverse transform.</p> <code>(tf_exp_eps_inv, tf_exp_eps)</code> <code>tfs_alpha</code> <code>Tuple[callable, callable]</code> <p>The first argument transforms to the raw value to be optimized; the second applies the inverse transform.</p> <code>(tf_exp_eps_inv, tf_exp_eps)</code> <code>torchify</code> <code>bool</code> <p>If <code>True</code>, use the <code>torch</code> backend. Set to <code>True</code> if computing gradients with respect to inputs and/or hyperparameters.</p> <code>False</code> <code>requires_grad_scale</code> <code>bool</code> <p>If <code>True</code> and <code>torchify</code>, set <code>requires_grad=True</code> for <code>scale</code>.</p> <code>True</code> <code>requires_grad_lengthscales</code> <code>bool</code> <p>If <code>True</code> and <code>torchify</code>, set <code>requires_grad=True</code> for <code>lengthscales</code>.</p> <code>True</code> <code>requires_grad_alpha</code> <code>bool</code> <p>If <code>True</code> and <code>torchify</code>, set <code>requires_grad=True</code> for <code>alpha</code>.</p> <code>True</code> <code>device</code> <code>device</code> <p>If <code>torchify</code>, put things onto this device.</p> <code>'cpu'</code> <code>compile_call</code> <code>bool</code> <p>If <code>True</code>, <code>torch.compile</code> the <code>parsed___call__</code> method.</p> <code>False</code> <code>comiple_call_kwargs</code> <code>dict</code> <p>When <code>compile_call</code> is <code>True</code>, pass these keyword arguments to <code>torch.compile</code>.</p> <code>{}</code> Source code in <code>qmcpy/kernel/common_kernels.py</code> <pre><code>def __init__(\n    self,\n    d,\n    scale=1.0,\n    lengthscales=1.0,\n    alpha=1.0,\n    shape_scale=[1],\n    shape_lengthscales=None,\n    shape_alpha=[1],\n    tfs_scale=(tf_exp_eps_inv, tf_exp_eps),\n    tfs_lengthscales=(tf_exp_eps_inv, tf_exp_eps),\n    tfs_alpha=(tf_exp_eps_inv, tf_exp_eps),\n    torchify=False,\n    requires_grad_scale=True,\n    requires_grad_lengthscales=True,\n    requires_grad_alpha=True,\n    device=\"cpu\",\n    compile_call=False,\n    comiple_call_kwargs={},\n):\n    r\"\"\"\n    Args:\n        d (int): Dimension.\n        scale (Union[np.ndarray, torch.Tensor]): Scaling factor $S$.\n        lengthscales (Union[np.ndarray, torch.Tensor]): Lengthscales $\\boldsymbol{\\gamma}$.\n        alpha (Union[np.ndarray, torch.Tensor]): Scale mixture parameter $\\alpha$.\n        shape_scale (list): Shape of `scale` when `np.isscalar(scale)`.\n        shape_lengthscales (list): Shape of `lengthscales` when `np.isscalar(lengthscales)`\n        shape_alpha (list): Shape of `alpha` when `np.isscalar(alpha)`\n        tfs_scale (Tuple[callable,callable]): The first argument transforms to the raw value to be optimized; the second applies the inverse transform.\n        tfs_lengthscales (Tuple[callable,callable]): The first argument transforms to the raw value to be optimized; the second applies the inverse transform.\n        tfs_alpha (Tuple[callable,callable]): The first argument transforms to the raw value to be optimized; the second applies the inverse transform.\n        torchify (bool): If `True`, use the `torch` backend. Set to `True` if computing gradients with respect to inputs and/or hyperparameters.\n        requires_grad_scale (bool): If `True` and `torchify`, set `requires_grad=True` for `scale`.\n        requires_grad_lengthscales (bool): If `True` and `torchify`, set `requires_grad=True` for `lengthscales`.\n        requires_grad_alpha (bool): If `True` and `torchify`, set `requires_grad=True` for `alpha`.\n        device (torch.device): If `torchify`, put things onto this device.\n        compile_call (bool): If `True`, `torch.compile` the `parsed___call__` method.\n        comiple_call_kwargs (dict): When `compile_call` is `True`, pass these keyword arguments to `torch.compile`.\n    \"\"\"\n    super().__init__(\n        d=d,\n        scale=scale,\n        lengthscales=lengthscales,\n        shape_scale=shape_scale,\n        shape_lengthscales=shape_lengthscales,\n        tfs_scale=tfs_scale,\n        tfs_lengthscales=tfs_lengthscales,\n        torchify=torchify,\n        requires_grad_scale=requires_grad_scale,\n        requires_grad_lengthscales=requires_grad_lengthscales,\n        device=device,\n        compile_call=compile_call,\n        comiple_call_kwargs=comiple_call_kwargs,\n    )\n    self.raw_alpha = self.parse_assign_param(\n        pname=\"alpha\",\n        param=alpha,\n        shape_param=shape_alpha,\n        requires_grad_param=requires_grad_alpha,\n        tfs_param=tfs_alpha,\n        endsize_ops=[1],\n        constraints=[\"POSITIVE\"],\n    )\n    self.tfs_alpha = tfs_alpha\n    self.batch_param_names.append(\"alpha\")\n</code></pre>"},{"location":"api/kernels/#kernelmatern12","title":"<code>KernelMatern12</code>","text":"<p>               Bases: <code>AbstractKernelScaleLengthscales</code></p> <p>Matern kernel with \\(\\alpha=1/2\\).</p> \\[K(\\boldsymbol{x},\\boldsymbol{z}) = S \\exp\\left(-d_{\\boldsymbol{\\gamma}}(\\boldsymbol{x},\\boldsymbol{z})\\right), \\qquad d_{\\boldsymbol{\\gamma}}(\\boldsymbol{x},\\boldsymbol{z}) = \\left\\lVert\\frac{\\boldsymbol{x}-\\boldsymbol{z}}{\\sqrt{2}\\boldsymbol{\\gamma}}\\right\\rVert_2.\\] <p>Examples:</p> <pre><code>&gt;&gt;&gt; rng = np.random.Generator(np.random.PCG64(7))\n&gt;&gt;&gt; kernel = KernelMatern12(d=2)\n&gt;&gt;&gt; x = rng.uniform(low=0,high=1,size=(4,2))\n&gt;&gt;&gt; kernel(x[0],x[0]).item()\n1.0\n&gt;&gt;&gt; kernel(x,x)\narray([1., 1., 1., 1.])\n&gt;&gt;&gt; kernel(x[:,None,:],x[None,:,:])\narray([[1.        , 0.61448839, 0.79424131, 0.64302787],\n       [0.61448839, 1.        , 0.56635268, 0.50219691],\n       [0.79424131, 0.56635268, 1.        , 0.80913986],\n       [0.64302787, 0.50219691, 0.80913986, 1.        ]])\n</code></pre> <p>Multiple randomizations</p> <pre><code>&gt;&gt;&gt; x = rng.uniform(low=0,high=1,size=(6,5,2))\n&gt;&gt;&gt; kernel(x,x).shape\n(6, 5)\n&gt;&gt;&gt; kernel(x[:,:,None,:],x[:,None,:,:]).shape\n(6, 5, 5)\n&gt;&gt;&gt; kernel(x[:,None,:,None,:],x[None,:,None,:,:]).shape\n(6, 6, 5, 5)\n</code></pre> <p>Batch hyperparameters</p> <pre><code>&gt;&gt;&gt; kernel = KernelMatern12(\n...     d = 2,\n...     shape_scale = [4,3,1],\n...     shape_lengthscales = [3,1])\n&gt;&gt;&gt; kernel(x,x).shape\n(4, 3, 6, 5)\n&gt;&gt;&gt; kernel(x[:,:,None,:],x[:,None,:,:]).shape\n(4, 3, 6, 5, 5)\n&gt;&gt;&gt; kernel(x[:,None,:,None,:],x[None,:,None,:,:]).shape\n(4, 3, 6, 6, 5, 5)\n</code></pre> <p>PyTorch</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; rng = np.random.Generator(np.random.PCG64(7))\n&gt;&gt;&gt; kernel = KernelMatern12(d=2,torchify=True)\n&gt;&gt;&gt; x = torch.from_numpy(rng.uniform(low=0,high=1,size=(4,2)))\n&gt;&gt;&gt; kernel(x[0],x[0]).item()\n1.0\n&gt;&gt;&gt; kernel(x,x)\ntensor([1., 1., 1., 1.], dtype=torch.float64, grad_fn=&lt;MulBackward0&gt;)\n&gt;&gt;&gt; kernel(x[:,None,:],x[None,:,:])\ntensor([[1.0000, 0.6145, 0.7942, 0.6430],\n        [0.6145, 1.0000, 0.5664, 0.5022],\n        [0.7942, 0.5664, 1.0000, 0.8091],\n        [0.6430, 0.5022, 0.8091, 1.0000]], dtype=torch.float64,\n       grad_fn=&lt;MulBackward0&gt;)\n</code></pre> <p>Multiple randomizations</p> <pre><code>&gt;&gt;&gt; x = torch.from_numpy(rng.uniform(low=0,high=1,size=(6,5,2)))\n&gt;&gt;&gt; kernel(x,x).shape\ntorch.Size([6, 5])\n&gt;&gt;&gt; kernel(x[:,:,None,:],x[:,None,:,:]).shape\ntorch.Size([6, 5, 5])\n&gt;&gt;&gt; kernel(x[:,None,:,None,:],x[None,:,None,:,:]).shape\ntorch.Size([6, 6, 5, 5])\n</code></pre> <p>Batch hyperparameters</p> <pre><code>&gt;&gt;&gt; kernel = KernelMatern12(\n...     d = 2,\n...     shape_scale = [4,3,1],\n...     shape_lengthscales = [3,1],\n...     torchify = True)\n&gt;&gt;&gt; kernel(x,x).shape\ntorch.Size([4, 3, 6, 5])\n&gt;&gt;&gt; kernel(x[:,:,None,:],x[:,None,:,:]).shape\ntorch.Size([4, 3, 6, 5, 5])\n&gt;&gt;&gt; kernel(x[:,None,:,None,:],x[None,:,None,:,:]).shape\ntorch.Size([4, 3, 6, 6, 5, 5])\n</code></pre> Source code in <code>qmcpy/kernel/abstract_kernel.py</code> <pre><code>def __init__(\n    self,\n    d,\n    scale=1.0,\n    lengthscales=1.0,\n    shape_scale=[1],\n    shape_lengthscales=None,\n    tfs_scale=(tf_exp_eps_inv, tf_exp_eps),\n    tfs_lengthscales=(tf_exp_eps_inv, tf_exp_eps),\n    torchify=False,\n    requires_grad_scale=True,\n    requires_grad_lengthscales=True,\n    device=\"cpu\",\n    compile_call=False,\n    comiple_call_kwargs={},\n):\n    r\"\"\"\n    Args:\n        d (int): Dimension.\n        scale (Union[np.ndarray, torch.Tensor]): Scaling factor $S$.\n        lengthscales (Union[np.ndarray, torch.Tensor]): Lengthscales $\\boldsymbol{\\gamma}$.\n        shape_scale (list): Shape of `scale` when `np.isscalar(scale)`.\n        shape_lengthscales (list): Shape of `lengthscales` when `np.isscalar(lengthscales)`\n        tfs_scale (Tuple[callable,callable]): The first argument transforms to the raw value to be optimized; the second applies the inverse transform.\n        tfs_lengthscales (Tuple[callable,callable]): The first argument transforms to the raw value to be optimized; the second applies the inverse transform.\n        torchify (bool): If `True`, use the `torch` backend. Set to `True` if computing gradients with respect to inputs and/or hyperparameters.\n        requires_grad_scale (bool): If `True` and `torchify`, set `requires_grad=True` for `scale`.\n        requires_grad_lengthscales (bool): If `True` and `torchify`, set `requires_grad=True` for `lengthscales`.\n        device (torch.device): If `torchify`, put things onto this device.\n        compile_call (bool): If `True`, `torch.compile` the `parsed___call__` method.\n        comiple_call_kwargs (dict): When `compile_call` is `True`, pass these keyword arguments to `torch.compile`.\n    \"\"\"\n    super().__init__(\n        d=d,\n        torchify=torchify,\n        device=device,\n        compile_call=compile_call,\n        comiple_call_kwargs=comiple_call_kwargs,\n    )\n    self.raw_scale = self.parse_assign_param(\n        pname=\"scale\",\n        param=scale,\n        shape_param=shape_scale,\n        requires_grad_param=requires_grad_scale,\n        tfs_param=tfs_scale,\n        endsize_ops=[1],\n        constraints=[\"POSITIVE\"],\n    )\n    self.tfs_scale = tfs_scale\n    self.batch_param_names.append(\"scale\")\n    self.raw_lengthscales = self.parse_assign_param(\n        pname=\"lengthscales\",\n        param=lengthscales,\n        shape_param=[self.d] if shape_lengthscales is None else shape_lengthscales,\n        requires_grad_param=requires_grad_lengthscales,\n        tfs_param=tfs_lengthscales,\n        endsize_ops=[1, self.d],\n        constraints=[\"POSITIVE\"],\n    )\n    self.tfs_lengthscales = tfs_lengthscales\n    self.batch_param_names.append(\"lengthscales\")\n</code></pre>"},{"location":"api/kernels/#kernelmatern32","title":"<code>KernelMatern32</code>","text":"<p>               Bases: <code>AbstractKernelScaleLengthscales</code></p> <p>Matern kernel with \\(\\alpha=3/2\\).</p> \\[K(\\boldsymbol{x},\\boldsymbol{z}) = S \\left(1+\\sqrt{3} d_{\\boldsymbol{\\gamma}}(\\boldsymbol{x},\\boldsymbol{z})\\right)\\exp\\left(-\\sqrt{3}d_{\\boldsymbol{\\gamma}}(\\boldsymbol{x},\\boldsymbol{z})\\right), \\qquad d_{\\boldsymbol{\\gamma}}(\\boldsymbol{x},\\boldsymbol{z}) = \\left\\lVert\\frac{\\boldsymbol{x}-\\boldsymbol{z}}{\\sqrt{2}\\boldsymbol{\\gamma}}\\right\\rVert_2.\\] <p>Examples:</p> <pre><code>&gt;&gt;&gt; rng = np.random.Generator(np.random.PCG64(7))\n&gt;&gt;&gt; kernel = KernelMatern32(d=2)\n&gt;&gt;&gt; x = rng.uniform(low=0,high=1,size=(4,2))\n&gt;&gt;&gt; kernel(x[0],x[0]).item()\n1.0\n&gt;&gt;&gt; kernel(x,x)\narray([1., 1., 1., 1.])\n&gt;&gt;&gt; kernel(x[:,None,:],x[None,:,:])\narray([[1.        , 0.79309639, 0.93871358, 0.82137958],\n       [0.79309639, 1.        , 0.74137353, 0.66516872],\n       [0.93871358, 0.74137353, 1.        , 0.9471166 ],\n       [0.82137958, 0.66516872, 0.9471166 , 1.        ]])\n</code></pre> <p>Multiple randomizations</p> <pre><code>&gt;&gt;&gt; x = rng.uniform(low=0,high=1,size=(6,5,2))\n&gt;&gt;&gt; kernel(x,x).shape\n(6, 5)\n&gt;&gt;&gt; kernel(x[:,:,None,:],x[:,None,:,:]).shape\n(6, 5, 5)\n&gt;&gt;&gt; kernel(x[:,None,:,None,:],x[None,:,None,:,:]).shape\n(6, 6, 5, 5)\n</code></pre> <p>Batch hyperparameters</p> <pre><code>&gt;&gt;&gt; kernel = KernelMatern32(\n...     d = 2,\n...     shape_scale = [4,3,1],\n...     shape_lengthscales = [3,1])\n&gt;&gt;&gt; kernel(x,x).shape\n(4, 3, 6, 5)\n&gt;&gt;&gt; kernel(x[:,:,None,:],x[:,None,:,:]).shape\n(4, 3, 6, 5, 5)\n&gt;&gt;&gt; kernel(x[:,None,:,None,:],x[None,:,None,:,:]).shape\n(4, 3, 6, 6, 5, 5)\n</code></pre> <p>PyTorch</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; rng = np.random.Generator(np.random.PCG64(7))\n&gt;&gt;&gt; kernel = KernelMatern32(d=2,torchify=True)\n&gt;&gt;&gt; x = torch.from_numpy(rng.uniform(low=0,high=1,size=(4,2)))\n&gt;&gt;&gt; kernel(x[0],x[0]).item()\n1.0\n&gt;&gt;&gt; kernel(x,x)\ntensor([1., 1., 1., 1.], dtype=torch.float64, grad_fn=&lt;MulBackward0&gt;)\n&gt;&gt;&gt; kernel(x[:,None,:],x[None,:,:])\ntensor([[1.0000, 0.7931, 0.9387, 0.8214],\n        [0.7931, 1.0000, 0.7414, 0.6652],\n        [0.9387, 0.7414, 1.0000, 0.9471],\n        [0.8214, 0.6652, 0.9471, 1.0000]], dtype=torch.float64,\n       grad_fn=&lt;MulBackward0&gt;)\n</code></pre> <p>Multiple randomizations</p> <pre><code>&gt;&gt;&gt; x = torch.from_numpy(rng.uniform(low=0,high=1,size=(6,5,2)))\n&gt;&gt;&gt; kernel(x,x).shape\ntorch.Size([6, 5])\n&gt;&gt;&gt; kernel(x[:,:,None,:],x[:,None,:,:]).shape\ntorch.Size([6, 5, 5])\n&gt;&gt;&gt; kernel(x[:,None,:,None,:],x[None,:,None,:,:]).shape\ntorch.Size([6, 6, 5, 5])\n</code></pre> <p>Batch hyperparameters</p> <pre><code>&gt;&gt;&gt; kernel = KernelMatern32(\n...     d = 2,\n...     shape_scale = [4,3,1],\n...     shape_lengthscales = [3,1],\n...     torchify = True)\n&gt;&gt;&gt; kernel(x,x).shape\ntorch.Size([4, 3, 6, 5])\n&gt;&gt;&gt; kernel(x[:,:,None,:],x[:,None,:,:]).shape\ntorch.Size([4, 3, 6, 5, 5])\n&gt;&gt;&gt; kernel(x[:,None,:,None,:],x[None,:,None,:,:]).shape\ntorch.Size([4, 3, 6, 6, 5, 5])\n</code></pre> Source code in <code>qmcpy/kernel/abstract_kernel.py</code> <pre><code>def __init__(\n    self,\n    d,\n    scale=1.0,\n    lengthscales=1.0,\n    shape_scale=[1],\n    shape_lengthscales=None,\n    tfs_scale=(tf_exp_eps_inv, tf_exp_eps),\n    tfs_lengthscales=(tf_exp_eps_inv, tf_exp_eps),\n    torchify=False,\n    requires_grad_scale=True,\n    requires_grad_lengthscales=True,\n    device=\"cpu\",\n    compile_call=False,\n    comiple_call_kwargs={},\n):\n    r\"\"\"\n    Args:\n        d (int): Dimension.\n        scale (Union[np.ndarray, torch.Tensor]): Scaling factor $S$.\n        lengthscales (Union[np.ndarray, torch.Tensor]): Lengthscales $\\boldsymbol{\\gamma}$.\n        shape_scale (list): Shape of `scale` when `np.isscalar(scale)`.\n        shape_lengthscales (list): Shape of `lengthscales` when `np.isscalar(lengthscales)`\n        tfs_scale (Tuple[callable,callable]): The first argument transforms to the raw value to be optimized; the second applies the inverse transform.\n        tfs_lengthscales (Tuple[callable,callable]): The first argument transforms to the raw value to be optimized; the second applies the inverse transform.\n        torchify (bool): If `True`, use the `torch` backend. Set to `True` if computing gradients with respect to inputs and/or hyperparameters.\n        requires_grad_scale (bool): If `True` and `torchify`, set `requires_grad=True` for `scale`.\n        requires_grad_lengthscales (bool): If `True` and `torchify`, set `requires_grad=True` for `lengthscales`.\n        device (torch.device): If `torchify`, put things onto this device.\n        compile_call (bool): If `True`, `torch.compile` the `parsed___call__` method.\n        comiple_call_kwargs (dict): When `compile_call` is `True`, pass these keyword arguments to `torch.compile`.\n    \"\"\"\n    super().__init__(\n        d=d,\n        torchify=torchify,\n        device=device,\n        compile_call=compile_call,\n        comiple_call_kwargs=comiple_call_kwargs,\n    )\n    self.raw_scale = self.parse_assign_param(\n        pname=\"scale\",\n        param=scale,\n        shape_param=shape_scale,\n        requires_grad_param=requires_grad_scale,\n        tfs_param=tfs_scale,\n        endsize_ops=[1],\n        constraints=[\"POSITIVE\"],\n    )\n    self.tfs_scale = tfs_scale\n    self.batch_param_names.append(\"scale\")\n    self.raw_lengthscales = self.parse_assign_param(\n        pname=\"lengthscales\",\n        param=lengthscales,\n        shape_param=[self.d] if shape_lengthscales is None else shape_lengthscales,\n        requires_grad_param=requires_grad_lengthscales,\n        tfs_param=tfs_lengthscales,\n        endsize_ops=[1, self.d],\n        constraints=[\"POSITIVE\"],\n    )\n    self.tfs_lengthscales = tfs_lengthscales\n    self.batch_param_names.append(\"lengthscales\")\n</code></pre>"},{"location":"api/kernels/#kernelmatern52","title":"<code>KernelMatern52</code>","text":"<p>               Bases: <code>AbstractKernelScaleLengthscales</code></p> <p>Matern kernel with \\(\\alpha=5/2\\).</p> \\[K(\\boldsymbol{x},\\boldsymbol{z}) = S \\left(1+\\sqrt{5} d_{\\boldsymbol{\\gamma}}(\\boldsymbol{x},\\boldsymbol{z}) + \\frac{5}{3} d_{\\boldsymbol{\\gamma}}^2(\\boldsymbol{x},\\boldsymbol{z})\\right)\\exp\\left(-\\sqrt{5}d_{\\boldsymbol{\\gamma}}(\\boldsymbol{x},\\boldsymbol{z})\\right), \\qquad d_{\\boldsymbol{\\gamma}}(\\boldsymbol{x},\\boldsymbol{z}) = \\left\\lVert\\frac{\\boldsymbol{x}-\\boldsymbol{z}}{\\sqrt{2}\\boldsymbol{\\gamma}}\\right\\rVert_2.\\] <p>Examples:</p> <pre><code>&gt;&gt;&gt; rng = np.random.Generator(np.random.PCG64(7))\n&gt;&gt;&gt; kernel = KernelMatern52(d=2)\n&gt;&gt;&gt; x = rng.uniform(low=0,high=1,size=(4,2))\n&gt;&gt;&gt; kernel(x[0],x[0]).item()\n1.0\n&gt;&gt;&gt; kernel(x,x)\narray([1., 1., 1., 1.])\n&gt;&gt;&gt; kernel(x[:,None,:],x[None,:,:])\narray([[1.        , 0.83612941, 0.95801903, 0.861472  ],\n       [0.83612941, 1.        , 0.78812397, 0.71396963],\n       [0.95801903, 0.78812397, 1.        , 0.96425994],\n       [0.861472  , 0.71396963, 0.96425994, 1.        ]])\n</code></pre> <p>Multiple randomizations</p> <pre><code>&gt;&gt;&gt; x = rng.uniform(low=0,high=1,size=(6,5,2))\n&gt;&gt;&gt; kernel(x,x).shape\n(6, 5)\n&gt;&gt;&gt; kernel(x[:,:,None,:],x[:,None,:,:]).shape\n(6, 5, 5)\n&gt;&gt;&gt; kernel(x[:,None,:,None,:],x[None,:,None,:,:]).shape\n(6, 6, 5, 5)\n</code></pre> <p>Batch hyperparameters</p> <pre><code>&gt;&gt;&gt; kernel = KernelMatern52(\n...     d = 2,\n...     shape_scale = [4,3,1],\n...     shape_lengthscales = [3,1])\n&gt;&gt;&gt; kernel(x,x).shape\n(4, 3, 6, 5)\n&gt;&gt;&gt; kernel(x[:,:,None,:],x[:,None,:,:]).shape\n(4, 3, 6, 5, 5)\n&gt;&gt;&gt; kernel(x[:,None,:,None,:],x[None,:,None,:,:]).shape\n(4, 3, 6, 6, 5, 5)\n</code></pre> <p>PyTorch</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; rng = np.random.Generator(np.random.PCG64(7))\n&gt;&gt;&gt; kernel = KernelMatern52(d=2,torchify=True)\n&gt;&gt;&gt; x = torch.from_numpy(rng.uniform(low=0,high=1,size=(4,2)))\n&gt;&gt;&gt; kernel(x[0],x[0]).item()\n1.0\n&gt;&gt;&gt; kernel(x,x)\ntensor([1., 1., 1., 1.], dtype=torch.float64, grad_fn=&lt;MulBackward0&gt;)\n&gt;&gt;&gt; kernel(x[:,None,:],x[None,:,:])\ntensor([[1.0000, 0.8361, 0.9580, 0.8615],\n        [0.8361, 1.0000, 0.7881, 0.7140],\n        [0.9580, 0.7881, 1.0000, 0.9643],\n        [0.8615, 0.7140, 0.9643, 1.0000]], dtype=torch.float64,\n       grad_fn=&lt;MulBackward0&gt;)\n</code></pre> <p>Multiple randomizations</p> <pre><code>&gt;&gt;&gt; x = torch.from_numpy(rng.uniform(low=0,high=1,size=(6,5,2)))\n&gt;&gt;&gt; kernel(x,x).shape\ntorch.Size([6, 5])\n&gt;&gt;&gt; kernel(x[:,:,None,:],x[:,None,:,:]).shape\ntorch.Size([6, 5, 5])\n&gt;&gt;&gt; kernel(x[:,None,:,None,:],x[None,:,None,:,:]).shape\ntorch.Size([6, 6, 5, 5])\n</code></pre> <p>Batch hyperparameters</p> <pre><code>&gt;&gt;&gt; kernel = KernelMatern52(\n...     d = 2,\n...     shape_scale = [4,3,1],\n...     shape_lengthscales = [3,1],\n...     torchify = True)\n&gt;&gt;&gt; kernel(x,x).shape\ntorch.Size([4, 3, 6, 5])\n&gt;&gt;&gt; kernel(x[:,:,None,:],x[:,None,:,:]).shape\ntorch.Size([4, 3, 6, 5, 5])\n&gt;&gt;&gt; kernel(x[:,None,:,None,:],x[None,:,None,:,:]).shape\ntorch.Size([4, 3, 6, 6, 5, 5])\n</code></pre> Source code in <code>qmcpy/kernel/abstract_kernel.py</code> <pre><code>def __init__(\n    self,\n    d,\n    scale=1.0,\n    lengthscales=1.0,\n    shape_scale=[1],\n    shape_lengthscales=None,\n    tfs_scale=(tf_exp_eps_inv, tf_exp_eps),\n    tfs_lengthscales=(tf_exp_eps_inv, tf_exp_eps),\n    torchify=False,\n    requires_grad_scale=True,\n    requires_grad_lengthscales=True,\n    device=\"cpu\",\n    compile_call=False,\n    comiple_call_kwargs={},\n):\n    r\"\"\"\n    Args:\n        d (int): Dimension.\n        scale (Union[np.ndarray, torch.Tensor]): Scaling factor $S$.\n        lengthscales (Union[np.ndarray, torch.Tensor]): Lengthscales $\\boldsymbol{\\gamma}$.\n        shape_scale (list): Shape of `scale` when `np.isscalar(scale)`.\n        shape_lengthscales (list): Shape of `lengthscales` when `np.isscalar(lengthscales)`\n        tfs_scale (Tuple[callable,callable]): The first argument transforms to the raw value to be optimized; the second applies the inverse transform.\n        tfs_lengthscales (Tuple[callable,callable]): The first argument transforms to the raw value to be optimized; the second applies the inverse transform.\n        torchify (bool): If `True`, use the `torch` backend. Set to `True` if computing gradients with respect to inputs and/or hyperparameters.\n        requires_grad_scale (bool): If `True` and `torchify`, set `requires_grad=True` for `scale`.\n        requires_grad_lengthscales (bool): If `True` and `torchify`, set `requires_grad=True` for `lengthscales`.\n        device (torch.device): If `torchify`, put things onto this device.\n        compile_call (bool): If `True`, `torch.compile` the `parsed___call__` method.\n        comiple_call_kwargs (dict): When `compile_call` is `True`, pass these keyword arguments to `torch.compile`.\n    \"\"\"\n    super().__init__(\n        d=d,\n        torchify=torchify,\n        device=device,\n        compile_call=compile_call,\n        comiple_call_kwargs=comiple_call_kwargs,\n    )\n    self.raw_scale = self.parse_assign_param(\n        pname=\"scale\",\n        param=scale,\n        shape_param=shape_scale,\n        requires_grad_param=requires_grad_scale,\n        tfs_param=tfs_scale,\n        endsize_ops=[1],\n        constraints=[\"POSITIVE\"],\n    )\n    self.tfs_scale = tfs_scale\n    self.batch_param_names.append(\"scale\")\n    self.raw_lengthscales = self.parse_assign_param(\n        pname=\"lengthscales\",\n        param=lengthscales,\n        shape_param=[self.d] if shape_lengthscales is None else shape_lengthscales,\n        requires_grad_param=requires_grad_lengthscales,\n        tfs_param=tfs_lengthscales,\n        endsize_ops=[1, self.d],\n        constraints=[\"POSITIVE\"],\n    )\n    self.tfs_lengthscales = tfs_lengthscales\n    self.batch_param_names.append(\"lengthscales\")\n</code></pre>"},{"location":"api/kernels/#kernelmultitask","title":"<code>KernelMultiTask</code>","text":"<p>               Bases: <code>AbstractKernel</code></p> <p>Multi-task kernel</p> \\[K((i,\\boldsymbol{x}),(j,\\boldsymbol{z})) = K_{\\mathrm{task}}(i,j) K_{\\mathrm{base}}(\\boldsymbol{x},\\boldsymbol{z})\\] <p>parameterized for \\(T\\) tasks by a factor \\(\\mathsf{F} \\in \\mathbb{R}^{T \\times r}\\) and a diagonal \\(\\boldsymbol{v} \\in \\mathbb{R}^T\\) so that</p> \\[\\left[K_{\\mathrm{task}}(i,j)\\right]_{i,j=1}^T = \\mathsf{F} \\mathsf{F}^T + \\mathrm{diag}(\\boldsymbol{v}).\\] <p>Examples:</p> <pre><code>&gt;&gt;&gt; kmt = KernelMultiTask(KernelGaussian(d=2),num_tasks=3,diag=[1,2,3])\n&gt;&gt;&gt; x = np.random.rand(4,2)\n&gt;&gt;&gt; task = np.arange(3)\n&gt;&gt;&gt; kmt(task[1],task[1],x[0],x[0]).item()\n3.0\n&gt;&gt;&gt; kmt(task,task,x[0],x[0])\narray([2., 3., 4.])\n&gt;&gt;&gt; kmt(task[:,None],task[None,:],x[0],x[0]).shape\n(3, 3)\n&gt;&gt;&gt; kmt(task[1],task[1],x,x,)\narray([3., 3., 3., 3.])\n&gt;&gt;&gt; kmt(task,task,x[:3],x[:3])\narray([2., 3., 4.])\n&gt;&gt;&gt; kmt(task[:,None],task[:,None],x,x).shape\n(3, 4)\n&gt;&gt;&gt; v = kmt(task,task,x[:3,None,:],x[None,:3,:])\n&gt;&gt;&gt; v.shape\n(3, 3)\n&gt;&gt;&gt; np.allclose(v,kmt.base_kernel(x[:3,None,:],x[None,:3,:])*kmt.taskmat[task,task])\nTrue\n&gt;&gt;&gt; kmt(task[:,None,None],task[:,None,None],x[:,None,:],x[None,:,:]).shape\n(3, 4, 4)\n&gt;&gt;&gt; kmt(task[:,None,None,None],task[None,:,None,None],x[:,None,:],x[None,:,:]).shape\n(3, 3, 4, 4)\n</code></pre> <p>Batched inference</p> <pre><code>&gt;&gt;&gt; kernel_base = KernelGaussian(d=10,shape_lengthscales=(5,1),shape_scale=(3,5,1))\n&gt;&gt;&gt; rng = np.random.Generator(np.random.PCG64(7))\n&gt;&gt;&gt; kmt = KernelMultiTask(kernel_base,\n...     num_tasks=4,\n...     shape_factor=(6,3,5,4,2),\n...     diag = rng.uniform(low=0,high=1,size=(3,5,4)))\n&gt;&gt;&gt; x = np.random.rand(8,10)\n&gt;&gt;&gt; task = np.arange(4)\n&gt;&gt;&gt; kmt(task[0],task[0],x[0],x[0]).shape\n(6, 3, 5)\n&gt;&gt;&gt; kmt(task,task,x[0],x[0]).shape\n(6, 3, 5, 4)\n&gt;&gt;&gt; kmt(task[0],task[0],x,x).shape\n(6, 3, 5, 8)\n&gt;&gt;&gt; v = kmt(task,task,x[:4,None,:],x[None,:4,:])\n&gt;&gt;&gt; v.shape\n(6, 3, 5, 4, 4)\n&gt;&gt;&gt; kmat_x = kmt.base_kernel(x[:4,None,:],x[None,:4,:])\n&gt;&gt;&gt; kmat_x.shape\n(3, 5, 4, 4)\n&gt;&gt;&gt; kmat_tasks = kmt.taskmat[...,task,task]\n&gt;&gt;&gt; kmat_tasks.shape\n(6, 3, 5, 4)\n&gt;&gt;&gt; np.allclose(v,kmat_tasks[...,None,:]*kmat_x)\nTrue\n&gt;&gt;&gt; np.allclose(v,kmat_tasks[...,:,None]*kmat_x)\nFalse\n&gt;&gt;&gt; kmt(task[:,None,None,None],task[None,:,None,None],x[:,None,:],x[None,:,:]).shape\n(6, 3, 5, 4, 4, 8, 8)\n&gt;&gt;&gt; kmt(task[:,None],task[None,:],x[:,None,None,None,:],x[None,:,None,None,:]).shape\n(6, 3, 5, 8, 8, 4, 4)\n</code></pre> <p>Integrals</p> <pre><code>&gt;&gt;&gt; kernel = KernelGaussian(\n...     d = 2,\n...     scale = rng.uniform(low=0,high=1,size=(3,1)),\n...     lengthscales = rng.uniform(low=0,high=1,size=(1,2)))\n&gt;&gt;&gt; kmt = KernelMultiTask(\n...     base_kernel=kernel,\n...     num_tasks=5,\n...     diag = rng.uniform(low=0,high=1,size=(6,3,5)),\n...     factor = rng.uniform(low=0,high=1,size=(3,5,2)))\n&gt;&gt;&gt; task = np.arange(5)\n&gt;&gt;&gt; kmt.double_integral_01d(task0=task[0],task1=task[1]).shape\n(6, 3)\n&gt;&gt;&gt; kmt.double_integral_01d(task0=task,task1=task).shape\n(6, 3, 5)\n&gt;&gt;&gt; kmt.double_integral_01d(task0=task[:,None],task1=task[None,:]).shape\n(6, 3, 5, 5)\n&gt;&gt;&gt; x = rng.uniform(low=0,high=1,size=(7,4,2))\n&gt;&gt;&gt; kmt.single_integral_01d(task0=task[0],task1=task[1],x=x).shape\n(6, 3, 7, 4)\n&gt;&gt;&gt; kmt.single_integral_01d(task0=task[:,None,None],task1=task[:,None,None],x=x).shape\n(6, 3, 5, 7, 4)\n&gt;&gt;&gt; kmt.single_integral_01d(task0=task[:,None,None,None],task1=task[None,:,None,None],x=x).shape\n(6, 3, 5, 5, 7, 4)\n</code></pre> <p>Cholesky Construction</p> <pre><code>&gt;&gt;&gt; kmt = KernelMultiTask(\n...     KernelGaussian(5),\n...     num_tasks = 3,\n...     method = \"CHOLESKY\",\n...     factor = 2,\n...     diag = 1.5,\n...     )\n&gt;&gt;&gt; kmt.taskmat\narray([[ 2.25,  3.  ,  3.  ],\n       [ 3.  ,  6.25,  7.  ],\n       [ 3.  ,  7.  , 10.25]])\n&gt;&gt;&gt; kmt = KernelMultiTask(\n...     KernelGaussian(5),\n...     num_tasks = 3,\n...     method = \"CHOLESKY\",\n...     factor = 2,\n...     diag = 1.5,\n...     shape_factor = [2,3*(1+3)//2-3],\n...     shape_diag = [3],\n...     )\n&gt;&gt;&gt; kmt.taskmat\narray([[[ 2.25,  3.  ,  3.  ],\n        [ 3.  ,  6.25,  7.  ],\n        [ 3.  ,  7.  , 10.25]],\n\n       [[ 2.25,  3.  ,  3.  ],\n        [ 3.  ,  6.25,  7.  ],\n        [ 3.  ,  7.  , 10.25]]])\n&gt;&gt;&gt; kmt = KernelMultiTask(\n...     KernelGaussian(5),\n...     num_tasks = 3,\n...     method = \"CHOLESKY\",\n...     factor = 2,\n...     diag = 1.5,\n...     shape_factor = [3*(1+3)//2-3],\n...     shape_diag = [2,3],\n...     )\n&gt;&gt;&gt; kmt.taskmat\narray([[[ 2.25,  3.  ,  3.  ],\n        [ 3.  ,  6.25,  7.  ],\n        [ 3.  ,  7.  , 10.25]],\n\n       [[ 2.25,  3.  ,  3.  ],\n        [ 3.  ,  6.25,  7.  ],\n        [ 3.  ,  7.  , 10.25]]])\n&gt;&gt;&gt; kmt = KernelMultiTask(\n...     KernelGaussian(5),\n...     num_tasks = 3,\n...     method = \"CHOLESKY\",\n...     factor = 2,\n...     diag = 1.5,\n...     shape_factor = [2,3*(1+3)//2-3],\n...     shape_diag = [2,3],\n...     )\n&gt;&gt;&gt; kmt.taskmat\narray([[[ 2.25,  3.  ,  3.  ],\n        [ 3.  ,  6.25,  7.  ],\n        [ 3.  ,  7.  , 10.25]],\n\n       [[ 2.25,  3.  ,  3.  ],\n        [ 3.  ,  6.25,  7.  ],\n        [ 3.  ,  7.  , 10.25]]])\n</code></pre> <p>PyTorch</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; kmt = KernelMultiTask(KernelGaussian(d=2,torchify=True),num_tasks=3,diag=[1,2,3])\n&gt;&gt;&gt; x = torch.from_numpy(np.random.rand(4,2))\n&gt;&gt;&gt; task = np.arange(3)\n&gt;&gt;&gt; kmt(task[1],task[1],x[0],x[0]).item()\n3.0\n&gt;&gt;&gt; kmt(task,task,x[0],x[0])\ntensor([2., 3., 4.], dtype=torch.float64, grad_fn=&lt;SelectBackward0&gt;)\n&gt;&gt;&gt; kmt(task[:,None],task[None,:],x[0],x[0]).shape\ntorch.Size([3, 3])\n&gt;&gt;&gt; kmt(task[1],task[1],x,x)\ntensor([3., 3., 3., 3.], dtype=torch.float64, grad_fn=&lt;SelectBackward0&gt;)\n&gt;&gt;&gt; kmt(task,task,x[:3],x[:3])\ntensor([2., 3., 4.], dtype=torch.float64, grad_fn=&lt;SelectBackward0&gt;)\n&gt;&gt;&gt; kmt(task[:,None],task[:,None],x,x).shape\ntorch.Size([3, 4])\n&gt;&gt;&gt; v = kmt(task,task,x[:3,None,:],x[None,:3,:])\n&gt;&gt;&gt; v.shape\ntorch.Size([3, 3])\n&gt;&gt;&gt; torch.allclose(v,kmt.base_kernel(x[:3,None,:],x[None,:3,:])*kmt.taskmat[task,task])\nTrue\n&gt;&gt;&gt; kmt(task[:,None,None],task[:,None,None],x[:,None,:],x[None,:,:]).shape\ntorch.Size([3, 4, 4])\n&gt;&gt;&gt; kmt(task[:,None,None,None],task[None,:,None,None],x[:,None,:],x[None,:,:]).shape\ntorch.Size([3, 3, 4, 4])\n</code></pre> <p>Batched inference</p> <pre><code>&gt;&gt;&gt; kernel_base = KernelGaussian(d=10,shape_lengthscales=(5,1),shape_scale=(3,5,1),torchify=True)\n&gt;&gt;&gt; rng = np.random.Generator(np.random.PCG64(7))\n&gt;&gt;&gt; kmt = KernelMultiTask(kernel_base,\n...     num_tasks=4,\n...     shape_factor=(6,3,5,4,2),\n...     diag = rng.uniform(low=0,high=1,size=(3,5,4)))\n&gt;&gt;&gt; x = torch.from_numpy(np.random.rand(8,10))\n&gt;&gt;&gt; task = torch.arange(4)\n&gt;&gt;&gt; kmt(task[0],task[0],x[0],x[0]).shape\ntorch.Size([6, 3, 5])\n&gt;&gt;&gt; kmt(task,task,x[0],x[0]).shape\ntorch.Size([6, 3, 5, 4])\n&gt;&gt;&gt; kmt(task[0],task[0],x,x).shape\ntorch.Size([6, 3, 5, 8])\n&gt;&gt;&gt; v = kmt(task,task,x[:4,None,:],x[None,:4,:])\n&gt;&gt;&gt; v.shape\ntorch.Size([6, 3, 5, 4, 4])\n&gt;&gt;&gt; kmat_x = kmt.base_kernel(x[:4,None,:],x[None,:4,:])\n&gt;&gt;&gt; kmat_x.shape\ntorch.Size([3, 5, 4, 4])\n&gt;&gt;&gt; kmat_tasks = kmt.taskmat[...,task,task]\n&gt;&gt;&gt; kmat_tasks.shape\ntorch.Size([6, 3, 5, 4])\n&gt;&gt;&gt; torch.allclose(v,kmat_tasks[...,None,:]*kmat_x)\nTrue\n&gt;&gt;&gt; torch.allclose(v,kmat_tasks[...,:,None]*kmat_x)\nFalse\n&gt;&gt;&gt; kmt(task[:,None,None,None],task[None,:,None,None],x[:,None,:],x[None,:,:]).shape\ntorch.Size([6, 3, 5, 4, 4, 8, 8])\n&gt;&gt;&gt; kmt(task[:,None],task[None,:],x[:,None,None,None,:],x[None,:,None,None,:]).shape\ntorch.Size([6, 3, 5, 8, 8, 4, 4])\n&gt;&gt;&gt; kmt.factor.dtype\ntorch.float32\n&gt;&gt;&gt; kmt.diag.dtype\ntorch.float64\n</code></pre> <p>Integrals</p> <pre><code>&gt;&gt;&gt; kernel = KernelGaussian(\n...     d = 2,\n...     torchify = True,\n...     scale = rng.uniform(low=0,high=1,size=(3,1)),\n...     lengthscales = rng.uniform(low=0,high=1,size=(1,2)))\n&gt;&gt;&gt; kmt = KernelMultiTask(\n...     base_kernel=kernel,\n...     num_tasks=5,\n...     diag = rng.uniform(low=0,high=1,size=(6,3,5)),\n...     factor = rng.uniform(low=0,high=1,size=(3,5,2)))\n&gt;&gt;&gt; task = torch.arange(5)\n&gt;&gt;&gt; kmt.double_integral_01d(task0=task[0],task1=task[1]).shape\ntorch.Size([6, 3])\n&gt;&gt;&gt; kmt.double_integral_01d(task0=task,task1=task).shape\ntorch.Size([6, 3, 5])\n&gt;&gt;&gt; kmt.double_integral_01d(task0=task[:,None],task1=task[None,:]).shape\ntorch.Size([6, 3, 5, 5])\n&gt;&gt;&gt; x = torch.from_numpy(rng.uniform(low=0,high=1,size=(7,4,2)))\n&gt;&gt;&gt; kmt.single_integral_01d(task0=task[0],task1=task[1],x=x).shape\ntorch.Size([6, 3, 7, 4])\n&gt;&gt;&gt; kmt.single_integral_01d(task0=task[:,None,None],task1=task[:,None,None],x=x).shape\ntorch.Size([6, 3, 5, 7, 4])\n&gt;&gt;&gt; kmt.single_integral_01d(task0=task[:,None,None,None],task1=task[None,:,None,None],x=x).shape\ntorch.Size([6, 3, 5, 5, 7, 4])\n</code></pre> <p>Cholesky Construction</p> <pre><code>&gt;&gt;&gt; kmt = KernelMultiTask(\n...     KernelGaussian(5,torchify=True),\n...     num_tasks = 3,\n...     method = \"CHOLESKY\",\n...     factor = 2,\n...     diag = 1.5,\n...     )\n&gt;&gt;&gt; kmt.taskmat\ntensor([[ 2.2500,  3.0000,  3.0000],\n        [ 3.0000,  6.2500,  7.0000],\n        [ 3.0000,  7.0000, 10.2500]], grad_fn=&lt;ViewBackward0&gt;)\n&gt;&gt;&gt; kmt = KernelMultiTask(\n...     KernelGaussian(5,torchify=True),\n...     num_tasks = 3,\n...     method = \"CHOLESKY\",\n...     factor = 2,\n...     diag = 1.5,\n...     shape_factor = [2,3*(1+3)//2-3],\n...     shape_diag = [3],\n...     )\n&gt;&gt;&gt; kmt.taskmat\ntensor([[[ 2.2500,  3.0000,  3.0000],\n         [ 3.0000,  6.2500,  7.0000],\n         [ 3.0000,  7.0000, 10.2500]],\n\n        [[ 2.2500,  3.0000,  3.0000],\n         [ 3.0000,  6.2500,  7.0000],\n         [ 3.0000,  7.0000, 10.2500]]], grad_fn=&lt;ViewBackward0&gt;)\n&gt;&gt;&gt; kmt = KernelMultiTask(\n...     KernelGaussian(5,torchify=True),\n...     num_tasks = 3,\n...     method = \"CHOLESKY\",\n...     factor = 2,\n...     diag = 1.5,\n...     shape_factor = [3*(1+3)//2-3],\n...     shape_diag = [2,3],\n...     )\n&gt;&gt;&gt; kmt.taskmat\ntensor([[[ 2.2500,  3.0000,  3.0000],\n         [ 3.0000,  6.2500,  7.0000],\n         [ 3.0000,  7.0000, 10.2500]],\n\n        [[ 2.2500,  3.0000,  3.0000],\n         [ 3.0000,  6.2500,  7.0000],\n         [ 3.0000,  7.0000, 10.2500]]], grad_fn=&lt;ViewBackward0&gt;)\n&gt;&gt;&gt; kmt = KernelMultiTask(\n...     KernelGaussian(5,torchify=True),\n...     num_tasks = 3,\n...     method = \"CHOLESKY\",\n...     factor = 2,\n...     diag = 1.5,\n...     shape_factor = [2,3*(1+3)//2-3],\n...     shape_diag = [2,3],\n...     )\n&gt;&gt;&gt; kmt.taskmat\ntensor([[[ 2.2500,  3.0000,  3.0000],\n         [ 3.0000,  6.2500,  7.0000],\n         [ 3.0000,  7.0000, 10.2500]],\n\n        [[ 2.2500,  3.0000,  3.0000],\n         [ 3.0000,  6.2500,  7.0000],\n         [ 3.0000,  7.0000, 10.2500]]], grad_fn=&lt;ViewBackward0&gt;)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>base_kernel</code> <code>AbstractKernel</code> <p>\\(K_{\\mathrm{base}}\\).</p> required <code>num_tasks</code> <code>int</code> <p>Number of tasks \\(T&gt;1\\).</p> required <code>factor</code> <code>Union[ndarray, Tensor]</code> <p>Factor \\(\\mathsf{F}\\).</p> <code>1.0</code> <code>diag</code> <code>Union[ndarray, Tensor]</code> <p>Diagonal parameter \\(\\boldsymbol{v}\\).</p> <code>1.0</code> <code>shape_factor</code> <code>list</code> <p>Shape of <code>factor</code> when <code>np.isscalar(factor)</code>.</p> <code>None</code> <code>shape_diag</code> <code>list</code> <p>Shape of <code>diag</code> when <code>np.isscalar(diag)</code>.</p> <code>None</code> <code>tfs_factor</code> <code>Tuple[callable, callable]</code> <p>The first argument transforms to the raw value to be optimized; the second applies the inverse transform.</p> <code>(tf_identity, tf_identity)</code> <code>tfs_diag</code> <code>Tuple[callable, callable]</code> <p>The first argument transforms to the raw value to be optimized; the second applies the inverse transform.</p> <code>(tf_exp_eps_inv, tf_exp_eps)</code> <code>requires_grad_factor</code> <code>bool</code> <p>If <code>True</code> and <code>torchify</code>, set <code>requires_grad=True</code> for <code>factor</code>.</p> <code>True</code> <code>requires_grad_diag</code> <code>bool</code> <p>If <code>True</code> and <code>torchify</code>, set <code>requires_grad=True</code> for <code>diag</code>.</p> <code>True</code> <code>method</code> <code>str</code> <p><code>\"LOW RANK\"</code> or \"CHOLESKY\"</p> <code>'LOW RANK'</code> Source code in <code>qmcpy/kernel/multitask_kernel.py</code> <pre><code>def __init__(\n    self,\n    base_kernel,\n    num_tasks,\n    factor=1.0,\n    diag=1.0,\n    shape_factor=None,\n    shape_diag=None,\n    tfs_factor=(tf_identity, tf_identity),\n    tfs_diag=(tf_exp_eps_inv, tf_exp_eps),\n    requires_grad_factor=True,\n    requires_grad_diag=True,\n    rank_factor=1,\n    method=\"LOW RANK\",\n):\n    r\"\"\"\n    Args:\n        base_kernel (AbstractKernel): $K_{\\mathrm{base}}$.\n        num_tasks (int): Number of tasks $T&gt;1$.\n        factor (Union[np.ndarray, torch.Tensor]): Factor $\\mathsf{F}$.\n        diag (Union[np.ndarray, torch.Tensor]): Diagonal parameter $\\boldsymbol{v}$.\n        shape_factor (list): Shape of `factor` when `np.isscalar(factor)`.\n        shape_diag (list): Shape of `diag` when `np.isscalar(diag)`.\n        tfs_factor (Tuple[callable,callable]): The first argument transforms to the raw value to be optimized; the second applies the inverse transform.\n        tfs_diag (Tuple[callable,callable]): The first argument transforms to the raw value to be optimized; the second applies the inverse transform.\n        requires_grad_factor (bool): If `True` and `torchify`, set `requires_grad=True` for `factor`.\n        requires_grad_diag (bool): If `True` and `torchify`, set `requires_grad=True` for `diag`.\n        method (str): `\"LOW RANK\"` or \"CHOLESKY\"\n    \"\"\"\n    assert isinstance(base_kernel, AbstractKernel)\n    super().__init__(\n        d=base_kernel.d,\n        torchify=base_kernel.torchify,\n        device=base_kernel.device,\n        compile_call=False,\n        comiple_call_kwargs={},\n    )\n    self.base_kernel = base_kernel\n    self.AUTOGRADKERNEL = base_kernel.AUTOGRADKERNEL\n    assert np.isscalar(num_tasks) and num_tasks % 1 == 0\n    self.num_tasks = num_tasks\n    assert (\n        np.isscalar(rank_factor)\n        and rank_factor % 1 == 0\n        and 0 &lt;= rank_factor &lt;= self.num_tasks\n    )\n    self.method = str(method).upper().replace(\"_\", \" \").strip()\n    if self.method == \"LOW RANK\":\n        if shape_factor is None:\n            shape_factor = [self.num_tasks, rank_factor]\n        factor_endsize_ops = list(range(self.num_tasks + 1))\n        diag_endsize_ops = [1, self.num_tasks]\n    elif self.method == \"CHOLESKY\":\n        if self.torchify:\n            self.lti0, self.lti1 = self.npt.tril_indices(num_tasks, num_tasks, -1)\n        else:\n            self.lti0, self.lti1 = self.npt.tril_indices(num_tasks, -1)\n        if shape_factor is None:\n            shape_factor = [len(self.lti0)]\n        factor_endsize_ops = [len(self.lti0)]\n        diag_endsize_ops = [self.num_tasks]\n    else:\n        raise Exception(\n            \"invalid method = %s, must be in ['LOW RANK','CHOLESKY']\" % self.method\n        )\n    self.raw_factor = self.parse_assign_param(\n        pname=\"factor\",\n        param=factor,\n        shape_param=shape_factor,\n        requires_grad_param=requires_grad_factor,\n        tfs_param=tfs_factor,\n        endsize_ops=factor_endsize_ops,\n        constraints=[],\n    )\n    self.tfs_factor = tfs_factor\n    if self.method == \"LOW RANK\":\n        assert self.raw_factor.shape[-2] == self.num_tasks\n    self.raw_diag = self.parse_assign_param(\n        pname=\"diag\",\n        param=diag,\n        shape_param=[self.num_tasks] if shape_diag is None else shape_diag,\n        requires_grad_param=requires_grad_diag,\n        tfs_param=tfs_diag,\n        endsize_ops=diag_endsize_ops,\n        constraints=[\"NON-NEGATIVE\"],\n    )\n    self.tfs_diag = tfs_diag\n    self.eye_num_tasks = self.npt.eye(self.num_tasks, **self.nptkwargs)\n    self.batch_param_names.append(\"taskmat\")\n    self._nbdim_base = None\n</code></pre>"},{"location":"api/kernels/#qmcpy.kernel.multitask_kernel.KernelMultiTask.__call__","title":"__call__","text":"<pre><code>__call__(task0, task1, x0, x1, beta0=None, beta1=None, c=None)\n</code></pre> <p>Evaluate the kernel with (optional) partial derivatives</p> \\[\\sum_{\\ell=1}^p c_\\ell \\partial_{\\boldsymbol{x}_0}^{\\boldsymbol{\\beta}_{\\ell,0}} \\partial_{\\boldsymbol{x}_1}^{\\boldsymbol{\\beta}_{\\ell,1}} K((i_0,\\boldsymbol{x}_0),(i_1,\\boldsymbol{x}_1)).\\] <p>Parameters:</p> Name Type Description Default <code>task0</code> <code>Union[int, ndarray, Tensor]</code> <p>First task indices \\(i_0\\).</p> required <code>task1</code> <code>Union[int, ndarray, Tensor]</code> <p>Second task indices \\(i_1\\).</p> required <code>x0</code> <code>Union[ndarray, Tensor]</code> <p>Shape <code>x0.shape=(...,d)</code> first input to kernel.</p> required <code>x1</code> <code>Union[ndarray, Tensor]</code> <p>Shape <code>x1.shape=(...,d)</code> second input to kernel.</p> required <code>beta0</code> <code>Union[ndarray, Tensor]</code> <p>Shape <code>beta0.shape=(p,d)</code> derivative orders with respect to first inputs, \\(\\boldsymbol{\\beta}_0\\).</p> <code>None</code> <code>beta1</code> <code>Union[ndarray, Tensor]</code> <p>Shape <code>beta1.shape=(p,d)</code> derivative orders with respect to first inputs, \\(\\boldsymbol{\\beta}_1\\).</p> <code>None</code> <code>c</code> <code>Union[ndarray, Tensor]</code> <p>Shape <code>c.shape=(p,)</code> coefficients of derivatives.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>k</code> <code>Union[ndarray, Tensor]</code> <p>Kernel evaluations with batched shape, see the doctests for examples.</p> Source code in <code>qmcpy/kernel/multitask_kernel.py</code> <pre><code>def __call__(self, task0, task1, x0, x1, beta0=None, beta1=None, c=None):\n    r\"\"\"\n    Evaluate the kernel with (optional) partial derivatives\n\n    $$\\sum_{\\ell=1}^p c_\\ell \\partial_{\\boldsymbol{x}_0}^{\\boldsymbol{\\beta}_{\\ell,0}} \\partial_{\\boldsymbol{x}_1}^{\\boldsymbol{\\beta}_{\\ell,1}} K((i_0,\\boldsymbol{x}_0),(i_1,\\boldsymbol{x}_1)).$$\n\n    Args:\n        task0 (Union[int, np.ndarray, torch.Tensor]): First task indices $i_0$.\n        task1 (Union[int, np.ndarray, torch.Tensor]): Second task indices $i_1$.\n        x0 (Union[np.ndarray, torch.Tensor]): Shape `x0.shape=(...,d)` first input to kernel.\n        x1 (Union[np.ndarray, torch.Tensor]): Shape `x1.shape=(...,d)` second input to kernel.\n        beta0 (Union[np.ndarray, torch.Tensor]): Shape `beta0.shape=(p,d)` derivative orders with respect to first inputs, $\\boldsymbol{\\beta}_0$.\n        beta1 (Union[np.ndarray, torch.Tensor]): Shape `beta1.shape=(p,d)` derivative orders with respect to first inputs, $\\boldsymbol{\\beta}_1$.\n        c (Union[np.ndarray, torch.Tensor]): Shape `c.shape=(p,)` coefficients of derivatives.\n\n    Returns:\n        k (Union[np.ndarray, torch.Tensor]): Kernel evaluations with batched shape, see the doctests for examples.\n    \"\"\"\n    kmat_x = self.base_kernel.__call__(x0, x1, beta0, beta1, c)\n    return self._parsed__call__(task0, task1, kmat_x)\n</code></pre>"},{"location":"api/kernels/#qmcpy.kernel.multitask_kernel.KernelMultiTask.single_integral_01d","title":"single_integral_01d","text":"<pre><code>single_integral_01d(task0, task1, x)\n</code></pre> <p>Evaluate the integral of the kernel over the unit cube</p> \\[\\tilde{K}((i_0,\\boldsymbol{x}),i_1) = \\int_{[0,1]^d} K((i_0,\\boldsymbol{x}),(i_1,\\boldsymbol{z}) \\; \\mathrm{d} \\boldsymbol{z}.\\] <p>Parameters:</p> Name Type Description Default <code>task0</code> <code>Union[int, ndarray, Tensor]</code> <p>First task indices \\(i_0\\).</p> required <code>task1</code> <code>Union[int, ndarray, Tensor]</code> <p>Second task indices \\(i_1\\).</p> required <code>x</code> <code>Union[ndarray, Tensor]</code> <p>Shape <code>x0.shape=(...,d)</code> first input to kernel with</p> required <p>Returns:</p> Name Type Description <code>tildek</code> <code>Union[ndarray, Tensor]</code> <p>Shape <code>y.shape=x.shape[:-1]</code> integral kernel evaluations.</p> Source code in <code>qmcpy/kernel/multitask_kernel.py</code> <pre><code>def single_integral_01d(self, task0, task1, x):\n    r\"\"\"\n    Evaluate the integral of the kernel over the unit cube\n\n    $$\\tilde{K}((i_0,\\boldsymbol{x}),i_1) = \\int_{[0,1]^d} K((i_0,\\boldsymbol{x}),(i_1,\\boldsymbol{z}) \\; \\mathrm{d} \\boldsymbol{z}.$$\n\n    Args:\n        task0 (Union[int, np.ndarray, torch.Tensor]): First task indices $i_0$.\n        task1 (Union[int, np.ndarray, torch.Tensor]): Second task indices $i_1$.\n        x (Union[np.ndarray, torch.Tensor]): Shape `x0.shape=(...,d)` first input to kernel with\n\n    Returns:\n        tildek (Union[np.ndarray, torch.Tensor]): Shape `y.shape=x.shape[:-1]` integral kernel evaluations.\n    \"\"\"\n    kint_x = self.base_kernel.single_integral_01d(x)\n    return self._parsed__call__(task0, task1, kint_x)\n</code></pre>"},{"location":"api/kernels/#qmcpy.kernel.multitask_kernel.KernelMultiTask.double_integral_01d","title":"double_integral_01d","text":"<pre><code>double_integral_01d(task0, task1)\n</code></pre> <p>Evaluate the integral of the kernel over the unit cube</p> \\[\\tilde{K}(i_0,i_1) = \\int_{[0,1]^d} \\int_{[0,1]^d} K((i_0,\\boldsymbol{x}),(i_1,\\boldsymbol{z})) \\; \\mathrm{d} \\boldsymbol{x} \\; \\mathrm{d} \\boldsymbol{z}.\\] <p>Parameters:</p> Name Type Description Default <code>task0</code> <code>Union[int, ndarray, Tensor]</code> <p>First task indices \\(i_0\\).</p> required <code>task1</code> <code>Union[int, ndarray, Tensor]</code> <p>Second task indices \\(i_1\\).</p> required <p>Returns:</p> Name Type Description <code>tildek</code> <code>Union[ndarray, Tensor]</code> <p>Double integral kernel evaluations.</p> Source code in <code>qmcpy/kernel/multitask_kernel.py</code> <pre><code>def double_integral_01d(self, task0, task1):\n    r\"\"\"\n    Evaluate the integral of the kernel over the unit cube\n\n    $$\\tilde{K}(i_0,i_1) = \\int_{[0,1]^d} \\int_{[0,1]^d} K((i_0,\\boldsymbol{x}),(i_1,\\boldsymbol{z})) \\; \\mathrm{d} \\boldsymbol{x} \\; \\mathrm{d} \\boldsymbol{z}.$$\n\n    Args:\n        task0 (Union[int, np.ndarray, torch.Tensor]): First task indices $i_0$.\n        task1 (Union[int, np.ndarray, torch.Tensor]): Second task indices $i_1$.\n\n    Returns:\n        tildek (Union[np.ndarray, torch.Tensor]): Double integral kernel evaluations.\n    \"\"\"\n    kint_x = self.base_kernel.double_integral_01d()\n    return self._parsed__call__(task0, task1, kint_x)\n</code></pre>"},{"location":"api/kernels/#uml-specific","title":"UML Specific","text":""},{"location":"api/stopping_criteria/","title":"Stopping Criteria","text":""},{"location":"api/stopping_criteria/#stopping-criteria","title":"Stopping Criteria","text":""},{"location":"api/stopping_criteria/#uml-overview","title":"UML Overview","text":""},{"location":"api/stopping_criteria/#abstractstoppingcriterion","title":"<code>AbstractStoppingCriterion</code>","text":"<p>               Bases: <code>object</code></p> <p>Parameters:</p> Name Type Description Default <code>allowed_distribs</code> <code>list</code> <p>Allowed discrete distribution classes.</p> required <code>allow_vectorized_integrals</code> <code>bool</code> <p>Whether or not to allow integrands with vectorized outputs, i.e., those with <code>integrand.d_indv!=()</code>.</p> required Source code in <code>qmcpy/stopping_criterion/abstract_stopping_criterion.py</code> <pre><code>def __init__(self, allowed_distribs, allow_vectorized_integrals):\n    \"\"\"\n    Args:\n        allowed_distribs (list): Allowed discrete distribution classes.\n        allow_vectorized_integrals (bool): Whether or not to allow integrands with vectorized outputs,\n            i.e., those with `integrand.d_indv!=()`.\n    \"\"\"\n    sname = type(self).__name__\n    prefix = \"A concrete implementation of StoppingCriterion must have \"\n    # integrand check\n    if (not hasattr(self, \"integrand\")) or (\n        not isinstance(self.integrand, AbstractIntegrand)\n    ):\n        raise ParameterError(prefix + \"self.integrand, an Integrand instance\")\n    # true measure check\n    if (not hasattr(self, \"true_measure\")) or (\n        self.true_measure != self.integrand.true_measure\n    ):\n        raise ParameterError(\n            prefix + \"self.true_measure=self.integrand.true_measure\"\n        )\n    # discrete distribution check\n    if (not hasattr(self, \"discrete_distrib\")) or (\n        self.discrete_distrib != self.integrand.discrete_distrib\n    ):\n        raise ParameterError(\n            prefix + \"self.discrete_distrib=self.integrand.discrete_distrib\"\n        )\n    if not isinstance(self.discrete_distrib, tuple(allowed_distribs)):\n        raise DistributionCompatibilityError(\n            \"%s must have a DiscreteDistribution in %s\"\n            % (sname, str(allowed_distribs))\n        )\n    if (not allow_vectorized_integrals) and len(self.integrand.d_indv) &gt; 0:\n        raise ParameterError(\n            \"Vectorized integrals (with d_indv!=() outputs per sample) are not supported by this stopping criterion\"\n        )\n    # parameter checks\n    if not hasattr(self, \"parameters\"):\n        self.parameters = []\n</code></pre>"},{"location":"api/stopping_criteria/#qmcpy.stopping_criterion.abstract_stopping_criterion.AbstractStoppingCriterion.integrate","title":"integrate","text":"<pre><code>integrate()\n</code></pre> <p>Abstract method to determine the number of samples needed to satisfy the tolerance(s).</p> <p>Returns:</p> Name Type Description <code>solution</code> <code>Union[float, ndarray]</code> <p>Approximation to the integral with shape <code>integrand.d_comb</code>.</p> <code>data</code> <code>Data</code> <p>A data object.</p> Source code in <code>qmcpy/stopping_criterion/abstract_stopping_criterion.py</code> <pre><code>def integrate(self):\n    \"\"\"\n    *Abstract method* to determine the number of samples needed to satisfy the tolerance(s).\n\n    Returns:\n        solution (Union[float, np.ndarray]): Approximation to the integral with shape `integrand.d_comb`.\n        data (Data): A data object.\n    \"\"\"\n    raise MethodImplementationError(self, \"integrate\")\n</code></pre>"},{"location":"api/stopping_criteria/#qmcpy.stopping_criterion.abstract_stopping_criterion.AbstractStoppingCriterion.set_tolerance","title":"set_tolerance","text":"<pre><code>set_tolerance(abs_tol=None, rel_tol=None, rmse_tol=None)\n</code></pre> <p>Reset the tolerances.</p> <p>Parameters:</p> Name Type Description Default <code>abs_tol</code> <code>float</code> <p>Absolute tolerance (if applicable). Reset if supplied, ignored otherwise.</p> <code>None</code> <code>rel_tol</code> <code>float</code> <p>Relative tolerance (if applicable). Reset if supplied, ignored otherwise.</p> <code>None</code> <code>rmse_tol</code> <code>float</code> <p>RMSE tolerance (if applicable). Reset if supplied, ignored if not. If <code>rmse_tol</code> is not supplied but <code>abs_tol</code> is, then <code>rmse_tol = abs_tol / norm.ppf(1-alpha/2)</code>.</p> <code>None</code> Source code in <code>qmcpy/stopping_criterion/abstract_stopping_criterion.py</code> <pre><code>def set_tolerance(self, abs_tol=None, rel_tol=None, rmse_tol=None):\n    \"\"\"\n    Reset the tolerances.\n\n    Args:\n        abs_tol (float): Absolute tolerance (if applicable). Reset if supplied, ignored otherwise.\n        rel_tol (float): Relative tolerance (if applicable). Reset if supplied, ignored otherwise.\n        rmse_tol (float): RMSE tolerance (if applicable). Reset if supplied, ignored if not.\n            If `rmse_tol` is not supplied but `abs_tol` is, then `rmse_tol = abs_tol / norm.ppf(1-alpha/2)`.\n    \"\"\"\n    raise MethodImplementationError(self, \"integrate\")\n</code></pre>"},{"location":"api/stopping_criteria/#qmc","title":"QMC","text":""},{"location":"api/stopping_criteria/#cubqmcnetg","title":"<code>CubQMCNetG</code>","text":"<p>               Bases: <code>AbstractCubQMCLDG</code></p> <p>Quasi-Monte Carlo stopping criterion using digital net cubature with guarantees for cones of functions with a predictable decay in the Walsh coefficients.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; k = Keister(DigitalNetB2(seed=7))\n&gt;&gt;&gt; sc = CubQMCNetG(k,abs_tol=1e-3,rel_tol=0,check_cone=True)\n&gt;&gt;&gt; solution,data = sc.integrate()\n&gt;&gt;&gt; solution\narray(1.38046669)\n&gt;&gt;&gt; data\nData (Data)\n    solution        1.380\n    comb_bound_low  1.380\n    comb_bound_high 1.381\n    comb_bound_diff 6.72e-04\n    comb_flags      1\n    n_total         2^(11)\n    n               2^(11)\n    time_integrate  ...\nCubQMCNetG (AbstractStoppingCriterion)\n    abs_tol         0.001\n    rel_tol         0\n    n_init          2^(10)\n    n_limit         2^(35)\nKeister (AbstractIntegrand)\nGaussian (AbstractTrueMeasure)\n    mean            0\n    covariance      2^(-1)\n    decomp_type     PCA\nDigitalNetB2 (AbstractLDDiscreteDistribution)\n    d               1\n    replications    1\n    randomize       LMS DS\n    gen_mats_source joe_kuo.6.21201.txt\n    order           RADICAL INVERSE\n    t               63\n    alpha           1\n    n_limit         2^(32)\n    entropy         7\n</code></pre> <p>Vector outputs</p> <pre><code>&gt;&gt;&gt; f = BoxIntegral(DigitalNetB2(3,seed=7),s=[-1,1])\n&gt;&gt;&gt; abs_tol = 1e-3\n&gt;&gt;&gt; sc = CubQMCNetG(f,abs_tol=abs_tol,rel_tol=0,check_cone=True)\n&gt;&gt;&gt; solution,data = sc.integrate()\n&gt;&gt;&gt; solution\narray([1.19003352, 0.96068403])\n&gt;&gt;&gt; data\nData (Data)\n    solution        [1.19  0.961]\n    comb_bound_low  [1.189 0.96 ]\n    comb_bound_high [1.191 0.962]\n    comb_bound_diff [0.001 0.002]\n    comb_flags      [ True  True]\n    n_total         2^(14)\n    n               [16384  1024]\n    time_integrate  ...\nCubQMCNetG (AbstractStoppingCriterion)\n    abs_tol         0.001\n    rel_tol         0\n    n_init          2^(10)\n    n_limit         2^(35)\nBoxIntegral (AbstractIntegrand)\n    s               [-1  1]\nUniform (AbstractTrueMeasure)\n    lower_bound     0\n    upper_bound     1\nDigitalNetB2 (AbstractLDDiscreteDistribution)\n    d               3\n    replications    1\n    randomize       LMS DS\n    gen_mats_source joe_kuo.6.21201.txt\n    order           RADICAL INVERSE\n    t               63\n    alpha           1\n    n_limit         2^(32)\n    entropy         7\n&gt;&gt;&gt; sol3neg1 = -np.pi/4-1/2*np.log(2)+np.log(5+3*np.sqrt(3))\n&gt;&gt;&gt; sol31 = np.sqrt(3)/4+1/2*np.log(2+np.sqrt(3))-np.pi/24\n&gt;&gt;&gt; true_value = np.array([sol3neg1,sol31])\n&gt;&gt;&gt; assert (abs(true_value-solution)&lt;abs_tol).all()\n</code></pre> <p>Sensitivity indices</p> <pre><code>&gt;&gt;&gt; function = Genz(DigitalNetB2(3,seed=7))\n&gt;&gt;&gt; integrand = SensitivityIndices(function)\n&gt;&gt;&gt; sc = CubQMCNetG(integrand,abs_tol=5e-4,rel_tol=0,check_cone=True)\n&gt;&gt;&gt; solution,data = sc.integrate()\n&gt;&gt;&gt; data\nData (Data)\n    solution        [[0.02  0.196 0.667]\n                     [0.036 0.303 0.782]]\n    comb_bound_low  [[0.019 0.195 0.667]\n                     [0.035 0.303 0.781]]\n    comb_bound_high [[0.02  0.196 0.667]\n                     [0.036 0.303 0.782]]\n    comb_bound_diff [[0.001 0.    0.001]\n                     [0.001 0.001 0.001]]\n    comb_flags      [[ True  True  True]\n                     [ True  True  True]]\n    n_total         2^(16)\n    n               [[[16384 65536 65536]\n                      [16384 65536 65536]\n                      [16384 65536 65536]]\n\n                     [[ 2048 16384 32768]\n                      [ 2048 16384 32768]\n                      [ 2048 16384 32768]]]\n    time_integrate  ...\nCubQMCNetG (AbstractStoppingCriterion)\n    abs_tol         5.00e-04\n    rel_tol         0\n    n_init          2^(10)\n    n_limit         2^(35)\nSensitivityIndices (AbstractIntegrand)\n    indices         [[ True False False]\n                     [False  True False]\n                     [False False  True]]\nUniform (AbstractTrueMeasure)\n    lower_bound     0\n    upper_bound     1\nDigitalNetB2 (AbstractLDDiscreteDistribution)\n    d               3\n    replications    1\n    randomize       LMS DS\n    gen_mats_source joe_kuo.6.21201.txt\n    order           RADICAL INVERSE\n    t               63\n    alpha           1\n    n_limit         2^(32)\n    entropy         7\n</code></pre> <p>Control Variates</p> <pre><code>&gt;&gt;&gt; dnb2 = DigitalNetB2(dimension=4,seed=7)\n&gt;&gt;&gt; integrand = CustomFun(\n...     true_measure = Uniform(dnb2),\n...     g = lambda t: np.stack([\n...         1*t[...,0]+2*t[...,0]**2+3*t[...,0]**3,\n...         2*t[...,1]+3*t[...,1]**2+4*t[...,1]**3,\n...         3*t[...,2]+4*t[...,2]**2+5*t[...,2]**3]),\n...     dimension_indv = (3,))\n&gt;&gt;&gt; control_variates = [\n...     CustomFun(\n...         true_measure = Uniform(dnb2),\n...         g = lambda t: np.stack([t[...,0],t[...,1],t[...,2]],axis=0),\n...         dimension_indv = (3,)),\n...     CustomFun(\n...         true_measure = Uniform(dnb2),\n...         g = lambda t: np.stack([t[...,0]**2,t[...,1]**2,t[...,2]**2],axis=0),\n...         dimension_indv = (3,))]\n&gt;&gt;&gt; control_variate_means = np.array([[1/2,1/2,1/2],[1/3,1/3,1/3]])\n&gt;&gt;&gt; true_value = np.array([23/12,3,49/12])\n&gt;&gt;&gt; abs_tol = 1e-6\n&gt;&gt;&gt; sc = CubQMCNetG(integrand,abs_tol=abs_tol,rel_tol=0,control_variates=control_variates,control_variate_means=control_variate_means,update_cv_coeffs=False)\n&gt;&gt;&gt; solution,data = sc.integrate()\n&gt;&gt;&gt; solution\narray([1.91666667, 3.        , 4.08333333])\n&gt;&gt;&gt; data.n\narray([ 8192,  8192, 16384])\n&gt;&gt;&gt; assert (np.abs(true_value-solution)&lt;abs_tol).all()\n&gt;&gt;&gt; sc = CubQMCNetG(integrand,abs_tol=abs_tol,rel_tol=0,control_variates=control_variates,control_variate_means=control_variate_means,update_cv_coeffs=True)\n&gt;&gt;&gt; solution,data = sc.integrate()\n&gt;&gt;&gt; solution\narray([1.91666667, 3.        , 4.08333333])\n&gt;&gt;&gt; data.n\narray([16384, 16384, 16384])\n&gt;&gt;&gt; assert (np.abs(true_value-solution)&lt;abs_tol).all()\n</code></pre> <p>References:</p> <ol> <li> <p>Hickernell, Fred J., and Llu\u00eds Antoni Jim\u00e9nez Rugama.     \"Reliable adaptive cubature using digital sequences.\"     Monte Carlo and Quasi-Monte Carlo Methods: MCQMC, Leuven, Belgium, April 2014.     Springer International Publishing, 2016.</p> </li> <li> <p>Sou-Cheng T. Choi, Yuhan Ding, Fred J. Hickernell, Lan Jiang, Lluis Antoni Jimenez Rugama,     Da Li, Jagadeeswaran Rathinavel, Xin Tong, Kan Zhang, Yizhi Zhang, and Xuan Zhou,     GAIL: Guaranteed Automatic Integration Library (Version 2.3) [MATLAB Software], 2019.     http://gailgithub.github.io/GAIL_Dev/.     https://github.com/GailGithub/GAIL_Dev/blob/master/Algorithms/IntegrationExpectation/cubSobol_g.m.</p> </li> </ol> <p>Parameters:</p> Name Type Description Default <code>integrand</code> <code>AbstractIntegrand</code> <p>The integrand.</p> required <code>abs_tol</code> <code>ndarray</code> <p>Absolute error tolerance.</p> <code>0.01</code> <code>rel_tol</code> <code>ndarray</code> <p>Relative error tolerance.</p> <code>0.0</code> <code>n_init</code> <code>int</code> <p>Initial number of samples.</p> <code>2 ** 10</code> <code>n_limit</code> <code>int</code> <p>Maximum number of samples.</p> <code>2 ** 35</code> <code>error_fun</code> <code>Union[str, callable]</code> <p>Function mapping the approximate solution, absolute error tolerance, and relative error tolerance to the current error bound.</p> <ul> <li><code>'EITHER'</code>, the default, requires the approximation error must be below either the absolue or relative tolerance.     Equivalent to setting     <pre><code>error_fun = lambda sv,abs_tol,rel_tol: np.maximum(abs_tol,abs(sv)*rel_tol)\n</code></pre></li> <li><code>'BOTH'</code> requires the approximation error to be below both the absolue and relative tolerance.     Equivalent to setting     <pre><code>error_fun = lambda sv,abs_tol,rel_tol: np.minimum(abs_tol,abs(sv)*rel_tol)\n</code></pre></li> </ul> <code>'EITHER'</code> <code>fudge</code> <code>function</code> <p>Positive function multiplying the finite sum of the Fourier coefficients specified in the cone of functions.</p> <code>lambda m: 5.0 * 2.0 ** -m</code> <code>check_cone</code> <code>bool</code> <p>Whether or not to check if the function falls in the cone.</p> <code>False</code> <code>control_variates</code> <code>list</code> <p>Integrands to use as control variates, each with the same underlying discrete distribution instance.</p> <code>[]</code> <code>control_variate_means</code> <code>ndarray</code> <p>Means of each control variate.</p> <code>[]</code> <code>update_cv_coeffs</code> <code>bool</code> <p>If set to true, the control variate coefficients are recomputed at each iteration. Otherwise they are estimated once after the initial sampling and then fixed.</p> <code>False</code> Source code in <code>qmcpy/stopping_criterion/cub_qmc_net_g.py</code> <pre><code>def __init__(\n    self,\n    integrand,\n    abs_tol=1e-2,\n    rel_tol=0.0,\n    n_init=2**10,\n    n_limit=2**35,\n    error_fun=\"EITHER\",\n    fudge=lambda m: 5.0 * 2.0 ** (-m),\n    check_cone=False,\n    control_variates=[],\n    control_variate_means=[],\n    update_cv_coeffs=False,\n):\n    r\"\"\"\n    Args:\n        integrand (AbstractIntegrand): The integrand.\n        abs_tol (np.ndarray): Absolute error tolerance.\n        rel_tol (np.ndarray): Relative error tolerance.\n        n_init (int): Initial number of samples.\n        n_limit (int): Maximum number of samples.\n        error_fun (Union[str, callable]): Function mapping the approximate solution, absolute error tolerance, and relative error tolerance to the current error bound.\n\n            - `'EITHER'`, the default, requires the approximation error must be below either the absolue *or* relative tolerance.\n                Equivalent to setting\n                ```python\n                error_fun = lambda sv,abs_tol,rel_tol: np.maximum(abs_tol,abs(sv)*rel_tol)\n                ```\n            - `'BOTH'` requires the approximation error to be below both the absolue *and* relative tolerance.\n                Equivalent to setting\n                ```python\n                error_fun = lambda sv,abs_tol,rel_tol: np.minimum(abs_tol,abs(sv)*rel_tol)\n                ```\n        fudge (function): Positive function multiplying the finite sum of the Fourier coefficients specified in the cone of functions.\n        check_cone (bool): Whether or not to check if the function falls in the cone.\n        control_variates (list): Integrands to use as control variates, each with the same underlying discrete distribution instance.\n        control_variate_means (np.ndarray): Means of each control variate.\n        update_cv_coeffs (bool): If set to true, the control variate coefficients are recomputed at each iteration.\n            Otherwise they are estimated once after the initial sampling and then fixed.\n    \"\"\"\n    super(CubQMCNetG, self).__init__(\n        integrand,\n        abs_tol,\n        rel_tol,\n        n_init,\n        n_limit,\n        fudge,\n        check_cone,\n        control_variates,\n        control_variate_means,\n        update_cv_coeffs,\n        ptransform=\"none\",\n        ft=fwht,\n        omega=omega_fwht,\n        allowed_distribs=[DigitalNetB2],\n        cast_complex=False,\n        error_fun=error_fun,\n    )\n    if self.discrete_distrib.order != \"RADICAL INVERSE\":\n        raise ParameterError(\n            \"CubQMCNet_g requires DigitalNetB2 with 'RADICAL INVERSE' order\"\n        )\n</code></pre>"},{"location":"api/stopping_criteria/#cubqmclatticeg","title":"<code>CubQMCLatticeG</code>","text":"<p>               Bases: <code>AbstractCubQMCLDG</code></p> <p>Quasi-Monte Carlo stopping criterion using rank-1 lattice cubature with guarantees for cones of functions with a predictable decay in the Fourier coefficients.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; k = Keister(Lattice(seed=7))\n&gt;&gt;&gt; sc = CubQMCLatticeG(k,abs_tol=1e-3,rel_tol=0,check_cone=True)\n&gt;&gt;&gt; solution,data = sc.integrate()\n&gt;&gt;&gt; solution\narray(1.38037385)\n&gt;&gt;&gt; data\nData (Data)\n    solution        1.380\n    comb_bound_low  1.380\n    comb_bound_high 1.381\n    comb_bound_diff 0.001\n    comb_flags      1\n    n_total         2^(11)\n    n               2^(11)\n    time_integrate  ...\nCubQMCLatticeG (AbstractStoppingCriterion)\n    abs_tol         0.001\n    rel_tol         0\n    n_init          2^(10)\n    n_limit         2^(30)\nKeister (AbstractIntegrand)\nGaussian (AbstractTrueMeasure)\n    mean            0\n    covariance      2^(-1)\n    decomp_type     PCA\nLattice (AbstractLDDiscreteDistribution)\n    d               1\n    replications    1\n    randomize       SHIFT\n    gen_vec_source  kuo.lattice-33002-1024-1048576.9125.txt\n    order           RADICAL INVERSE\n    n_limit         2^(20)\n    entropy         7\n</code></pre> <p>Vector outputs</p> <pre><code>&gt;&gt;&gt; f = BoxIntegral(Lattice(3,seed=11),s=[-1,1])\n&gt;&gt;&gt; abs_tol = 1e-3\n&gt;&gt;&gt; sc = CubQMCLatticeG(f,abs_tol=abs_tol,rel_tol=0,check_cone=True)\n&gt;&gt;&gt; solution,data = sc.integrate()\n&gt;&gt;&gt; solution\narray([1.18947477, 0.96060862])\n&gt;&gt;&gt; data\nData (Data)\n    solution        [1.189 0.961]\n    comb_bound_low  [1.189 0.96 ]\n    comb_bound_high [1.19  0.961]\n    comb_bound_diff [0.001 0.001]\n    comb_flags      [ True  True]\n    n_total         2^(13)\n    n               [8192 1024]\n    time_integrate  ...\nCubQMCLatticeG (AbstractStoppingCriterion)\n    abs_tol         0.001\n    rel_tol         0\n    n_init          2^(10)\n    n_limit         2^(30)\nBoxIntegral (AbstractIntegrand)\n    s               [-1  1]\nUniform (AbstractTrueMeasure)\n    lower_bound     0\n    upper_bound     1\nLattice (AbstractLDDiscreteDistribution)\n    d               3\n    replications    1\n    randomize       SHIFT\n    gen_vec_source  kuo.lattice-33002-1024-1048576.9125.txt\n    order           RADICAL INVERSE\n    n_limit         2^(20)\n    entropy         11\n&gt;&gt;&gt; sol3neg1 = -np.pi/4-1/2*np.log(2)+np.log(5+3*np.sqrt(3))\n&gt;&gt;&gt; sol31 = np.sqrt(3)/4+1/2*np.log(2+np.sqrt(3))-np.pi/24\n&gt;&gt;&gt; true_value = np.array([sol3neg1,sol31])\n&gt;&gt;&gt; assert (abs(true_value-solution)&lt;abs_tol).all()\n</code></pre> <p>Sensitivity indices</p> <pre><code>&gt;&gt;&gt; function = Genz(Lattice(3,seed=7))\n&gt;&gt;&gt; integrand = SensitivityIndices(function)\n&gt;&gt;&gt; sc = CubQMCLatticeG(integrand,abs_tol=5e-4,rel_tol=0,check_cone=True)\n&gt;&gt;&gt; solution,data = sc.integrate()\n&gt;&gt;&gt; data\nData (Data)\n    solution        [[0.021 0.196 0.667]\n                     [0.036 0.303 0.782]]\n    comb_bound_low  [[0.02  0.196 0.667]\n                     [0.035 0.303 0.781]]\n    comb_bound_high [[0.021 0.196 0.667]\n                     [0.036 0.303 0.782]]\n    comb_bound_diff [[0.001 0.001 0.   ]\n                     [0.001 0.001 0.001]]\n    comb_flags      [[ True  True  True]\n                     [ True  True  True]]\n    n_total         2^(16)\n    n               [[[16384 32768 65536]\n                      [16384 32768 65536]\n                      [16384 32768 65536]]\n\n                     [[ 2048 16384 32768]\n                      [ 2048 16384 32768]\n                      [ 2048 16384 32768]]]\n    time_integrate  ...\nCubQMCLatticeG (AbstractStoppingCriterion)\n    abs_tol         5.00e-04\n    rel_tol         0\n    n_init          2^(10)\n    n_limit         2^(30)\nSensitivityIndices (AbstractIntegrand)\n    indices         [[ True False False]\n                     [False  True False]\n                     [False False  True]]\nUniform (AbstractTrueMeasure)\n    lower_bound     0\n    upper_bound     1\nLattice (AbstractLDDiscreteDistribution)\n    d               3\n    replications    1\n    randomize       SHIFT\n    gen_vec_source  kuo.lattice-33002-1024-1048576.9125.txt\n    order           RADICAL INVERSE\n    n_limit         2^(20)\n    entropy         7\n</code></pre> <p>References:</p> <ol> <li> <p>Lluis Antoni Jimenez Rugama and Fred J. Hickernell.     \"Adaptive multidimensional integration based on rank-1 lattices,\"     Monte Carlo and Quasi-Monte Carlo Methods: MCQMC, Leuven, Belgium,     April 2014 (R. Cools and D. Nuyens, eds.), Springer Proceedings in Mathematics.     and Statistics, vol. 163, Springer-Verlag, Berlin, 2016, arXiv:1411.1966, pp. 407-422.</p> </li> <li> <p>Sou-Cheng T. Choi, Yuhan Ding, Fred J. Hickernell, Lan Jiang, Lluis Antoni Jimenez Rugama,     Da Li, Jagadeeswaran Rathinavel, Xin Tong, Kan Zhang, Yizhi Zhang, and Xuan Zhou,     GAIL: Guaranteed Automatic Integration Library (Version 2.3) [MATLAB Software], 2019.     http://gailgithub.github.io/GAIL_Dev/.     https://github.com/GailGithub/GAIL_Dev/blob/master/Algorithms/IntegrationExpectation/cubLattice_g.m.</p> </li> </ol> <p>Parameters:</p> Name Type Description Default <code>integrand</code> <code>AbstractIntegrand</code> <p>The integrand.</p> required <code>abs_tol</code> <code>ndarray</code> <p>Absolute error tolerance.</p> <code>0.01</code> <code>rel_tol</code> <code>ndarray</code> <p>Relative error tolerance.</p> <code>0.0</code> <code>n_init</code> <code>int</code> <p>Initial number of samples.</p> <code>2 ** 10</code> <code>n_limit</code> <code>int</code> <p>Maximum number of samples.</p> <code>2 ** 30</code> <code>error_fun</code> <code>Union[str, callable]</code> <p>Function mapping the approximate solution, absolute error tolerance, and relative error tolerance to the current error bound.</p> <ul> <li><code>'EITHER'</code>, the default, requires the approximation error must be below either the absolue or relative tolerance.     Equivalent to setting     <pre><code>error_fun = lambda sv,abs_tol,rel_tol: np.maximum(abs_tol,abs(sv)*rel_tol)\n</code></pre></li> <li><code>'BOTH'</code> requires the approximation error to be below both the absolue and relative tolerance.     Equivalent to setting     <pre><code>error_fun = lambda sv,abs_tol,rel_tol: np.minimum(abs_tol,abs(sv)*rel_tol)\n</code></pre></li> </ul> <code>'EITHER'</code> <code>fudge</code> <code>function</code> <p>Positive function multiplying the finite sum of the Fourier coefficients specified in the cone of functions.</p> <code>lambda m: 5.0 * 2.0 ** -m</code> <code>check_cone</code> <code>bool</code> <p>Whether or not to check if the function falls in the cone.</p> <code>False</code> <code>ptransform</code> <code>str</code> <p>Periodization transform, see the options in [<code>AbstractIntegrand.f</code>][qmcpy.AbstractIntegrand.f].</p> <code>'BAKER'</code> Source code in <code>qmcpy/stopping_criterion/cub_qmc_lattice_g.py</code> <pre><code>def __init__(\n    self,\n    integrand,\n    abs_tol=1e-2,\n    rel_tol=0.0,\n    n_init=2**10,\n    n_limit=2**30,\n    error_fun=\"EITHER\",\n    fudge=lambda m: 5.0 * 2.0 ** (-m),\n    check_cone=False,\n    ptransform=\"BAKER\",\n):\n    r\"\"\"\n    Args:\n        integrand (AbstractIntegrand): The integrand.\n        abs_tol (np.ndarray): Absolute error tolerance.\n        rel_tol (np.ndarray): Relative error tolerance.\n        n_init (int): Initial number of samples.\n        n_limit (int): Maximum number of samples.\n        error_fun (Union[str, callable]): Function mapping the approximate solution, absolute error tolerance, and relative error tolerance to the current error bound.\n\n            - `'EITHER'`, the default, requires the approximation error must be below either the absolue *or* relative tolerance.\n                Equivalent to setting\n                ```python\n                error_fun = lambda sv,abs_tol,rel_tol: np.maximum(abs_tol,abs(sv)*rel_tol)\n                ```\n            - `'BOTH'` requires the approximation error to be below both the absolue *and* relative tolerance.\n                Equivalent to setting\n                ```python\n                error_fun = lambda sv,abs_tol,rel_tol: np.minimum(abs_tol,abs(sv)*rel_tol)\n                ```\n        fudge (function): Positive function multiplying the finite sum of the Fourier coefficients specified in the cone of functions.\n        check_cone (bool): Whether or not to check if the function falls in the cone.\n        ptransform (str): Periodization transform, see the options in [`AbstractIntegrand.f`][qmcpy.AbstractIntegrand.f].\n    \"\"\"\n    super(CubQMCLatticeG, self).__init__(\n        integrand,\n        abs_tol,\n        rel_tol,\n        n_init,\n        n_limit,\n        fudge,\n        check_cone,\n        control_variates=[],\n        control_variate_means=[],\n        update_beta=False,\n        ptransform=ptransform,\n        ft=fftbr,\n        omega=omega_fftbr,\n        allowed_distribs=[Lattice],\n        cast_complex=True,\n        error_fun=error_fun,\n    )\n    if self.discrete_distrib.order != \"RADICAL INVERSE\":\n        raise ParameterError(\n            \"CubLattice_g requires Lattice with 'RADICAL INVERSE' order\"\n        )\n</code></pre>"},{"location":"api/stopping_criteria/#cubqmcbayesnetg","title":"<code>CubQMCBayesNetG</code>","text":"<p>               Bases: <code>AbstractCubBayesLDG</code></p> <p>Quasi-Monte Carlo stopping criterion using fast Bayesian cubature and digital nets with guarantees for Gaussian processes having certain digitally shift invariant kernels.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; k = Keister(DigitalNetB2(2, seed=123456789))\n&gt;&gt;&gt; sc = CubQMCBayesNetG(k,abs_tol=5e-3)\n&gt;&gt;&gt; solution,data = sc.integrate()\n&gt;&gt;&gt; data\nData (Data)\n    solution        1.809\n    comb_bound_low  1.807\n    comb_bound_high 1.810\n    comb_bound_diff 0.003\n    comb_flags      1\n    n_total         2^(10)\n    n               2^(10)\n    time_integrate  ...\nCubQMCBayesNetG (AbstractStoppingCriterion)\n    abs_tol         0.005\n    rel_tol         0\n    n_init          2^(8)\n    n_limit         2^(22)\n    order           1\nKeister (AbstractIntegrand)\nGaussian (AbstractTrueMeasure)\n    mean            0\n    covariance      2^(-1)\n    decomp_type     PCA\nDigitalNetB2 (AbstractLDDiscreteDistribution)\n    d               2^(1)\n    replications    1\n    randomize       LMS DS\n    gen_mats_source joe_kuo.6.21201.txt\n    order           RADICAL INVERSE\n    t               63\n    alpha           1\n    n_limit         2^(32)\n    entropy         123456789\n</code></pre> <p>Vector outputs</p> <pre><code>&gt;&gt;&gt; f = BoxIntegral(DigitalNetB2(3,seed=7),s=[-1,1])\n&gt;&gt;&gt; abs_tol = 1e-2\n&gt;&gt;&gt; sc = CubQMCBayesNetG(f,abs_tol=abs_tol,rel_tol=0)\n&gt;&gt;&gt; solution,data = sc.integrate()\n&gt;&gt;&gt; solution\narray([1.18750491, 0.96076395])\n&gt;&gt;&gt; data\nData (Data)\n    solution        [1.188 0.961]\n    comb_bound_low  [1.18 0.96]\n    comb_bound_high [1.195 0.962]\n    comb_bound_diff [0.014 0.002]\n    comb_flags      [ True  True]\n    n_total         2^(11)\n    n               [2048  256]\n    time_integrate  ...\nCubQMCBayesNetG (AbstractStoppingCriterion)\n    abs_tol         0.010\n    rel_tol         0\n    n_init          2^(8)\n    n_limit         2^(22)\n    order           1\nBoxIntegral (AbstractIntegrand)\n    s               [-1  1]\nUniform (AbstractTrueMeasure)\n    lower_bound     0\n    upper_bound     1\nDigitalNetB2 (AbstractLDDiscreteDistribution)\n    d               3\n    replications    1\n    randomize       LMS DS\n    gen_mats_source joe_kuo.6.21201.txt\n    order           RADICAL INVERSE\n    t               63\n    alpha           1\n    n_limit         2^(32)\n    entropy         7\n&gt;&gt;&gt; sol3neg1 = -np.pi/4-1/2*np.log(2)+np.log(5+3*np.sqrt(3))\n&gt;&gt;&gt; sol31 = np.sqrt(3)/4+1/2*np.log(2+np.sqrt(3))-np.pi/24\n&gt;&gt;&gt; true_value = np.array([sol3neg1,sol31])\n&gt;&gt;&gt; assert (abs(true_value-solution)&lt;abs_tol).all()\n</code></pre> <p>Sensitivity indices</p> <pre><code>&gt;&gt;&gt; function = Genz(DigitalNetB2(3,seed=7))\n&gt;&gt;&gt; integrand = SensitivityIndices(function)\n&gt;&gt;&gt; sc = CubQMCBayesNetG(integrand,abs_tol=5e-2,rel_tol=0)\n&gt;&gt;&gt; solution,data = sc.integrate()\n&gt;&gt;&gt; data\nData (Data)\n    solution        [[0.009 0.194 0.657]\n                     [0.036 0.312 0.783]]\n    comb_bound_low  [[0.007 0.178 0.636]\n                     [0.033 0.297 0.762]]\n    comb_bound_high [[0.012 0.209 0.679]\n                     [0.038 0.327 0.804]]\n    comb_bound_diff [[0.005 0.031 0.043]\n                     [0.004 0.03  0.042]]\n    comb_flags      [[ True  True  True]\n                     [ True  True  True]]\n    n_total         2^(8)\n    n               [[[256 256 256]\n                      [256 256 256]\n                      [256 256 256]]\n\n                     [[256 256 256]\n                      [256 256 256]\n                      [256 256 256]]]\n    time_integrate  ...\nCubQMCBayesNetG (AbstractStoppingCriterion)\n    abs_tol         0.050\n    rel_tol         0\n    n_init          2^(8)\n    n_limit         2^(22)\n    order           1\nSensitivityIndices (AbstractIntegrand)\n    indices         [[ True False False]\n                     [False  True False]\n                     [False False  True]]\nUniform (AbstractTrueMeasure)\n    lower_bound     0\n    upper_bound     1\nDigitalNetB2 (AbstractLDDiscreteDistribution)\n    d               3\n    replications    1\n    randomize       LMS DS\n    gen_mats_source joe_kuo.6.21201.txt\n    order           RADICAL INVERSE\n    t               63\n    alpha           1\n    n_limit         2^(32)\n    entropy         7\n</code></pre> <p>References:</p> <ol> <li> <p>Jagadeeswaran, Rathinavel, and Fred J. Hickernell.     \"Fast automatic Bayesian cubature using Sobol\u2019sampling.\"     Advances in Modeling and Simulation: Festschrift for Pierre L'Ecuyer.     Springer International Publishing, 2022. 301-318.</p> </li> <li> <p>Jagadeeswaran Rathinavel,     Fast automatic Bayesian cubature using matching kernels and designs,     PhD thesis, Illinois Institute of Technology, 2019.</p> </li> <li> <p>Sou-Cheng T. Choi, Yuhan Ding, Fred J. Hickernell, Lan Jiang, Lluis Antoni Jimenez Rugama,     Da Li, Jagadeeswaran Rathinavel, Xin Tong, Kan Zhang, Yizhi Zhang, and Xuan Zhou,     GAIL: Guaranteed Automatic Integration Library (Version 2.3) [MATLAB Software], 2019.     http://gailgithub.github.io/GAIL_Dev/.     https://github.com/GailGithub/GAIL_Dev/blob/master/Algorithms/IntegrationExpectation/cubBayesNet_g.m.</p> </li> </ol> <p>Parameters:</p> Name Type Description Default <code>integrand</code> <code>AbstractIntegrand</code> <p>The integrand.</p> required <code>abs_tol</code> <code>ndarray</code> <p>Absolute error tolerance.</p> <code>0.01</code> <code>rel_tol</code> <code>ndarray</code> <p>Relative error tolerance.</p> <code>0</code> <code>n_init</code> <code>int</code> <p>Initial number of samples.</p> <code>2 ** 8</code> <code>n_limit</code> <code>int</code> <p>Maximum number of samples.</p> <code>2 ** 22</code> <code>error_fun</code> <code>Union[str, callable]</code> <p>Function mapping the approximate solution, absolute error tolerance, and relative error tolerance to the current error bound.</p> <ul> <li><code>'EITHER'</code>, the default, requires the approximation error must be below either the absolue or relative tolerance.     Equivalent to setting     <pre><code>error_fun = lambda sv,abs_tol,rel_tol: np.maximum(abs_tol,abs(sv)*rel_tol)\n</code></pre></li> <li><code>'BOTH'</code> requires the approximation error to be below both the absolue and relative tolerance.     Equivalent to setting     <pre><code>error_fun = lambda sv,abs_tol,rel_tol: np.minimum(abs_tol,abs(sv)*rel_tol)\n</code></pre></li> </ul> <code>'EITHER'</code> <code>alpha</code> <code>ndarray</code> <p>Uncertainty level in \\((0,1)\\).</p> <code>0.01</code> <code>errbd_type</code> <code>str</code> <p>Options are</p> <ul> <li><code>'MLE'</code>: Marginal Log Likelihood.</li> <li><code>'GCV'</code>: Generalized Cross Validation.</li> <li><code>'FULL'</code>: Full Bayes.</li> </ul> <code>'MLE'</code> Source code in <code>qmcpy/stopping_criterion/cub_qmc_bayes_net_g.py</code> <pre><code>def __init__(\n    self,\n    integrand,\n    abs_tol=1e-2,\n    rel_tol=0,\n    n_init=2**8,\n    n_limit=2**22,\n    error_fun=\"EITHER\",\n    alpha=0.01,\n    errbd_type=\"MLE\",\n):\n    r\"\"\"\n    Args:\n        integrand (AbstractIntegrand): The integrand.\n        abs_tol (np.ndarray): Absolute error tolerance.\n        rel_tol (np.ndarray): Relative error tolerance.\n        n_init (int): Initial number of samples.\n        n_limit (int): Maximum number of samples.\n        error_fun (Union[str, callable]): Function mapping the approximate solution, absolute error tolerance, and relative error tolerance to the current error bound.\n\n            - `'EITHER'`, the default, requires the approximation error must be below either the absolue *or* relative tolerance.\n                Equivalent to setting\n                ```python\n                error_fun = lambda sv,abs_tol,rel_tol: np.maximum(abs_tol,abs(sv)*rel_tol)\n                ```\n            - `'BOTH'` requires the approximation error to be below both the absolue *and* relative tolerance.\n                Equivalent to setting\n                ```python\n                error_fun = lambda sv,abs_tol,rel_tol: np.minimum(abs_tol,abs(sv)*rel_tol)\n                ```\n        alpha (np.ndarray): Uncertainty level in $(0,1)$.\n        errbd_type (str): Options are\n\n            - `'MLE'`: Marginal Log Likelihood.\n            - `'GCV'`: Generalized Cross Validation.\n            - `'FULL'`: Full Bayes.\n    \"\"\"\n    super(CubQMCBayesNetG, self).__init__(\n        integrand,\n        ft=fwht,\n        omega=omega_fwht,\n        ptransform=None,\n        allowed_distribs=[DigitalNetB2],\n        kernel=self._shift_inv_kernel_digital,\n        abs_tol=abs_tol,\n        rel_tol=rel_tol,\n        n_init=n_init,\n        n_limit=n_limit,\n        alpha=alpha,\n        error_fun=error_fun,\n        errbd_type=errbd_type,\n    )\n    self.order = 1  # Currently supports only order=1\n    # private properties\n    # Full Bayes - assumes m and s^2 as hyperparameters\n    # GCV - Generalized cross validation\n    self.kernType = 1  # Type-1:\n    self._xfullundtype = np.uint64\n    if self.discrete_distrib.order != \"RADICAL INVERSE\":\n        raise ParameterError(\n            \"CubQMCNet_g requires DigitalNetB2 with 'RADICAL INVERSE' order\"\n        )\n</code></pre>"},{"location":"api/stopping_criteria/#cubqmcbayeslatticeg","title":"<code>CubQMCBayesLatticeG</code>","text":"<p>               Bases: <code>AbstractCubBayesLDG</code></p> <p>Quasi-Monte Carlo stopping criterion using fast Bayesian cubature and rank-1 lattices with guarantees for Gaussian processes having certain shift invariant kernels.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; k = Keister(Lattice(2, seed=123456789))\n&gt;&gt;&gt; sc = CubQMCBayesLatticeG(k,abs_tol=1e-4)\n&gt;&gt;&gt; solution,data = sc.integrate()\n&gt;&gt;&gt; data\nData (Data)\n    solution        1.808\n    comb_bound_low  1.808\n    comb_bound_high 1.808\n    comb_bound_diff 3.60e-05\n    comb_flags      1\n    n_total         2^(10)\n    n               2^(10)\n    time_integrate  ...\nCubQMCBayesLatticeG (AbstractStoppingCriterion)\n    abs_tol         1.00e-04\n    rel_tol         0\n    n_init          2^(8)\n    n_limit         2^(22)\n    order           2^(1)\nKeister (AbstractIntegrand)\nGaussian (AbstractTrueMeasure)\n    mean            0\n    covariance      2^(-1)\n    decomp_type     PCA\nLattice (AbstractLDDiscreteDistribution)\n    d               2^(1)\n    replications    1\n    randomize       SHIFT\n    gen_vec_source  kuo.lattice-33002-1024-1048576.9125.txt\n    order           RADICAL INVERSE\n    n_limit         2^(20)\n    entropy         123456789\n</code></pre> <p>Vector outputs</p> <pre><code>&gt;&gt;&gt; f = BoxIntegral(Lattice(3,seed=7),s=[-1,1])\n&gt;&gt;&gt; abs_tol = 1e-2\n&gt;&gt;&gt; sc = CubQMCBayesLatticeG(f,abs_tol=abs_tol,rel_tol=0)\n&gt;&gt;&gt; solution,data = sc.integrate()\n&gt;&gt;&gt; solution\narray([1.18837601, 0.95984299])\n&gt;&gt;&gt; data\nData (Data)\n    solution        [1.188 0.96 ]\n    comb_bound_low  [1.183 0.95 ]\n    comb_bound_high [1.194 0.969]\n    comb_bound_diff [0.011 0.019]\n    comb_flags      [ True  True]\n    n_total         2^(9)\n    n               [512 256]\n    time_integrate  ...\nCubQMCBayesLatticeG (AbstractStoppingCriterion)\n    abs_tol         0.010\n    rel_tol         0\n    n_init          2^(8)\n    n_limit         2^(22)\n    order           2^(1)\nBoxIntegral (AbstractIntegrand)\n    s               [-1  1]\nUniform (AbstractTrueMeasure)\n    lower_bound     0\n    upper_bound     1\nLattice (AbstractLDDiscreteDistribution)\n    d               3\n    replications    1\n    randomize       SHIFT\n    gen_vec_source  kuo.lattice-33002-1024-1048576.9125.txt\n    order           RADICAL INVERSE\n    n_limit         2^(20)\n    entropy         7\n&gt;&gt;&gt; sol3neg1 = -np.pi/4-1/2*np.log(2)+np.log(5+3*np.sqrt(3))\n&gt;&gt;&gt; sol31 = np.sqrt(3)/4+1/2*np.log(2+np.sqrt(3))-np.pi/24\n&gt;&gt;&gt; true_value = np.array([sol3neg1,sol31])\n&gt;&gt;&gt; assert (abs(true_value-solution)&lt;abs_tol).all()\n</code></pre> <p>Sensitivity indices</p> <pre><code>&gt;&gt;&gt; function = Genz(Lattice(3,seed=7))\n&gt;&gt;&gt; integrand = SensitivityIndices(function)\n&gt;&gt;&gt; sc = CubQMCBayesLatticeG(integrand,abs_tol=5e-2,rel_tol=0)\n&gt;&gt;&gt; solution,data = sc.integrate()\n&gt;&gt;&gt; data\nData (Data)\n    solution        [[0.057 0.131 0.269]\n                     [0.386 0.523 0.741]]\n    comb_bound_low  [[0.048 0.12  0.256]\n                     [0.377 0.511 0.721]]\n    comb_bound_high [[0.066 0.141 0.283]\n                     [0.395 0.535 0.762]]\n    comb_bound_diff [[0.018 0.021 0.027]\n                     [0.018 0.024 0.04 ]]\n    comb_flags      [[ True  True  True]\n                     [ True  True  True]]\n    n_total         2^(10)\n    n               [[[1024 1024 1024]\n                      [1024 1024 1024]\n                      [1024 1024 1024]]\n\n                     [[1024 1024 1024]\n                      [1024 1024 1024]\n                      [1024 1024 1024]]]\n    time_integrate  ...\nCubQMCBayesLatticeG (AbstractStoppingCriterion)\n    abs_tol         0.050\n    rel_tol         0\n    n_init          2^(8)\n    n_limit         2^(22)\n    order           2^(1)\nSensitivityIndices (AbstractIntegrand)\n    indices         [[ True False False]\n                     [False  True False]\n                     [False False  True]]\nUniform (AbstractTrueMeasure)\n    lower_bound     0\n    upper_bound     1\nLattice (AbstractLDDiscreteDistribution)\n    d               3\n    replications    1\n    randomize       SHIFT\n    gen_vec_source  kuo.lattice-33002-1024-1048576.9125.txt\n    order           RADICAL INVERSE\n    n_limit         2^(20)\n    entropy         7\n</code></pre> <p>References:</p> <ol> <li> <p>Jagadeeswaran, Rathinavel, and Fred J. Hickernell.     \"Fast automatic Bayesian cubature using lattice sampling.\"     Statistics and Computing 29.6 (2019): 1215-1229.</p> </li> <li> <p>Jagadeeswaran Rathinavel and Fred J. Hickernell,     Fast automatic Bayesian cubature using lattice sampling.     Stat Comput 29, 1215-1229 (2019).     Available from Springer https://doi.org/10.1007/s11222-019-09895-9.</p> </li> <li> <p>Sou-Cheng T. Choi, Yuhan Ding, Fred J. Hickernell, Lan Jiang, Lluis Antoni Jimenez Rugama,     Da Li, Jagadeeswaran Rathinavel, Xin Tong, Kan Zhang, Yizhi Zhang, and Xuan Zhou,     GAIL: Guaranteed Automatic Integration Library (Version 2.3) [MATLAB Software], 2019.     http://gailgithub.github.io/GAIL_Dev/.     https://github.com/GailGithub/GAIL_Dev/blob/master/Algorithms/IntegrationExpectation/cubBayesLattice_g.m.</p> </li> </ol> <p>Parameters:</p> Name Type Description Default <code>integrand</code> <code>AbstractIntegrand</code> <p>The integrand.</p> required <code>abs_tol</code> <code>ndarray</code> <p>Absolute error tolerance.</p> <code>0.01</code> <code>rel_tol</code> <code>ndarray</code> <p>Relative error tolerance.</p> <code>0</code> <code>n_init</code> <code>int</code> <p>Initial number of samples.</p> <code>2 ** 8</code> <code>n_limit</code> <code>int</code> <p>Maximum number of samples.</p> <code>2 ** 22</code> <code>error_fun</code> <code>Union[str, callable]</code> <p>Function mapping the approximate solution, absolute error tolerance, and relative error tolerance to the current error bound.</p> <ul> <li><code>'EITHER'</code>, the default, requires the approximation error must be below either the absolue or relative tolerance.     Equivalent to setting     <pre><code>error_fun = lambda sv,abs_tol,rel_tol: np.maximum(abs_tol,abs(sv)*rel_tol)\n</code></pre></li> <li><code>'BOTH'</code> requires the approximation error to be below both the absolue and relative tolerance.     Equivalent to setting     <pre><code>error_fun = lambda sv,abs_tol,rel_tol: np.minimum(abs_tol,abs(sv)*rel_tol)\n</code></pre></li> </ul> <code>'EITHER'</code> <code>alpha</code> <code>ndarray</code> <p>Uncertainty level in \\((0,1)\\).</p> <code>0.01</code> <code>ptransform</code> <code>str</code> <p>Periodization transform, see the options in [<code>AbstractIntegrand.f</code>][qmcpy.AbstractIntegrand.f].</p> <code>'C1SIN'</code> <code>errbd_type</code> <code>str</code> <p>Options are</p> <ul> <li><code>'MLE'</code>: Marginal Log Likelihood.</li> <li><code>'GCV'</code>: Generalized Cross Validation.</li> <li><code>'FULL'</code>: Full Bayes.</li> </ul> <code>'MLE'</code> <code>order</code> <code>int</code> <p>Bernoulli kernel's order. If zero, choose order automatically</p> <code>2</code> Source code in <code>qmcpy/stopping_criterion/cub_qmc_bayes_lattice_g.py</code> <pre><code>def __init__(\n    self,\n    integrand,\n    abs_tol=1e-2,\n    rel_tol=0,\n    n_init=2**8,\n    n_limit=2**22,\n    error_fun=\"EITHER\",\n    alpha=0.01,\n    ptransform=\"C1SIN\",\n    errbd_type=\"MLE\",\n    order=2,\n):\n    r\"\"\"\n    Args:\n        integrand (AbstractIntegrand): The integrand.\n        abs_tol (np.ndarray): Absolute error tolerance.\n        rel_tol (np.ndarray): Relative error tolerance.\n        n_init (int): Initial number of samples.\n        n_limit (int): Maximum number of samples.\n        error_fun (Union[str, callable]): Function mapping the approximate solution, absolute error tolerance, and relative error tolerance to the current error bound.\n\n            - `'EITHER'`, the default, requires the approximation error must be below either the absolue *or* relative tolerance.\n                Equivalent to setting\n                ```python\n                error_fun = lambda sv,abs_tol,rel_tol: np.maximum(abs_tol,abs(sv)*rel_tol)\n                ```\n            - `'BOTH'` requires the approximation error to be below both the absolue *and* relative tolerance.\n                Equivalent to setting\n                ```python\n                error_fun = lambda sv,abs_tol,rel_tol: np.minimum(abs_tol,abs(sv)*rel_tol)\n                ```\n        alpha (np.ndarray): Uncertainty level in $(0,1)$.\n        ptransform (str): Periodization transform, see the options in [`AbstractIntegrand.f`][qmcpy.AbstractIntegrand.f].\n        errbd_type (str): Options are\n\n            - `'MLE'`: Marginal Log Likelihood.\n            - `'GCV'`: Generalized Cross Validation.\n            - `'FULL'`: Full Bayes.\n        order (int): Bernoulli kernel's order. If zero, choose order automatically\n    \"\"\"\n    super(CubQMCBayesLatticeG, self).__init__(\n        integrand,\n        ft=fftbr,\n        omega=omega_fftbr,\n        ptransform=ptransform,\n        allowed_distribs=[Lattice],\n        kernel=self._shift_inv_kernel,\n        abs_tol=abs_tol,\n        rel_tol=rel_tol,\n        n_init=n_init,\n        n_limit=n_limit,\n        alpha=alpha,\n        error_fun=error_fun,\n        errbd_type=errbd_type,\n    )\n    self.order = (\n        order  # Bernoulli kernel's order. If zero, choose order automatically\n    )\n    # private properties\n    # full_Bayes - Full Bayes - assumes m and s^2 as hyperparameters\n    # GCV - Generalized cross validation\n    self.kernType = 1  # Type-1: Bernoulli polynomial based algebraic convergence, Type-2: Truncated series\n    self._xfullundtype = float\n    if self.discrete_distrib.order != \"RADICAL INVERSE\":\n        raise ParameterError(\n            \"CubLattice_g requires Lattice with 'RADICAL INVERSE' order\"\n        )\n</code></pre>"},{"location":"api/stopping_criteria/#cubqmcrepstudentt","title":"<code>CubQMCRepStudentT</code>","text":"<p>               Bases: <code>AbstractStoppingCriterion</code></p> <p>Quasi-Monte Carlo stopping criterion based on Student's \\(t\\)-distribution for multiple replications.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; k = Keister(DigitalNetB2(seed=7,replications=25))\n&gt;&gt;&gt; sc = CubQMCRepStudentT(k,abs_tol=1e-3,rel_tol=0)\n&gt;&gt;&gt; solution,data = sc.integrate()\n&gt;&gt;&gt; solution\narray(1.3803927)\n&gt;&gt;&gt; data\nData (Data)\n    solution        1.380\n    comb_bound_low  1.380\n    comb_bound_high 1.381\n    comb_bound_diff 0.002\n    comb_flags      1\n    n_total         6400\n    n               6400\n    n_rep           2^(8)\n    time_integrate  ...\nCubQMCRepStudentT (AbstractStoppingCriterion)\n    inflate         1\n    alpha           0.010\n    abs_tol         0.001\n    rel_tol         0\n    n_init          2^(8)\n    n_limit         2^(30)\nKeister (AbstractIntegrand)\nGaussian (AbstractTrueMeasure)\n    mean            0\n    covariance      2^(-1)\n    decomp_type     PCA\nDigitalNetB2 (AbstractLDDiscreteDistribution)\n    d               1\n    replications    25\n    randomize       LMS DS\n    gen_mats_source joe_kuo.6.21201.txt\n    order           RADICAL INVERSE\n    t               63\n    alpha           1\n    n_limit         2^(32)\n    entropy         7\n</code></pre> <p>Vector outputs</p> <pre><code>&gt;&gt;&gt; f = BoxIntegral(DigitalNetB2(3,seed=7,replications=25),s=[-1,1])\n&gt;&gt;&gt; abs_tol = 1e-3\n&gt;&gt;&gt; sc = CubQMCRepStudentT(f,abs_tol=abs_tol,rel_tol=0)\n&gt;&gt;&gt; solution,data = sc.integrate()\n&gt;&gt;&gt; solution\narray([1.19025707, 0.96062762])\n&gt;&gt;&gt; data\nData (Data)\n    solution        [1.19  0.961]\n    comb_bound_low  [1.19  0.961]\n    comb_bound_high [1.191 0.961]\n    comb_bound_diff [0.001 0.   ]\n    comb_flags      [ True  True]\n    n_total         409600\n    n               [409600   6400]\n    n_rep           [16384   256]\n    time_integrate  ...\nCubQMCRepStudentT (AbstractStoppingCriterion)\n    inflate         1\n    alpha           0.010\n    abs_tol         0.001\n    rel_tol         0\n    n_init          2^(8)\n    n_limit         2^(30)\nBoxIntegral (AbstractIntegrand)\n    s               [-1  1]\nUniform (AbstractTrueMeasure)\n    lower_bound     0\n    upper_bound     1\nDigitalNetB2 (AbstractLDDiscreteDistribution)\n    d               3\n    replications    25\n    randomize       LMS DS\n    gen_mats_source joe_kuo.6.21201.txt\n    order           RADICAL INVERSE\n    t               63\n    alpha           1\n    n_limit         2^(32)\n    entropy         7\n&gt;&gt;&gt; sol3neg1 = -np.pi/4-1/2*np.log(2)+np.log(5+3*np.sqrt(3))\n&gt;&gt;&gt; sol31 = np.sqrt(3)/4+1/2*np.log(2+np.sqrt(3))-np.pi/24\n&gt;&gt;&gt; true_value = np.array([sol3neg1,sol31])\n&gt;&gt;&gt; assert (abs(true_value-solution)&lt;abs_tol).all()\n</code></pre> <p>Sensitivity indices</p> <pre><code>&gt;&gt;&gt; function = Genz(DigitalNetB2(3,seed=7,replications=25))\n&gt;&gt;&gt; integrand = SensitivityIndices(function)\n&gt;&gt;&gt; sc = CubQMCRepStudentT(integrand,abs_tol=5e-4,rel_tol=0)\n&gt;&gt;&gt; solution,data = sc.integrate()\n&gt;&gt;&gt; data\nData (Data)\n    solution        [[0.02  0.196 0.667]\n                     [0.036 0.303 0.782]]\n    comb_bound_low  [[0.019 0.195 0.667]\n                     [0.035 0.303 0.781]]\n    comb_bound_high [[0.02  0.196 0.667]\n                     [0.036 0.303 0.782]]\n    comb_bound_diff [[0.001 0.001 0.001]\n                     [0.001 0.001 0.001]]\n    comb_flags      [[ True  True  True]\n                     [ True  True  True]]\n    n_total         204800\n    n               [[[102400 204800 204800]\n                      [102400 204800 204800]\n                      [102400 204800 204800]]\n\n                     [[ 12800 102400 102400]\n                      [ 12800 102400 102400]\n                      [ 12800 102400 102400]]]\n    n_rep           [[[4096 8192 8192]\n                      [4096 8192 8192]\n                      [4096 8192 8192]]\n\n                     [[ 512 4096 4096]\n                      [ 512 4096 4096]\n                      [ 512 4096 4096]]]\n    time_integrate  ...\nCubQMCRepStudentT (AbstractStoppingCriterion)\n    inflate         1\n    alpha           0.010\n    abs_tol         5.00e-04\n    rel_tol         0\n    n_init          2^(8)\n    n_limit         2^(30)\nSensitivityIndices (AbstractIntegrand)\n    indices         [[ True False False]\n                     [False  True False]\n                     [False False  True]]\nUniform (AbstractTrueMeasure)\n    lower_bound     0\n    upper_bound     1\nDigitalNetB2 (AbstractLDDiscreteDistribution)\n    d               3\n    replications    25\n    randomize       LMS DS\n    gen_mats_source joe_kuo.6.21201.txt\n    order           RADICAL INVERSE\n    t               63\n    alpha           1\n    n_limit         2^(32)\n    entropy         7\n</code></pre> <p>References:</p> <ol> <li> <p>Art B. Owen. \"Practical Quasi-Monte Carlo Integration.\" 2023.     https://artowen.su.domains/mc/.</p> </li> <li> <p>Pierre l\u2019Ecuyer et al.     \"Confidence intervals for randomized quasi-Monte Carlo estimators.\"     2023 Winter Simulation Conference (WSC). IEEE, 2023.     https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10408613.</p> </li> </ol> <p>Parameters:</p> Name Type Description Default <code>integrand</code> <code>AbstractIntegrand</code> <p>The integrand.</p> required <code>abs_tol</code> <code>ndarray</code> <p>Absolute error tolerance.</p> <code>0.01</code> <code>rel_tol</code> <code>ndarray</code> <p>Relative error tolerance.</p> <code>0.0</code> <code>n_init</code> <code>int</code> <p>Initial number of samples.</p> <code>256.0</code> <code>n_limit</code> <code>int</code> <p>Maximum number of samples.</p> <code>2 ** 30</code> <code>error_fun</code> <code>Union[str, callable]</code> <p>Function mapping the approximate solution, absolute error tolerance, and relative error tolerance to the current error bound.</p> <ul> <li><code>'EITHER'</code>, the default, requires the approximation error must be below either the absolue or relative tolerance.     Equivalent to setting     <pre><code>error_fun = lambda sv,abs_tol,rel_tol: np.maximum(abs_tol,abs(sv)*rel_tol)\n</code></pre></li> <li><code>'BOTH'</code> requires the approximation error to be below both the absolue and relative tolerance.     Equivalent to setting     <pre><code>error_fun = lambda sv,abs_tol,rel_tol: np.minimum(abs_tol,abs(sv)*rel_tol)\n</code></pre></li> </ul> <code>'EITHER'</code> <code>inflate</code> <code>float</code> <p>Inflation factor \\(\\geq 1\\) to multiply by the variance estimate to make it more conservative.</p> <code>1</code> <code>alpha</code> <code>ndarray</code> <p>Uncertainty level in \\((0,1)\\).</p> <code>0.01</code> Source code in <code>qmcpy/stopping_criterion/cub_qmc_rep_student_t.py</code> <pre><code>def __init__(\n    self,\n    integrand,\n    abs_tol=1e-2,\n    rel_tol=0.0,\n    n_init=256.0,\n    n_limit=2**30,\n    error_fun=\"EITHER\",\n    inflate=1,\n    alpha=0.01,\n):\n    r\"\"\"\n    Args:\n        integrand (AbstractIntegrand): The integrand.\n        abs_tol (np.ndarray): Absolute error tolerance.\n        rel_tol (np.ndarray): Relative error tolerance.\n        n_init (int): Initial number of samples.\n        n_limit (int): Maximum number of samples.\n        error_fun (Union[str, callable]): Function mapping the approximate solution, absolute error tolerance, and relative error tolerance to the current error bound.\n\n            - `'EITHER'`, the default, requires the approximation error must be below either the absolue *or* relative tolerance.\n                Equivalent to setting\n                ```python\n                error_fun = lambda sv,abs_tol,rel_tol: np.maximum(abs_tol,abs(sv)*rel_tol)\n                ```\n            - `'BOTH'` requires the approximation error to be below both the absolue *and* relative tolerance.\n                Equivalent to setting\n                ```python\n                error_fun = lambda sv,abs_tol,rel_tol: np.minimum(abs_tol,abs(sv)*rel_tol)\n                ```\n        inflate (float): Inflation factor $\\geq 1$ to multiply by the variance estimate to make it more conservative.\n        alpha (np.ndarray): Uncertainty level in $(0,1)$.\n    \"\"\"\n    self.parameters = [\n        \"inflate\",\n        \"alpha\",\n        \"abs_tol\",\n        \"rel_tol\",\n        \"n_init\",\n        \"n_limit\",\n    ]\n    # Input Checks\n    if np.log2(n_init) % 1 != 0:\n        warnings.warn(\n            \"n_init must be a power of two. Using n_init = 2**5\", ParameterWarning\n        )\n        n_init = 2**5\n    if np.log2(n_limit) % 1 != 0:\n        warnings.warn(\n            \"n_init must be a power of two. Using n_limit = 2**30\", ParameterWarning\n        )\n        n_limit = 2**30\n    # Set Attributes\n    self.n_init = int(n_init)\n    self.n_limit = int(n_limit)\n    assert isinstance(error_fun, str) or callable(error_fun)\n    if isinstance(error_fun, str):\n        if error_fun.upper() == \"EITHER\":\n            error_fun = lambda sv, abs_tol, rel_tol: np.maximum(\n                abs_tol, abs(sv) * rel_tol\n            )\n        elif error_fun.upper() == \"BOTH\":\n            error_fun = lambda sv, abs_tol, rel_tol: np.minimum(\n                abs_tol, abs(sv) * rel_tol\n            )\n        else:\n            raise ParameterError(\"str error_fun must be 'EITHER' or 'BOTH'\")\n    self.error_fun = error_fun\n    self.alpha = alpha\n    self.inflate = float(inflate)\n    assert self.inflate &gt;= 1\n    assert 0 &lt; self.alpha &lt; 1\n    # QMCPy Objs\n    self.integrand = integrand\n    self.true_measure = self.integrand.true_measure\n    self.discrete_distrib = self.integrand.discrete_distrib\n    super(CubQMCRepStudentT, self).__init__(\n        allowed_distribs=[AbstractLDDiscreteDistribution],\n        allow_vectorized_integrals=True,\n    )\n    assert (\n        self.integrand.discrete_distrib.replications &gt; 1\n    ), \"Require the discrete distribution has replications&gt;1\"\n    assert (\n        self.integrand.discrete_distrib.randomize != \"FALSE\"\n    ), \"Require discrete distribution is randomized\"\n    self.alphas_indv, identity_dependency = self._compute_indv_alphas(\n        np.full(self.integrand.d_comb, self.alpha)\n    )\n    self.set_tolerance(abs_tol, rel_tol)\n    self.t_star = -t.ppf(\n        self.alphas_indv / 2, df=self.integrand.discrete_distrib.replications - 1\n    )\n</code></pre>"},{"location":"api/stopping_criteria/#iid-mc","title":"IID MC","text":""},{"location":"api/stopping_criteria/#cubmccltvec","title":"<code>CubMCCLTVec</code>","text":"<p>               Bases: <code>AbstractStoppingCriterion</code></p> <p>IID Monte Carlo stopping criterion stopping criterion based on the Central Limit Theorem with doubling sample sizes.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; k = Keister(IIDStdUniform(seed=7))\n&gt;&gt;&gt; sc = CubMCCLTVec(k,abs_tol=5e-2,rel_tol=0)\n&gt;&gt;&gt; solution,data = sc.integrate()\n&gt;&gt;&gt; solution\narray(1.38366791)\n&gt;&gt;&gt; data\nData (Data)\n    solution        1.384\n    comb_bound_low  1.343\n    comb_bound_high 1.424\n    comb_bound_diff 0.080\n    comb_flags      1\n    n_total         1024\n    n               2^(10)\n    time_integrate  ...\nCubMCCLTVec (AbstractStoppingCriterion)\n    inflate         1\n    alpha           0.010\n    abs_tol         0.050\n    rel_tol         0\n    n_init          2^(8)\n    n_limit         2^(30)\nKeister (AbstractIntegrand)\nGaussian (AbstractTrueMeasure)\n    mean            0\n    covariance      2^(-1)\n    decomp_type     PCA\nIIDStdUniform (AbstractIIDDiscreteDistribution)\n    d               1\n    replications    1\n    entropy         7\n</code></pre> <p>Vector outputs</p> <pre><code>&gt;&gt;&gt; f = BoxIntegral(IIDStdUniform(3,seed=7),s=[-1,1])\n&gt;&gt;&gt; abs_tol = 2.5e-2\n&gt;&gt;&gt; sc = CubMCCLTVec(f,abs_tol=abs_tol,rel_tol=0)\n&gt;&gt;&gt; solution,data = sc.integrate()\n&gt;&gt;&gt; solution\narray([1.18448043, 0.95435347])\n&gt;&gt;&gt; data\nData (Data)\n    solution        [1.184 0.954]\n    comb_bound_low  [1.165 0.932]\n    comb_bound_high [1.203 0.977]\n    comb_bound_diff [0.038 0.045]\n    comb_flags      [ True  True]\n    n_total         8192\n    n               [8192 1024]\n    time_integrate  ...\nCubMCCLTVec (AbstractStoppingCriterion)\n    inflate         1\n    alpha           0.010\n    abs_tol         0.025\n    rel_tol         0\n    n_init          2^(8)\n    n_limit         2^(30)\nBoxIntegral (AbstractIntegrand)\n    s               [-1  1]\nUniform (AbstractTrueMeasure)\n    lower_bound     0\n    upper_bound     1\nIIDStdUniform (AbstractIIDDiscreteDistribution)\n    d               3\n    replications    1\n    entropy         7\n&gt;&gt;&gt; sol3neg1 = -np.pi/4-1/2*np.log(2)+np.log(5+3*np.sqrt(3))\n&gt;&gt;&gt; sol31 = np.sqrt(3)/4+1/2*np.log(2+np.sqrt(3))-np.pi/24\n&gt;&gt;&gt; true_value = np.array([sol3neg1,sol31])\n&gt;&gt;&gt; assert (abs(true_value-solution)&lt;abs_tol).all()\n</code></pre> <p>Sensitivity indices</p> <pre><code>&gt;&gt;&gt; function = Genz(IIDStdUniform(3,seed=7))\n&gt;&gt;&gt; integrand = SensitivityIndices(function)\n&gt;&gt;&gt; sc = CubMCCLTVec(integrand,abs_tol=2.5e-2,rel_tol=0)\n&gt;&gt;&gt; solution,data = sc.integrate()\n&gt;&gt;&gt; data\nData (Data)\n    solution        [[0.024 0.203 0.662]\n                     [0.044 0.308 0.78 ]]\n    comb_bound_low  [[0.006 0.186 0.644]\n                     [0.02  0.286 0.761]]\n    comb_bound_high [[0.042 0.221 0.681]\n                     [0.067 0.329 0.798]]\n    comb_bound_diff [[0.036 0.035 0.037]\n                     [0.047 0.043 0.037]]\n    comb_flags      [[ True  True  True]\n                     [ True  True  True]]\n    n_total         262144\n    n               [[[  4096  65536 262144]\n                      [  4096  65536 262144]\n                      [  4096  65536 262144]]\n\n                     [[   512  32768 262144]\n                      [   512  32768 262144]\n                      [   512  32768 262144]]]\n    time_integrate  ...\nCubMCCLTVec (AbstractStoppingCriterion)\n    inflate         1\n    alpha           0.010\n    abs_tol         0.025\n    rel_tol         0\n    n_init          2^(8)\n    n_limit         2^(30)\nSensitivityIndices (AbstractIntegrand)\n    indices         [[ True False False]\n                     [False  True False]\n                     [False False  True]]\nUniform (AbstractTrueMeasure)\n    lower_bound     0\n    upper_bound     1\nIIDStdUniform (AbstractIIDDiscreteDistribution)\n    d               3\n    replications    1\n    entropy         7\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>integrand</code> <code>AbstractIntegrand</code> <p>The integrand.</p> required <code>abs_tol</code> <code>ndarray</code> <p>Absolute error tolerance.</p> <code>0.01</code> <code>rel_tol</code> <code>ndarray</code> <p>Relative error tolerance.</p> <code>0.0</code> <code>n_init</code> <code>int</code> <p>Initial number of samples.</p> <code>256.0</code> <code>n_limit</code> <code>int</code> <p>Maximum number of samples.</p> <code>2 ** 30</code> <code>error_fun</code> <code>Union[str, callable]</code> <p>Function mapping the approximate solution, absolute error tolerance, and relative error tolerance to the current error bound.</p> <ul> <li><code>'EITHER'</code>, the default, requires the approximation error must be below either the absolue or relative tolerance.     Equivalent to setting     <pre><code>error_fun = lambda sv,abs_tol,rel_tol: np.maximum(abs_tol,abs(sv)*rel_tol)\n</code></pre></li> <li><code>'BOTH'</code> requires the approximation error to be below both the absolue and relative tolerance.     Equivalent to setting     <pre><code>error_fun = lambda sv,abs_tol,rel_tol: np.minimum(abs_tol,abs(sv)*rel_tol)\n</code></pre></li> </ul> <code>'EITHER'</code> <code>inflate</code> <code>float</code> <p>Inflation factor \\(\\geq 1\\) to multiply by the variance estimate to make it more conservative.</p> <code>1</code> <code>alpha</code> <code>ndarray</code> <p>Uncertainty level in \\((0,1)\\).</p> <code>0.01</code> Source code in <code>qmcpy/stopping_criterion/cub_mc_clt_vec.py</code> <pre><code>def __init__(\n    self,\n    integrand,\n    abs_tol=1e-2,\n    rel_tol=0.0,\n    n_init=256.0,\n    n_limit=2**30,\n    error_fun=\"EITHER\",\n    inflate=1,\n    alpha=0.01,\n):\n    r\"\"\"\n    Args:\n        integrand (AbstractIntegrand): The integrand.\n        abs_tol (np.ndarray): Absolute error tolerance.\n        rel_tol (np.ndarray): Relative error tolerance.\n        n_init (int): Initial number of samples.\n        n_limit (int): Maximum number of samples.\n        error_fun (Union[str, callable]): Function mapping the approximate solution, absolute error tolerance, and relative error tolerance to the current error bound.\n\n            - `'EITHER'`, the default, requires the approximation error must be below either the absolue *or* relative tolerance.\n                Equivalent to setting\n                ```python\n                error_fun = lambda sv,abs_tol,rel_tol: np.maximum(abs_tol,abs(sv)*rel_tol)\n                ```\n            - `'BOTH'` requires the approximation error to be below both the absolue *and* relative tolerance.\n                Equivalent to setting\n                ```python\n                error_fun = lambda sv,abs_tol,rel_tol: np.minimum(abs_tol,abs(sv)*rel_tol)\n                ```\n        inflate (float): Inflation factor $\\geq 1$ to multiply by the variance estimate to make it more conservative.\n        alpha (np.ndarray): Uncertainty level in $(0,1)$.\n    \"\"\"\n    self.parameters = [\n        \"inflate\",\n        \"alpha\",\n        \"abs_tol\",\n        \"rel_tol\",\n        \"n_init\",\n        \"n_limit\",\n    ]\n    # Input Checks\n    if np.log2(n_init) % 1 != 0:\n        warnings.warn(\n            \"n_init must be a power of two. Using n_init = 2**5\", ParameterWarning\n        )\n        n_init = 2**5\n    if np.log2(n_limit) % 1 != 0:\n        warnings.warn(\n            \"n_init must be a power of two. Using n_limit = 2**30\", ParameterWarning\n        )\n        n_limit = 2**30\n    # Set Attributes\n    self.n_init = int(n_init)\n    self.n_limit = int(n_limit)\n    assert isinstance(error_fun, str) or callable(error_fun)\n    if isinstance(error_fun, str):\n        if error_fun.upper() == \"EITHER\":\n            error_fun = lambda sv, abs_tol, rel_tol: np.maximum(\n                abs_tol, abs(sv) * rel_tol\n            )\n        elif error_fun.upper() == \"BOTH\":\n            error_fun = lambda sv, abs_tol, rel_tol: np.minimum(\n                abs_tol, abs(sv) * rel_tol\n            )\n        else:\n            raise ParameterError(\"str error_fun must be 'EITHER' or 'BOTH'\")\n    self.error_fun = error_fun\n    self.alpha = alpha\n    self.inflate = float(inflate)\n    assert self.inflate &gt;= 1\n    # QMCPy Objs\n    self.integrand = integrand\n    self.true_measure = self.integrand.true_measure\n    self.discrete_distrib = self.integrand.discrete_distrib\n    super(CubMCCLTVec, self).__init__(\n        allowed_distribs=[AbstractIIDDiscreteDistribution],\n        allow_vectorized_integrals=True,\n    )\n    assert (\n        self.integrand.discrete_distrib.no_replications == True\n    ), \"Require the discrete distribution has replications=None\"\n    self.alphas_indv, identity_dependency = self._compute_indv_alphas(\n        np.full(self.integrand.d_comb, self.alpha)\n    )\n    self.set_tolerance(abs_tol, rel_tol)\n    self.z_star = -norm.ppf(self.alphas_indv / 2)\n</code></pre>"},{"location":"api/stopping_criteria/#cubmcg","title":"<code>CubMCG</code>","text":"<p>               Bases: <code>AbstractStoppingCriterion</code></p> <p>IID Monte Carlo stopping criterion using Berry-Esseen inequalities in a two step method with guarantees for functions with bounded kurtosis.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; ao = FinancialOption(IIDStdUniform(52,seed=7))\n&gt;&gt;&gt; sc = CubMCG(ao,abs_tol=.05)\n&gt;&gt;&gt; solution,data = sc.integrate()\n&gt;&gt;&gt; data\nData (Data)\n    solution        1.779\n    bound_low       1.729\n    bound_high      1.829\n    bound_diff      0.100\n    n_total         112314\n    time_integrate  ...\nCubMCG (AbstractStoppingCriterion)\n    abs_tol         0.050\n    rel_tol         0\n    n_init          2^(10)\n    n_limit         2^(30)\n    inflate         1.200\n    alpha           0.010\n    kurtmax         1.478\nFinancialOption (AbstractIntegrand)\n    option          ASIAN\n    call_put        CALL\n    volatility      2^(-1)\n    start_price     30\n    strike_price    35\n    interest_rate   0\n    t_final         1\n    asian_mean      ARITHMETIC\nGeometricBrownianMotion (AbstractTrueMeasure)\n    time_vec        [0.019 0.038 0.058 ... 0.962 0.981 1.   ]\n    drift           0\n    diffusion       2^(-2)\n    mean_gbm        [30. 30. 30. ... 30. 30. 30.]\n    covariance_gbm  [[  4.337   4.337   4.337 ...   4.337   4.337   4.337]\n                     [  4.337   8.696   8.696 ...   8.696   8.696   8.696]\n                     [  4.337   8.696  13.075 ...  13.075  13.075  13.075]\n                     ...\n                     [  4.337   8.696  13.075 ... 244.564 244.564 244.564]\n                     [  4.337   8.696  13.075 ... 244.564 250.08  250.08 ]\n                     [  4.337   8.696  13.075 ... 244.564 250.08  255.623]]\n    decomp_type     PCA\nIIDStdUniform (AbstractIIDDiscreteDistribution)\n    d               52\n    replications    1\n    entropy         7\n</code></pre> <p>Control variates</p> <pre><code>&gt;&gt;&gt; iid = IIDStdUniform(52,seed=7)\n&gt;&gt;&gt; ao = FinancialOption(iid,option=\"ASIAN\")\n&gt;&gt;&gt; eo = FinancialOption(iid,option=\"EUROPEAN\")\n&gt;&gt;&gt; lin0 = Linear0(iid)\n&gt;&gt;&gt; sc = CubMCG(ao,abs_tol=.05,\n...     control_variates = [eo,lin0],\n...     control_variate_means = [eo.get_exact_value(),0])\n&gt;&gt;&gt; solution,data = sc.integrate()\n&gt;&gt;&gt; data\nData (Data)\n    solution        1.787\n    bound_low       1.737\n    bound_high      1.837\n    bound_diff      0.100\n    n_total         52147\n    time_integrate  ...\nCubMCG (AbstractStoppingCriterion)\n    abs_tol         0.050\n    rel_tol         0\n    n_init          2^(10)\n    n_limit         2^(30)\n    inflate         1.200\n    alpha           0.010\n    kurtmax         1.478\n    cv              [FinancialOption (AbstractIntegrand)\n                         option          EUROPEAN\n                         call_put        CALL\n                         volatility      2^(-1)\n                         start_price     30\n                         strike_price    35\n                         interest_rate   0\n                         t_final         1               Linear0 (AbstractIntegrand)]\n    cv_mu           [4.211 0.   ]\nFinancialOption (AbstractIntegrand)\n    option          ASIAN\n    call_put        CALL\n    volatility      2^(-1)\n    start_price     30\n    strike_price    35\n    interest_rate   0\n    t_final         1\n    asian_mean      ARITHMETIC\nGeometricBrownianMotion (AbstractTrueMeasure)\n    time_vec        [0.019 0.038 0.058 ... 0.962 0.981 1.   ]\n    drift           0\n    diffusion       2^(-2)\n    mean_gbm        [30. 30. 30. ... 30. 30. 30.]\n    covariance_gbm  [[  4.337   4.337   4.337 ...   4.337   4.337   4.337]\n                     [  4.337   8.696   8.696 ...   8.696   8.696   8.696]\n                     [  4.337   8.696  13.075 ...  13.075  13.075  13.075]\n                     ...\n                     [  4.337   8.696  13.075 ... 244.564 244.564 244.564]\n                     [  4.337   8.696  13.075 ... 244.564 250.08  250.08 ]\n                     [  4.337   8.696  13.075 ... 244.564 250.08  255.623]]\n    decomp_type     PCA\nIIDStdUniform (AbstractIIDDiscreteDistribution)\n    d               52\n    replications    1\n    entropy         7\n</code></pre> <p>Relative tolerance</p> <pre><code>&gt;&gt;&gt; ao = FinancialOption(IIDStdUniform(52,seed=7))\n&gt;&gt;&gt; sc = CubMCG(ao,abs_tol=1e-3,rel_tol=5e-2)\n&gt;&gt;&gt; solution,data = sc.integrate()\n&gt;&gt;&gt; data\nData (Data)\n    solution        1.743\n    bound_low       1.661\n    bound_high      1.825\n    bound_diff      0.164\n    n_total         27503\n    time_integrate  ...\nCubMCG (AbstractStoppingCriterion)\n    abs_tol         0.001\n    rel_tol         0.050\n    n_init          2^(10)\n    n_limit         2^(30)\n    inflate         1.200\n    alpha           0.010\n    kurtmax         1.478\nFinancialOption (AbstractIntegrand)\n    option          ASIAN\n    call_put        CALL\n    volatility      2^(-1)\n    start_price     30\n    strike_price    35\n    interest_rate   0\n    t_final         1\n    asian_mean      ARITHMETIC\nGeometricBrownianMotion (AbstractTrueMeasure)\n    time_vec        [0.019 0.038 0.058 ... 0.962 0.981 1.   ]\n    drift           0\n    diffusion       2^(-2)\n    mean_gbm        [30. 30. 30. ... 30. 30. 30.]\n    covariance_gbm  [[  4.337   4.337   4.337 ...   4.337   4.337   4.337]\n                     [  4.337   8.696   8.696 ...   8.696   8.696   8.696]\n                     [  4.337   8.696  13.075 ...  13.075  13.075  13.075]\n                     ...\n                     [  4.337   8.696  13.075 ... 244.564 244.564 244.564]\n                     [  4.337   8.696  13.075 ... 244.564 250.08  250.08 ]\n                     [  4.337   8.696  13.075 ... 244.564 250.08  255.623]]\n    decomp_type     PCA\nIIDStdUniform (AbstractIIDDiscreteDistribution)\n    d               52\n    replications    1\n    entropy         7\n</code></pre> <p>Relative tolerance and control variates</p> <pre><code>&gt;&gt;&gt; iid = IIDStdUniform(52,seed=7)\n&gt;&gt;&gt; ao = FinancialOption(iid,option=\"ASIAN\")\n&gt;&gt;&gt; eo = FinancialOption(iid,option=\"EUROPEAN\")\n&gt;&gt;&gt; lin0 = Linear0(iid)\n&gt;&gt;&gt; sc = CubMCG(ao,abs_tol=1e-3,rel_tol=5e-2,\n...     control_variates = [eo,lin0],\n...     control_variate_means = [eo.get_exact_value(),0])\n&gt;&gt;&gt; solution,data = sc.integrate()\n&gt;&gt;&gt; data\nData (Data)\n    solution        1.776\n    bound_low       1.692\n    bound_high      1.859\n    bound_diff      0.167\n    n_total         12074\n    time_integrate  ...\nCubMCG (AbstractStoppingCriterion)\n    abs_tol         0.001\n    rel_tol         0.050\n    n_init          2^(10)\n    n_limit         2^(30)\n    inflate         1.200\n    alpha           0.010\n    kurtmax         1.478\n    cv              [FinancialOption (AbstractIntegrand)\n                         option          EUROPEAN\n                         call_put        CALL\n                         volatility      2^(-1)\n                         start_price     30\n                         strike_price    35\n                         interest_rate   0\n                         t_final         1               Linear0 (AbstractIntegrand)]\n    cv_mu           [4.211 0.   ]\nFinancialOption (AbstractIntegrand)\n    option          ASIAN\n    call_put        CALL\n    volatility      2^(-1)\n    start_price     30\n    strike_price    35\n    interest_rate   0\n    t_final         1\n    asian_mean      ARITHMETIC\nGeometricBrownianMotion (AbstractTrueMeasure)\n    time_vec        [0.019 0.038 0.058 ... 0.962 0.981 1.   ]\n    drift           0\n    diffusion       2^(-2)\n    mean_gbm        [30. 30. 30. ... 30. 30. 30.]\n    covariance_gbm  [[  4.337   4.337   4.337 ...   4.337   4.337   4.337]\n                     [  4.337   8.696   8.696 ...   8.696   8.696   8.696]\n                     [  4.337   8.696  13.075 ...  13.075  13.075  13.075]\n                     ...\n                     [  4.337   8.696  13.075 ... 244.564 244.564 244.564]\n                     [  4.337   8.696  13.075 ... 244.564 250.08  250.08 ]\n                     [  4.337   8.696  13.075 ... 244.564 250.08  255.623]]\n    decomp_type     PCA\nIIDStdUniform (AbstractIIDDiscreteDistribution)\n    d               52\n    replications    1\n    entropy         7\n</code></pre> <p>References:</p> <ol> <li> <p>Fred J. Hickernell, Lan Jiang, Yuewei Liu, and Art B. Owen,     \"Guaranteed conservative fixed width confidence intervals via Monte Carlo sampling,\"     Monte Carlo and Quasi-Monte Carlo Methods 2012 (J. Dick, F. Y. Kuo, G. W. Peters, and I. H. Sloan, eds.), pp. 105-128,     Springer-Verlag, Berlin, 2014. DOI: 10.1007/978-3-642-41095-6_5</p> </li> <li> <p>Sou-Cheng T. Choi, Yuhan Ding, Fred J. Hickernell, Lan Jiang, Lluis Antoni Jimenez Rugama,     Da Li, Jagadeeswaran Rathinavel, Xin Tong, Kan Zhang, Yizhi Zhang, and Xuan Zhou,     GAIL: Guaranteed Automatic Integration Library (Version 2.3) [MATLAB Software], 2019.     http://gailgithub.github.io/GAIL_Dev/.     https://github.com/GailGithub/GAIL_Dev/blob/master/Algorithms/IntegrationExpectation/meanMC_g.m.</p> </li> </ol> <p>Parameters:</p> Name Type Description Default <code>integrand</code> <code>AbstractIntegrand</code> <p>The integrand.</p> required <code>abs_tol</code> <code>ndarray</code> <p>Absolute error tolerance.</p> <code>0.01</code> <code>rel_tol</code> <code>ndarray</code> <p>Relative error tolerance.</p> <code>0.0</code> <code>n_init</code> <code>int</code> <p>Initial number of samples.</p> <code>1024</code> <code>n_limit</code> <code>int</code> <p>Maximum number of samples.</p> <code>2 ** 30</code> <code>inflate</code> <code>float</code> <p>Inflation factor \\(\\geq 1\\) to multiply by the variance estimate to make it more conservative.</p> <code>1.2</code> <code>alpha</code> <code>ndarray</code> <p>Uncertainty level in \\((0,1)\\).</p> <code>0.01</code> <code>control_variates</code> <code>list</code> <p>Integrands to use as control variates, each with the same underlying discrete distribution instance.</p> <code>[]</code> <code>control_variate_means</code> <code>ndarray</code> <p>Means of each control variate.</p> <code>[]</code> Source code in <code>qmcpy/stopping_criterion/cub_mc_g.py</code> <pre><code>def __init__(\n    self,\n    integrand,\n    abs_tol=1e-2,\n    rel_tol=0.0,\n    n_init=1024,\n    n_limit=2**30,\n    inflate=1.2,\n    alpha=0.01,\n    control_variates=[],\n    control_variate_means=[],\n):\n    r\"\"\"\n    Args:\n        integrand (AbstractIntegrand): The integrand.\n        abs_tol (np.ndarray): Absolute error tolerance.\n        rel_tol (np.ndarray): Relative error tolerance.\n        n_init (int): Initial number of samples.\n        n_limit (int): Maximum number of samples.\n        inflate (float): Inflation factor $\\geq 1$ to multiply by the variance estimate to make it more conservative.\n        alpha (np.ndarray): Uncertainty level in $(0,1)$.\n        control_variates (list): Integrands to use as control variates, each with the same underlying discrete distribution instance.\n        control_variate_means (np.ndarray): Means of each control variate.\n    \"\"\"\n    self.parameters = [\n        \"abs_tol\",\n        \"rel_tol\",\n        \"n_init\",\n        \"n_limit\",\n        \"inflate\",\n        \"alpha\",\n        \"kurtmax\",\n    ]\n    # Set Attributes\n    self.abs_tol = float(abs_tol)\n    self.rel_tol = float(rel_tol)\n    self.n_init = float(n_init)\n    self.n_limit = float(n_limit)\n    self.alpha = float(alpha)\n    self.inflate = float(inflate)\n    self.alpha_sigma = (\n        float(self.alpha) / 2.0\n    )  # the uncertainty for variance estimation\n    self.kurtmax = (n_init - 3) / (n_init - 1) + (self.alpha_sigma * n_init) / (\n        1 - self.alpha_sigma\n    ) * (1 - 1.0 / self.inflate**2) ** 2\n    # QMCPy Objs\n    self.integrand = integrand\n    self.true_measure = self.integrand.true_measure\n    self.discrete_distrib = self.integrand.discrete_distrib\n    super(CubMCG, self).__init__(\n        allowed_distribs=[AbstractIIDDiscreteDistribution],\n        allow_vectorized_integrals=False,\n    )\n    assert self.integrand.d_indv == ()\n    # control variates\n    self.cv_mu = np.atleast_1d(control_variate_means)\n    self.cv = control_variates\n    if isinstance(self.cv, AbstractIntegrand):\n        self.cv = [self.cv]\n        self.cv_mu = self.cv_mu[None, ...]\n    assert isinstance(\n        self.cv, list\n    ), \"cv must be a list of AbstractIntegrand objects\"\n    for cv in self.cv:\n        if (\n            (not isinstance(cv, AbstractIntegrand))\n            or (cv.discrete_distrib != self.discrete_distrib)\n            or (cv.d_indv != self.integrand.d_indv)\n        ):\n            raise ParameterError(\n                \"\"\"\n                    Each control variates discrete distribution must be an AbstractIntegrand instance \n                    with the same discrete distribution as the main integrand. d_indv must also match \n                    that of the main integrand instance for each control variate.\"\"\"\n            )\n    self.ncv = len(self.cv)\n    if self.ncv &gt; 0:\n        assert self.cv_mu.shape == (\n            (self.ncv,) + self.integrand.d_indv\n        ), \"Control variate means should have shape (len(control variates),d_indv).\"\n        self.parameters += [\"cv\", \"cv_mu\"]\n</code></pre>"},{"location":"api/stopping_criteria/#cubmcclt","title":"<code>CubMCCLT</code>","text":"<p>               Bases: <code>AbstractStoppingCriterion</code></p> <p>IID Monte Carlo stopping criterion based on the Central Limit Theorem in a two step method.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; ao = FinancialOption(IIDStdUniform(52,seed=7))\n&gt;&gt;&gt; sc = CubMCCLT(ao,abs_tol=.05)\n&gt;&gt;&gt; solution,data = sc.integrate()\n&gt;&gt;&gt; data\nData (Data)\n    solution        1.777\n    bound_low       1.723\n    bound_high      1.831\n    bound_diff      0.107\n    n_total         74235\n    time_integrate  ...\nCubMCCLT (AbstractStoppingCriterion)\n    abs_tol         0.050\n    rel_tol         0\n    n_init          2^(10)\n    n_limit         2^(30)\n    inflate         1.200\n    alpha           0.010\nFinancialOption (AbstractIntegrand)\n    option          ASIAN\n    call_put        CALL\n    volatility      2^(-1)\n    start_price     30\n    strike_price    35\n    interest_rate   0\n    t_final         1\n    asian_mean      ARITHMETIC\nGeometricBrownianMotion (AbstractTrueMeasure)\n    time_vec        [0.019 0.038 0.058 ... 0.962 0.981 1.   ]\n    drift           0\n    diffusion       2^(-2)\n    mean_gbm        [30. 30. 30. ... 30. 30. 30.]\n    covariance_gbm  [[  4.337   4.337   4.337 ...   4.337   4.337   4.337]\n                     [  4.337   8.696   8.696 ...   8.696   8.696   8.696]\n                     [  4.337   8.696  13.075 ...  13.075  13.075  13.075]\n                     ...\n                     [  4.337   8.696  13.075 ... 244.564 244.564 244.564]\n                     [  4.337   8.696  13.075 ... 244.564 250.08  250.08 ]\n                     [  4.337   8.696  13.075 ... 244.564 250.08  255.623]]\n    decomp_type     PCA\nIIDStdUniform (AbstractIIDDiscreteDistribution)\n    d               52\n    replications    1\n    entropy         7\n</code></pre> <p>Control variates</p> <pre><code>&gt;&gt;&gt; iid = IIDStdUniform(52,seed=7)\n&gt;&gt;&gt; ao = FinancialOption(iid,option=\"ASIAN\")\n&gt;&gt;&gt; eo = FinancialOption(iid,option=\"EUROPEAN\")\n&gt;&gt;&gt; lin0 = Linear0(iid)\n&gt;&gt;&gt; sc = CubMCCLT(ao,abs_tol=.05,\n...     control_variates = [eo,lin0],\n...     control_variate_means = [eo.get_exact_value(),0])\n&gt;&gt;&gt; solution,data = sc.integrate()\n&gt;&gt;&gt; data\nData (Data)\n    solution        1.790\n    bound_low       1.738\n    bound_high      1.843\n    bound_diff      0.104\n    n_total         27777\n    time_integrate  ...\nCubMCCLT (AbstractStoppingCriterion)\n    abs_tol         0.050\n    rel_tol         0\n    n_init          2^(10)\n    n_limit         2^(30)\n    inflate         1.200\n    alpha           0.010\n    cv              [FinancialOption (AbstractIntegrand)\n                         option          EUROPEAN\n                         call_put        CALL\n                         volatility      2^(-1)\n                         start_price     30\n                         strike_price    35\n                         interest_rate   0\n                         t_final         1               Linear0 (AbstractIntegrand)]\n    cv_mu           [4.211 0.   ]\nFinancialOption (AbstractIntegrand)\n    option          ASIAN\n    call_put        CALL\n    volatility      2^(-1)\n    start_price     30\n    strike_price    35\n    interest_rate   0\n    t_final         1\n    asian_mean      ARITHMETIC\nGeometricBrownianMotion (AbstractTrueMeasure)\n    time_vec        [0.019 0.038 0.058 ... 0.962 0.981 1.   ]\n    drift           0\n    diffusion       2^(-2)\n    mean_gbm        [30. 30. 30. ... 30. 30. 30.]\n    covariance_gbm  [[  4.337   4.337   4.337 ...   4.337   4.337   4.337]\n                     [  4.337   8.696   8.696 ...   8.696   8.696   8.696]\n                     [  4.337   8.696  13.075 ...  13.075  13.075  13.075]\n                     ...\n                     [  4.337   8.696  13.075 ... 244.564 244.564 244.564]\n                     [  4.337   8.696  13.075 ... 244.564 250.08  250.08 ]\n                     [  4.337   8.696  13.075 ... 244.564 250.08  255.623]]\n    decomp_type     PCA\nIIDStdUniform (AbstractIIDDiscreteDistribution)\n    d               52\n    replications    1\n    entropy         7\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>integrand</code> <code>AbstractIntegrand</code> <p>The integrand.</p> required <code>abs_tol</code> <code>ndarray</code> <p>Absolute error tolerance.</p> <code>0.01</code> <code>rel_tol</code> <code>ndarray</code> <p>Relative error tolerance.</p> <code>0.0</code> <code>n_init</code> <code>int</code> <p>Initial number of samples.</p> <code>1024</code> <code>n_limit</code> <code>int</code> <p>Maximum number of samples.</p> <code>2 ** 30</code> <code>inflate</code> <code>float</code> <p>Inflation factor \\(\\geq 1\\) to multiply by the variance estimate to make it more conservative.</p> <code>1.2</code> <code>alpha</code> <code>ndarray</code> <p>Uncertainty level in \\((0,1)\\).</p> <code>0.01</code> <code>control_variates</code> <code>list</code> <p>Integrands to use as control variates, each with the same underlying discrete distribution instance.</p> <code>[]</code> <code>control_variate_means</code> <code>ndarray</code> <p>Means of each control variate.</p> <code>[]</code> Source code in <code>qmcpy/stopping_criterion/cub_mc_clt.py</code> <pre><code>def __init__(\n    self,\n    integrand,\n    abs_tol=1e-2,\n    rel_tol=0.0,\n    n_init=1024,\n    n_limit=2**30,\n    inflate=1.2,\n    alpha=0.01,\n    control_variates=[],\n    control_variate_means=[],\n):\n    r\"\"\"\n    Args:\n        integrand (AbstractIntegrand): The integrand.\n        abs_tol (np.ndarray): Absolute error tolerance.\n        rel_tol (np.ndarray): Relative error tolerance.\n        n_init (int): Initial number of samples.\n        n_limit (int): Maximum number of samples.\n        inflate (float): Inflation factor $\\geq 1$ to multiply by the variance estimate to make it more conservative.\n        alpha (np.ndarray): Uncertainty level in $(0,1)$.\n        control_variates (list): Integrands to use as control variates, each with the same underlying discrete distribution instance.\n        control_variate_means (np.ndarray): Means of each control variate.\n    \"\"\"\n    self.parameters = [\n        \"abs_tol\",\n        \"rel_tol\",\n        \"n_init\",\n        \"n_limit\",\n        \"inflate\",\n        \"alpha\",\n    ]\n    # Set Attributes\n    self.abs_tol = abs_tol\n    self.rel_tol = rel_tol\n    self.n_init = n_init\n    self.n_limit = n_limit\n    assert self.n_limit &gt; (\n        2 * self.n_init\n    ), \"require n_limit is at least twic as much as n_init\"\n    self.alpha = alpha\n    self.inflate = inflate\n    assert self.inflate &gt;= 1\n    assert 0 &lt; self.alpha &lt; 1\n    # QMCPy Objs\n    self.integrand = integrand\n    self.true_measure = self.integrand.true_measure\n    self.discrete_distrib = self.integrand.discrete_distrib\n    super(CubMCCLT, self).__init__(\n        allowed_distribs=[AbstractIIDDiscreteDistribution],\n        allow_vectorized_integrals=True,\n    )\n    assert self.integrand.d_indv == ()\n    # control variates\n    self.cv_mu = np.atleast_1d(control_variate_means)\n    self.cv = control_variates\n    if isinstance(self.cv, AbstractIntegrand):\n        self.cv = [self.cv]\n        self.cv_mu = self.cv_mu[None, ...]\n    assert isinstance(\n        self.cv, list\n    ), \"cv must be a list of AbstractIntegrand objects\"\n    for cv in self.cv:\n        if (\n            (not isinstance(cv, AbstractIntegrand))\n            or (cv.discrete_distrib != self.discrete_distrib)\n            or (cv.d_indv != self.integrand.d_indv)\n        ):\n            raise ParameterError(\n                \"\"\"\n                    Each control variates discrete distribution must be an AbstractIntegrand instance \n                    with the same discrete distribution as the main integrand. d_indv must also match \n                    that of the main integrand instance for each control variate.\"\"\"\n            )\n    self.ncv = len(self.cv)\n    if self.ncv &gt; 0:\n        assert self.cv_mu.shape == (\n            (self.ncv,) + self.integrand.d_indv\n        ), \"Control variate means should have shape (len(control variates),d_indv).\"\n        self.parameters += [\"cv\", \"cv_mu\"]\n    self.z_star = -norm.ppf(self.alpha / 2.0)\n</code></pre>"},{"location":"api/stopping_criteria/#multilevel-qmc","title":"Multilevel QMC","text":""},{"location":"api/stopping_criteria/#cubmlqmccont","title":"<code>CubMLQMCCont</code>","text":"<p>               Bases: <code>AbstractCubMLQMC</code></p> <p>Multilevel Quasi-Monte Carlo stopping criterion with continuation.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; fo = FinancialOption(DigitalNetB2(seed=7,replications=32))\n&gt;&gt;&gt; sc = CubMLQMCCont(fo,abs_tol=1e-3)\n&gt;&gt;&gt; solution,data = sc.integrate()\n&gt;&gt;&gt; data\nData (Data)\n    solution        1.784\n    n_total         4718592\n    levels          2^(2)\n    n_level         [65536 32768 32768 16384]\n    mean_level      [1.718 0.051 0.012 0.003]\n    var_level       [1.169e-08 2.569e-08 1.850e-08 5.209e-08]\n    bias_estimate   2.78e-04\n    time_integrate  ...\nCubMLQMCCont (AbstractStoppingCriterion)\n    rmse_tol        3.88e-04\n    n_init          2^(8)\n    n_limit         10000000000\n    replications    2^(5)\n    levels_min      2^(1)\n    levels_max      10\n    n_tols          10\n    inflate         1.668\n    theta_init      2^(-1)\n    theta           2^(-3)\nFinancialOption (AbstractIntegrand)\n    option          ASIAN\n    call_put        CALL\n    volatility      2^(-1)\n    start_price     30\n    strike_price    35\n    interest_rate   0\n    t_final         1\n    asian_mean      ARITHMETIC\nGeometricBrownianMotion (AbstractTrueMeasure)\n    time_vec        1\n    drift           0\n    diffusion       2^(-2)\n    mean_gbm        30\n    covariance_gbm  255.623\n    decomp_type     PCA\nDigitalNetB2 (AbstractLDDiscreteDistribution)\n    d               1\n    replications    2^(5)\n    randomize       LMS DS\n    gen_mats_source joe_kuo.6.21201.txt\n    order           RADICAL INVERSE\n    t               63\n    alpha           1\n    n_limit         2^(32)\n    entropy         7\n</code></pre> <p>References:</p> <ol> <li>https://github.com/PieterjanRobbe/MultilevelEstimators.jl.</li> </ol> <p>Parameters:</p> Name Type Description Default <code>integrand</code> <code>AbstractIntegrand</code> <p>The integrand.</p> required <code>abs_tol</code> <code>ndarray</code> <p>Absolute error tolerance.</p> <code>0.05</code> <code>rmse_tol</code> <code>ndarray</code> <p>Root mean squared error tolerance. If supplied, then absolute tolerance and alpha are ignored in favor of the rmse tolerance.</p> <code>None</code> <code>n_init</code> <code>int</code> <p>Initial number of samples.</p> <code>256</code> <code>n_limit</code> <code>int</code> <p>Maximum number of samples.</p> <code>10000000000.0</code> <code>inflate</code> <code>float</code> <p>Coarser tolerance multiplication factor \\(\\geq 1\\).</p> <code>100 ** (1 / 9)</code> <code>alpha</code> <code>ndarray</code> <p>Uncertainty level in \\((0,1)\\).</p> <code>0.01</code> <code>levels_min</code> <code>int</code> <p>Minimum level of refinement \\(\\geq 2\\).</p> <code>2</code> <code>levels_max</code> <code>int</code> <p>Maximum level of refinement \\(\\geq\\) <code>levels_min</code>.</p> <code>10</code> <code>n_tols</code> <code>int</code> <p>Number of coarser tolerances to run.</p> <code>10</code> <code>theta_init</code> <code>float</code> <p>Initial error splitting constant.</p> <code>0.5</code> Source code in <code>qmcpy/stopping_criterion/cub_mlqmc_cont.py</code> <pre><code>def __init__(\n    self,\n    integrand,\n    abs_tol=0.05,\n    rmse_tol=None,\n    n_init=256,\n    n_limit=1e10,\n    inflate=100 ** (1 / 9),\n    alpha=0.01,\n    levels_min=2,\n    levels_max=10,\n    n_tols=10,\n    theta_init=0.5,\n):\n    r\"\"\"\n    Args:\n        integrand (AbstractIntegrand): The integrand.\n        abs_tol (np.ndarray): Absolute error tolerance.\n        rmse_tol (np.ndarray): Root mean squared error tolerance.\n            If supplied, then absolute tolerance and alpha are ignored in favor of the rmse tolerance.\n        n_init (int): Initial number of samples.\n        n_limit (int): Maximum number of samples.\n        inflate (float): Coarser tolerance multiplication factor $\\geq 1$.\n        alpha (np.ndarray): Uncertainty level in $(0,1)$.\n        levels_min (int): Minimum level of refinement $\\geq 2$.\n        levels_max (int): Maximum level of refinement $\\geq$ `levels_min`.\n        n_tols (int): Number of coarser tolerances to run.\n        theta_init (float): Initial error splitting constant.\n    \"\"\"\n    self.parameters = [\n        \"rmse_tol\",\n        \"n_init\",\n        \"n_limit\",\n        \"replications\",\n        \"levels_min\",\n        \"levels_max\",\n        \"n_tols\",\n        \"inflate\",\n        \"theta_init\",\n        \"theta\",\n    ]\n    # initialization\n    if rmse_tol:\n        self.target_tol = float(rmse_tol)\n    else:  # use absolute tolerance\n        self.target_tol = float(abs_tol) / norm.ppf(1 - alpha / 2)\n    self.n_init = n_init\n    self.n_limit = n_limit\n    self.levels_min = levels_min\n    self.levels_max = levels_max\n    self.theta_init = theta_init\n    self.theta = theta_init\n    self.n_tols = n_tols\n    self.alpha = alpha\n    self.inflate = inflate\n    assert self.inflate &gt;= 1\n    assert 0 &lt; self.alpha &lt; 1\n    # QMCPy Objs\n    self.integrand = integrand\n    self.true_measure = self.integrand.true_measure\n    self.discrete_distrib = self.integrand.discrete_distrib\n    super(CubMLQMCCont, self).__init__(\n        allowed_distribs=[AbstractLDDiscreteDistribution],\n        allow_vectorized_integrals=False,\n    )\n    self.replications = self.discrete_distrib.replications\n    assert self.replications &gt;= 4, \"require at least 4 replications\"\n</code></pre>"},{"location":"api/stopping_criteria/#cubmlqmc","title":"<code>CubMLQMC</code>","text":"<p>               Bases: <code>AbstractCubMLQMC</code></p> <p>Multilevel Quasi-Monte Carlo stopping criterion.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; fo = FinancialOption(DigitalNetB2(seed=7,replications=32))\n&gt;&gt;&gt; sc = CubMLQMC(fo,abs_tol=3e-3)\n&gt;&gt;&gt; solution,data = sc.integrate()\n&gt;&gt;&gt; data\nData (Data)\n    solution        1.784\n    n_total         2359296\n    levels          2^(2)\n    n_level         [32768 16384 16384  8192]\n    mean_level      [1.718 0.051 0.012 0.003]\n    var_level       [7.119e-08 1.409e-07 9.668e-08 1.852e-07]\n    bias_estimate   2.99e-04\n    time_integrate  ...\nCubMLQMC (AbstractStoppingCriterion)\n    rmse_tol        0.001\n    n_init          2^(8)\n    n_limit         10000000000\n    replications    2^(5)\nFinancialOption (AbstractIntegrand)\n    option          ASIAN\n    call_put        CALL\n    volatility      2^(-1)\n    start_price     30\n    strike_price    35\n    interest_rate   0\n    t_final         1\n    asian_mean      ARITHMETIC\nGeometricBrownianMotion (AbstractTrueMeasure)\n    time_vec        1\n    drift           0\n    diffusion       2^(-2)\n    mean_gbm        30\n    covariance_gbm  255.623\n    decomp_type     PCA\nDigitalNetB2 (AbstractLDDiscreteDistribution)\n    d               1\n    replications    2^(5)\n    randomize       LMS DS\n    gen_mats_source joe_kuo.6.21201.txt\n    order           RADICAL INVERSE\n    t               63\n    alpha           1\n    n_limit         2^(32)\n    entropy         7\n</code></pre> <p>References:</p> <ol> <li>M.B. Giles and B.J. Waterhouse.     'Multilevel quasi-Monte Carlo path simulation'.     pp.165-181 in Advanced Financial Modelling, in Radon Series on Computational and Applied Mathematics, de Gruyter, 2009.     http://people.maths.ox.ac.uk/~gilesm/files/radon.pdf.</li> </ol> <p>Parameters:</p> Name Type Description Default <code>integrand</code> <code>AbstractIntegrand</code> <p>The integrand.</p> required <code>abs_tol</code> <code>ndarray</code> <p>Absolute error tolerance.</p> <code>0.05</code> <code>rmse_tol</code> <code>ndarray</code> <p>Root mean squared error tolerance. If supplied, then absolute tolerance and alpha are ignored in favor of the rmse tolerance.</p> <code>None</code> <code>n_init</code> <code>int</code> <p>Initial number of samples.</p> <code>256</code> <code>n_limit</code> <code>int</code> <p>Maximum number of samples.</p> <code>10000000000.0</code> <code>alpha</code> <code>ndarray</code> <p>Uncertainty level in \\((0,1)\\).</p> <code>0.01</code> <code>levels_min</code> <code>int</code> <p>Minimum level of refinement \\(\\geq 2\\).</p> <code>2</code> <code>levels_max</code> <code>int</code> <p>Maximum level of refinement \\(\\geq\\) <code>levels_min</code>.</p> <code>10</code> Source code in <code>qmcpy/stopping_criterion/cub_mlqmc.py</code> <pre><code>def __init__(\n    self,\n    integrand,\n    abs_tol=0.05,\n    rmse_tol=None,\n    n_init=256,\n    n_limit=1e10,\n    alpha=0.01,\n    levels_min=2,\n    levels_max=10,\n):\n    r\"\"\"\n    Args:\n        integrand (AbstractIntegrand): The integrand.\n        abs_tol (np.ndarray): Absolute error tolerance.\n        rmse_tol (np.ndarray): Root mean squared error tolerance.\n            If supplied, then absolute tolerance and alpha are ignored in favor of the rmse tolerance.\n        n_init (int): Initial number of samples.\n        n_limit (int): Maximum number of samples.\n        alpha (np.ndarray): Uncertainty level in $(0,1)$.\n        levels_min (int): Minimum level of refinement $\\geq 2$.\n        levels_max (int): Maximum level of refinement $\\geq$ `levels_min`.\n    \"\"\"\n    self.parameters = [\"rmse_tol\", \"n_init\", \"n_limit\", \"replications\"]\n    # initialization\n    if rmse_tol:\n        self.rmse_tol = float(rmse_tol)\n    else:  # use absolute tolerance\n        self.rmse_tol = float(abs_tol) / norm.ppf(1 - alpha / 2)\n    self.alpha = alpha\n    assert 0 &lt; self.alpha &lt; 1\n    self.n_init = n_init\n    self.n_limit = n_limit\n    self.levels_min = levels_min\n    self.levels_max = levels_max\n    # QMCPy Objs\n    self.integrand = integrand\n    self.true_measure = self.integrand.true_measure\n    self.discrete_distrib = self.integrand.discrete_distrib\n    super(CubMLQMC, self).__init__(\n        allowed_distribs=[AbstractLDDiscreteDistribution],\n        allow_vectorized_integrals=False,\n    )\n    self.replications = self.discrete_distrib.replications\n    assert self.replications &gt;= 4, \"require at least 4 replications\"\n</code></pre>"},{"location":"api/stopping_criteria/#multilevel-iid-mc","title":"Multilevel IID MC","text":""},{"location":"api/stopping_criteria/#cubmlmccont","title":"<code>CubMLMCCont</code>","text":"<p>               Bases: <code>AbstractCubMLMC</code></p> <p>Multilevel IID Monte Carlo stopping criterion with continuation.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; fo = FinancialOption(IIDStdUniform(seed=7))\n&gt;&gt;&gt; sc = CubMLMCCont(fo,abs_tol=1.5e-2)\n&gt;&gt;&gt; solution,data = sc.integrate()\n&gt;&gt;&gt; data\nData (Data)\n    solution        1.771\n    n_total         2291120\n    levels          3\n    n_level         [1094715  222428   79666     912     256]\n    mean_level      [1.71  0.048 0.012]\n    var_level       [21.826  1.768  0.453]\n    cost_per_sample [2. 4. 8.]\n    alpha           1.970\n    beta            1.965\n    gamma           1.000\n    time_integrate  ...\nCubMLMCCont (AbstractStoppingCriterion)\n    rmse_tol        0.006\n    n_init          2^(8)\n    levels_min      2^(1)\n    levels_max      10\n    n_tols          10\n    inflate         1.668\n    theta_init      2^(-1)\n    theta           0.010\nFinancialOption (AbstractIntegrand)\n    option          ASIAN\n    call_put        CALL\n    volatility      2^(-1)\n    start_price     30\n    strike_price    35\n    interest_rate   0\n    t_final         1\n    asian_mean      ARITHMETIC\nGeometricBrownianMotion (AbstractTrueMeasure)\n    time_vec        1\n    drift           0\n    diffusion       2^(-2)\n    mean_gbm        30\n    covariance_gbm  255.623\n    decomp_type     PCA\nIIDStdUniform (AbstractIIDDiscreteDistribution)\n    d               1\n    replications    1\n    entropy         7\n</code></pre> <p>References:</p> <ol> <li>https://github.com/PieterjanRobbe/MultilevelEstimators.jl.</li> </ol> <p>Parameters:</p> Name Type Description Default <code>integrand</code> <code>AbstractIntegrand</code> <p>The integrand.</p> required <code>abs_tol</code> <code>ndarray</code> <p>Absolute error tolerance.</p> <code>0.05</code> <code>rmse_tol</code> <code>ndarray</code> <p>Root mean squared error tolerance. If supplied, then absolute tolerance and alpha are ignored in favor of the rmse tolerance.</p> <code>None</code> <code>n_init</code> <code>int</code> <p>Initial number of samples.</p> <code>256</code> <code>n_limit</code> <code>int</code> <p>Maximum number of samples.</p> <code>10000000000.0</code> <code>inflate</code> <code>float</code> <p>Coarser tolerance multiplication factor \\(\\geq 1\\).</p> <code>100 ** (1 / 9)</code> <code>alpha</code> <code>ndarray</code> <p>Uncertainty level in \\((0,1)\\).</p> <code>0.01</code> <code>levels_min</code> <code>int</code> <p>Minimum level of refinement \\(\\geq 2\\).</p> <code>2</code> <code>levels_max</code> <code>int</code> <p>Maximum level of refinement \\(\\geq\\) <code>levels_min</code>.</p> <code>10</code> <code>n_tols</code> <code>int</code> <p>Number of coarser tolerances to run.</p> <code>10</code> <code>theta_init</code> <code>float</code> <p>Initial error splitting constant.</p> <code>0.5</code> Source code in <code>qmcpy/stopping_criterion/cub_mlmc_cont.py</code> <pre><code>def __init__(\n    self,\n    integrand,\n    abs_tol=0.05,\n    rmse_tol=None,\n    n_init=256,\n    n_limit=1e10,\n    inflate=100 ** (1 / 9),\n    alpha=0.01,\n    levels_min=2,\n    levels_max=10,\n    n_tols=10,\n    theta_init=0.5,\n):\n    r\"\"\"\n    Args:\n        integrand (AbstractIntegrand): The integrand.\n        abs_tol (np.ndarray): Absolute error tolerance.\n        rmse_tol (np.ndarray): Root mean squared error tolerance.\n            If supplied, then absolute tolerance and alpha are ignored in favor of the rmse tolerance.\n        n_init (int): Initial number of samples.\n        n_limit (int): Maximum number of samples.\n        inflate (float): Coarser tolerance multiplication factor $\\geq 1$.\n        alpha (np.ndarray): Uncertainty level in $(0,1)$.\n        levels_min (int): Minimum level of refinement $\\geq 2$.\n        levels_max (int): Maximum level of refinement $\\geq$ `levels_min`.\n        n_tols (int): Number of coarser tolerances to run.\n        theta_init (float): Initial error splitting constant.\n    \"\"\"\n    self.parameters = [\n        \"rmse_tol\",\n        \"n_init\",\n        \"levels_min\",\n        \"levels_max\",\n        \"n_tols\",\n        \"inflate\",\n        \"theta_init\",\n        \"theta\",\n    ]\n    if levels_min &lt; 2:\n        raise ParameterError(\"needs levels_min &gt;= 2\")\n    if levels_max &lt; levels_min:\n        raise ParameterError(\"needs levels_max &gt;= levels_min\")\n    if n_init &lt;= 0:\n        raise ParameterError(\"needs n_init &gt; 0\")\n    # initialization\n    if rmse_tol:\n        self.target_tol = float(rmse_tol)\n    else:  # use absolute tolerance\n        self.target_tol = float(abs_tol) / norm.ppf(1 - alpha / 2)\n    self.n_init = n_init\n    self.n_limit = n_limit\n    self.levels_min = levels_min\n    self.levels_max = levels_max\n    self.theta_init = theta_init\n    self.theta = theta_init\n    self.n_tols = n_tols\n    self.inflate = inflate\n    self.integrand = integrand\n    self.true_measure = self.integrand.true_measure\n    self.discrete_distrib = self.integrand.discrete_distrib\n    self.alpha0 = -1\n    self.beta0 = -1\n    self.gamma0 = -1\n    self.alpha = alpha\n    self.inflate = inflate\n    assert self.inflate &gt;= 1\n    assert 0 &lt; self.alpha &lt; 1\n    super(CubMLMCCont, self).__init__(\n        allowed_distribs=[AbstractIIDDiscreteDistribution],\n        allow_vectorized_integrals=False,\n    )\n</code></pre>"},{"location":"api/stopping_criteria/#cubmlmc","title":"<code>CubMLMC</code>","text":"<p>               Bases: <code>AbstractCubMLMC</code></p> <p>Multilevel IID Monte Carlo stopping criterion.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; fo = FinancialOption(IIDStdUniform(seed=7))\n&gt;&gt;&gt; sc = CubMLMC(fo,abs_tol=1.5e-2)\n&gt;&gt;&gt; solution,data = sc.integrate()\n&gt;&gt;&gt; data\nData (Data)\n    solution        1.785\n    n_total         3577556\n    levels          2^(2)\n    n_level         [2438191  490331  207606   62905]\n    mean_level      [1.715 0.053 0.013 0.003]\n    var_level       [21.829  1.761  0.452  0.11 ]\n    cost_per_sample [ 2.  4.  8. 16.]\n    alpha           2.008\n    beta            1.997\n    gamma           1.000\n    time_integrate  ...\nCubMLMC (AbstractStoppingCriterion)\n    rmse_tol        0.006\n    n_init          2^(8)\n    levels_min      2^(1)\n    levels_max      10\n    theta           2^(-1)\nFinancialOption (AbstractIntegrand)\n    option          ASIAN\n    call_put        CALL\n    volatility      2^(-1)\n    start_price     30\n    strike_price    35\n    interest_rate   0\n    t_final         1\n    asian_mean      ARITHMETIC\nGeometricBrownianMotion (AbstractTrueMeasure)\n    time_vec        1\n    drift           0\n    diffusion       2^(-2)\n    mean_gbm        30\n    covariance_gbm  255.623\n    decomp_type     PCA\nIIDStdUniform (AbstractIIDDiscreteDistribution)\n    d               1\n    replications    1\n    entropy         7\n</code></pre> <p>References:</p> <ol> <li> <p>M.B. Giles. 'Multi-level Monte Carlo path simulation'.     Operations Research, 56(3):607-617, 2008.     http://people.maths.ox.ac.uk/~gilesm/files/OPRE_2008.pdf.</p> </li> <li> <p>http://people.maths.ox.ac.uk/~gilesm/mlmc/#MATLAB.</p> </li> </ol> <p>Parameters:</p> Name Type Description Default <code>integrand</code> <code>AbstractIntegrand</code> <p>The integrand.</p> required <code>abs_tol</code> <code>ndarray</code> <p>Absolute error tolerance.</p> <code>0.05</code> <code>rmse_tol</code> <code>ndarray</code> <p>Root mean squared error tolerance. If supplied, then absolute tolerance and alpha are ignored in favor of the rmse tolerance.</p> <code>None</code> <code>n_init</code> <code>int</code> <p>Initial number of samples.</p> <code>256</code> <code>n_limit</code> <code>int</code> <p>Maximum number of samples.</p> <code>10000000000.0</code> <code>alpha</code> <code>ndarray</code> <p>Uncertainty level in \\((0,1)\\).</p> <code>0.01</code> <code>levels_min</code> <code>int</code> <p>Minimum level of refinement \\(\\geq 2\\).</p> <code>2</code> <code>levels_max</code> <code>int</code> <p>Maximum level of refinement \\(\\geq\\) <code>levels_min</code>.</p> <code>10</code> <code>alpha0</code> <code>float</code> <p>Weak error is \\(\\mathcal{O}(2^{-\\alpha_0\\ell})\\) in the level \\(\\ell\\). If <code>alpha0</code>\\(\\leq 0\\) then it will be estimated.</p> <code>-1.0</code> <code>beta0</code> <code>float</code> <p>Variance is \\(\\mathcal{O}(2^{-\\beta_0\\ell})\\) in the level \\(\\ell\\). If <code>beta0</code>\\(\\leq 0\\) then it will be estimated.</p> <code>-1.0</code> <code>gamma0</code> <code>float</code> <p>Sample cost is \\(\\mathcal{O}(2^{\\gamma_0\\ell})\\) in the level \\(\\ell\\). If <code>gamma0</code>\\(\\leq 0\\) then it will be estimated.</p> <code>-1.0</code> Source code in <code>qmcpy/stopping_criterion/cub_mlmc.py</code> <pre><code>def __init__(\n    self,\n    integrand,\n    abs_tol=0.05,\n    rmse_tol=None,\n    n_init=256,\n    n_limit=1e10,\n    alpha=0.01,\n    levels_min=2,\n    levels_max=10,\n    alpha0=-1.0,\n    beta0=-1.0,\n    gamma0=-1.0,\n):\n    r\"\"\"\n    Args:\n        integrand (AbstractIntegrand): The integrand.\n        abs_tol (np.ndarray): Absolute error tolerance.\n        rmse_tol (np.ndarray): Root mean squared error tolerance.\n            If supplied, then absolute tolerance and alpha are ignored in favor of the rmse tolerance.\n        n_init (int): Initial number of samples.\n        n_limit (int): Maximum number of samples.\n        alpha (np.ndarray): Uncertainty level in $(0,1)$.\n        levels_min (int): Minimum level of refinement $\\geq 2$.\n        levels_max (int): Maximum level of refinement $\\geq$ `levels_min`.\n        alpha0 (float): Weak error is $\\mathcal{O}(2^{-\\alpha_0\\ell})$ in the level $\\ell$. If `alpha0`$\\leq 0$ then it will be estimated.\n        beta0 (float): Variance is $\\mathcal{O}(2^{-\\beta_0\\ell})$ in the level $\\ell$. If `beta0`$\\leq 0$ then it will be estimated.\n        gamma0 (float): Sample cost is $\\mathcal{O}(2^{\\gamma_0\\ell})$ in the level $\\ell$. If `gamma0`$\\leq 0$ then it will be estimated.\n    \"\"\"\n    self.parameters = [\"rmse_tol\", \"n_init\", \"levels_min\", \"levels_max\", \"theta\"]\n    if levels_min &lt; 2:\n        raise ParameterError(\"needs levels_min &gt;= 2\")\n    if levels_max &lt; levels_min:\n        raise ParameterError(\"needs levels_max &gt;= levels_min\")\n    if n_init &lt;= 0:\n        raise ParameterError(\"needs n_init &gt; 0\")\n    # initialization\n    if rmse_tol:\n        self.rmse_tol = float(rmse_tol)\n    else:  # use absolute tolerance\n        self.rmse_tol = float(abs_tol) / norm.ppf(1 - alpha / 2)\n    self.alpha = alpha\n    assert 0 &lt; self.alpha &lt; 1\n    self.n_init = n_init\n    self.n_limit = n_limit\n    self.levels_min = levels_min\n    self.levels_max = levels_max\n    self.theta = 0.5\n    self.alpha0 = alpha0\n    self.beta0 = beta0\n    self.gamma0 = gamma0\n    # QMCPy Objs\n    self.integrand = integrand\n    self.true_measure = self.integrand.true_measure\n    self.discrete_distrib = self.integrand.discrete_distrib\n    super(CubMLMC, self).__init__(\n        allowed_distribs=[AbstractIIDDiscreteDistribution],\n        allow_vectorized_integrals=False,\n    )\n</code></pre>"},{"location":"api/stopping_criteria/#failure-probability","title":"Failure Probability","text":""},{"location":"api/stopping_criteria/#pfgpci","title":"<code>PFGPCI</code>","text":"<p>               Bases: <code>AbstractStoppingCriterion</code></p> <p>Probability of failure estimation using adaptive Gaussian process construction and resulting credible intervals.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; pfgpci = PFGPCI(\n...     integrand = Ishigami(DigitalNetB2(3,seed=17)),\n...     failure_threshold = 0,\n...     failure_above_threshold = False,\n...     abs_tol = 2.5e-2,\n...     alpha = 1e-1,\n...     n_init = 64,\n...     init_samples = None,\n...     batch_sampler = PFSampleErrorDensityAR(verbose=False),\n...     n_batch = 16,\n...     n_limit = 128,\n...     n_approx = 2**18,\n...     gpytorch_prior_mean = gpytorch.means.ZeroMean(),\n...     gpytorch_prior_cov = gpytorch.kernels.ScaleKernel(\n...         gpytorch.kernels.MaternKernel(nu=2.5,lengthscale_constraint = gpytorch.constraints.Interval(.5,1)),\n...         outputscale_constraint = gpytorch.constraints.Interval(1e-8,.5)),\n...     gpytorch_likelihood = gpytorch.likelihoods.GaussianLikelihood(noise_constraint = gpytorch.constraints.Interval(1e-12,1e-8)),\n...     gpytorch_marginal_log_likelihood_func = lambda likelihood,gpyt_model: gpytorch.mlls.ExactMarginalLogLikelihood(likelihood,gpyt_model),\n...     torch_optimizer_func = lambda gpyt_model: torch.optim.Adam(gpyt_model.parameters(),lr=0.1),\n...     gpytorch_train_iter = 100,\n...     gpytorch_use_gpu = False,\n...     verbose = False,\n...     n_ref_approx = 2**22,\n...     seed_ref_approx = 11)\n&gt;&gt;&gt; solution,data = pfgpci.integrate(seed=7,refit=True)\n&gt;&gt;&gt; data\nPFGPCIData (Data)\n    solution        0.158\n    error_bound     0.022\n    bound_low       0.136\n    bound_high      0.180\n    n_total         112\n    time_integrate  ...\nPFGPCI (AbstractStoppingCriterion)\n    abs_tol         0.025\n    n_init          2^(6)\n    n_limit         2^(7)\n    n_batch         2^(4)\nIshigami (AbstractIntegrand)\nUniform (AbstractTrueMeasure)\n    lower_bound     -3.142\n    upper_bound     3.142\nDigitalNetB2 (AbstractLDDiscreteDistribution)\n    d               3\n    replications    1\n    randomize       LMS DS\n    gen_mats_source joe_kuo.6.21201.txt\n    order           RADICAL INVERSE\n    t               63\n    alpha           1\n    n_limit         2^(32)\n    entropy         17\n&gt;&gt;&gt; df = data.get_results_dict()\n&gt;&gt;&gt; with np.printoptions(formatter={\"float\": lambda x: \"%-10.2e\"%x, \"int\": lambda x: \"%-10d\"%x, \"bool\": lambda x: \"%-10s\"%x}):\n...     for k,v in df.items():\n...         print(\"%15s: %s\"%(k,str(v)))\n           iter: [0          1          2          3         ]\n          n_sum: [64         80         96         112       ]\n        n_batch: [64         16         16         16        ]\n   error_bounds: [5.58e-02   3.92e-02   3.05e-02   2.16e-02  ]\n         ci_low: [8.66e-02   1.16e-01   1.19e-01   1.36e-01  ]\n        ci_high: [1.98e-01   1.95e-01   1.80e-01   1.80e-01  ]\n      solutions: [1.42e-01   1.55e-01   1.50e-01   1.58e-01  ]\n  solutions_ref: [1.62e-01   1.62e-01   1.62e-01   1.62e-01  ]\n      error_ref: [2.01e-02   7.02e-03   1.28e-02   4.52e-03  ]\n          in_ci: [True       True       True       True      ]\n</code></pre> <p>References:</p> <ol> <li>Sorokin, Aleksei G., and Vishwas Rao.     \"Credible Intervals for Probability of Failure with Gaussian Processes.\"     arXiv preprint arXiv:2311.07733 (2023).</li> </ol> <p>Parameters:</p> Name Type Description Default <code>integrand</code> <code>AbstractIntegrand</code> <p>The integrand.</p> required <code>failure_threshold</code> <code>float</code> <p>Thresholds for failure.</p> required <code>failure_above_threshold</code> <code>bool</code> <p>Set to <code>True</code> if failure occurs when the simulation exceeds <code>failure_threshold</code> and False otherwise.</p> required <code>abs_tol</code> <code>float</code> <p>The desired maximum distance from the estimate to either end of the credible interval.</p> <code>0.005</code> <code>n_init</code> <code>float</code> <p>Initial number of samples from integrand.discrete_distrib from which to build the first surrogate GP</p> <code>64</code> <code>n_limit</code> <code>int</code> <p>Budget of simulations.</p> <code>1000</code> <code>n_batch</code> <code>int</code> <p>The number of samples per batch to draw from batch_sampler.</p> <code>4</code> <code>alpha</code> <code>float</code> <p>The credible interval is constructed to hold with probability at least 1 - alpha</p> <code>0.01</code> <code>init_samples</code> <code>float</code> <p>If the simulation has already been run, pass in (x,y) where x are past samples from the discrete distribution and y are corresponding simulation evaluations.</p> <code>None</code> <code>batch_sampler</code> <code>Suggester or AbstractDiscreteDistribution</code> <p>A suggestion scheme for future samples.</p> <code>PFSampleErrorDensityAR()</code> <code>n_approx</code> <code>int</code> <p>Number of points from integrand.discrete_distrib used to approximate estimate and credible interval bounds</p> <code>2 ** 20</code> <code>gpytorch_prior_mean</code> <code>means</code> <p>prior mean function of the GP</p> <code>ZeroMean()</code> <code>gpytorch_prior_cov</code> <code>kernels</code> <p>Prior covariance kernel of the GP</p> <code>ScaleKernel(MaternKernel(nu=2.5))</code> <code>gpytorch_likelihood</code> <code>likelihoods</code> <p>GP likelihood, require one of gpytorch.likelihoods.{GaussianLikelihood, GaussianLikelihoodWithMissingObs, FixedNoiseGaussianLikelihood}</p> <code>GaussianLikelihood(noise_constraint=Interval(1e-12, 1e-08))</code> <code>gpytorch_marginal_log_likelihood_func</code> <code>callable</code> <p>Function taking in the likelihood and gpytorch model and returning a marginal log likelihood from gpytorch.mlls</p> <code>lambda likelihood, gpyt_model: ExactMarginalLogLikelihood(likelihood, gpyt_model)</code> <code>torch_optimizer_func</code> <code>callable</code> <p>Function taking in the gpytorch model and returning an optimizer from torch.optim</p> <code>lambda gpyt_model: Adam(parameters(), lr=0.1)</code> <code>gpytorch_train_iter</code> <code>int</code> <p>Training iterations for the GP in gpytorch</p> <code>100</code> <code>gpytorch_use_gpu</code> <code>bool</code> <p>If True, have gpytorch use a GPU for fitting and trining the GP</p> <code>False</code> <code>verbose</code> <code>int</code> <p>If verbose &gt; 0, print information through the call to integrate()</p> <code>False</code> <code>n_ref_approx</code> <code>int</code> <p>If n_ref_approx &gt; 0, use n_ref_approx points to get a reference QMC approximation of the true solution. Caution: If n_ref_approx &gt; 0, it should be a large int e.g. 2**22, in which case it is only helpful for cheap to evaluate simulations</p> <code>2 ** 22</code> <code>seed_ref_approx</code> <code>int</code> <p>Seed for the reference approximation. Only applies when n_ref_approx&gt;0</p> <code>None</code> Source code in <code>qmcpy/stopping_criterion/pf_gp_ci.py</code> <pre><code>def __init__(\n    self,\n    integrand,\n    failure_threshold,\n    failure_above_threshold,\n    abs_tol=5e-3,\n    n_init=64,\n    n_limit=1000,\n    alpha=1e-2,\n    init_samples=None,\n    batch_sampler=PFSampleErrorDensityAR(),\n    n_batch=4,\n    n_approx=2**20,\n    gpytorch_prior_mean=gpytorch.means.ZeroMean(),\n    gpytorch_prior_cov=gpytorch.kernels.ScaleKernel(\n        gpytorch.kernels.MaternKernel(nu=2.5)\n    ),\n    gpytorch_likelihood=gpytorch.likelihoods.GaussianLikelihood(\n        noise_constraint=gpytorch.constraints.Interval(1e-12, 1e-8)\n    ),\n    gpytorch_marginal_log_likelihood_func=lambda likelihood, gpyt_model: gpytorch.mlls.ExactMarginalLogLikelihood(\n        likelihood, gpyt_model\n    ),\n    torch_optimizer_func=lambda gpyt_model: torch.optim.Adam(\n        gpyt_model.parameters(), lr=0.1\n    ),\n    gpytorch_train_iter=100,\n    gpytorch_use_gpu=False,\n    verbose=False,\n    n_ref_approx=2**22,\n    seed_ref_approx=None,\n):\n    \"\"\"\n    Args:\n        integrand (AbstractIntegrand): The integrand.\n        failure_threshold (float): Thresholds for failure.\n        failure_above_threshold (bool): Set to `True` if failure occurs when the simulation exceeds `failure_threshold` and False otherwise.\n        abs_tol (float): The desired maximum distance from the estimate to either end of the credible interval.\n        n_init (float): Initial number of samples from integrand.discrete_distrib from which to build the first surrogate GP\n        n_limit (int): Budget of simulations.\n        n_batch (int): The number of samples per batch to draw from batch_sampler.\n        alpha (float): The credible interval is constructed to hold with probability at least 1 - alpha\n        init_samples (float): If the simulation has already been run, pass in (x,y) where x are past samples from the discrete distribution and y are corresponding simulation evaluations.\n        batch_sampler (Suggester or AbstractDiscreteDistribution): A suggestion scheme for future samples.\n        n_approx (int): Number of points from integrand.discrete_distrib used to approximate estimate and credible interval bounds\n        gpytorch_prior_mean (gpytorch.means): prior mean function of the GP\n        gpytorch_prior_cov (gpytorch.kernels): Prior covariance kernel of the GP\n        gpytorch_likelihood (gpytorch.likelihoods): GP likelihood, require one of gpytorch.likelihoods.{GaussianLikelihood, GaussianLikelihoodWithMissingObs, FixedNoiseGaussianLikelihood}\n        gpytorch_marginal_log_likelihood_func (callable): Function taking in the likelihood and gpytorch model and returning a marginal log likelihood from gpytorch.mlls\n        torch_optimizer_func (callable): Function taking in the gpytorch model and returning an optimizer from torch.optim\n        gpytorch_train_iter (int): Training iterations for the GP in gpytorch\n        gpytorch_use_gpu (bool): If True, have gpytorch use a GPU for fitting and trining the GP\n        verbose (int): If verbose &gt; 0, print information through the call to integrate()\n        n_ref_approx (int): If n_ref_approx &gt; 0, use n_ref_approx points to get a reference QMC approximation of the true solution.\n            Caution: If n_ref_approx &gt; 0, it should be a large int e.g. 2**22, in which case it is only helpful for cheap to evaluate simulations\n        seed_ref_approx (int): Seed for the reference approximation. Only applies when n_ref_approx&gt;0\n    \"\"\"\n    self.parameters = [\"abs_tol\", \"n_init\", \"n_limit\", \"n_batch\"]\n    self.integrand = integrand\n    self.true_measure = self.integrand.true_measure\n    self.discrete_distrib = self.integrand.discrete_distrib\n    self.d = self.integrand.d\n    self.sampler = self.d\n    self.failure_threshold = failure_threshold\n    self.failure_above_threshold = failure_above_threshold\n    self.abs_tol = abs_tol\n    self.alpha = alpha\n    assert 0 &lt; self.alpha &lt; 1\n    self.n_init = n_init\n    self.init_samples = init_samples is not None\n    if self.init_samples:\n        self.x_init, self.y_init = init_samples\n        assert self.x_init.ndim == 2 and self.y_init.ndim == 1\n        assert self.x_init.shape[1] == self.d and len(self.y_init) == len(\n            self.x_init\n        )\n        assert self.n_init == len(self.x_init)\n        self.ytf_init = self._affine_tf(self.y_init)\n    self.batch_sampler = batch_sampler\n    self.n_batch = n_batch\n    self.n_limit = n_limit\n    assert self.n_limit &gt;= self.n_init\n    self.n_approx = n_approx\n    assert (self.n_approx + self.n_init) &lt;= 2**32\n    self.gpytorch_prior_mean = gpytorch_prior_mean\n    self.gpytorch_prior_cov = gpytorch_prior_cov\n    self.gpytorch_likelihood = gpytorch_likelihood\n    self.gpytorch_marginal_log_likelihood_func = (\n        gpytorch_marginal_log_likelihood_func\n    )\n    self.torch_optimizer_func = torch_optimizer_func\n    self.gpytorch_train_iter = gpytorch_train_iter\n    self.gpytorch_use_gpu = gpytorch_use_gpu\n    self.verbose = verbose\n    self.approx_true_solution = n_ref_approx &gt; 0\n    if self.approx_true_solution:\n        x = DigitalNetB2(self.d, order=\"GRAY\", seed=seed_ref_approx)(n_ref_approx)\n        y = self.integrand.f(x).squeeze()\n        ytf = self._affine_tf(y)\n        self.ref_approx = (ytf &gt;= 0).mean(0)\n        if self.verbose:\n            print(\n                \"reference approximation with d=%d: %s\" % (self.d, self.ref_approx)\n            )\n    super(PFGPCI, self).__init__(\n        allowed_distribs=[AbstractDiscreteDistribution],\n        allow_vectorized_integrals=False,\n    )\n</code></pre>"},{"location":"api/stopping_criteria/#uml-specific","title":"UML Specific","text":""},{"location":"api/true_measures/","title":"True Measures","text":""},{"location":"api/true_measures/#true-measures","title":"True Measures","text":""},{"location":"api/true_measures/#uml-overview","title":"UML Overview","text":""},{"location":"api/true_measures/#abstracttruemeasure","title":"<code>AbstractTrueMeasure</code>","text":"<p>               Bases: <code>object</code></p> Source code in <code>qmcpy/true_measure/abstract_true_measure.py</code> <pre><code>def __init__(self):\n    prefix = \"A concrete implementation of TrueMeasure must have \"\n    if not hasattr(self, \"domain\"):\n        raise ParameterError(\n            prefix\n            + \"self.domain, 2xd ndarray of domain lower bounds (first col) and upper bounds (second col)\"\n        )\n    if not hasattr(self, \"range\"):\n        raise ParameterError(\n            prefix\n            + \"self.range, 2xd ndarray of range lower bounds (first col) and upper bounds (second col)\"\n        )\n    if not hasattr(self, \"parameters\"):\n        self.parameters = []\n</code></pre>"},{"location":"api/true_measures/#qmcpy.true_measure.abstract_true_measure.AbstractTrueMeasure.__call__","title":"__call__","text":"<pre><code>__call__(n=None, n_min=None, n_max=None, return_weights=False, warn=True)\n</code></pre> <ul> <li>If just <code>n</code> is supplied, generate samples from the sequence at indices 0,...,<code>n</code>-1.</li> <li>If <code>n_min</code> and <code>n_max</code> are supplied, generate samples from the sequence at indices <code>n_min</code>,...,<code>n_max</code>-1.</li> <li>If <code>n</code> and <code>n_min</code> are supplied, then generate samples from the sequence at indices <code>n</code>,...,<code>n_min</code>-1.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>Union[None, int]</code> <p>Number of points to generate.</p> <code>None</code> <code>n_min</code> <code>Union[None, int]</code> <p>Starting index of sequence.</p> <code>None</code> <code>n_max</code> <code>Union[None, int]</code> <p>Final index of sequence.</p> <code>None</code> <code>return_weights</code> <code>bool</code> <p>If <code>True</code>, return <code>weights</code> as well</p> <code>False</code> <code>warn</code> <code>bool</code> <p>If <code>False</code>, disable warnings when generating samples.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>t</code> <code>ndarray</code> <p>Samples from the sequence.</p> <ul> <li>If <code>replications</code> is <code>None</code> then this will be of size (<code>n_max</code>-<code>n_min</code>) \\(\\times\\) <code>dimension</code></li> <li>If <code>replications</code> is a positive int, then <code>t</code> will be of size <code>replications</code> \\(\\times\\) (<code>n_max</code>-<code>n_min</code>) \\(\\times\\) <code>dimension</code></li> </ul> <code>weights</code> <code>ndarray</code> <p>Only returned when <code>return_weights=True</code>. The Jacobian weights for the transformation</p> Source code in <code>qmcpy/true_measure/abstract_true_measure.py</code> <pre><code>def __call__(self, n=None, n_min=None, n_max=None, return_weights=False, warn=True):\n    r\"\"\"\n    - If just `n` is supplied, generate samples from the sequence at indices 0,...,`n`-1.\n    - If `n_min` and `n_max` are supplied, generate samples from the sequence at indices `n_min`,...,`n_max`-1.\n    - If `n` and `n_min` are supplied, then generate samples from the sequence at indices `n`,...,`n_min`-1.\n\n    Args:\n        n (Union[None, int]): Number of points to generate.\n        n_min (Union[None, int]): Starting index of sequence.\n        n_max (Union[None, int]): Final index of sequence.\n        return_weights (bool): If `True`, return `weights` as well\n        warn (bool): If `False`, disable warnings when generating samples.\n\n    Returns:\n        t (np.ndarray): Samples from the sequence.\n\n            - If `replications` is `None` then this will be of size (`n_max`-`n_min`) $\\times$ `dimension`\n            - If `replications` is a positive int, then `t` will be of size `replications` $\\times$ (`n_max`-`n_min`) $\\times$ `dimension`\n        weights (np.ndarray): Only returned when `return_weights=True`. The Jacobian weights for the transformation\n    \"\"\"\n    return self.gen_samples(\n        n=n, n_min=n_min, n_max=n_max, return_weights=return_weights, warn=warn\n    )\n</code></pre>"},{"location":"api/true_measures/#qmcpy.true_measure.abstract_true_measure.AbstractTrueMeasure.spawn","title":"spawn","text":"<pre><code>spawn(s=1, dimensions=None)\n</code></pre> <p>Spawn new instances of the current true measure but with new seeds and dimensions. Used by multi-level QMC algorithms which require different seeds and dimensions on each level.</p> Note <p>Use <code>replications</code> instead of using <code>spawn</code> when possible, e.g., when spawning copies which all have the same dimension.</p> <p>Parameters:</p> Name Type Description Default <code>s</code> <code>int</code> <p>Number of copies to spawn</p> <code>1</code> <code>dimensions</code> <code>ndarray</code> <p>Length <code>s</code> array of dimensions for each copy. Defaults to the current dimension.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>spawned_true_measures</code> <code>list</code> <p>True measure with new seeds and dimensions.</p> Source code in <code>qmcpy/true_measure/abstract_true_measure.py</code> <pre><code>def spawn(self, s=1, dimensions=None):\n    r\"\"\"\n    Spawn new instances of the current true measure but with new seeds and dimensions.\n    Used by multi-level QMC algorithms which require different seeds and dimensions on each level.\n\n    Note:\n        Use `replications` instead of using `spawn` when possible, e.g., when spawning copies which all have the same dimension.\n\n    Args:\n        s (int): Number of copies to spawn\n        dimensions (np.ndarray): Length `s` array of dimensions for each copy. Defaults to the current dimension.\n\n    Returns:\n        spawned_true_measures (list): True measure with new seeds and dimensions.\n    \"\"\"\n    sampler = self.discrete_distrib if self.transform == self else self.transform\n    sampler_spawns = sampler.spawn(s=s, dimensions=dimensions)\n    spawned_true_measures = [None] * len(sampler_spawns)\n    for i in range(s):\n        spawned_true_measures[i] = self._spawn(\n            sampler_spawns[i], sampler_spawns[i].d\n        )\n    return spawned_true_measures\n</code></pre>"},{"location":"api/true_measures/#scipywrapper","title":"<code>SciPyWrapper</code>","text":"<p>               Bases: <code>AbstractTrueMeasure</code></p> <p>True measure that wraps SciPy style distributions.</p> <p>This class keeps the original behavior of SciPyWrapper with independent 1D marginals and adds an optional \"joint\" mode for dependent distributions.</p> <p>Examples:</p> <p>Independent marginals from <code>scipy.stats</code>:</p> <pre><code>&gt;&gt;&gt; import scipy.stats as stats\n&gt;&gt;&gt; tm = SciPyWrapper(\n...     sampler=DigitalNetB2(3, seed=7),\n...     scipy_distribs=[\n...         stats.uniform(loc=1, scale=2),\n...         stats.norm(loc=0, scale=1),\n...         stats.gamma(a=5, loc=0, scale=2)])\n&gt;&gt;&gt; x = tm(2)\n&gt;&gt;&gt; x.shape\n(2, 3)\n</code></pre> <p>Joint multivariate normal passed as a single object:</p> <pre><code>&gt;&gt;&gt; mvn = stats.multivariate_normal(\n...     mean=[0.0, 0.0],\n...     cov=[[1.0, 0.8], [0.8, 1.0]])\n&gt;&gt;&gt; tm_joint = SciPyWrapper(DigitalNetB2(2, seed=7), mvn)\n&gt;&gt;&gt; tm_joint(2).shape\n(2, 2)\n</code></pre> <p>2D Student t distribution (independent marginals):</p> <pre><code>&gt;&gt;&gt; df = 5\n&gt;&gt;&gt; true_measure = SciPyWrapper(\n...     sampler=DigitalNetB2(2, seed=13),\n...     scipy_distribs=[\n...         stats.t(df=df, loc=0.0, scale=1.0),\n...         stats.t(df=df, loc=1.0, scale=2.0),\n...     ],\n... )\n&gt;&gt;&gt; xs = true_measure(4)\n&gt;&gt;&gt; xs.shape\n(4, 2)\n</code></pre>"},{"location":"api/true_measures/#qmcpy.true_measure.scipy_wrapper.SciPyWrapper--parameters","title":"Parameters","text":"<p>sampler : AbstractDiscreteDistribution     Low discrepancy or iid sampler in dimension d, living on [0,1)^d. scipy_distribs :     One of the following:</p> <pre><code>- A single SciPy 1D continuous frozen distribution.\n- A list of such frozen distributions (independent marginals).\n- A custom 1D distribution object with ``ppf`` and ``pdf`` or\n  ``logpdf`` methods.\n- A joint object with:\n    * ``transform(u)`` method\n    * optional ``logpdf(x)`` method\n    * ``dim`` or ``dimension`` attribute (otherwise ``sampler.d``).\n</code></pre> Source code in <code>qmcpy/true_measure/scipy_wrapper.py</code> <pre><code>def __init__(self, sampler, scipy_distribs):\n    \"\"\"\n    Parameters\n    ----------\n    sampler : AbstractDiscreteDistribution\n        Low discrepancy or iid sampler in dimension d, living on [0,1)^d.\n    scipy_distribs :\n        One of the following:\n\n        - A single SciPy 1D continuous frozen distribution.\n        - A list of such frozen distributions (independent marginals).\n        - A custom 1D distribution object with ``ppf`` and ``pdf`` or\n          ``logpdf`` methods.\n        - A joint object with:\n            * ``transform(u)`` method\n            * optional ``logpdf(x)`` method\n            * ``dim`` or ``dimension`` attribute (otherwise ``sampler.d``).\n    \"\"\"\n    self.domain = np.array([[0.0, 1.0]])\n\n    if not isinstance(sampler, AbstractDiscreteDistribution):\n        if not (\n            hasattr(sampler, \"d\")\n            and hasattr(sampler, \"gen_samples\")\n            and callable(sampler.gen_samples)\n        ):\n            raise ParameterError(\n                \"SciPyWrapper requires sampler be an AbstractDiscreteDistribution.\"\n            )\n    self._parse_sampler(sampler)\n\n    # Remember what the user originally passed in so that _spawn can reuse it.\n    self._user_distrib_arg = scipy_distribs\n\n    # Flags and holders for the two modes.\n    self._is_joint = False\n    self._joint = None\n    self._joint_has_logpdf = False\n    self._warned_missing_pdf = False\n\n    if self._looks_like_joint(scipy_distribs):\n        # Configure joint mode.\n        self._setup_joint(scipy_distribs)\n    else:\n        # Configure independent marginals mode.\n        self._setup_marginals(scipy_distribs)\n\n    super(SciPyWrapper, self).__init__()\n</code></pre>"},{"location":"api/true_measures/#uniform","title":"<code>Uniform</code>","text":"<p>               Bases: <code>AbstractTrueMeasure</code></p> <p>Uniform distribution, see https://en.wikipedia.org/wiki/Continuous_uniform_distribution.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; true_measure = Uniform(DigitalNetB2(2,seed=7),lower_bound=[0,.5],upper_bound=[2,3])\n&gt;&gt;&gt; true_measure(4)\narray([[1.44324713, 2.7873875 ],\n       [0.32691107, 1.5741214 ],\n       [1.97352511, 0.58590959],\n       [0.8591331 , 1.89690854]])\n&gt;&gt;&gt; true_measure\nUniform (AbstractTrueMeasure)\n    lower_bound     [0.  0.5]\n    upper_bound     [2 3]\n</code></pre> <p>With independent replications</p> <pre><code>&gt;&gt;&gt; x = Uniform(DigitalNetB2(3,seed=7,replications=2),lower_bound=[.25,.5,.75],upper_bound=[1.75,1.5,1.25])(4)\n&gt;&gt;&gt; x.shape\n(2, 4, 3)\n&gt;&gt;&gt; x\narray([[[0.61979915, 0.6821862 , 1.12366296],\n        [1.27229355, 1.16169442, 0.9644598 ],\n        [0.97209782, 1.29818233, 0.79100643],\n        [1.62311988, 0.79520621, 1.13747905]],\n\n       [[0.92315337, 1.35899604, 1.0027484 ],\n        [1.05453886, 0.54353443, 0.91782473],\n        [0.59821215, 0.79281506, 0.78420518],\n        [1.37943573, 1.10241448, 1.13481488]]])\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>sampler</code> <code>Union[AbstractDiscreteDistribution, AbstractTrueMeasure]</code> <p>Either</p> <ul> <li>a discrete distribution from which to transform samples, or</li> <li>a true measure by which to compose a transform.</li> </ul> required <code>lower_bound</code> <code>Union[float, ndarray]</code> <p>Lower bound.</p> <code>0</code> <code>upper_bound</code> <code>Union[float, ndarray]</code> <p>Upper bound.</p> <code>1</code> Source code in <code>qmcpy/true_measure/uniform.py</code> <pre><code>def __init__(self, sampler, lower_bound=0, upper_bound=1):\n    r\"\"\"\n    Args:\n        sampler (Union[AbstractDiscreteDistribution, AbstractTrueMeasure]): Either\n\n            - a discrete distribution from which to transform samples, or\n            - a true measure by which to compose a transform.\n        lower_bound (Union[float, np.ndarray]): Lower bound.\n        upper_bound (Union[float, np.ndarray]): Upper bound.\n    \"\"\"\n    self.parameters = [\"lower_bound\", \"upper_bound\"]\n    self.domain = np.array([[0, 1]])\n    self._parse_sampler(sampler)\n    self.lower_bound = lower_bound\n    self.upper_bound = upper_bound\n    if np.isscalar(self.lower_bound):\n        lower_bound = np.tile(self.lower_bound, self.d)\n    if np.isscalar(self.upper_bound):\n        upper_bound = np.tile(self.upper_bound, self.d)\n    self.a = np.array(lower_bound)\n    self.b = np.array(upper_bound)\n    if len(self.a) != self.d or len(self.b) != self.d:\n        raise DimensionError(\n            \"upper bound and lower bound must be of length dimension\"\n        )\n    self.delta = self.b - self.a\n    self.inv_delta_prod = 1 / self.delta.prod()\n    self.range = np.hstack(\n        (self.a.reshape((self.d, 1)), self.b.reshape((self.d, 1)))\n    )\n    super(Uniform, self).__init__()\n    assert self.a.shape == (self.d,) and self.b.shape == (self.d,)\n</code></pre>"},{"location":"api/true_measures/#gaussian","title":"<code>Gaussian</code>","text":"<p>               Bases: <code>AbstractTrueMeasure</code></p> <p>Gaussian (Normal) distribution as described in https://en.wikipedia.org/wiki/Multivariate_normal_distribution.</p> Note <ul> <li><code>Normal</code> is an alias for <code>Gaussian</code></li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; true_measure = Gaussian(DigitalNetB2(2,seed=7),mean=[1,2],covariance=[[9,4],[4,5]])\n&gt;&gt;&gt; true_measure(4)\narray([[ 3.83994612,  1.19097885],\n       [-1.9727727 ,  0.49405353],\n       [ 5.87242307,  8.41341485],\n       [ 0.61222205,  1.48402653]])\n&gt;&gt;&gt; true_measure\nGaussian (AbstractTrueMeasure)\n    mean            [1 2]\n    covariance      [[9 4]\n                     [4 5]]\n    decomp_type     PCA\n</code></pre> <p>With independent replications</p> <pre><code>&gt;&gt;&gt; x = Gaussian(DigitalNetB2(3,seed=7,replications=2),mean=0,covariance=3)(4)\n&gt;&gt;&gt; x.shape\n(2, 4, 3)\n&gt;&gt;&gt; x\narray([[[-1.18721904, -1.57108272,  1.15371635],\n        [ 0.81749123,  0.72242445, -0.31025434],\n        [-0.0807895 ,  1.44651585, -2.41042379],\n        [ 2.38133494, -0.93225637,  1.30817519]],\n\n       [[-0.22304017,  1.86337427,  0.02386568],\n        [ 0.15807672, -2.96365385, -0.73502346],\n        [-1.26753687, -0.94427848, -2.57683314],\n        [ 1.1844196 ,  0.44964332,  1.27760936]]])\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>sampler</code> <code>Union[AbstractDiscreteDistribution, AbstractTrueMeasure]</code> <p>Either</p> <ul> <li>a discrete distribution from which to transform samples, or</li> <li>a true measure by which to compose a transform.</li> </ul> required <code>mean</code> <code>Union[float, ndarray]</code> <p>Mean vector.</p> <code>0.0</code> <code>covariance</code> <code>Union[float, ndarray]</code> <p>Covariance matrix. A float or vector will be expanded into a diagonal matrix.</p> <code>1.0</code> <code>decomp_type</code> <code>str</code> <p>Method for decomposition for covariance matrix. Options include</p> <ul> <li><code>'PCA'</code> for principal component analysis, or</li> <li><code>'Cholesky'</code> for cholesky decomposition.</li> </ul> <code>'PCA'</code> Source code in <code>qmcpy/true_measure/gaussian.py</code> <pre><code>def __init__(self, sampler, mean=0.0, covariance=1.0, decomp_type=\"PCA\"):\n    \"\"\"\n    Args:\n        sampler (Union[AbstractDiscreteDistribution, AbstractTrueMeasure]): Either\n\n            - a discrete distribution from which to transform samples, or\n            - a true measure by which to compose a transform.\n        mean (Union[float, np.ndarray]): Mean vector.\n        covariance (Union[float, np.ndarray]): Covariance matrix. A float or vector will be expanded into a diagonal matrix.\n        decomp_type (str): Method for decomposition for covariance matrix. Options include\n\n            - `'PCA'` for principal component analysis, or\n            - `'Cholesky'` for cholesky decomposition.\n    \"\"\"\n    self.parameters = [\"mean\", \"covariance\", \"decomp_type\"]\n    # default to transform from standard uniform\n    self.domain = np.array([[0, 1]])\n    self._parse_sampler(sampler)\n    self._parse_gaussian_params(mean, covariance, decomp_type)\n    self.range = np.array([[-np.inf, np.inf]])\n    super(Gaussian, self).__init__()\n    assert self.mu.shape == (self.d,) and self.a.shape == (self.d, self.d)\n</code></pre>"},{"location":"api/true_measures/#qmcpy.true_measure.gaussian.Gaussian.a","title":"a  <code>property</code> <code>writable</code>","text":"<pre><code>a\n</code></pre> <p>Lazy-loaded decomposition matrix.</p>"},{"location":"api/true_measures/#qmcpy.true_measure.gaussian.Gaussian.mvn_scipy","title":"mvn_scipy  <code>property</code> <code>writable</code>","text":"<pre><code>mvn_scipy\n</code></pre> <p>Lazy-loaded scipy multivariate normal.</p>"},{"location":"api/true_measures/#brownianmotion","title":"<code>BrownianMotion</code>","text":"<p>               Bases: <code>Gaussian</code></p> <p>Brownian Motion as described in https://en.wikipedia.org/wiki/Brownian_motion. For a standard Brownian Motion \\(W\\) we define the Brownian Motion \\(B\\) with initial value \\(B_0\\), drift \\(\\gamma\\), and diffusion \\(\\sigma^2\\) to be</p> \\[B(t) = B_0 + \\gamma t + \\sigma W(t).\\] <p>Examples:</p> <pre><code>&gt;&gt;&gt; true_measure = BrownianMotion(DigitalNetB2(4,seed=7),t_final=2,drift=2)\n&gt;&gt;&gt; true_measure(2)\narray([[0.82189263, 2.7851793 , 3.60126805, 3.98054724],\n       [0.2610643 , 0.06170064, 1.06448269, 2.30990767]])\n&gt;&gt;&gt; true_measure\nBrownianMotion (AbstractTrueMeasure)\n    time_vec        [0.5 1.  1.5 2. ]\n    drift           2^(1)\n    mean            [1. 2. 3. 4.]\n    covariance      [[0.5 0.5 0.5 0.5]\n                     [0.5 1.  1.  1. ]\n                     [0.5 1.  1.5 1.5]\n                     [0.5 1.  1.5 2. ]]\n    decomp_type     PCA\n</code></pre> <p>With independent replications</p> <pre><code>&gt;&gt;&gt; x = BrownianMotion(DigitalNetB2(3,seed=7,replications=2),t_final=2,drift=2)(4)\n&gt;&gt;&gt; x.shape\n(2, 4, 3)\n&gt;&gt;&gt; x\narray([[[0.66154685, 1.50620966, 3.52322901],\n        [1.77064217, 3.32782204, 4.45013223],\n        [1.33558688, 3.26017547, 3.40692337],\n        [2.10317345, 3.78961839, 6.17948096]],\n\n       [[1.77868019, 2.75347902, 3.41161419],\n        [0.44891984, 2.53987304, 4.7224811 ],\n        [0.23147948, 2.25289769, 3.00039101],\n        [2.06762574, 3.21756319, 4.93375923]]])\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>sampler</code> <code>Union[AbstractDiscreteDistribution, AbstractTrueMeasure]</code> <p>Either</p> <ul> <li>a discrete distribution from which to transform samples, or</li> <li>a true measure by which to compose a transform.</li> </ul> required <code>t_final</code> <code>float</code> <p>End time.</p> <code>1</code> <code>initial_value</code> <code>float</code> <p>Initial value \\(B_0\\).</p> <code>0</code> <code>drift</code> <code>int</code> <p>Drift \\(\\gamma\\).</p> <code>0</code> <code>diffusion</code> <code>int</code> <p>Diffusion \\(\\sigma^2\\).</p> <code>1</code> <code>decomp_type</code> <code>str</code> <p>Method for decomposition for covariance matrix. Options include</p> <ul> <li><code>'PCA'</code> for principal component analysis, or</li> <li><code>'Cholesky'</code> for cholesky decomposition.</li> </ul> <code>'PCA'</code> <code>lazy_decomp</code> <code>bool</code> <p>If True, defer expensive matrix decomposition until needed.</p> <code>True</code> Source code in <code>qmcpy/true_measure/brownian_motion.py</code> <pre><code>def __init__(\n    self,\n    sampler,\n    t_final=1,\n    initial_value=0,\n    drift=0,\n    diffusion=1,\n    decomp_type=\"PCA\",\n    lazy_decomp=True,\n):\n    r\"\"\"\n    Args:\n        sampler (Union[AbstractDiscreteDistribution, AbstractTrueMeasure]): Either\n\n            - a discrete distribution from which to transform samples, or\n            - a true measure by which to compose a transform.\n        t_final (float): End time.\n        initial_value (float): Initial value $B_0$.\n        drift (int): Drift $\\gamma$.\n        diffusion (int): Diffusion $\\sigma^2$.\n        decomp_type (str): Method for decomposition for covariance matrix. Options include\n\n            - `'PCA'` for principal component analysis, or\n            - `'Cholesky'` for cholesky decomposition.\n        lazy_decomp (bool): If True, defer expensive matrix decomposition until needed.\n    \"\"\"\n    self.parameters = [\"time_vec\", \"drift\", \"mean\", \"covariance\", \"decomp_type\"]\n    # default to transform from standard uniform\n    self.domain = np.array([[0, 1]])\n    self._parse_sampler(sampler)\n    self.t = t_final  # exercise time\n    self.initial_value = initial_value\n    self.drift = drift\n    self.diffusion = diffusion\n    self.time_vec = np.linspace(self.t / self.d, self.t, self.d)  # evenly spaced\n    self.diffused_sigma_bm = self.diffusion * np.minimum.outer(\n        self.time_vec, self.time_vec\n    )\n    self.drift_time_vec_plus_init = (\n        self.drift * self.time_vec + self.initial_value\n    )  # mean\n    self._parse_gaussian_params(\n        self.drift_time_vec_plus_init,\n        self.diffused_sigma_bm,\n        decomp_type,\n        lazy_decomp,\n    )\n    self.range = np.array([[-np.inf, np.inf]])\n    super(Gaussian, self).__init__()\n</code></pre>"},{"location":"api/true_measures/#geometricbrownianmotion","title":"<code>GeometricBrownianMotion</code>","text":"<p>               Bases: <code>BrownianMotion</code></p> <p>A Geometric Brownian Motion (GBM) with initial value \\(S_0\\), drift \\(\\gamma\\), and diffusion \\(\\sigma^2\\) is</p> \\[\\mathrm{GBM}(t) = S_0 \\exp[(\\gamma - \\sigma^2/2) t + \\sigma \\mathrm{BM}(t)]\\] <p>where BM is a Brownian Motion drift \\(\\gamma\\) and diffusion \\(\\sigma^2\\).</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; gbm = GeometricBrownianMotion(DigitalNetB2(4,seed=7), t_final=2, drift=0.1, diffusion=0.2)\n&gt;&gt;&gt; gbm.gen_samples(2)\narray([[0.92343761, 1.42069027, 1.30851806, 0.99133819],\n       [0.7185916 , 0.42028013, 0.42080335, 0.4696196 ]])\n&gt;&gt;&gt; gbm\nGeometricBrownianMotion (AbstractTrueMeasure)\n    time_vec        [0.5 1.  1.5 2. ]\n    drift           0.100\n    diffusion       0.200\n    mean_gbm        [1.051 1.105 1.162 1.221]\n    covariance_gbm  [[0.116 0.122 0.128 0.135]\n                     [0.122 0.27  0.284 0.299]\n                     [0.128 0.284 0.472 0.496]\n                     [0.135 0.299 0.496 0.734]]\n    decomp_type     PCA\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>sampler</code> <code>DiscreteDistribution / TrueMeasure</code> <p>A discrete distribution or true measure.</p> required <code>t_final</code> <code>float</code> <p>End time for the geometric Brownian motion, non-negative.</p> <code>1</code> <code>initial_value</code> <code>float</code> <p>Positive initial value of the process, \\(S_0\\).</p> <code>1</code> <code>drift</code> <code>float</code> <p>Drift coefficient \\(\\gamma\\).</p> <code>0</code> <code>diffusion</code> <code>float</code> <p>Positive diffusion coefficient \\(\\sigma^2\\), where \\(\\sigma\\) is volatility.</p> <code>1</code> <code>decomp_type</code> <code>str</code> <p>Method of decomposition, either \"PCA\" or \"Cholesky\".</p> <code>'PCA'</code> <code>lazy_load</code> <code>bool</code> <p>If True, defer GBM-specific computations until needed.</p> <code>True</code> <code>lazy_decomp</code> <code>bool</code> <p>If True, defer expensive matrix decomposition until needed.</p> <code>True</code> Source code in <code>qmcpy/true_measure/geometric_brownian_motion.py</code> <pre><code>def __init__(\n    self,\n    sampler,\n    t_final=1,\n    initial_value=1,\n    drift=0,\n    diffusion=1,\n    decomp_type=\"PCA\",\n    lazy_load=True,\n    lazy_decomp=True,\n):\n    r\"\"\"\n    Args:\n        sampler (DiscreteDistribution/TrueMeasure): A discrete distribution or true measure.\n        t_final (float): End time for the geometric Brownian motion, non-negative.\n        initial_value (float): Positive initial value of the process, $S_0$.\n        drift (float): Drift coefficient $\\gamma$.\n        diffusion (float): Positive diffusion coefficient $\\sigma^2$, where $\\sigma$ is volatility.\n        decomp_type (str): Method of decomposition, either \"PCA\" or \"Cholesky\".\n        lazy_load (bool): If True, defer GBM-specific computations until needed.\n        lazy_decomp (bool): If True, defer expensive matrix decomposition until needed.\n    \"\"\"\n    super().__init__(\n        sampler,\n        t_final=t_final,\n        drift=0,\n        diffusion=diffusion,\n        decomp_type=decomp_type,\n        lazy_decomp=lazy_decomp,\n    )\n    self.parameters = [\n        \"time_vec\",\n        \"drift\",\n        \"diffusion\",\n        \"mean_gbm\",\n        \"covariance_gbm\",\n        \"decomp_type\",\n    ]\n    self.initial_value = initial_value\n    self.drift = drift\n    self.diffusion = diffusion\n    self.lazy_load = lazy_load\n    self.lazy_decomp = lazy_decomp\n\n    # Cache for lazy-loaded properties\n    self._mean_gbm_cache = None\n    self._covariance_gbm_cache = None\n    self._log_mvn_scipy_cache = None\n\n    # Large step optimization - use fast path for large problems\n    self.large_step_threshold = 1000\n    self.use_large_step_optimization = (\n        len(self.time_vec) &gt; self.large_step_threshold\n    )\n\n    # Validate input early (fast operation)\n    self._validate_input()\n\n    if not lazy_load:\n        # Compute everything immediately for backwards compatibility\n        self.mean_gbm = self._compute_gbm_mean()\n        self.covariance_gbm = self._compute_gbm_covariance()\n        self._setup_lognormal_distribution()\n</code></pre>"},{"location":"api/true_measures/#qmcpy.true_measure.geometric_brownian_motion.GeometricBrownianMotion.mean_gbm","title":"mean_gbm  <code>property</code> <code>writable</code>","text":"<pre><code>mean_gbm\n</code></pre> <p>Lazy-loaded GBM mean vector.</p>"},{"location":"api/true_measures/#qmcpy.true_measure.geometric_brownian_motion.GeometricBrownianMotion.covariance_gbm","title":"covariance_gbm  <code>property</code> <code>writable</code>","text":"<pre><code>covariance_gbm\n</code></pre> <p>Lazy-loaded GBM covariance matrix.</p>"},{"location":"api/true_measures/#qmcpy.true_measure.geometric_brownian_motion.GeometricBrownianMotion.log_mvn_scipy","title":"log_mvn_scipy  <code>property</code>","text":"<pre><code>log_mvn_scipy\n</code></pre> <p>Lazy-loaded scipy multivariate normal distribution.</p>"},{"location":"api/true_measures/#qmcpy.true_measure.geometric_brownian_motion.GeometricBrownianMotion.gen_samples","title":"gen_samples","text":"<pre><code>gen_samples(n=None, n_min=None, n_max=None, return_weights=False, warn=True)\n</code></pre> <p>Generate GBM samples using the parent's transform pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>int</code> <p>number of samples to generate</p> <code>None</code> <code>n_min</code> <code>int</code> <p>minimum index of sequence</p> <code>None</code> <code>n_max</code> <code>int</code> <p>maximum index of sequence  </p> <code>None</code> <code>return_weights</code> <code>bool</code> <p>whether to return Jacobian weights</p> <code>False</code> <code>warn</code> <code>bool</code> <p>whether to warn about sample generation</p> <code>True</code> <p>Returns:</p> Name Type Description <code>samples</code> <code>Union[ndarray, tuple]</code> <p>GBM samples, optionally with weights if return_weights=True</p> Source code in <code>qmcpy/true_measure/geometric_brownian_motion.py</code> <pre><code>def gen_samples(\n    self, n=None, n_min=None, n_max=None, return_weights=False, warn=True\n) -&gt; Union[ndarray, Tuple[ndarray, ndarray]]:\n    \"\"\"\n    Generate GBM samples using the parent's transform pipeline.\n\n    Args:\n        n (int): number of samples to generate\n        n_min (int): minimum index of sequence\n        n_max (int): maximum index of sequence  \n        return_weights (bool): whether to return Jacobian weights\n        warn (bool): whether to warn about sample generation\n\n    Returns:\n        samples (Union[ndarray,tuple]): GBM samples, optionally with weights if return_weights=True\n    \"\"\"\n    return super().gen_samples(n=n, n_min=n_min, n_max=n_max, return_weights=return_weights, warn=warn)\n</code></pre>"},{"location":"api/true_measures/#materngp","title":"<code>MaternGP</code>","text":"<p>               Bases: <code>Gaussian</code></p> <p>A Gaussian process with Mat\u00e9rn covariance kernel.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; true_measure = MaternGP(DigitalNetB2(dimension=3,seed=7),points=np.linspace(0,1,3)[:,None],nu=3/2,length_scale=[3,4,5],variance=0.01,mean=np.array([.3,.4,.5]))\n&gt;&gt;&gt; true_measure(4)\narray([[0.3515401 , 0.43083384, 0.51801119],\n       [0.20272448, 0.31312011, 0.4241431 ],\n       [0.40189226, 0.53502934, 0.63826677],\n       [0.29943567, 0.38491661, 0.48296594]])\n&gt;&gt;&gt; true_measure\nMaternGP (AbstractTrueMeasure)\n    mean            [0.3 0.4 0.5]\n    covariance      [[0.01  0.01  0.01 ]\n                     [0.01  0.01  0.01 ]\n                     [0.009 0.01  0.01 ]]\n    decomp_type     PCA\n</code></pre> <p>With independent replications</p> <pre><code>&gt;&gt;&gt; x = MaternGP(DigitalNetB2(dimension=3,seed=7,replications=2),points=np.linspace(0,1,3)[:,None],nu=3/2,length_scale=[3,4,5],variance=0.01,mean=np.array([.3,.4,.5]))(4)\n&gt;&gt;&gt; x.shape\n(2, 4, 3)\n&gt;&gt;&gt; x\narray([[[0.21490091, 0.33078241, 0.45151042],\n        [0.35465127, 0.44705898, 0.53793358],\n        [0.31091595, 0.39868187, 0.47660193],\n        [0.42419919, 0.53572415, 0.64674883]],\n\n       [[0.31010701, 0.38522001, 0.46670381],\n        [0.27221177, 0.413546  , 0.54101758],\n        [0.2147053 , 0.33293508, 0.43572791],\n        [0.37343973, 0.46534628, 0.56356714]]])\n</code></pre> <p>References:</p> <ol> <li> <p><code>sklearn.gaussian_process.kernels.Matern</code>.</p> </li> <li> <p>https://en.wikipedia.org/wiki/Mat%C3%A9rn_covariance_function.</p> </li> </ol> <p>Parameters:</p> Name Type Description Default <code>sampler</code> <code>Union[AbstractDiscreteDistribution, AbstractTrueMeasure]</code> <p>Either</p> <ul> <li>a discrete distribution from which to transform samples, or</li> <li>a true measure by which to compose a transform.</li> </ul> required <code>points</code> <code>ndarray</code> <p>The positions of points on a metric space. The array should have shape \\((d,k)\\) where \\(d\\) is the dimension of the sampler and \\(k\\) is the latent dimension.</p> required <code>nu</code> <code>float</code> <p>The \"smoothness\" of the MaternGP function, e.g.,</p> <ul> <li>\\(\\nu = 1/2\\) is equivalent to the absolute exponential kernel,</li> <li>\\(\\nu = 3/2\\) implies a once-differentiable function,</li> <li>\\(\\nu = 5/2\\) implies twice differentiability.</li> <li>as \\(\\nu \\to \\infty\\) the kernel becomes equivalent to the RBF kernel, see <code>sklearn.gaussian_process.kernels.RBF</code>.</li> </ul> <p>Note that when \\(\\nu \\notin \\{1/2, 3/2, 5/2, \\infty \\}\\) the kernel is around \\(10\\) times slower to evaluate.</p> <code>1.5</code> <code>length_scale</code> <code>Union[float, ndarray]</code> <p>Determines \"peakiness\", or how correlated two points are based on their distance.</p> <code>1.0</code> <code>variance</code> <code>float</code> <p>Global scaling factor.</p> <code>1.0</code> <code>mean</code> <code>Union[float, ndarray]</code> <p>Mean vector for multivariate <code>Gaussian</code>.</p> <code>0.0</code> <code>nugget</code> <code>float</code> <p>Positive nugget to add to diagonal.</p> <code>1e-06</code> <code>decomp_type</code> <code>str</code> <p>Method for decomposition for covariance matrix. Options include</p> <ul> <li><code>'PCA'</code> for principal component analysis, or</li> <li><code>'Cholesky'</code> for cholesky decomposition.</li> </ul> <code>'PCA'</code> Source code in <code>qmcpy/true_measure/matern_gp.py</code> <pre><code>def __init__(\n    self,\n    sampler,\n    points,\n    length_scale=1.0,\n    nu=1.5,\n    variance=1.0,\n    mean=0.0,\n    nugget=1e-6,\n    decomp_type=\"PCA\",\n):\n    r\"\"\"\n    Args:\n        sampler (Union[AbstractDiscreteDistribution, AbstractTrueMeasure]): Either\n\n            - a discrete distribution from which to transform samples, or\n            - a true measure by which to compose a transform.\n        points (np.ndarray): The positions of points on a metric space. The array should have shape $(d,k)$ where $d$ is the dimension of the sampler and $k$ is the latent dimension.\n        nu (float): The \"smoothness\" of the MaternGP function, e.g.,\n\n            - $\\nu = 1/2$ is equivalent to the absolute exponential kernel,\n            - $\\nu = 3/2$ implies a once-differentiable function,\n            - $\\nu = 5/2$ implies twice differentiability.\n            - as $\\nu \\to \\infty$ the kernel becomes equivalent to the RBF kernel, see [`sklearn.gaussian_process.kernels.RBF`](https://scikit-learn.org/stable/modules/generated/sklearn.gaussian_process.kernels.RBF.html#sklearn.gaussian_process.kernels.RBF).\n\n            Note that when $\\nu \\notin \\{1/2, 3/2, 5/2, \\infty \\}$ the kernel is around $10$ times slower to evaluate.\n        length_scale (Union[float, np.ndarray]): Determines \"peakiness\", or how correlated two points are based on their distance.\n        variance (float): Global scaling factor.\n        mean (Union[float, np.ndarray]): Mean vector for multivariate `Gaussian`.\n        nugget (float): Positive nugget to add to diagonal.\n        decomp_type (str): Method for decomposition for covariance matrix. Options include\n\n            - `'PCA'` for principal component analysis, or\n            - `'Cholesky'` for cholesky decomposition.\n    \"\"\"\n    if not (\n        isinstance(sampler, AbstractDiscreteDistribution)\n        or isinstance(sampler, AbstractTrueMeasure)\n    ):\n        raise ParameterError(\n            \"sampler input should either be an AbstractDiscreteDistribution or AbstractTrueMeasure.\"\n        )\n    if not (\n        isinstance(points, np.ndarray) and (points.ndim == 1 or points.ndim == 2)\n    ):\n        raise ParameterError(\"points must be a one or two dimensional np.ndarray.\")\n    if points.ndim == 1:\n        points = points[:, None]\n    assert (\n        points.ndim == 2 and points.shape[0] == sampler.d\n    ), \"points should be a two dimension array with the number of points equal to the dimension of the sampler\"\n    mean = np.array(mean)\n    if mean.size == 1:\n        mean = mean.item() * np.ones(sampler.d)\n    assert mean.shape == (sampler.d,), \"mean should be a length d vector\"\n    assert np.isscalar(nu) and nu &gt; 0, \"nu should be a positive scalar\"\n    length_scale = np.array(length_scale)\n    if length_scale.size == 1:\n        length_scale = length_scale.item() * np.ones(sampler.d)\n    assert (\n        length_scale.shape == (sampler.d,) and (length_scale &gt; 0).all()\n    ), \"length_scale should be a vector with length equal to the dimension of the sampler\"\n    assert (\n        np.isscalar(variance) and variance &gt; 0\n    ), \"length_scale should be a positive scalar\"\n    assert np.isscalar(nugget) and nugget &gt; 0, \"nugget should be a positive scalar\"\n    self.points = points\n    self.length_scale = length_scale\n    self.nu = nu\n    self.variance = variance\n    dists = np.linalg.norm(\n        points[..., :, None, :] - points[..., None, :, :], axis=-1\n    )\n    if nu == 1 / 2:\n        covariance = np.exp(-dists / self.length_scale)\n    elif nu == 3 / 2:\n        covariance = (1 + np.sqrt(3) * dists / self.length_scale) * np.exp(\n            -np.sqrt(3) * dists / self.length_scale\n        )\n    elif nu == 5 / 2:\n        covariance = (\n            1\n            + np.sqrt(5) * dists / self.length_scale\n            + 5 * dists**2 / (3 * self.length_scale**2)\n        ) * np.exp(-np.sqrt(5) * dists / self.length_scale)\n    elif nu == np.inf:\n        covariance = np.exp(-(dists**2) / (2 * self.length_scale**2))\n    else:\n        k = np.sqrt(2 * nu) * dists / self.length_scale\n        covariance = 2 ** (1 - nu) / gamma(nu) * k**nu * kv(nu, k)\n    covariance = variance * covariance + nugget * np.eye(sampler.d)\n    super().__init__(\n        sampler, mean=mean, covariance=covariance, decomp_type=decomp_type\n    )\n</code></pre>"},{"location":"api/true_measures/#lebesgue","title":"<code>Lebesgue</code>","text":"<p>               Bases: <code>AbstractTrueMeasure</code></p> <p>Lebesgue measure as described in https://en.wikipedia.org/wiki/Lebesgue_measure.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; Lebesgue(Gaussian(DigitalNetB2(2,seed=7)))\nLebesgue (AbstractTrueMeasure)\n    transform       Gaussian (AbstractTrueMeasure)\n                        mean            0\n                        covariance      1\n                        decomp_type     PCA\n&gt;&gt;&gt; Lebesgue(Uniform(DigitalNetB2(2,seed=7)))\nLebesgue (AbstractTrueMeasure)\n    transform       Uniform (AbstractTrueMeasure)\n                        lower_bound     0\n                        upper_bound     1\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>sampler</code> <code>AbstractTrueMeasure</code> <p>A true measure by which to compose a transform.</p> required Source code in <code>qmcpy/true_measure/lebesgue.py</code> <pre><code>def __init__(self, sampler):\n    r\"\"\"\n    Args:\n        sampler (AbstractTrueMeasure): A true measure by which to compose a transform.\n    \"\"\"\n    self.parameters = []\n    if not isinstance(sampler, AbstractTrueMeasure):\n        raise ParameterError(\n            \"Lebesgue sampler must be an AbstractTrueMeasure by which to transform samples.\"\n        )\n    self.domain = (\n        sampler.range\n    )  # hack to make sure Lebesgue is compatible with any transform\n    self.range = sampler.range\n    self._parse_sampler(sampler)\n    super(Lebesgue, self).__init__()\n</code></pre>"},{"location":"api/true_measures/#bernoullicont","title":"<code>BernoulliCont</code>","text":"<p>               Bases: <code>AbstractTrueMeasure</code></p> <p>Continuous Bernoulli distribution with independent marginals as described in https://en.wikipedia.org/wiki/Continuous_Bernoulli_distribution.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; true_measure = BernoulliCont(DigitalNetB2(2,seed=7),lam=.2)\n&gt;&gt;&gt; true_measure(4)\narray([[0.56205914, 0.83607872],\n       [0.09433983, 0.28057299],\n       [0.97190779, 0.01883497],\n       [0.28050753, 0.39178506]])\n&gt;&gt;&gt; true_measure\nBernoulliCont (AbstractTrueMeasure)\n    lam             0.200\n</code></pre> <p>With independent replications</p> <pre><code>&gt;&gt;&gt; x = BernoulliCont(DigitalNetB2(3,seed=7,replications=2),lam=[.25,.5,.75])(4)\n&gt;&gt;&gt; x.shape\n(2, 4, 3)\n&gt;&gt;&gt; x\narray([[[0.16343492, 0.1821862 , 0.83209443],\n        [0.55140696, 0.66169442, 0.56381501],\n        [0.35229402, 0.79818233, 0.13825119],\n        [0.85773226, 0.29520621, 0.85203898]],\n\n       [[0.32359293, 0.85899604, 0.63591945],\n        [0.40278251, 0.04353443, 0.46749989],\n        [0.1530438 , 0.29281506, 0.116725  ],\n        [0.6345258 , 0.60241448, 0.84822692]]])\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>sampler</code> <code>Union[AbstractDiscreteDistribution, AbstractTrueMeasure]</code> <p>Either</p> <ul> <li>a discrete distribution from which to transform samples, or</li> <li>a true measure by which to compose a transform.</li> </ul> required <code>lam</code> <code>Union[float, ndarray]</code> <p>Vector of shape parameters, each in \\((0,1)\\).</p> <code>1 / 2</code> Source code in <code>qmcpy/true_measure/bernoulli_cont.py</code> <pre><code>def __init__(self, sampler, lam=1 / 2):\n    r\"\"\"\n    Args:\n        sampler (Union[AbstractDiscreteDistribution, AbstractTrueMeasure]): Either\n\n            - a discrete distribution from which to transform samples, or\n            - a true measure by which to compose a transform.\n        lam (Union[float, np.ndarray]): Vector of shape parameters, each in $(0,1)$.\n    \"\"\"\n    self.parameters = [\"lam\"]\n    self.domain = np.array([[0, 1]])\n    self.range = np.array([[0, 1]])\n    self._parse_sampler(sampler)\n    self.lam = lam\n    self.l = np.array(lam)\n    if self.l.size == 1:\n        self.l = self.l.item() * np.ones(self.d)\n    if not (\n        self.l.shape == (self.d,) and (0 &lt;= self.l).all() and (self.l &lt;= 1).all()\n    ):\n        raise DimensionError(\n            \"lam must be scalar or have length equal to dimension and must be in (0,1).\"\n        )\n    super(BernoulliCont, self).__init__()\n</code></pre>"},{"location":"api/true_measures/#johnsonssu","title":"<code>JohnsonsSU</code>","text":"<p>               Bases: <code>AbstractTrueMeasure</code></p> <p>Johnson's \\(S_U\\)-distribution with independent marginals as described in https://en.wikipedia.org/wiki/Johnson%27s_SU-distribution.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; true_measure = JohnsonsSU(DigitalNetB2(2,seed=7),gamma=1,xi=2,delta=3,lam=4)\n&gt;&gt;&gt; true_measure(4)\narray([[ 1.44849599,  2.49715741],\n       [-0.83646172,  0.38970902],\n       [ 3.67068094, -2.33911196],\n       [ 0.38940887,  0.84843818]])\n&gt;&gt;&gt; true_measure\nJohnsonsSU (AbstractTrueMeasure)\n    gamma           1\n    xi              2^(1)\n    delta           3\n    lam             2^(2)\n</code></pre> <p>With independent replications</p> <pre><code>&gt;&gt;&gt; x = JohnsonsSU(DigitalNetB2(3,seed=7,replications=2),gamma=1,xi=2,delta=3,lam=4)(4)\n&gt;&gt;&gt; x.shape\n(2, 4, 3)\n&gt;&gt;&gt; x\narray([[[-0.36735335, -0.71750135,  1.55387818],\n        [ 1.29233112,  1.21788962,  0.3870404 ],\n        [ 0.57599197,  1.78008445, -1.53756327],\n        [ 2.50112084, -0.14204369,  1.67333839]],\n\n       [[ 0.45920696,  2.10110361,  0.66122546],\n        [ 0.76973983, -2.12724026,  0.02868419],\n        [-0.43948474, -0.1525475 , -1.71041918],\n        [ 1.57765245,  1.00275   ,  1.64972468]]])\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>sampler</code> <code>Union[AbstractDiscreteDistribution, AbstractTrueMeasure]</code> <p>Either</p> <ul> <li>a discrete distribution from which to transform samples, or</li> <li>a true measure by which to compose a transform.</li> </ul> required <code>gamma</code> <code>Union[float, ndarray]</code> <p>First parameter \\(\\gamma\\).</p> <code>1</code> <code>xi</code> <code>Union[float, ndarray]</code> <p>Second parameter \\(\\xi\\).</p> <code>1</code> <code>delta</code> <code>Union[float, ndarray]</code> <p>Third parameter \\(\\delta &gt; 0\\).</p> <code>2</code> <code>lam</code> <code>Union[float, ndarray]</code> <p>Fourth parameter \\(\\lambda &gt; 0\\).</p> <code>2</code> Source code in <code>qmcpy/true_measure/johnsons_su.py</code> <pre><code>def __init__(self, sampler, gamma=1, xi=1, delta=2, lam=2):\n    r\"\"\"\n    Args:\n        sampler (Union[AbstractDiscreteDistribution, AbstractTrueMeasure]): Either\n\n            - a discrete distribution from which to transform samples, or\n            - a true measure by which to compose a transform.\n        gamma (Union[float, np.ndarray]): First parameter $\\gamma$.\n        xi (Union[float, np.ndarray]): Second parameter $\\xi$.\n        delta (Union[float, np.ndarray]): Third parameter $\\delta &gt; 0$.\n        lam (Union[float, np.ndarray]): Fourth parameter $\\lambda &gt; 0$.\n    \"\"\"\n    self.parameters = [\"gamma\", \"xi\", \"delta\", \"lam\"]\n    self.domain = np.array([[0, 1]])\n    self.range = np.array([[-np.inf, np.inf]])\n    self._parse_sampler(sampler)\n    self.gamma = gamma\n    self.xi = xi\n    self.delta = delta\n    self.lam = lam\n    self._gamma = np.array(gamma)\n    if self._gamma.size == 1:\n        self._gamma = self._gamma.item() * np.ones(self.d)\n    self._xi = np.array(xi)\n    if self._xi.size == 1:\n        self._xi = self._xi.item() * np.ones(self.d)\n    self._delta = np.array(delta)\n    if self._delta.size == 1:\n        self._delta = self._delta.item() * np.ones(self.d)\n    self._lam = np.array(lam)\n    if self._lam.size == 1:\n        self._lam = self._lam.item() * np.ones(self.d)\n    if not (\n        self._gamma.shape == (self.d,)\n        and self._xi.shape == (self.d,)\n        and self._delta.shape == (self.d,)\n        and self._lam.shape == (self.d,)\n    ):\n        raise DimensionError(\n            \"all Johnson's S_U parameters be scalar or have length equal to dimension.\"\n        )\n    if not ((self._delta &gt; 0).all() and (self._lam &gt; 0).all()):\n        raise ParameterError(\"delta and lam must be all be positive\")\n    super(JohnsonsSU, self).__init__()\n    assert (\n        self._gamma.shape == (self.d,)\n        and self._xi.shape == (self.d,)\n        and self._delta.shape == (self.d,)\n        and self._lam.shape == (self.d,)\n    )\n</code></pre>"},{"location":"api/true_measures/#kumaraswamy","title":"<code>Kumaraswamy</code>","text":"<p>               Bases: <code>AbstractTrueMeasure</code></p> <p>Kumaraswamy distribution as described in https://en.wikipedia.org/wiki/Kumaraswamy_distribution.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; true_measure = Kumaraswamy(DigitalNetB2(2,seed=7),a=[1,2],b=[3,4])\n&gt;&gt;&gt; true_measure(4)\narray([[0.34705366, 0.6782161 ],\n       [0.0577568 , 0.36189538],\n       [0.76344358, 0.0932949 ],\n       [0.17065545, 0.43009386]])\n&gt;&gt;&gt; true_measure\nKumaraswamy (AbstractTrueMeasure)\n    a               [1 2]\n    b               [3 4]\n</code></pre> <p>With independent replications</p> <pre><code>&gt;&gt;&gt; x = Kumaraswamy(DigitalNetB2(3,seed=7,replications=2),a=[1,2,3],b=[3,4,5])(4)\n&gt;&gt;&gt; x.shape\n(2, 4, 3)\n&gt;&gt;&gt; x\narray([[[0.09004177, 0.22144305, 0.62190133],\n        [0.31710078, 0.48718217, 0.47325643],\n        [0.19657641, 0.57423463, 0.25697057],\n        [0.56103074, 0.28939035, 0.63654112]],\n\n       [[0.18006788, 0.62226635, 0.5083556 ],\n        [0.22602452, 0.10519477, 0.42823814],\n        [0.08428482, 0.28804621, 0.2414302 ],\n        [0.37253319, 0.45379743, 0.63366422]]])\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>sampler</code> <code>Union[AbstractDiscreteDistribution, AbstractTrueMeasure]</code> <p>Either</p> <ul> <li>a discrete distribution from which to transform samples, or</li> <li>a true measure by which to compose a transform.</li> </ul> required <code>a</code> <code>Union[float, ndarray]</code> <p>First parameter \\(\\alpha &gt; 0\\).</p> <code>2</code> <code>b</code> <code>Union[float, ndarray]</code> <p>Second parameter \\(\\beta &gt; 0\\).</p> <code>2</code> Source code in <code>qmcpy/true_measure/kumaraswamy.py</code> <pre><code>def __init__(self, sampler, a=2, b=2):\n    r\"\"\"\n    Args:\n        sampler (Union[AbstractDiscreteDistribution, AbstractTrueMeasure]): Either\n\n            - a discrete distribution from which to transform samples, or\n            - a true measure by which to compose a transform.\n        a (Union[float, np.ndarray]): First parameter $\\alpha &gt; 0$.\n        b (Union[float, np.ndarray]): Second parameter $\\beta &gt; 0$.\n    \"\"\"\n    self.parameters = [\"a\", \"b\"]\n    self.domain = np.array([[0, 1]])\n    self.range = np.array([[0, 1]])\n    self._parse_sampler(sampler)\n    self.a = a\n    self.b = b\n    self.alpha = np.array(a)\n    if self.alpha.size == 1:\n        self.alpha = self.alpha.item() * np.ones(self.d)\n        a = np.tile(self.a, self.d)\n    self.beta = np.array(b)\n    if self.beta.size == 1:\n        self.beta = self.beta.item() * np.ones(self.d)\n    if not (self.alpha.shape == (self.d,) and self.beta.shape == (self.d,)):\n        raise DimensionError(\n            \"a and b must be scalar or have length equal to dimension.\"\n        )\n    if not ((self.alpha &gt; 0).all() and (self.beta &gt; 0).all()):\n        raise ParameterError(\"Kumaraswamy requires a,b&gt;0.\")\n    super(Kumaraswamy, self).__init__()\n    assert self.alpha.shape == (self.d,) and self.beta.shape == (self.d,)\n</code></pre>"},{"location":"api/true_measures/#uml-specific","title":"UML Specific","text":""},{"location":"api/util/","title":"Util","text":""},{"location":"api/util/#utilities","title":"Utilities","text":""},{"location":"api/util/#plot_proj","title":"<code>plot_proj</code>","text":"<p>Parameters:</p> Name Type Description Default <code>sampler</code> <code>(DiscreteDistribution, TrueMeasure)</code> <p>The generator of samples to be plotted.</p> required <code>n</code> <code>Union[int, list]</code> <p>The number of samples or a list of samples(used for extensibility) to be plotted.</p> <code>64</code> <code>d_horizontal</code> <code>Union[int, list]</code> <p>The dimension or list of dimensions to be plotted on the horizontal axes.</p> <code>1</code> <code>d_vertical</code> <code>Union[int, list]</code> <p>The dimension or list of dimensions to be plotted on the vertical axes.</p> <code>2</code> <code>math_ind</code> <code>bool</code> <p>Setting to <code>True</code> will enable user to pass in math indices.</p> <code>True</code> <code>marker_size</code> <code>float</code> <p>The marker size (typographic points are 1/72 in.).</p> <code>5</code> <code>figfac</code> <code>float</code> <p>The figure size factor.</p> <code>5</code> <code>fig_title</code> <code>str</code> <p>The title of the figure.</p> <code>'Projection of Samples'</code> <code>axis_pad</code> <code>float</code> <p>The padding of the axis so that points on the boundaries can be seen.</p> <code>0</code> <code>want_grid</code> <code>bool</code> <p>Setting to <code>True</code> will enable grid on the plot.</p> <code>True</code> <code>font_family</code> <code>str</code> <p>The font family of the plot.</p> <code>'sans-serif'</code> <code>where_title</code> <code>float</code> <p>the position of the title on the plot. Default value is 1.</p> <code>1</code> <code>**kwargs</code> <code>dict</code> <p>Additional keyword arguments passed to <code>matplotlib.pyplot.scatter</code>.</p> <code>{}</code> Source code in <code>qmcpy/util/plot_functions.py</code> <pre><code>def plot_proj(\n    sampler,\n    n=64,\n    d_horizontal=1,\n    d_vertical=2,\n    math_ind=True,\n    marker_size=5,\n    figfac=5,\n    fig_title=\"Projection of Samples\",\n    axis_pad=0,\n    want_grid=True,\n    font_family=\"sans-serif\",\n    where_title=1,\n    **kwargs\n):\n    \"\"\"\n    Args:\n        sampler (DiscreteDistribution,TrueMeasure): The generator of samples to be plotted.\n        n (Union[int, list]): The number of samples or a list of samples(used for extensibility) to be plotted.\n        d_horizontal (Union[int, list]): The dimension or list of dimensions to be plotted on the horizontal axes.\n        d_vertical (Union[int, list]): The dimension or list of dimensions to be plotted on the vertical axes.\n        math_ind (bool): Setting to `True` will enable user to pass in math indices.\n        marker_size (float): The marker size (typographic points are 1/72 in.).\n        figfac (float): The figure size factor.\n        fig_title (str): The title of the figure.\n        axis_pad (float): The padding of the axis so that points on the boundaries can be seen.\n        want_grid (bool): Setting to `True` will enable grid on the plot.\n        font_family (str): The font family of the plot.\n        where_title (float): the position of the title on the plot. Default value is 1.\n        **kwargs (dict): Additional keyword arguments passed to `matplotlib.pyplot.scatter`.\n    \"\"\"\n    try:\n        import matplotlib.pyplot as plt\n        from matplotlib import colors\n\n        dir_path = os.path.dirname(os.path.realpath(__file__))\n        plt.style.use(os.path.join(dir_path, \"qmcpy.mplstyle\"))\n    except:\n        raise ImportError(\n            \"Missing matplotlib.pyplot as plt, Matplotlib must be installed to run plot_proj function\"\n        )\n    plt.rcParams[\"font.family\"] = font_family\n    n = np.atleast_1d(n)\n    d_horizontal = np.atleast_1d(d_horizontal)\n    d_vertical = np.atleast_1d(d_vertical)\n    samples = sampler(n[n.size - 1])\n    d = samples.shape[1]\n    fig, ax = plt.subplots(\n        nrows=d_horizontal.size,\n        ncols=d_vertical.size,\n        figsize=(figfac * d_horizontal.size, figfac * d_vertical.size),\n        squeeze=False,\n    )\n\n    colors = plt.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"]\n    for i in range(d_horizontal.size):\n        for j in range(d_vertical.size):\n            n_min = 0\n            for m in range(n.size):\n                n_max = n[m]\n                if d_horizontal[i] == d_vertical[j]:\n                    ax[i, j].remove()\n                    break\n                if math_ind is True:\n                    x = d_horizontal[i] - 1\n                    y = d_vertical[j] - 1\n                    x_label_num = d_horizontal[i]\n                    y_label_num = d_vertical[j]\n                else:\n                    x = d_horizontal[i]\n                    y = d_vertical[j]\n                    x_label_num = d_horizontal[i] + 1\n                    y_label_num = d_vertical[j] + 1\n\n                if isinstance(sampler, qp.AbstractDiscreteDistribution):\n                    ax[i, j].set_xlim([0 - axis_pad, 1 + axis_pad])\n                    ax[i, j].set_ylim([0 - axis_pad, 1 + axis_pad])\n                    ax[i, j].set_xticks([0, 1 / 4, 1 / 2, 3 / 4, 1])\n                    ax[i, j].set_yticks([0, 1 / 4, 1 / 2, 3 / 4, 1])\n                    ax[i, j].set_aspect(1)\n                    if not want_grid:\n                        ax[i, j].grid(False)\n                        ax[i, j].tick_params(\n                            axis=\"both\", which=\"both\", direction=\"in\", length=5\n                        )\n                    ax[i, j].grid(want_grid)\n                    x_label = r\"$x_{i%d}$\" % (x_label_num)\n                    y_label = r\"$x_{i%d}$\" % (y_label_num)\n                else:\n                    x_label = r\"$t_{i%d}$\" % (x_label_num)\n                    y_label = r\"$t_{i%d}$\" % (y_label_num)\n\n                ax[i, j].set_xlabel(x_label, fontsize=15)\n                if d &gt; 1:\n                    ax[i, j].set_ylabel(y_label, fontsize=15)\n                    y_axis = samples[n_min:n_max, y]\n                else:\n                    y_axis = []\n                    for h in range(n_max - n_min):\n                        y_axis.append(0.5)\n                ax[i, j].scatter(\n                    samples[n_min:n_max, x],\n                    y_axis,\n                    s=marker_size,\n                    color=colors[m],\n                    label=\"n_min = %d, n_max = %d\" % (n_min, n_max),\n                    **kwargs,\n                )\n                n_min = n[m]\n    plt.suptitle(fig_title, fontsize=20, y=where_title)\n    # fig.text(0.55,0.55,fig_title, ha = 'center', va = 'center', fontsize = 20) %replaced by plt.suptitle\n    fig.tight_layout()  # pad=2)\n    return fig, ax\n</code></pre>"},{"location":"api/util/#mlmc_test","title":"<code>mlmc_test</code>","text":"<p>Multilevel Monte Carlo test routine.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; fo = qp.FinancialOption(\n...     sampler=qp.IIDStdUniform(seed=7),\n...     option = \"ASIAN\",\n...     asian_mean = \"GEOMETRIC\",\n...     volatility = 0.2, \n...     start_price = 100, \n...     strike_price = 100, \n...     interest_rate = 0.05, \n...     t_final = 1)\n&gt;&gt;&gt; print('Exact Value: %s'%fo.get_exact_value_inf_dim())\nExact Value: 5.546818633789201\n&gt;&gt;&gt; mlmc_test(fo)\nConvergence tests, kurtosis, telescoping sum check using N =  20000 samples\n    l              ave(Pf-Pc)     ave(Pf)        var(Pf-Pc)     var(Pf)        kurtosis       check          cost\n    0              5.4486e+00     5.4486e+00     5.673e+01      5.673e+01      0.00e+00       0.00e+00       2.00e+00\n    1              1.4925e-01     5.5937e+00     3.839e+00      5.838e+01      5.48e+00       1.14e-02       4.00e+00\n    2              3.5921e-02     5.6024e+00     9.585e-01      5.990e+01      5.59e+00       7.86e-02       8.00e+00\n    3              8.7217e-03     5.5128e+00     2.332e-01      5.828e+01      5.36e+00       2.92e-01       1.60e+01\n    4              1.9773e-03     5.6850e+00     6.021e-02      6.081e+01      5.46e+00       5.12e-01       3.20e+01\n    5              9.5925e-04     5.5628e+00     1.512e-02      5.939e+01      5.37e+00       3.71e-01       6.40e+01\n    6              8.5998e-04     5.5706e+00     3.773e-03      5.995e+01      5.48e+00       2.10e-02       1.28e+02\n    7              1.3592e-04     5.4359e+00     9.285e-04      5.808e+01      5.51e+00       4.13e-01       2.56e+02\n    8              3.4520e-05     5.5322e+00     2.313e-04      5.881e+01      5.57e+00       2.96e-01       5.12e+02\nLinear regression estimates of MLMC parameters\n    alpha = 1.617207  (exponent for MLMC weak convergence)\n    beta  = 2.000355  (exponent for MLMC variance)\n    gamma = 1.000000  (exponent for MLMC cost)\nMLMC complexity tests\n    rmse_tol       value          mlmc_cost      std_cost       savings        N_l\n    5.000e-03      5.545e+00      3.339e+07      1.038e+08      3.11           8605392      1566846      559701       198886       70359        \n    1.000e-02      5.539e+00      7.272e+06      1.243e+07      1.71           2009192      365451       130781       46623        \n    2.000e-02      5.549e+00      1.827e+06      3.108e+06      1.70           503397       91821        33196        11736        \n    5.000e-02      5.474e+00      2.324e+05      2.556e+05      1.10           71432        13143        4617         \n    1.000e-01      5.466e+00      6.220e+04      6.389e+04      1.03           19477        3361         1225         \n</code></pre> <p>Parameters:</p> Name Type Description Default <code>integrand</code> <code>AbstractIntegrand</code> <p>multilevel integrand</p> required <code>n</code> <code>int</code> <p>number of samples for convergence tests</p> <code>20000</code> <code>l</code> <code>int</code> <p>number of levels for convergence tests</p> <code>8</code> <code>n_init</code> <code>int</code> <p>initial number of samples for MLMC calcs</p> <code>200</code> <code>rmse_tols</code> <code>ndarray</code> <p>desired accuracy array for MLMC calcs</p> <code>array([0.005, 0.01, 0.02, 0.05, 0.1])</code> <code>levels_min</code> <code>int</code> <p>minimum number of levels for MLMC calcs</p> <code>2</code> <code>levels_max</code> <code>int</code> <p>maximum number of levels for MLMC calcs</p> <code>10</code> Source code in <code>qmcpy/util/mlmc_test.py</code> <pre><code>def mlmc_test(\n    integrand,\n    n = 20000,\n    l = 8,\n    n_init = 200,\n    rmse_tols = np.array([.005, 0.01, 0.02, 0.05, 0.1]),\n    levels_min = 2,\n    levels_max = 10,\n    ):\n    r\"\"\"\n    Multilevel Monte Carlo test routine.\n\n    Examples:\n        &gt;&gt;&gt; fo = qp.FinancialOption(\n        ...     sampler=qp.IIDStdUniform(seed=7),\n        ...     option = \"ASIAN\",\n        ...     asian_mean = \"GEOMETRIC\",\n        ...     volatility = 0.2, \n        ...     start_price = 100, \n        ...     strike_price = 100, \n        ...     interest_rate = 0.05, \n        ...     t_final = 1)\n        &gt;&gt;&gt; print('Exact Value: %s'%fo.get_exact_value_inf_dim())\n        Exact Value: 5.546818633789201\n        &gt;&gt;&gt; mlmc_test(fo)\n        Convergence tests, kurtosis, telescoping sum check using N =  20000 samples\n            l              ave(Pf-Pc)     ave(Pf)        var(Pf-Pc)     var(Pf)        kurtosis       check          cost\n            0              5.4486e+00     5.4486e+00     5.673e+01      5.673e+01      0.00e+00       0.00e+00       2.00e+00\n            1              1.4925e-01     5.5937e+00     3.839e+00      5.838e+01      5.48e+00       1.14e-02       4.00e+00\n            2              3.5921e-02     5.6024e+00     9.585e-01      5.990e+01      5.59e+00       7.86e-02       8.00e+00\n            3              8.7217e-03     5.5128e+00     2.332e-01      5.828e+01      5.36e+00       2.92e-01       1.60e+01\n            4              1.9773e-03     5.6850e+00     6.021e-02      6.081e+01      5.46e+00       5.12e-01       3.20e+01\n            5              9.5925e-04     5.5628e+00     1.512e-02      5.939e+01      5.37e+00       3.71e-01       6.40e+01\n            6              8.5998e-04     5.5706e+00     3.773e-03      5.995e+01      5.48e+00       2.10e-02       1.28e+02\n            7              1.3592e-04     5.4359e+00     9.285e-04      5.808e+01      5.51e+00       4.13e-01       2.56e+02\n            8              3.4520e-05     5.5322e+00     2.313e-04      5.881e+01      5.57e+00       2.96e-01       5.12e+02\n        Linear regression estimates of MLMC parameters\n            alpha = 1.617207  (exponent for MLMC weak convergence)\n            beta  = 2.000355  (exponent for MLMC variance)\n            gamma = 1.000000  (exponent for MLMC cost)\n        MLMC complexity tests\n            rmse_tol       value          mlmc_cost      std_cost       savings        N_l\n            5.000e-03      5.545e+00      3.339e+07      1.038e+08      3.11           8605392      1566846      559701       198886       70359        \n            1.000e-02      5.539e+00      7.272e+06      1.243e+07      1.71           2009192      365451       130781       46623        \n            2.000e-02      5.549e+00      1.827e+06      3.108e+06      1.70           503397       91821        33196        11736        \n            5.000e-02      5.474e+00      2.324e+05      2.556e+05      1.10           71432        13143        4617         \n            1.000e-01      5.466e+00      6.220e+04      6.389e+04      1.03           19477        3361         1225         \n\n    Args:\n        integrand (AbstractIntegrand): multilevel integrand\n        n (int): number of samples for convergence tests\n        l (int): number of levels for convergence tests\n        n_init (int): initial number of samples for MLMC calcs\n        rmse_tols (np.ndarray): desired accuracy array for MLMC calcs\n        levels_min (int): minimum number of levels for MLMC calcs\n        levels_max (int): maximum number of levels for MLMC calcs\n    \"\"\"\n    # first, convergence tests\n    n = 100*np.ceil(n/100) # make N a multiple of 100\n    print('Convergence tests, kurtosis, telescoping sum check using N =%7d samples'%n)\n    print('    %-15s%-15s%-15s%-15s%-15s%-15s%-15s%s'\\\n        %('l','ave(Pf-Pc)','ave(Pf)','var(Pf-Pc)','var(Pf)','kurtosis','check','cost'))\n    del1 = np.array([])\n    del2 = np.array([])\n    var1 = np.array([])\n    var2 = np.array([])\n    kur1 = np.array([])\n    chk1 = np.array([])\n    cost = np.array([])\n    integrand_spawns = integrand.spawn(levels=np.arange(l+1))\n    for ll in range(l+1):\n        sums = np.zeros(6)\n        cst = 0\n        integrand_spawn = integrand_spawns[ll]\n        for j in range(1,101):\n            # evaluate integral at sampleing points samples\n            samples = integrand_spawn.discrete_distrib.gen_samples(n=n/100)\n            Pc,Pf = integrand_spawn.f(samples)\n            dP = Pf-Pc\n            sums_j = np.array([\n                np.sum(dP),\n                np.sum(dP**2),\n                np.sum(dP**3),\n                np.sum(dP**4),\n                np.sum(Pf),\n                np.sum(Pf**2),\n            ])\n            cst_j = integrand_spawn.cost*(n/100)\n            sums = sums + sums_j/n\n            cst = cst + cst_j/n\n        if ll == 0:\n            kurt = 0.\n        else:\n            kurt = ( sums[3] - 4*sums[2]*sums[0] + 6*sums[1]*sums[0]**2 - \n                     3*sums[0]*sums[0]**3 ) /  (sums[1]-sums[0]**2)**2.\n        cost = np.hstack((cost, cst))\n        del1 = np.hstack((del1, sums[0]))\n        del2 = np.hstack((del2, sums[4]))\n        var1 = np.hstack((var1, sums[1]-sums[0]**2))\n        var2 = np.hstack((var2, sums[5]-sums[4]**2))\n        var2 = np.maximum(var2, 1e-10) # fix for cases with var=0\n        kur1 = np.hstack((kur1, kurt))\n        if ll == 0:\n            check = 0\n        else:\n            check = abs( del1[ll] + del2[ll-1] - del2[ll]) / \\\n                    ( 3.*( np.sqrt(var1[ll]) + np.sqrt(var2[ll-1]) + np.sqrt(var2[ll]) ) / np.sqrt(n))\n        chk1 = np.hstack((chk1, check))\n\n        print('    %-15d%-15.4e%-15.4e%-15.3e%-15.3e%-15.2e%-15.2e%.2e'\\\n              %(ll,del1[ll],del2[ll],var1[ll],var2[ll],kur1[ll],chk1[ll],cst))\n    # print out a warning if kurtosis or consistency check looks bad\n    if kur1[-1] &gt; 100.:\n        print('WARNING: kurtosis on finest level = %f'%kur1[-1])\n        print(' indicates MLMC correction dominated by a few rare paths;')\n        print(' for information on the connection to variance of sample variances,')\n        print(' see http://mathworld.wolfram.com/SampleVarianceDistribution.html\\n')\n    if np.max(chk1) &gt; 1.:\n        print('WARNING: maximum consistency error = %f'%max(chk1))\n        print(' indicates identity E[Pf-Pc] = E[Pf] - E[Pc] not satisfied;')\n        print(' to be more certain, re-run mlmc_test with larger N\\n')\n    # use linear regression to estimate alpha, beta and gamma\n    l1 = 2\n    l2 = l+1\n    x = np.ones((l2+1-l1,2))\n    x[:,1] = np.arange(l1,l2+1)\n    pa = np.linalg.lstsq(x,np.log2(np.absolute(del1[(l1-1):l2])),rcond=None)[0]\n    alpha = -pa[1]\n    pb = np.linalg.lstsq(x,np.log2(np.absolute(var1[(l1-1):l2])),rcond=None)[0]\n    beta = -pb[1]\n    pg = np.linalg.lstsq(x,np.log2(np.absolute(cost[(l1-1):l2])),rcond=None)[0]\n    gamma = pg[1]\n    print('Linear regression estimates of MLMC parameters')\n    print('    alpha = %f  (exponent for MLMC weak convergence)'%alpha)\n    print('    beta  = %f  (exponent for MLMC variance)'%beta)\n    print('    gamma = %f  (exponent for MLMC cost)'%gamma)\n    #second, mlmc complexity tests\n    print('MLMC complexity tests')\n    print('    %-15s%-15s%-15s%-15s%-15s%s'\\\n        %('rmse_tol','value','mlmc_cost','std_cost','savings','N_l'))\n    alpha = np.maximum(alpha,0.5)\n    beta  = np.maximum(beta,0.5)\n    theta = 0.25\n    for i in range(len(rmse_tols)):\n        mlmc_qmcpy = qp.CubMLMC(integrand,\n            rmse_tol = rmse_tols[i],\n            n_init = n_init,\n            levels_min = levels_min,\n            levels_max = levels_max,\n            alpha0 = alpha,\n            beta0 = beta,\n            gamma0 = gamma)\n        sol,data = mlmc_qmcpy.integrate()\n        p = data.solution\n        nl = data.n_level\n        cl = data.cost_per_sample\n        mlmc_cost = sum(nl*cl)\n        idx = np.minimum(len(var2),len(nl))-1\n        std_cost = var2[idx]*cl[-1] / ((1.-theta)*rmse_tols[i]**2)\n        print('    %-15.3e%-15.3e%-15.3e%-15.3e%-15.2f%s'\\\n            %(rmse_tols[i], p, mlmc_cost, std_cost, std_cost/mlmc_cost,''.join('%-13d'%nli for nli in nl)))\n</code></pre>"},{"location":"api/util/#shift-invariant-ops","title":"Shift Invariant Ops","text":""},{"location":"api/util/#utilbernoulli_poly","title":"<code>util.bernoulli_poly</code>","text":"<p>\\(n^\\text{th}\\) Bernoulli polynomial</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; x = np.arange(6).reshape((2,3))/6\n&gt;&gt;&gt; available_n = list(BERNOULLIPOLYSDICT.keys())\n&gt;&gt;&gt; available_n\n[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n&gt;&gt;&gt; for n in available_n:\n...     y = bernoulli_poly(n,x)\n...     with np.printoptions(precision=2):\n...         print(\"n = %d\\n%s\"%(n,y))\nn = 1\n[[-0.5  -0.33 -0.17]\n [ 0.    0.17  0.33]]\nn = 2\n[[ 0.17  0.03 -0.06]\n [-0.08 -0.06  0.03]]\nn = 3\n[[ 0.    0.05  0.04]\n [ 0.   -0.04 -0.05]]\nn = 4\n[[-0.03 -0.01  0.02]\n [ 0.03  0.02 -0.01]]\nn = 5\n[[ 0.00e+00 -2.19e-02 -2.06e-02]\n [ 1.39e-17  2.06e-02  2.19e-02]]\nn = 6\n[[ 0.02  0.01 -0.01]\n [-0.02 -0.01  0.01]]\nn = 7\n[[ 0.00e+00  2.28e-02  2.24e-02]\n [-1.39e-17 -2.24e-02 -2.28e-02]]\nn = 8\n[[-0.03 -0.02  0.02]\n [ 0.03  0.02 -0.02]]\nn = 9\n[[ 0.   -0.04 -0.04]\n [ 0.    0.04  0.04]]\nn = 10\n[[ 0.08  0.04 -0.04]\n [-0.08 -0.04  0.04]]\n&gt;&gt;&gt; import scipy.special\n&gt;&gt;&gt; for n in available_n:\n...     bpoly_coeffs = BERNOULLIPOLYSDICT[n].coeffs\n...     bpoly_coeffs_true = scipy.special.bernoulli(n)*scipy.special.comb(n,np.arange(n,-1,-1))\n...     assert np.allclose(bpoly_coeffs_true,bpoly_coeffs,atol=1e-12)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>int</code> <p>Polynomial order.</p> required <code>x</code> <code>Union[ndarray, Tensor]</code> <p>Points at which to evaluate the Bernoulli polynomial.</p> required <p>Returns:</p> Name Type Description <code>y</code> <code>Union[ndarray, Tensor]</code> <p>Bernoulli polynomial values.</p> Source code in <code>qmcpy/util/shift_invar_ops.py</code> <pre><code>def bernoulli_poly(n, x):\n    r\"\"\"\n    $n^\\text{th}$ Bernoulli polynomial\n\n    Examples:\n        &gt;&gt;&gt; x = np.arange(6).reshape((2,3))/6\n        &gt;&gt;&gt; available_n = list(BERNOULLIPOLYSDICT.keys())\n        &gt;&gt;&gt; available_n\n        [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n        &gt;&gt;&gt; for n in available_n:\n        ...     y = bernoulli_poly(n,x)\n        ...     with np.printoptions(precision=2):\n        ...         print(\"n = %d\\n%s\"%(n,y))\n        n = 1\n        [[-0.5  -0.33 -0.17]\n         [ 0.    0.17  0.33]]\n        n = 2\n        [[ 0.17  0.03 -0.06]\n         [-0.08 -0.06  0.03]]\n        n = 3\n        [[ 0.    0.05  0.04]\n         [ 0.   -0.04 -0.05]]\n        n = 4\n        [[-0.03 -0.01  0.02]\n         [ 0.03  0.02 -0.01]]\n        n = 5\n        [[ 0.00e+00 -2.19e-02 -2.06e-02]\n         [ 1.39e-17  2.06e-02  2.19e-02]]\n        n = 6\n        [[ 0.02  0.01 -0.01]\n         [-0.02 -0.01  0.01]]\n        n = 7\n        [[ 0.00e+00  2.28e-02  2.24e-02]\n         [-1.39e-17 -2.24e-02 -2.28e-02]]\n        n = 8\n        [[-0.03 -0.02  0.02]\n         [ 0.03  0.02 -0.02]]\n        n = 9\n        [[ 0.   -0.04 -0.04]\n         [ 0.    0.04  0.04]]\n        n = 10\n        [[ 0.08  0.04 -0.04]\n         [-0.08 -0.04  0.04]]\n        &gt;&gt;&gt; import scipy.special\n        &gt;&gt;&gt; for n in available_n:\n        ...     bpoly_coeffs = BERNOULLIPOLYSDICT[n].coeffs\n        ...     bpoly_coeffs_true = scipy.special.bernoulli(n)*scipy.special.comb(n,np.arange(n,-1,-1))\n        ...     assert np.allclose(bpoly_coeffs_true,bpoly_coeffs,atol=1e-12)\n\n    Args:\n        n (int): Polynomial order.\n        x (Union[np.ndarray, torch.Tensor]): Points at which to evaluate the Bernoulli polynomial.\n\n    Returns:\n        y (Union[np.ndarray, torch.Tensor]): Bernoulli polynomial values.\n    \"\"\"\n    assert isinstance(n, int)\n    assert n in BERNOULLIPOLYSDICT, \"n = %d not in BERNOULLIPOLYSDICT\" % n\n    bpoly = BERNOULLIPOLYSDICT[n]\n    y = bpoly(x)\n    return y\n</code></pre>"},{"location":"api/util/#digitally-shift-invariant-ops","title":"Digitally Shift Invariant Ops","text":""},{"location":"api/util/#utilweighted_walsh_funcs","title":"<code>util.weighted_walsh_funcs</code>","text":"<p>Weighted walsh functions</p> \\[\\sum_{k=0}^\\infty \\mathrm{wal}_k(x) 2^{-\\mu_\\alpha(k)}\\] <p>where \\(\\mathrm{wal}_k\\) is the \\(k^\\text{th}\\) Walsh function and \\(\\mu_\\alpha\\) is the Dick weight function which sums the first \\(\\alpha\\) largest indices of \\(1\\) bits in the binary expansion of \\(k\\) e.g. \\(k=13=1101_2\\) has 1-bit indexes \\((4,3,1)\\) so</p> \\[\\mu_1(k) = 4, \\mu_2(k) = 4+3, \\mu_3(k) = 4+3+1 = \\mu_4(k) = \\mu_5(k) = \\dots\\] <p>Examples:</p> <pre><code>&gt;&gt;&gt; t = 3\n&gt;&gt;&gt; rng = np.random.Generator(np.random.SFC64(11))\n&gt;&gt;&gt; xb = rng.integers(low=0,high=2**t,size=(2,3))\n&gt;&gt;&gt; available_alpha = list(WEIGHTEDWALSHFUNCSPOS.keys())\n&gt;&gt;&gt; available_alpha\n[2, 3, 4]\n&gt;&gt;&gt; for alpha in available_alpha:\n...     y = weighted_walsh_funcs(alpha,xb,t)\n...     with np.printoptions(precision=2):\n...         print(\"alpha = %d\\n%s\"%(alpha,y))\nalpha = 2\n[[1.81 1.38 1.81]\n [0.62 1.81 2.5 ]]\nalpha = 3\n[[1.85 1.43 1.85]\n [0.62 1.85 2.39]]\nalpha = 4\n[[1.85 1.43 1.85]\n [0.62 1.85 2.38]]\n&gt;&gt;&gt; import torch\n&gt;&gt;&gt; for alpha in available_alpha:\n...     y = weighted_walsh_funcs(alpha,torch.from_numpy(xb),t)\n...     with torch._tensor_str.printoptions(precision=2):\n...         print(\"alpha = %d\\n%s\"%(alpha,y))\nalpha = 2\ntensor([[1.81, 1.38, 1.81],\n        [0.62, 1.81, 2.50]])\nalpha = 3\ntensor([[1.85, 1.43, 1.85],\n        [0.62, 1.85, 2.39]])\nalpha = 4\ntensor([[1.85, 1.43, 1.85],\n        [0.62, 1.85, 2.38]])\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>alpha</code> <code>int</code> <p>Weighted walsh functions order.</p> required <code>xb</code> <code>Union[ndarray, Tensor]</code> <p>Integer points at which to evaluate the weighted Walsh function.</p> required <code>t</code> <code>int</code> <p>Number of bits in each integer in xb.</p> required <p>Returns:</p> Name Type Description <code>y</code> <code>Union[ndarray, Tensor]</code> <p>Weighted Walsh function values.</p> <p>References:</p> <ol> <li> <p>Dick, Josef.     \"Walsh spaces containing smooth functions and quasi\u2013Monte Carlo rules of arbitrary high order.\"     SIAM Journal on Numerical Analysis 46.3 (2008): 1519-1553.</p> </li> <li> <p>Dick, Josef.     \"The decay of the Walsh coefficients of smooth functions.\"     Bulletin of the Australian Mathematical Society 80.3 (2009): 430-453.</p> </li> </ol> Source code in <code>qmcpy/util/dig_shift_invar_ops.py</code> <pre><code>def weighted_walsh_funcs(alpha, xb, t):\n    r\"\"\"\n    Weighted walsh functions\n\n    $$\\sum_{k=0}^\\infty \\mathrm{wal}_k(x) 2^{-\\mu_\\alpha(k)}$$\n\n    where $\\mathrm{wal}_k$ is the $k^\\text{th}$ Walsh function\n    and $\\mu_\\alpha$ is the Dick weight function which sums the first $\\alpha$ largest indices of $1$ bits in the binary expansion of $k$\n    e.g. $k=13=1101_2$ has 1-bit indexes $(4,3,1)$ so\n\n    $$\\mu_1(k) = 4, \\mu_2(k) = 4+3, \\mu_3(k) = 4+3+1 = \\mu_4(k) = \\mu_5(k) = \\dots$$\n\n    Examples:\n        &gt;&gt;&gt; t = 3\n        &gt;&gt;&gt; rng = np.random.Generator(np.random.SFC64(11))\n        &gt;&gt;&gt; xb = rng.integers(low=0,high=2**t,size=(2,3))\n        &gt;&gt;&gt; available_alpha = list(WEIGHTEDWALSHFUNCSPOS.keys())\n        &gt;&gt;&gt; available_alpha\n        [2, 3, 4]\n        &gt;&gt;&gt; for alpha in available_alpha:\n        ...     y = weighted_walsh_funcs(alpha,xb,t)\n        ...     with np.printoptions(precision=2):\n        ...         print(\"alpha = %d\\n%s\"%(alpha,y))\n        alpha = 2\n        [[1.81 1.38 1.81]\n         [0.62 1.81 2.5 ]]\n        alpha = 3\n        [[1.85 1.43 1.85]\n         [0.62 1.85 2.39]]\n        alpha = 4\n        [[1.85 1.43 1.85]\n         [0.62 1.85 2.38]]\n        &gt;&gt;&gt; import torch\n        &gt;&gt;&gt; for alpha in available_alpha:\n        ...     y = weighted_walsh_funcs(alpha,torch.from_numpy(xb),t)\n        ...     with torch._tensor_str.printoptions(precision=2):\n        ...         print(\"alpha = %d\\n%s\"%(alpha,y))\n        alpha = 2\n        tensor([[1.81, 1.38, 1.81],\n                [0.62, 1.81, 2.50]])\n        alpha = 3\n        tensor([[1.85, 1.43, 1.85],\n                [0.62, 1.85, 2.39]])\n        alpha = 4\n        tensor([[1.85, 1.43, 1.85],\n                [0.62, 1.85, 2.38]])\n\n    Args:\n        alpha (int): Weighted walsh functions order.\n        xb (Union[np.ndarray, torch.Tensor]): Integer points at which to evaluate the weighted Walsh function.\n        t (int): Number of bits in each integer in xb.\n\n    returns:\n        y (Union[np.ndarray, torch.Tensor]): Weighted Walsh function values.\n\n    **References:**\n\n    1.  Dick, Josef.\n        \"Walsh spaces containing smooth functions and quasi\u2013Monte Carlo rules of arbitrary high order.\"\n        SIAM Journal on Numerical Analysis 46.3 (2008): 1519-1553.\n\n    2.  Dick, Josef.\n        \"The decay of the Walsh coefficients of smooth functions.\"\n        Bulletin of the Australian Mathematical Society 80.3 (2009): 430-453.\n    \"\"\"\n    assert isinstance(alpha, int)\n    assert alpha in WEIGHTEDWALSHFUNCSPOS, (\n        \"alpha = %d not in WEIGHTEDWALSHFUNCSPOS\" % alpha\n    )\n    assert alpha in WEIGHTEDWALSHFUNCSZEROS, (\n        \"alpha = %d not in WEIGHTEDWALSHFUNCSZEROS\" % alpha\n    )\n    if isinstance(xb, np.ndarray):\n        np_or_torch = np\n        y = np.ones(xb.shape)\n    else:\n        import torch\n\n        np_or_torch = torch\n        y = torch.ones(xb.shape, device=xb.device)\n    pidxs = xb &gt; 0\n    y[~pidxs] = WEIGHTEDWALSHFUNCSZEROS[alpha]\n    xfpidxs = (2 ** (-t)) * xb[pidxs]\n    betapidxs = -np_or_torch.floor(np_or_torch.log2(xfpidxs))\n    y[pidxs] = WEIGHTEDWALSHFUNCSPOS[alpha](betapidxs, xfpidxs, xb[pidxs], t)\n    return y\n</code></pre>"},{"location":"api/util/#utilk4sumterm","title":"<code>util.k4sumterm</code>","text":"\\[K_4(x) = \\sum_{a=0}^{t-1} \\frac{x_a}{2^{3a}}\\] <p>where \\(x_a\\) is the bit at index \\(a\\) in the binary expansion of \\(x\\) e.g. \\(x = 6\\) with \\(t=3\\) has \\((x_0,x_1,x_2) = (1,1,0)\\)</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; t = 3\n&gt;&gt;&gt; rng = np.random.Generator(np.random.SFC64(11))\n&gt;&gt;&gt; x = rng.integers(low=0,high=2**t,size=(5,4))\n&gt;&gt;&gt; with np.printoptions(precision=2):\n...     k4sumterm(x,t)\narray([[ 1.11,  0.89,  1.11, -0.89],\n       [ 1.11,  1.14,  1.11, -0.86],\n       [-0.89,  0.86, -1.11,  0.89],\n       [-1.11,  0.89, -0.89,  0.89],\n       [-1.14, -0.89, -0.89, -0.86]])\n&gt;&gt;&gt; import torch\n&gt;&gt;&gt; with torch._tensor_str.printoptions(precision=2):\n...     k4sumterm(torch.from_numpy(x),t)\ntensor([[ 1.11,  0.89,  1.11, -0.89],\n        [ 1.11,  1.14,  1.11, -0.86],\n        [-0.89,  0.86, -1.11,  0.89],\n        [-1.11,  0.89, -0.89,  0.89],\n        [-1.14, -0.89, -0.89, -0.86]])\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Union[np.ndarray torch.Tensor]</code> <p>Integer arrays.</p> required <code>t</code> <code>int</code> <p>Number of bits in each integer.</p> required <p>Returns:</p> Name Type Description <code>y</code> <code>Union[np.ndarray torch.Tensor]</code> <p>The \\(K_4\\) sum term.</p> Source code in <code>qmcpy/util/dig_shift_invar_ops.py</code> <pre><code>def k4sumterm(x, t, cutoff=1e-8):\n    r\"\"\"\n    $$K_4(x) = \\sum_{a=0}^{t-1} \\frac{x_a}{2^{3a}}$$\n\n    where $x_a$ is the bit at index $a$ in the binary expansion of $x$\n    e.g. $x = 6$ with $t=3$ has $(x_0,x_1,x_2) = (1,1,0)$\n\n    Examples:\n        &gt;&gt;&gt; t = 3\n        &gt;&gt;&gt; rng = np.random.Generator(np.random.SFC64(11))\n        &gt;&gt;&gt; x = rng.integers(low=0,high=2**t,size=(5,4))\n        &gt;&gt;&gt; with np.printoptions(precision=2):\n        ...     k4sumterm(x,t)\n        array([[ 1.11,  0.89,  1.11, -0.89],\n               [ 1.11,  1.14,  1.11, -0.86],\n               [-0.89,  0.86, -1.11,  0.89],\n               [-1.11,  0.89, -0.89,  0.89],\n               [-1.14, -0.89, -0.89, -0.86]])\n        &gt;&gt;&gt; import torch\n        &gt;&gt;&gt; with torch._tensor_str.printoptions(precision=2):\n        ...     k4sumterm(torch.from_numpy(x),t)\n        tensor([[ 1.11,  0.89,  1.11, -0.89],\n                [ 1.11,  1.14,  1.11, -0.86],\n                [-0.89,  0.86, -1.11,  0.89],\n                [-1.11,  0.89, -0.89,  0.89],\n                [-1.14, -0.89, -0.89, -0.86]])\n\n    Args:\n        x (Union[np.ndarray torch.Tensor]): Integer arrays.\n        t (int): Number of bits in each integer.\n\n    Returns:\n        y (Union[np.ndarray torch.Tensor]): The $K_4$ sum term.\n    \"\"\"\n    total = 0.0\n    for a in range(0, t):\n        factor = 1 / float(2.0 ** (3 * a))\n        if factor &lt; cutoff:\n            break\n        total += (-1.0) ** ((x &gt;&gt; (t - a - 1)) &amp; 1) * factor\n    return total\n</code></pre>"},{"location":"api/util/#utilto_float","title":"<code>util.to_float</code>","text":"<p>Convert binary representations of digital net samples in base \\(b=2\\) to floating point representations.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; xb = np.arange(8,dtype=np.uint64)\n&gt;&gt;&gt; xb\narray([0, 1, 2, 3, 4, 5, 6, 7], dtype=uint64)\n&gt;&gt;&gt; to_float(xb,3)\narray([0.   , 0.125, 0.25 , 0.375, 0.5  , 0.625, 0.75 , 0.875])\n&gt;&gt;&gt; xbtorch = bin_from_numpy_to_torch(xb)\n&gt;&gt;&gt; xbtorch\ntensor([0, 1, 2, 3, 4, 5, 6, 7])\n&gt;&gt;&gt; to_float(xbtorch,3)\ntensor([0.0000, 0.1250, 0.2500, 0.3750, 0.5000, 0.6250, 0.7500, 0.8750])\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Union[ndarray, Tensor]</code> <p>binary representation of samples with <code>dtype</code> either <code>np.uint64</code> or <code>torch.int64</code>.</p> required <code>t</code> <code>int</code> <p>number of bits in binary represtnations. Typically <code>dnb2.t</code> where <code>isinstance(dnb2,DigitalNetB2)</code>.</p> required <p>Returns:</p> Name Type Description <code>xf</code> <code>Unioin[ndarray, Tensor]</code> <p>floating point representation of samples.</p> Source code in <code>qmcpy/util/dig_shift_invar_ops.py</code> <pre><code>def to_float(x, t):\n    r\"\"\"\n    Convert binary representations of digital net samples in base $b=2$ to floating point representations.\n\n    Examples:\n        &gt;&gt;&gt; xb = np.arange(8,dtype=np.uint64)\n        &gt;&gt;&gt; xb\n        array([0, 1, 2, 3, 4, 5, 6, 7], dtype=uint64)\n        &gt;&gt;&gt; to_float(xb,3)\n        array([0.   , 0.125, 0.25 , 0.375, 0.5  , 0.625, 0.75 , 0.875])\n        &gt;&gt;&gt; xbtorch = bin_from_numpy_to_torch(xb)\n        &gt;&gt;&gt; xbtorch\n        tensor([0, 1, 2, 3, 4, 5, 6, 7])\n        &gt;&gt;&gt; to_float(xbtorch,3)\n        tensor([0.0000, 0.1250, 0.2500, 0.3750, 0.5000, 0.6250, 0.7500, 0.8750])\n\n    Args:\n        x (Union[np.ndarray, torch.Tensor]): binary representation of samples with `dtype` either `np.uint64` or `torch.int64`.\n        t (int): number of bits in binary represtnations. Typically `dnb2.t` where `isinstance(dnb2,DigitalNetB2)`.\n\n    Returns:\n        xf (Unioin[np.ndarray,torch.Tensor]): floating point representation of samples.\n    \"\"\"\n    npt = get_npt(x)\n    if npt == np:  # npt==torch\n        if x.dtype == np.uint64:\n            return x.astype(np.float64) * 2.0 ** (-t)\n        elif npt.is_floating_point(x):\n            return x\n        else:\n            raise ParameterError(\"x.dtype must be np.uint64, got %s\" % str(x.dtype))\n    else:\n        if x.dtype == npt.int64:\n            return x.to(npt.get_default_dtype()) * 2.0 ** (-t)\n        elif npt.is_floating_point(x):\n            return x\n        else:\n            raise ParameterError(\"x.dtype must be torch.int64, got %s\" % str(x.dtype))\n</code></pre>"},{"location":"api/util/#utilto_bin","title":"<code>util.to_bin</code>","text":"<p>Convert floating point representations of digital net samples in base \\(b=2\\) to binary representations.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; xf = np.random.Generator(np.random.PCG64(7)).uniform(low=0,high=1,size=(5))\n&gt;&gt;&gt; xf\narray([0.62509547, 0.8972138 , 0.77568569, 0.22520719, 0.30016628])\n&gt;&gt;&gt; xb = to_bin(xf,2)\n&gt;&gt;&gt; xb\narray([2, 3, 3, 0, 1], dtype=uint64)\n&gt;&gt;&gt; to_bin(xb,2)\narray([2, 3, 3, 0, 1], dtype=uint64)\n&gt;&gt;&gt; import torch\n&gt;&gt;&gt; xftorch = torch.from_numpy(xf)\n&gt;&gt;&gt; xftorch\ntensor([0.6251, 0.8972, 0.7757, 0.2252, 0.3002], dtype=torch.float64)\n&gt;&gt;&gt; xbtorch = to_bin(xftorch,2)\n&gt;&gt;&gt; xbtorch\ntensor([2, 3, 3, 0, 1])\n&gt;&gt;&gt; to_bin(xbtorch,2)\ntensor([2, 3, 3, 0, 1])\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Union[ndarray, Tensor]</code> <p>floating point representation of samples.</p> required <code>t</code> <code>int</code> <p>number of bits in binary represtnations. Typically <code>dnb2.t</code> where <code>isinstance(dnb2,DigitalNetB2)</code>.</p> required <p>Returns:</p> Name Type Description <code>xb</code> <code>Unioin[ndarray, Tensor]</code> <p>binary representation of samples with <code>dtype</code> either <code>np.uint64</code> or <code>torch.int64</code>.</p> Source code in <code>qmcpy/util/dig_shift_invar_ops.py</code> <pre><code>def to_bin(x, t):\n    r\"\"\"\n    Convert floating point representations of digital net samples in base $b=2$ to binary representations.\n\n    Examples:\n        &gt;&gt;&gt; xf = np.random.Generator(np.random.PCG64(7)).uniform(low=0,high=1,size=(5))\n        &gt;&gt;&gt; xf\n        array([0.62509547, 0.8972138 , 0.77568569, 0.22520719, 0.30016628])\n        &gt;&gt;&gt; xb = to_bin(xf,2)\n        &gt;&gt;&gt; xb\n        array([2, 3, 3, 0, 1], dtype=uint64)\n        &gt;&gt;&gt; to_bin(xb,2)\n        array([2, 3, 3, 0, 1], dtype=uint64)\n        &gt;&gt;&gt; import torch\n        &gt;&gt;&gt; xftorch = torch.from_numpy(xf)\n        &gt;&gt;&gt; xftorch\n        tensor([0.6251, 0.8972, 0.7757, 0.2252, 0.3002], dtype=torch.float64)\n        &gt;&gt;&gt; xbtorch = to_bin(xftorch,2)\n        &gt;&gt;&gt; xbtorch\n        tensor([2, 3, 3, 0, 1])\n        &gt;&gt;&gt; to_bin(xbtorch,2)\n        tensor([2, 3, 3, 0, 1])\n\n\n    Args:\n        x (Union[np.ndarray, torch.Tensor]): floating point representation of samples.\n        t (int): number of bits in binary represtnations. Typically `dnb2.t` where `isinstance(dnb2,DigitalNetB2)`.\n\n    Returns:\n        xb (Unioin[np.ndarray,torch.Tensor]): binary representation of samples with `dtype` either `np.uint64` or `torch.int64`.\n    \"\"\"\n    npt = get_npt(x)\n    if npt == np:\n        if npt.issubdtype(x.dtype, npt.floating):\n            return npt.floor((x % 1) * 2.0**t).astype(npt.uint64)\n        elif npt.issubdtype(x.dtype, npt.integer):\n            return x\n        else:\n            raise ParameterError(\"x.dtype must be float or int, got %s\" % str(x.dtype))\n    else:  # npt==torch\n        if npt.is_floating_point(x):\n            return npt.floor((x % 1) * 2.0**t).to(npt.int64)\n        elif (not npt.is_floating_point(x)) and (not npt.is_complex(x)):  # int type\n            return x\n        else:\n            raise ParameterError(\"x.dtype must be float or int, got %s\" % str(x.dtype))\n    return xb\n</code></pre>"},{"location":"api/util/#utilbin_from_numpy_to_torch","title":"<code>util.bin_from_numpy_to_torch</code>","text":"<p>Convert <code>numpy.uint64</code> to <code>torch.int64</code>, useful for converting binary samples from <code>DigitalNetB2</code> to torch representations.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; xb = np.arange(8,dtype=np.uint64)\n&gt;&gt;&gt; xb\narray([0, 1, 2, 3, 4, 5, 6, 7], dtype=uint64)\n&gt;&gt;&gt; bin_from_numpy_to_torch(xb)\ntensor([0, 1, 2, 3, 4, 5, 6, 7])\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>xb</code> <code>Union[ndarray]</code> <p>binary representation of samples with <code>dtype=np.uint64</code></p> required <p>Returns:</p> Name Type Description <code>xbtorch</code> <code>Unioin[Tensor]</code> <p>binary representation of samples with <code>dtype=torch.int64</code>.</p> Source code in <code>qmcpy/util/dig_shift_invar_ops.py</code> <pre><code>def bin_from_numpy_to_torch(xb):\n    r\"\"\"\n    Convert `numpy.uint64` to `torch.int64`, useful for converting binary samples from `DigitalNetB2` to torch representations.\n\n    Examples:\n        &gt;&gt;&gt; xb = np.arange(8,dtype=np.uint64)\n        &gt;&gt;&gt; xb\n        array([0, 1, 2, 3, 4, 5, 6, 7], dtype=uint64)\n        &gt;&gt;&gt; bin_from_numpy_to_torch(xb)\n        tensor([0, 1, 2, 3, 4, 5, 6, 7])\n\n    Args:\n        xb (Union[np.ndarray]): binary representation of samples with `dtype=np.uint64`\n\n    Returns:\n        xbtorch (Unioin[torch.Tensor]): binary representation of samples with `dtype=torch.int64`.\n    \"\"\"\n    assert xb.dtype == np.uint64\n    assert xb.max() &lt;= (2**63 - 1), \"require all xb &lt; 2^63\"\n    import torch\n\n    return torch.from_numpy(xb.astype(np.int64))\n</code></pre>"},{"location":"demos/asian-option-mlqmc/","title":"Multilevel (Q)MC for Option Pricing","text":"In\u00a0[1]: Copied! <pre>import matplotlib.pyplot as plt\nimport numpy as np\nimport qmcpy as qp\n</pre> import matplotlib.pyplot as plt import numpy as np import qmcpy as qp In\u00a0[2]: Copied! <pre>seed = 7\n</pre> seed = 7 <p>Compute the exact value of the Asian option with single level QMC, for an increasing number of time steps:</p> In\u00a0[4]: Copied! <pre>for level in range(5):\n    aco = qp.FinancialOption(qp.Sobol(2*2**level, seed=seed), option=\"ASIAN\", volatility=.2, start_price=100, strike_price=100, interest_rate=.05)\n    approx_solution, data = qp.CubQMCSobolG(aco, abs_tol=1e-4).integrate()\n    print(\"Asian Option true value (%d time steps): %.5f (to within 1e-4)\"%(2*2**level, approx_solution))\n</pre> for level in range(5):     aco = qp.FinancialOption(qp.Sobol(2*2**level, seed=seed), option=\"ASIAN\", volatility=.2, start_price=100, strike_price=100, interest_rate=.05)     approx_solution, data = qp.CubQMCSobolG(aco, abs_tol=1e-4).integrate()     print(\"Asian Option true value (%d time steps): %.5f (to within 1e-4)\"%(2*2**level, approx_solution)) <pre>Asian Option true value (2 time steps): 5.63593 (to within 1e-4)\nAsian Option true value (4 time steps): 5.73171 (to within 1e-4)\nAsian Option true value (8 time steps): 5.75526 (to within 1e-4)\nAsian Option true value (16 time steps): 5.76112 (to within 1e-4)\nAsian Option true value (32 time steps): 5.76260 (to within 1e-4)\n</pre> <p>This function compares 4 different algorithms: Multilevel Monte Carlo (<code>CubMLMC</code>), Multilevel Quasi-Monte Carlo (<code>CubMLQMC</code>), continuation Multilevel Monte Carlo (<code>CubMLMCCont</code>) and Multilevel Quasi-Monte Carlo (<code>CubMLQMCCont</code>):</p> In\u00a0[5]: Copied! <pre>def eval_option(option_mc, option_qmc, abs_tol):\n    stopping_criteria = {\n        \"MLMC\" : qp.CubMLMC(option_mc, abs_tol=abs_tol, levels_max=15),\n        \"continuation MLMC\" : qp.CubMLMCCont(option_mc, abs_tol=abs_tol, levels_max=15),\n        \"MLQMC\" : qp.CubMLQMC(option_qmc, abs_tol=abs_tol, levels_max=15),\n        \"continuation MLQMC\" : qp.CubMLQMCCont(option_qmc, abs_tol=abs_tol, levels_max=15)\n    }\n    \n    levels = []\n    times = []\n    for name, stopper in stopping_criteria.items():\n        sol, data = stopper.integrate()\n        levels.append(data.levels)\n        times.append(data.time_integrate)\n        print(\"\\t%-20s solution %-10.4f number of levels %-6d time %.3f\"%(name, sol, levels[-1], times[-1]))\n        \n    return levels, times\n</pre> def eval_option(option_mc, option_qmc, abs_tol):     stopping_criteria = {         \"MLMC\" : qp.CubMLMC(option_mc, abs_tol=abs_tol, levels_max=15),         \"continuation MLMC\" : qp.CubMLMCCont(option_mc, abs_tol=abs_tol, levels_max=15),         \"MLQMC\" : qp.CubMLQMC(option_qmc, abs_tol=abs_tol, levels_max=15),         \"continuation MLQMC\" : qp.CubMLQMCCont(option_qmc, abs_tol=abs_tol, levels_max=15)     }          levels = []     times = []     for name, stopper in stopping_criteria.items():         sol, data = stopper.integrate()         levels.append(data.levels)         times.append(data.time_integrate)         print(\"\\t%-20s solution %-10.4f number of levels %-6d time %.3f\"%(name, sol, levels[-1], times[-1]))              return levels, times <p>Define the Multilevel Asian options:</p> In\u00a0[8]: Copied! <pre>option_mc = qp.FinancialOption(qp.IIDStdUniform(seed=seed), option=\"asian\")\noption_qmc = qp.FinancialOption(qp.Lattice(seed=seed, replications=8), option=\"asian\")\n</pre> option_mc = qp.FinancialOption(qp.IIDStdUniform(seed=seed), option=\"asian\") option_qmc = qp.FinancialOption(qp.Lattice(seed=seed, replications=8), option=\"asian\") <p>Run and compare each of the 4 algorithms for the Asian option problem:</p> In\u00a0[9]: Copied! <pre>eval_option(option_mc, option_qmc, abs_tol=5e-3);\n</pre> eval_option(option_mc, option_qmc, abs_tol=5e-3); <pre>\tMLMC                 solution 1.7859     number of levels 5      time 14.886\n\tcontinuation MLMC    solution 1.7825     number of levels 3      time 9.339\n\tMLQMC                solution 1.7818     number of levels 3      time 0.492\n\tcontinuation MLQMC   solution 1.7812     number of levels 3      time 0.180\n</pre> <p>Repeat this comparison for a sequence of decreasing tolerances, with 5 different random seeds each. This will allow us to visualize the asymptotic cost complexity of each method.</p> In\u00a0[10]: Copied! <pre>repetitions = 5\ntolerances = 5*np.logspace(-1, -3, num=5)\n\nlevels = {}\ntimes = {}\nfor t in range(len(tolerances)):\n    for r in range(repetitions):\n        print(\"tolerance = %10.4e, repetition = %d/%d\"%(tolerances[t], r + 1, repetitions))\n        levels[t, r], times[t, r] = eval_option(option_mc, option_qmc, tolerances[t])\n</pre> repetitions = 5 tolerances = 5*np.logspace(-1, -3, num=5)  levels = {} times = {} for t in range(len(tolerances)):     for r in range(repetitions):         print(\"tolerance = %10.4e, repetition = %d/%d\"%(tolerances[t], r + 1, repetitions))         levels[t, r], times[t, r] = eval_option(option_mc, option_qmc, tolerances[t]) <pre>tolerance = 5.0000e-01, repetition = 1/5\n\tMLMC                 solution 1.7574     number of levels 3      time 0.012\n\tcontinuation MLMC    solution 1.7765     number of levels 3      time 0.014\n\tMLQMC                solution 1.7840     number of levels 3      time 0.012\n\tcontinuation MLQMC   solution 1.7897     number of levels 3      time 0.012\ntolerance = 5.0000e-01, repetition = 2/5\n\tMLMC                 solution 1.8907     number of levels 3      time 0.005\n\tcontinuation MLMC    solution 1.5579     number of levels 3      time 0.010\n\tMLQMC                solution 1.7822     number of levels 3      time 0.009\n\tcontinuation MLQMC   solution 1.7790     number of levels 3      time 0.011\ntolerance = 5.0000e-01, repetition = 3/5\n\tMLMC                 solution 1.5368     number of levels 3      time 0.005\n\tcontinuation MLMC    solution 1.5291     number of levels 3      time 0.009\n\tMLQMC                solution 1.8200     number of levels 3      time 0.009\n\tcontinuation MLQMC   solution 1.8018     number of levels 3      time 0.010\ntolerance = 5.0000e-01, repetition = 4/5\n\tMLMC                 solution 1.6869     number of levels 3      time 0.005\n\tcontinuation MLMC    solution 1.8590     number of levels 3      time 0.009\n\tMLQMC                solution 1.8063     number of levels 3      time 0.008\n\tcontinuation MLQMC   solution 1.8205     number of levels 3      time 0.010\ntolerance = 5.0000e-01, repetition = 5/5\n\tMLMC                 solution 1.5989     number of levels 3      time 0.006\n\tcontinuation MLMC    solution 1.7292     number of levels 3      time 0.009\n\tMLQMC                solution 1.7672     number of levels 3      time 0.007\n\tcontinuation MLQMC   solution 1.7521     number of levels 3      time 0.008\ntolerance = 1.5811e-01, repetition = 1/5\n\tMLMC                 solution 1.6774     number of levels 3      time 0.010\n\tcontinuation MLMC    solution 1.8315     number of levels 3      time 0.016\n\tMLQMC                solution 1.7883     number of levels 3      time 0.010\n\tcontinuation MLQMC   solution 1.7917     number of levels 3      time 0.011\ntolerance = 1.5811e-01, repetition = 2/5\n\tMLMC                 solution 1.7921     number of levels 3      time 0.012\n\tcontinuation MLMC    solution 1.6978     number of levels 3      time 0.017\n\tMLQMC                solution 1.7939     number of levels 3      time 0.015\n\tcontinuation MLQMC   solution 1.7888     number of levels 3      time 0.012\ntolerance = 1.5811e-01, repetition = 3/5\n\tMLMC                 solution 1.7723     number of levels 3      time 0.013\n\tcontinuation MLMC    solution 1.8004     number of levels 4      time 0.021\n\tMLQMC                solution 1.7896     number of levels 3      time 0.011\n\tcontinuation MLQMC   solution 1.7780     number of levels 3      time 0.009\ntolerance = 1.5811e-01, repetition = 4/5\n\tMLMC                 solution 1.7671     number of levels 3      time 0.011\n\tcontinuation MLMC    solution 1.7734     number of levels 4      time 0.015\n\tMLQMC                solution 1.7888     number of levels 3      time 0.012\n\tcontinuation MLQMC   solution 1.7693     number of levels 3      time 0.008\ntolerance = 1.5811e-01, repetition = 5/5\n\tMLMC                 solution 1.7919     number of levels 3      time 0.012\n\tcontinuation MLMC    solution 1.8158     number of levels 3      time 0.010\n\tMLQMC                solution 1.7866     number of levels 3      time 0.011\n\tcontinuation MLQMC   solution 1.7808     number of levels 3      time 0.012\ntolerance = 5.0000e-02, repetition = 1/5\n\tMLMC                 solution 1.7964     number of levels 3      time 0.127\n\tcontinuation MLMC    solution 1.8056     number of levels 4      time 0.065\n\tMLQMC                solution 1.7835     number of levels 3      time 0.042\n\tcontinuation MLQMC   solution 1.7734     number of levels 3      time 0.029\ntolerance = 5.0000e-02, repetition = 2/5\n\tMLMC                 solution 1.7826     number of levels 3      time 0.096\n\tcontinuation MLMC    solution 1.7682     number of levels 3      time 0.100\n\tMLQMC                solution 1.7884     number of levels 3      time 0.043\n\tcontinuation MLQMC   solution 1.7705     number of levels 3      time 0.029\ntolerance = 5.0000e-02, repetition = 3/5\n\tMLMC                 solution 1.7604     number of levels 3      time 0.072\n\tcontinuation MLMC    solution 1.8240     number of levels 3      time 0.064\n\tMLQMC                solution 1.7786     number of levels 3      time 0.050\n\tcontinuation MLQMC   solution 1.7769     number of levels 3      time 0.025\ntolerance = 5.0000e-02, repetition = 4/5\n\tMLMC                 solution 1.7693     number of levels 3      time 0.094\n\tcontinuation MLMC    solution 1.8409     number of levels 4      time 0.178\n\tMLQMC                solution 1.7839     number of levels 3      time 0.041\n\tcontinuation MLQMC   solution 1.7835     number of levels 3      time 0.039\ntolerance = 5.0000e-02, repetition = 5/5\n\tMLMC                 solution 1.7808     number of levels 3      time 0.090\n\tcontinuation MLMC    solution 1.7848     number of levels 3      time 0.062\n\tMLQMC                solution 1.7768     number of levels 3      time 0.027\n\tcontinuation MLQMC   solution 1.7758     number of levels 3      time 0.024\ntolerance = 1.5811e-02, repetition = 1/5\n\tMLMC                 solution 1.7800     number of levels 3      time 1.225\n\tcontinuation MLMC    solution 1.7838     number of levels 3      time 0.410\n\tMLQMC                solution 1.7835     number of levels 3      time 0.099\n\tcontinuation MLQMC   solution 1.7781     number of levels 3      time 0.059\ntolerance = 1.5811e-02, repetition = 2/5\n\tMLMC                 solution 1.7941     number of levels 4      time 1.002\n\tcontinuation MLMC    solution 1.7848     number of levels 3      time 0.380\n\tMLQMC                solution 1.7783     number of levels 3      time 0.096\n\tcontinuation MLQMC   solution 1.7760     number of levels 3      time 0.056\ntolerance = 1.5811e-02, repetition = 3/5\n\tMLMC                 solution 1.7787     number of levels 3      time 0.809\n\tcontinuation MLMC    solution 1.7846     number of levels 3      time 0.791\n\tMLQMC                solution 1.7820     number of levels 3      time 0.132\n\tcontinuation MLQMC   solution 1.7776     number of levels 3      time 0.059\ntolerance = 1.5811e-02, repetition = 4/5\n\tMLMC                 solution 1.7806     number of levels 4      time 1.074\n\tcontinuation MLMC    solution 1.7810     number of levels 3      time 0.482\n\tMLQMC                solution 1.7806     number of levels 3      time 0.151\n\tcontinuation MLQMC   solution 1.7823     number of levels 3      time 0.051\ntolerance = 1.5811e-02, repetition = 5/5\n\tMLMC                 solution 1.7914     number of levels 4      time 0.952\n\tcontinuation MLMC    solution 1.7858     number of levels 3      time 0.769\n\tMLQMC                solution 1.7830     number of levels 3      time 0.111\n\tcontinuation MLQMC   solution 1.7804     number of levels 3      time 0.072\ntolerance = 5.0000e-03, repetition = 1/5\n\tMLMC                 solution 1.7827     number of levels 5      time 18.777\n\tcontinuation MLMC    solution 1.7823     number of levels 3      time 4.222\n\tMLQMC                solution 1.7817     number of levels 3      time 0.291\n\tcontinuation MLQMC   solution 1.7814     number of levels 3      time 0.425\ntolerance = 5.0000e-03, repetition = 2/5\n\tMLMC                 solution 1.7844     number of levels 4      time 10.280\n\tcontinuation MLMC    solution 1.7807     number of levels 3      time 8.755\n\tMLQMC                solution 1.7811     number of levels 3      time 0.355\n\tcontinuation MLQMC   solution 1.7812     number of levels 3      time 0.460\ntolerance = 5.0000e-03, repetition = 3/5\n\tMLMC                 solution 1.7819     number of levels 4      time 16.041\n\tcontinuation MLMC    solution 1.7825     number of levels 3      time 6.892\n\tMLQMC                solution 1.7813     number of levels 3      time 0.272\n\tcontinuation MLQMC   solution 1.7814     number of levels 3      time 0.369\ntolerance = 5.0000e-03, repetition = 4/5\n\tMLMC                 solution 1.7843     number of levels 4      time 12.328\n\tcontinuation MLMC    solution 1.7804     number of levels 3      time 4.089\n\tMLQMC                solution 1.7805     number of levels 3      time 0.498\n\tcontinuation MLQMC   solution 1.7823     number of levels 3      time 0.162\ntolerance = 5.0000e-03, repetition = 5/5\n\tMLMC                 solution 1.7822     number of levels 4      time 10.548\n\tcontinuation MLMC    solution 1.7805     number of levels 3      time 9.212\n\tMLQMC                solution 1.7818     number of levels 3      time 0.460\n\tcontinuation MLQMC   solution 1.7805     number of levels 3      time 0.508\n</pre> <p>Compute and plot the asymptotic cost complexity.</p> In\u00a0[11]: Copied! <pre>avg_time = {}\nfor method in range(4):\n    avg_time[method] = [np.mean([times[t, r][method] for r in range(repetitions)]) for t in range(len(tolerances))]\n</pre> avg_time = {} for method in range(4):     avg_time[method] = [np.mean([times[t, r][method] for r in range(repetitions)]) for t in range(len(tolerances))] In\u00a0[12]: Copied! <pre>plt.figure(figsize=(10,7))\nplt.plot(tolerances, avg_time[0], label=\"MLMC\")\nplt.plot(tolerances, avg_time[1], label=\"continuation MLMC\")\nplt.plot(tolerances, avg_time[2], label=\"MLQMC\")\nplt.plot(tolerances, avg_time[3], label=\"continuation MLQMC\")\nplt.xscale(\"log\")\nplt.yscale(\"log\")\nplt.xlabel(\"requested absolute tolerance\")\nplt.ylabel(\"average run time in seconds\")\nplt.legend();\n</pre> plt.figure(figsize=(10,7)) plt.plot(tolerances, avg_time[0], label=\"MLMC\") plt.plot(tolerances, avg_time[1], label=\"continuation MLMC\") plt.plot(tolerances, avg_time[2], label=\"MLQMC\") plt.plot(tolerances, avg_time[3], label=\"continuation MLQMC\") plt.xscale(\"log\") plt.yscale(\"log\") plt.xlabel(\"requested absolute tolerance\") plt.ylabel(\"average run time in seconds\") plt.legend(); In\u00a0[13]: Copied! <pre>max_levels = {}\nfor method in range(4):\n    levels_rep = np.array([levels[len(tolerances)-1, r][method] for r in range(repetitions)])\n    max_levels[method] = [np.count_nonzero(levels_rep == level)/repetitions for level in range(15)]\n</pre> max_levels = {} for method in range(4):     levels_rep = np.array([levels[len(tolerances)-1, r][method] for r in range(repetitions)])     max_levels[method] = [np.count_nonzero(levels_rep == level)/repetitions for level in range(15)] In\u00a0[14]: Copied! <pre>plt.figure(figsize=(14,10))\nplt.subplot(2,2,1); plt.bar(range(15), max_levels[0], label=\"MLMC old\", color=\"C0\"); plt.xlabel(\"max level\"); plt.ylabel(\"fraction of runs\"); plt.ylim(0, 1); plt.legend()\nplt.subplot(2,2,2); plt.bar(range(15), max_levels[1], label=\"MLMC new\", color=\"C1\"); plt.xlabel(\"max level\"); plt.ylabel(\"fraction of runs\"); plt.ylim(0, 1); plt.legend()\nplt.subplot(2,2,3); plt.bar(range(15), max_levels[2], label=\"MLQMC old\", color=\"C2\"); plt.xlabel(\"max level\"); plt.ylabel(\"fraction of runs\"); plt.ylim(0, 1); plt.legend()\nplt.subplot(2,2,4); plt.bar(range(15), max_levels[3], label=\"MLQMC new\", color=\"C3\"); plt.xlabel(\"max level\"); plt.ylabel(\"fraction of runs\"); plt.ylim(0, 1); plt.legend();\n</pre> plt.figure(figsize=(14,10)) plt.subplot(2,2,1); plt.bar(range(15), max_levels[0], label=\"MLMC old\", color=\"C0\"); plt.xlabel(\"max level\"); plt.ylabel(\"fraction of runs\"); plt.ylim(0, 1); plt.legend() plt.subplot(2,2,2); plt.bar(range(15), max_levels[1], label=\"MLMC new\", color=\"C1\"); plt.xlabel(\"max level\"); plt.ylabel(\"fraction of runs\"); plt.ylim(0, 1); plt.legend() plt.subplot(2,2,3); plt.bar(range(15), max_levels[2], label=\"MLQMC old\", color=\"C2\"); plt.xlabel(\"max level\"); plt.ylabel(\"fraction of runs\"); plt.ylim(0, 1); plt.legend() plt.subplot(2,2,4); plt.bar(range(15), max_levels[3], label=\"MLQMC new\", color=\"C3\"); plt.xlabel(\"max level\"); plt.ylabel(\"fraction of runs\"); plt.ylim(0, 1); plt.legend(); In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"demos/asian-option-mlqmc/#comparison-of-multilevel-quasi-monte-carlo-for-an-asian-option-problem","title":"Comparison of multilevel (Quasi-)Monte Carlo for an Asian option problem\u00b6","text":""},{"location":"demos/control_variates/","title":"Control Variates","text":"In\u00a0[1]: Copied! <pre>from qmcpy import *\nfrom numpy import *\nfrom qmcpy import *\nfrom numpy import *\n</pre> from qmcpy import * from numpy import * from qmcpy import * from numpy import * In\u00a0[2]: Copied! <pre>from matplotlib import pyplot\n%matplotlib inline\nsize = 20\npyplot.rc('font', size=size)          # controls default text sizes\npyplot.rc('axes', titlesize=size)     # fontsize of the axes title\npyplot.rc('axes', labelsize=size)     # fontsize of the x and y labels\npyplot.rc('xtick', labelsize=size)    # fontsize of the tick labels\npyplot.rc('ytick', labelsize=size)    # fontsize of the tick labels\npyplot.rc('legend', fontsize=size)    # legend fontsize\npyplot.rc('figure', titlesize=size)   # fontsize of the figure title\n</pre> from matplotlib import pyplot %matplotlib inline size = 20 pyplot.rc('font', size=size)          # controls default text sizes pyplot.rc('axes', titlesize=size)     # fontsize of the axes title pyplot.rc('axes', labelsize=size)     # fontsize of the x and y labels pyplot.rc('xtick', labelsize=size)    # fontsize of the tick labels pyplot.rc('ytick', labelsize=size)    # fontsize of the tick labels pyplot.rc('legend', fontsize=size)    # legend fontsize pyplot.rc('figure', titlesize=size)   # fontsize of the figure title In\u00a0[3]: Copied! <pre>def compare(problem,discrete_distrib,stopping_crit,abs_tol):\n  g1,cvs,cvmus = problem(discrete_distrib)\n  sc1 = stopping_crit(g1,abs_tol=abs_tol)\n  name = type(sc1).__name__\n  print('Stopping Criterion: %-15s absolute tolerance: %-5.1e'%(name,abs_tol))\n  sol,data = sc1.integrate()\n  print('\\tW CV:  Solution %-10.2f time %-10.2f samples %.1e'%(sol,data.time_integrate,data.n_total))\n  sc1 = stopping_crit(g1,abs_tol=abs_tol,control_variates=cvs,control_variate_means=cvmus)\n  solcv,datacv = sc1.integrate()\n  print('\\tWO CV: Solution %-10.2f time %-10.2f samples %.1e'%(solcv,datacv.time_integrate,datacv.n_total))\n  print('\\tControl variates took %.1f%% the time and %.1f%% the samples\\n'%\\\n        (100*datacv.time_integrate/data.time_integrate,100*datacv.n_total/data.n_total))\n</pre> def compare(problem,discrete_distrib,stopping_crit,abs_tol):   g1,cvs,cvmus = problem(discrete_distrib)   sc1 = stopping_crit(g1,abs_tol=abs_tol)   name = type(sc1).__name__   print('Stopping Criterion: %-15s absolute tolerance: %-5.1e'%(name,abs_tol))   sol,data = sc1.integrate()   print('\\tW CV:  Solution %-10.2f time %-10.2f samples %.1e'%(sol,data.time_integrate,data.n_total))   sc1 = stopping_crit(g1,abs_tol=abs_tol,control_variates=cvs,control_variate_means=cvmus)   solcv,datacv = sc1.integrate()   print('\\tWO CV: Solution %-10.2f time %-10.2f samples %.1e'%(solcv,datacv.time_integrate,datacv.n_total))   print('\\tControl variates took %.1f%% the time and %.1f%% the samples\\n'%\\         (100*datacv.time_integrate/data.time_integrate,100*datacv.n_total/data.n_total)) In\u00a0[4]: Copied! <pre># parameters\ndef poly_problem(discrete_distrib):\n  g1 = CustomFun(Uniform(discrete_distrib,0,2),lambda t: 10*t[...,0]-5*t[...,1]**2+t[...,2]**3)\n  cv1 = CustomFun(Uniform(discrete_distrib,0,2),lambda t: t[...,0])\n  cv2 = CustomFun(Uniform(discrete_distrib,0,2),lambda t: t[...,1]**2)\n  return g1,[cv1,cv2],[1,4/3]\ncompare(poly_problem,IIDStdUniform(3,seed=7),CubMCCLT,abs_tol=1e-2)\ncompare(poly_problem,IIDStdUniform(3,seed=7),CubMCCLT,abs_tol=1e-2)\ncompare(poly_problem,Sobol(3,seed=7),CubQMCSobolG,abs_tol=1e-8)\n</pre> # parameters def poly_problem(discrete_distrib):   g1 = CustomFun(Uniform(discrete_distrib,0,2),lambda t: 10*t[...,0]-5*t[...,1]**2+t[...,2]**3)   cv1 = CustomFun(Uniform(discrete_distrib,0,2),lambda t: t[...,0])   cv2 = CustomFun(Uniform(discrete_distrib,0,2),lambda t: t[...,1]**2)   return g1,[cv1,cv2],[1,4/3] compare(poly_problem,IIDStdUniform(3,seed=7),CubMCCLT,abs_tol=1e-2) compare(poly_problem,IIDStdUniform(3,seed=7),CubMCCLT,abs_tol=1e-2) compare(poly_problem,Sobol(3,seed=7),CubQMCSobolG,abs_tol=1e-8) <pre>Stopping Criterion: CubMCCLT        absolute tolerance: 1.0e-02\n\tW CV:  Solution 5.33       time 1.11       samples 6.7e+06\n\tWO CV: Solution 5.34       time 0.12       samples 4.8e+05\n\tControl variates took 10.5% the time and 7.1% the samples\n\nStopping Criterion: CubMCCLT        absolute tolerance: 1.0e-02\n\tW CV:  Solution 5.33       time 0.98       samples 6.7e+06\n\tWO CV: Solution 5.34       time 0.09       samples 4.8e+05\n\tControl variates took 9.1% the time and 7.1% the samples\n\nStopping Criterion: CubQMCNetG      absolute tolerance: 1.0e-08\n\tW CV:  Solution 5.33       time 0.12       samples 2.6e+05\n\tWO CV: Solution 5.33       time 0.04       samples 6.6e+04\n\tControl variates took 33.4% the time and 25.0% the samples\n\n</pre> In\u00a0[5]: Copied! <pre>def keister_problem(discrete_distrib):\n  k = Keister(discrete_distrib)\n  cv1 = CustomFun(Uniform(discrete_distrib),lambda x: sin(pi*x).sum(-1))\n  cv2 = CustomFun(Uniform(discrete_distrib),lambda x: (-3*(x-.5)**2+1).sum(-1))\n  return k,[cv1,cv2],[2/pi,3/4]\ncompare(keister_problem,IIDStdUniform(1,seed=7),CubMCCLT,abs_tol=5e-4)\ncompare(keister_problem,IIDStdUniform(1,seed=7),CubMCCLT,abs_tol=4e-4)\ncompare(keister_problem,Sobol(1,seed=7),CubQMCSobolG,abs_tol=1e-7)\n</pre> def keister_problem(discrete_distrib):   k = Keister(discrete_distrib)   cv1 = CustomFun(Uniform(discrete_distrib),lambda x: sin(pi*x).sum(-1))   cv2 = CustomFun(Uniform(discrete_distrib),lambda x: (-3*(x-.5)**2+1).sum(-1))   return k,[cv1,cv2],[2/pi,3/4] compare(keister_problem,IIDStdUniform(1,seed=7),CubMCCLT,abs_tol=5e-4) compare(keister_problem,IIDStdUniform(1,seed=7),CubMCCLT,abs_tol=4e-4) compare(keister_problem,Sobol(1,seed=7),CubQMCSobolG,abs_tol=1e-7) <pre>Stopping Criterion: CubMCCLT        absolute tolerance: 5.0e-04\n\tW CV:  Solution 1.38       time 1.11       samples 9.5e+06\n\tWO CV: Solution 1.38       time 0.10       samples 3.4e+05\n\tControl variates took 9.1% the time and 3.5% the samples\n\nStopping Criterion: CubMCCLT        absolute tolerance: 4.0e-04\n\tW CV:  Solution 1.38       time 1.71       samples 1.5e+07\n\tWO CV: Solution 1.38       time 0.17       samples 6.8e+05\n\tControl variates took 10.2% the time and 4.6% the samples\n\nStopping Criterion: CubQMCNetG      absolute tolerance: 1.0e-07\n\tW CV:  Solution 1.38       time 0.31       samples 1.0e+06\n\tWO CV: Solution 1.38       time 0.41       samples 1.0e+06\n\tControl variates took 133.0% the time and 100.0% the samples\n\n</pre> In\u00a0[7]: Copied! <pre>call_put = 'call'\nstart_price = 100\nstrike_price = 125\nvolatility = .75\ninterest_rate = .01 # 1% interest\nt_final = 1 # 1 year\ndef option_problem(discrete_distrib):\n  eurocv = FinancialOption(\n    discrete_distrib,\n    option=\"EUROPEAN\",\n    volatility=volatility,\n    start_price=start_price,\n    strike_price=strike_price,\n    interest_rate=interest_rate,\n    t_final=t_final,\n    call_put=call_put)\n  aco = FinancialOption(\n    discrete_distrib,\n    option=\"ASIAN\",\n    volatility=volatility,\n    start_price=start_price,\n    strike_price=strike_price,\n    interest_rate=interest_rate,\n    t_final=t_final,\n    call_put=call_put)\n  mu_eurocv = eurocv.get_exact_value()\n  return aco,[eurocv],[mu_eurocv]\ncompare(option_problem,IIDStdUniform(4,seed=7),CubMCCLT,abs_tol=5e-2)\ncompare(option_problem,IIDStdUniform(4,seed=7),CubMCCLT,abs_tol=5e-2)\ncompare(option_problem,Sobol(4,seed=7),CubQMCSobolG,abs_tol=1e-3)\n</pre> call_put = 'call' start_price = 100 strike_price = 125 volatility = .75 interest_rate = .01 # 1% interest t_final = 1 # 1 year def option_problem(discrete_distrib):   eurocv = FinancialOption(     discrete_distrib,     option=\"EUROPEAN\",     volatility=volatility,     start_price=start_price,     strike_price=strike_price,     interest_rate=interest_rate,     t_final=t_final,     call_put=call_put)   aco = FinancialOption(     discrete_distrib,     option=\"ASIAN\",     volatility=volatility,     start_price=start_price,     strike_price=strike_price,     interest_rate=interest_rate,     t_final=t_final,     call_put=call_put)   mu_eurocv = eurocv.get_exact_value()   return aco,[eurocv],[mu_eurocv] compare(option_problem,IIDStdUniform(4,seed=7),CubMCCLT,abs_tol=5e-2) compare(option_problem,IIDStdUniform(4,seed=7),CubMCCLT,abs_tol=5e-2) compare(option_problem,Sobol(4,seed=7),CubQMCSobolG,abs_tol=1e-3) <pre>Stopping Criterion: CubMCCLT        absolute tolerance: 5.0e-02\n\tW CV:  Solution 9.55       time 1.66       samples 3.1e+06\n\tWO CV: Solution 9.54       time 0.81       samples 7.9e+05\n\tControl variates took 49.0% the time and 25.6% the samples\n\nStopping Criterion: CubMCCLT        absolute tolerance: 5.0e-02\n\tW CV:  Solution 9.55       time 1.44       samples 3.1e+06\n\tWO CV: Solution 9.54       time 0.66       samples 7.9e+05\n\tControl variates took 45.7% the time and 25.6% the samples\n\nStopping Criterion: CubQMCNetG      absolute tolerance: 1.0e-03\n\tW CV:  Solution 9.55       time 0.40       samples 5.2e+05\n\tWO CV: Solution 9.55       time 0.53       samples 5.2e+05\n\tControl variates took 132.0% the time and 100.0% the samples\n\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"demos/control_variates/#control-variates-in-qmcpy","title":"Control Variates in QMCPy\u00b6","text":"<p>This notebook demonstrates QMCPy's current support for control variates.</p>"},{"location":"demos/control_variates/#setup","title":"Setup\u00b6","text":""},{"location":"demos/control_variates/#problem-1-polynomial-function","title":"Problem 1: Polynomial Function\u00b6","text":"<p>We will integrate $$g(t) = 10t_1-5t_2^2+2t_3^3$$ with true measure $\\mathcal{U}[0,2]^3$ and control variates $$\\hat{g}_1(t) = t_1$$ and $$\\hat{g}_2(t) = t_2^2$$ using the same true measure.</p>"},{"location":"demos/control_variates/#problem-2-keister-function","title":"Problem 2: Keister Function\u00b6","text":"<p>This problem will integrate the Keister function while using control variates $$g_1(x) = \\sin(\\pi x)$$ and $$g_2(x) = -3(x-1/2)^2+1.$$ The following code does this problem in one-dimension for visualization purposes, but control variates are compatible with any dimension.</p>"},{"location":"demos/control_variates/#problem-3-option-pricing","title":"Problem 3: Option Pricing\u00b6","text":"<p>We will use a European Call Option as a control variate for pricing the Asian Call Option using various stopping criterion, as done for problem 1</p>"},{"location":"demos/digital_net_b2/","title":"Digital Nets in Base 2 and their Randomizations","text":"In\u00a0[1]: Copied! <pre>from qmcpy import *\nfrom numpy import *\nfrom matplotlib import pyplot\nfrom time import time\nimport os\n</pre> from qmcpy import * from numpy import * from matplotlib import pyplot from time import time import os In\u00a0[2]: Copied! <pre>s = DigitalNetB2(5,seed=7)\ns\n</pre> s = DigitalNetB2(5,seed=7) s Out[2]: <pre>DigitalNetB2 (AbstractLDDiscreteDistribution)\n    d               5\n    replications    1\n    randomize       LMS_DS\n    gen_mats_source joe_kuo.6.21201.txt\n    order           NATURAL\n    t               63\n    alpha           1\n    n_limit         2^(32)\n    entropy         7</pre> In\u00a0[3]: Copied! <pre>s.gen_samples(4) # generate Sobol' samples\n</pre> s.gen_samples(4) # generate Sobol' samples Out[3]: <pre>array([[0.864483  , 0.31330935, 0.09580848, 0.24636182, 0.13239161],\n       [0.03735175, 0.82581618, 0.71117322, 0.9042245 , 0.5285374 ],\n       [0.65627575, 0.57484149, 0.94341128, 0.67738059, 0.38460276],\n       [0.48731325, 0.06429227, 0.37126087, 0.45706178, 0.7690279 ]])</pre> In\u00a0[4]: Copied! <pre>s.gen_samples(n_min=2,n_max=4) # generate from specific range. If range is not powers of 2, use graycode\n</pre> s.gen_samples(n_min=2,n_max=4) # generate from specific range. If range is not powers of 2, use graycode Out[4]: <pre>array([[0.65627575, 0.57484149, 0.94341128, 0.67738059, 0.38460276],\n       [0.48731325, 0.06429227, 0.37126087, 0.45706178, 0.7690279 ]])</pre> In\u00a0[5]: Copied! <pre>t0 = time()\ns.gen_samples(2**25)\nprint('Time: %.2f'%(time()-t0))\n</pre> t0 = time() s.gen_samples(2**25) print('Time: %.2f'%(time()-t0)) <pre>Time: 4.61\n</pre> In\u00a0[6]: Copied! <pre>s = DigitalNetB2(2,randomize='LMS_DS') # linear matrix scramble with digital shift (default)\ns.gen_samples(2)\n</pre> s = DigitalNetB2(2,randomize='LMS_DS') # linear matrix scramble with digital shift (default) s.gen_samples(2) Out[6]: <pre>array([[0.5032812 , 0.3002277 ],\n       [0.37437374, 0.86989486]])</pre> In\u00a0[7]: Copied! <pre>s = DigitalNetB2(2,randomize='LMS') # just linear matrix scrambling\ns.gen_samples(2, warn=False) # suppress warning that the first point is still the origin\n</pre> s = DigitalNetB2(2,randomize='LMS') # just linear matrix scrambling s.gen_samples(2, warn=False) # suppress warning that the first point is still the origin Out[7]: <pre>array([[0.        , 0.        ],\n       [0.95339709, 0.93679526]])</pre> In\u00a0[8]: Copied! <pre>s = DigitalNetB2(2,randomize='DS') # just digital shift\ns.gen_samples(2)\n</pre> s = DigitalNetB2(2,randomize='DS') # just digital shift s.gen_samples(2) Out[8]: <pre>array([[0.72193614, 0.16766092],\n       [0.22193614, 0.66766092]])</pre> In\u00a0[9]: Copied! <pre>s = DigitalNetB2(2,randomize=False,graycode=False)\ns.gen_samples(n_min=4,n_max=8,warn=False) # don't warn about non-randomized samples including the origin\n</pre> s = DigitalNetB2(2,randomize=False,graycode=False) s.gen_samples(n_min=4,n_max=8,warn=False) # don't warn about non-randomized samples including the origin <pre>/Users/alegresor/Desktop/QMCSoftware/qmcpy/discrete_distribution/digital_net_b2/digital_net_b2.py:263: ParameterWarning: graycode argument deprecated, set order='GRAY' or order='NATURAL' instead. Using order='NATURAL'\n  warnings.warn(\"graycode argument deprecated, set order='GRAY' or order='NATURAL' instead. Using order='%s'\"%order,ParameterWarning)\n</pre> Out[9]: <pre>array([[0.125, 0.625],\n       [0.625, 0.125],\n       [0.375, 0.375],\n       [0.875, 0.875]])</pre> In\u00a0[10]: Copied! <pre>s = DigitalNetB2(2,randomize=False,graycode=True)\ns.gen_samples(n_min=4,n_max=8,warn=False)\n</pre> s = DigitalNetB2(2,randomize=False,graycode=True) s.gen_samples(n_min=4,n_max=8,warn=False) <pre>/Users/alegresor/Desktop/QMCSoftware/qmcpy/discrete_distribution/digital_net_b2/digital_net_b2.py:263: ParameterWarning: graycode argument deprecated, set order='GRAY' or order='NATURAL' instead. Using order='GRAY'\n  warnings.warn(\"graycode argument deprecated, set order='GRAY' or order='NATURAL' instead. Using order='%s'\"%order,ParameterWarning)\n</pre> Out[10]: <pre>array([[0.375, 0.375],\n       [0.875, 0.875],\n       [0.625, 0.125],\n       [0.125, 0.625]])</pre> In\u00a0[11]: Copied! <pre>s = DigitalNetB2(3,randomize=False)\ns.gen_samples(n_min=4,n_max=8)\n</pre> s = DigitalNetB2(3,randomize=False) s.gen_samples(n_min=4,n_max=8) Out[11]: <pre>array([[0.125, 0.625, 0.375],\n       [0.625, 0.125, 0.875],\n       [0.375, 0.375, 0.625],\n       [0.875, 0.875, 0.125]])</pre> In\u00a0[12]: Copied! <pre>s = DigitalNetB2([1,2],randomize=False) # use only the second and third dimensions in the sequence\ns.gen_samples(n_min=4,n_max=8)\n</pre> s = DigitalNetB2([1,2],randomize=False) # use only the second and third dimensions in the sequence s.gen_samples(n_min=4,n_max=8) Out[12]: <pre>array([[0.625, 0.375],\n       [0.125, 0.875],\n       [0.375, 0.625],\n       [0.875, 0.125]])</pre> In\u00a0[16]: Copied! <pre>def plt_ei(x,ax,x_cuts,y_cuts):\n    for ix in arange(1,x_cuts,dtype=float): ax.axvline(x=ix/x_cuts,color='r',alpha=.5)\n    for iy in arange(1,y_cuts,dtype=float): ax.axhline(y=iy/y_cuts,color='r',alpha=.5)\n    ax.scatter(x[:,0],x[:,1],color='b',s=25)\n    ax.set_xlim([0,1])\n    ax.set_xticks([0,1])\n    ax.set_ylim([0,1])\n    ax.set_yticks([0,1])\n    ax.set_aspect(1)\n</pre> def plt_ei(x,ax,x_cuts,y_cuts):     for ix in arange(1,x_cuts,dtype=float): ax.axvline(x=ix/x_cuts,color='r',alpha=.5)     for iy in arange(1,y_cuts,dtype=float): ax.axhline(y=iy/y_cuts,color='r',alpha=.5)     ax.scatter(x[:,0],x[:,1],color='b',s=25)     ax.set_xlim([0,1])     ax.set_xticks([0,1])     ax.set_ylim([0,1])     ax.set_yticks([0,1])     ax.set_aspect(1) In\u00a0[17]: Copied! <pre># unrandomized \nfig,ax = pyplot.subplots(ncols=3,nrows=2,figsize=(15,10))\ns = DigitalNetB2(2,randomize=False)\nplt_ei(s.gen_samples(2**2,warn=False),ax[0,0],2,2)\nplt_ei(s.gen_samples(2**3,warn=False),ax[1,0],8,0)\nplt_ei(s.gen_samples(2**4,warn=False),ax[0,1],4,4)\nplt_ei(s.gen_samples(2**4,warn=False),ax[1,1],2,8)\nplt_ei(s.gen_samples(2**6,warn=False),ax[0,2],8,8)\nplt_ei(s.gen_samples(2**6,warn=False),ax[1,2],16,4)\nfig.suptitle(\"Unrandomized Sobol' points\");\n</pre> # unrandomized  fig,ax = pyplot.subplots(ncols=3,nrows=2,figsize=(15,10)) s = DigitalNetB2(2,randomize=False) plt_ei(s.gen_samples(2**2,warn=False),ax[0,0],2,2) plt_ei(s.gen_samples(2**3,warn=False),ax[1,0],8,0) plt_ei(s.gen_samples(2**4,warn=False),ax[0,1],4,4) plt_ei(s.gen_samples(2**4,warn=False),ax[1,1],2,8) plt_ei(s.gen_samples(2**6,warn=False),ax[0,2],8,8) plt_ei(s.gen_samples(2**6,warn=False),ax[1,2],16,4) fig.suptitle(\"Unrandomized Sobol' points\"); In\u00a0[18]: Copied! <pre>fig,ax = pyplot.subplots(ncols=3,nrows=2,figsize=(15,10))\ns = DigitalNetB2(2,randomize='LMS_DS')\nplt_ei(s.gen_samples(2**2,warn=False),ax[0,0],2,2)\nplt_ei(s.gen_samples(2**3,warn=False),ax[1,0],8,0)\nplt_ei(s.gen_samples(2**4,warn=False),ax[0,1],4,4)\nplt_ei(s.gen_samples(2**4,warn=False),ax[1,1],2,8)\nplt_ei(s.gen_samples(2**6,warn=False),ax[0,2],8,8)\nplt_ei(s.gen_samples(2**6,warn=False),ax[1,2],16,4)\nfig.suptitle(\"Sobol' points with linear matrix scramble and digital shift\");\n</pre> fig,ax = pyplot.subplots(ncols=3,nrows=2,figsize=(15,10)) s = DigitalNetB2(2,randomize='LMS_DS') plt_ei(s.gen_samples(2**2,warn=False),ax[0,0],2,2) plt_ei(s.gen_samples(2**3,warn=False),ax[1,0],8,0) plt_ei(s.gen_samples(2**4,warn=False),ax[0,1],4,4) plt_ei(s.gen_samples(2**4,warn=False),ax[1,1],2,8) plt_ei(s.gen_samples(2**6,warn=False),ax[0,2],8,8) plt_ei(s.gen_samples(2**6,warn=False),ax[1,2],16,4) fig.suptitle(\"Sobol' points with linear matrix scramble and digital shift\"); In\u00a0[20]: Copied! <pre>fig,ax = pyplot.subplots(ncols=3,nrows=2,figsize=(15,10))\ns = DigitalNetB2(2,order=\"GRAY\",randomize='LMS')\nplt_ei(s.gen_samples(2**2,warn=False),ax[0,0],2,2)\nplt_ei(s.gen_samples(2**3,warn=False),ax[1,0],8,0)\nplt_ei(s.gen_samples(2**4,warn=False),ax[0,1],4,4)\nplt_ei(s.gen_samples(2**4,warn=False),ax[1,1],2,8)\nplt_ei(s.gen_samples(2**6,warn=False),ax[0,2],8,8)\nplt_ei(s.gen_samples(2**6,warn=False),ax[1,2],16,4)\nfig.suptitle(\"Sobol' points with linear matrix scrambling shift\");\n</pre> fig,ax = pyplot.subplots(ncols=3,nrows=2,figsize=(15,10)) s = DigitalNetB2(2,order=\"GRAY\",randomize='LMS') plt_ei(s.gen_samples(2**2,warn=False),ax[0,0],2,2) plt_ei(s.gen_samples(2**3,warn=False),ax[1,0],8,0) plt_ei(s.gen_samples(2**4,warn=False),ax[0,1],4,4) plt_ei(s.gen_samples(2**4,warn=False),ax[1,1],2,8) plt_ei(s.gen_samples(2**6,warn=False),ax[0,2],8,8) plt_ei(s.gen_samples(2**6,warn=False),ax[1,2],16,4) fig.suptitle(\"Sobol' points with linear matrix scrambling shift\"); In\u00a0[21]: Copied! <pre>fig,ax = pyplot.subplots(ncols=3,nrows=2,figsize=(15,10))\ns = DigitalNetB2(2,order=\"GRAY\",randomize='DS')\nplt_ei(s.gen_samples(2**2,warn=False),ax[0,0],2,2)\nplt_ei(s.gen_samples(2**3,warn=False),ax[1,0],8,0)\nplt_ei(s.gen_samples(2**4,warn=False),ax[0,1],4,4)\nplt_ei(s.gen_samples(2**4,warn=False),ax[1,1],2,8)\nplt_ei(s.gen_samples(2**6,warn=False),ax[0,2],8,8)\nplt_ei(s.gen_samples(2**6,warn=False),ax[1,2],16,4)\nfig.suptitle(\"Sobol' points with digital shift\");\n</pre> fig,ax = pyplot.subplots(ncols=3,nrows=2,figsize=(15,10)) s = DigitalNetB2(2,order=\"GRAY\",randomize='DS') plt_ei(s.gen_samples(2**2,warn=False),ax[0,0],2,2) plt_ei(s.gen_samples(2**3,warn=False),ax[1,0],8,0) plt_ei(s.gen_samples(2**4,warn=False),ax[0,1],4,4) plt_ei(s.gen_samples(2**4,warn=False),ax[1,1],2,8) plt_ei(s.gen_samples(2**6,warn=False),ax[0,2],8,8) plt_ei(s.gen_samples(2**6,warn=False),ax[1,2],16,4) fig.suptitle(\"Sobol' points with digital shift\"); In\u00a0[22]: Copied! <pre>fig,ax = pyplot.subplots(figsize=(15,5))\ns = DigitalNetB2([50,51],randomize='LMS_DS')\nplt_ei(s.gen_samples(2**4),ax,4,4)\nfig.suptitle(\"Sobol' points dimension 50 vs 51\");\n# nice properties do not necessary hold in higher dimensions\n</pre> fig,ax = pyplot.subplots(figsize=(15,5)) s = DigitalNetB2([50,51],randomize='LMS_DS') plt_ei(s.gen_samples(2**4),ax,4,4) fig.suptitle(\"Sobol' points dimension 50 vs 51\"); # nice properties do not necessary hold in higher dimensions In\u00a0[23]: Copied! <pre>fig,ax = pyplot.subplots(figsize=(15,5))\ns = DigitalNetB2(2,randomize='LMS_DS',graycode=True)\nplt_ei(s.gen_samples(n_min=1,n_max=16),ax,4,4)\nx16 = s.gen_samples(n_min=16,n_max=17)\nax.scatter(x16[:,0],x16[:,1],color='g',s=100)\nfig.suptitle(\"Sobol' points 2-17\");\n# better to take points 1-16 instead of 2-16\n</pre> fig,ax = pyplot.subplots(figsize=(15,5)) s = DigitalNetB2(2,randomize='LMS_DS',graycode=True) plt_ei(s.gen_samples(n_min=1,n_max=16),ax,4,4) x16 = s.gen_samples(n_min=16,n_max=17) ax.scatter(x16[:,0],x16[:,1],color='g',s=100) fig.suptitle(\"Sobol' points 2-17\"); # better to take points 1-16 instead of 2-16 <pre>/Users/alegresor/Desktop/QMCSoftware/qmcpy/discrete_distribution/digital_net_b2/digital_net_b2.py:263: ParameterWarning: graycode argument deprecated, set order='GRAY' or order='NATURAL' instead. Using order='GRAY'\n  warnings.warn(\"graycode argument deprecated, set order='GRAY' or order='NATURAL' instead. Using order='%s'\"%order,ParameterWarning)\n</pre> In\u00a0[26]: Copied! <pre>def plt_k2d_sobol(ax,rtype,colors,plts=[1,2,3]):\n    trials = 100\n    ms = arange(10,18)\n    ax.set_xscale('log',base=2)\n    ax.set_yscale('log',base=10)\n    epsilons = {}\n    if 1 in plts:\n        epsilons['$2^m$ points with skipped $1^{st}$ point'] = zeros((trials,len(ms)),dtype=double)\n    if 2 in plts:\n        epsilons['$2^m-1$ points with skipped $1^{st}$ point'] = zeros((trials,len(ms)),dtype=double)\n    if 3 in plts:\n        epsilons['$2^n$ points no skipping'] = zeros((trials,len(ms)),dtype=double)\n    for i,m in enumerate(ms):\n        for t in range(trials):\n            s = DigitalNetB2(2,randomize=rtype,order=\"GRAY\")\n            k = Keister(s)\n            solution = k.get_exact_value(d=2)\n            if 1 in plts:\n                epsilons['$2^m$ points with skipped $1^{st}$ point'][t,i] = \\\n                     abs(k.f(s.gen_samples(n_min=1,n_max=1+2**m)).mean()-solution)\n            if 2 in plts:\n                epsilons['$2^m-1$ points with skipped $1^{st}$ point'][t,i] = \\\n                     abs(k.f(s.gen_samples(n_min=1,n_max=2**m)).mean()-solution)\n            if 3 in plts:\n                epsilons['$2^n$ points no skipping'][t,i] = \\\n                     abs(k.f(s.gen_samples(n=2**m)).mean()-solution)\n    for i,(label,eps) in enumerate(epsilons.items()):\n        bot = percentile(eps, 40, axis=0)\n        med = percentile(eps, 50, axis=0)\n        top = percentile(eps, 60, axis=0)\n        ax.loglog(2**ms,med,label=label+' with %s randomization'%rtype,color=colors[i])\n        ax.fill_between(2**ms, bot, top, color=colors[i], alpha=.3)     \n</pre> def plt_k2d_sobol(ax,rtype,colors,plts=[1,2,3]):     trials = 100     ms = arange(10,18)     ax.set_xscale('log',base=2)     ax.set_yscale('log',base=10)     epsilons = {}     if 1 in plts:         epsilons['$2^m$ points with skipped $1^{st}$ point'] = zeros((trials,len(ms)),dtype=double)     if 2 in plts:         epsilons['$2^m-1$ points with skipped $1^{st}$ point'] = zeros((trials,len(ms)),dtype=double)     if 3 in plts:         epsilons['$2^n$ points no skipping'] = zeros((trials,len(ms)),dtype=double)     for i,m in enumerate(ms):         for t in range(trials):             s = DigitalNetB2(2,randomize=rtype,order=\"GRAY\")             k = Keister(s)             solution = k.get_exact_value(d=2)             if 1 in plts:                 epsilons['$2^m$ points with skipped $1^{st}$ point'][t,i] = \\                      abs(k.f(s.gen_samples(n_min=1,n_max=1+2**m)).mean()-solution)             if 2 in plts:                 epsilons['$2^m-1$ points with skipped $1^{st}$ point'][t,i] = \\                      abs(k.f(s.gen_samples(n_min=1,n_max=2**m)).mean()-solution)             if 3 in plts:                 epsilons['$2^n$ points no skipping'][t,i] = \\                      abs(k.f(s.gen_samples(n=2**m)).mean()-solution)     for i,(label,eps) in enumerate(epsilons.items()):         bot = percentile(eps, 40, axis=0)         med = percentile(eps, 50, axis=0)         top = percentile(eps, 60, axis=0)         ax.loglog(2**ms,med,label=label+' with %s randomization'%rtype,color=colors[i])         ax.fill_between(2**ms, bot, top, color=colors[i], alpha=.3)      In\u00a0[27]: Copied! <pre># compare randomization fig,ax = pyplot.subplots(figsize=(10,10))\nfig,ax = pyplot.subplots(figsize=(10,10))\nplt_k2d_sobol(ax,None,['r','g'],[1,2])\nplt_k2d_sobol(ax,'LMS_DS',['c','m','b'],[1,2,3])\nplt_k2d_sobol(ax,'DS',['y','k','orangered'],[1,2,3])\nax.legend()\nax.set_xlabel('n')\nax.set_ylabel('$\\\\epsilon$');\n</pre> # compare randomization fig,ax = pyplot.subplots(figsize=(10,10)) fig,ax = pyplot.subplots(figsize=(10,10)) plt_k2d_sobol(ax,None,['r','g'],[1,2]) plt_k2d_sobol(ax,'LMS_DS',['c','m','b'],[1,2,3]) plt_k2d_sobol(ax,'DS',['y','k','orangered'],[1,2,3]) ax.legend() ax.set_xlabel('n') ax.set_ylabel('$\\\\epsilon$'); In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"demos/digital_net_b2/#digital-net-base-2-generator","title":"Digital Net Base 2 Generator\u00b6","text":""},{"location":"demos/digital_net_b2/#basic-usage","title":"Basic usage\u00b6","text":""},{"location":"demos/digital_net_b2/#randomize-with-digital-shift-linear-matrix-scramble","title":"Randomize with digital shift / linear matrix scramble\u00b6","text":""},{"location":"demos/digital_net_b2/#support-for-graycode-and-natural-ordering","title":"Support for graycode and natural ordering\u00b6","text":""},{"location":"demos/digital_net_b2/#custom-dimensions","title":"Custom Dimensions\u00b6","text":""},{"location":"demos/digital_net_b2/#elementary-intervals","title":"Elementary intervals\u00b6","text":""},{"location":"demos/digital_net_b2/#skipping-points-vs-randomization","title":"Skipping points vs. randomization\u00b6","text":"<p>The first point in a Sobol' sequence is $\\vec{0}$. Therefore, some software packages skip this point as various transformations will map 0 to NAN. For example, the inverse CDF of a Gaussian density at $\\vec{0}$ is $-\\infty$. However, we argue that it is better to randomize points than to simply skip the first point for the following reasons:</p> <ol> <li>Skipping the first point does not give the uniformity advantages when taking powers of 2. For example, notice the green point in the above plot of \"Sobol' points 2-17\".</li> <li>Randomizing Sobol' points will return 0 with probability 0.</li> </ol> <p>A more thorough explanation can be found in Art Owen's paper On dropping the first Sobol' point</p> <p>So always randomize your Sobol' unless you specifically need unrandomized points. In QMCPy the Sobol' generator defaults to randomization with a linear matrix scramble.</p> <p>The below code runs tests comparing unrandomized vs. randomization with linear matrix scramble with digital shift vs. randomization with just digital shift. Furthermore, we compare taking the first $2^n$ points vs. dropping the first point and taking $2^n$ points vs. dropping the first point and taking $2^n-1$ points.</p> <p>The 2D keister function is used for testing purposes as it can be exactly integrated using Mathematica: <code>N[Integrate[E^(-x1^2 - x2^2) Cos[Sqrt[x1^2 + x2^2]], {x1, -Infinity, Infinity}, {x2, -Infinity, Infinity}]]</code></p> <p>Plots the median (line) and fills the middle 10% of observations</p>"},{"location":"demos/elliptic-pde/","title":"Single and Multilevel (Q)MC for an Elliptic PDE","text":"In\u00a0[1]: Copied! <pre>import copy\nimport numpy as np\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nfrom scipy.special import gamma, kv\nfrom qmcpy.integrand import Integrand\nfrom qmcpy.util.data import Data\nimport qmcpy as qp\n</pre> import copy import numpy as np import matplotlib as mpl import matplotlib.pyplot as plt from scipy.special import gamma, kv from qmcpy.integrand import Integrand from qmcpy.util.data import Data import qmcpy as qp In\u00a0[2]: Copied! <pre># matplotlib options\nrc_fonts = {\n    \"text.usetex\": True,\n    \"font.size\": 14,\n    \"mathtext.default\": \"regular\",\n    \"axes.titlesize\": 14,\n    \"axes.labelsize\": 14,\n    \"legend.fontsize\": 14,\n    \"xtick.labelsize\": 12,\n    \"ytick.labelsize\": 12,\n    \"figure.titlesize\": 16,\n    \"font.family\": \"serif\",\n    \"font.serif\": \"computer modern roman\",\n}\nmpl.rcParams.update(rc_fonts)\n</pre> # matplotlib options rc_fonts = {     \"text.usetex\": True,     \"font.size\": 14,     \"mathtext.default\": \"regular\",     \"axes.titlesize\": 14,     \"axes.labelsize\": 14,     \"legend.fontsize\": 14,     \"xtick.labelsize\": 12,     \"ytick.labelsize\": 12,     \"figure.titlesize\": 16,     \"font.family\": \"serif\",     \"font.serif\": \"computer modern roman\", } mpl.rcParams.update(rc_fonts) In\u00a0[3]: Copied! <pre># set random seed for reproducibility\nnp.random.seed(9999)\n</pre> # set random seed for reproducibility np.random.seed(9999) <p>We will apply various multilevel Monte Carlo and multilevel quasi-Monte Carlo methods to approximate the expected value of a quantity of interest derived from the solution of a one-dimensional partial differential equation (PDE), where the diffusion coefficient of the PDE is a lognormal Gaussian random field. This example problem serves as an important benchmark problem for various methods in the uncertainty quantification and quasi-Monte Carlo literature. It is often referred to as the fruitfly problem of uncertainty quantification.</p> <p>Let $Q$ be a quantity of interest derived from the solution $u(x, \\omega)$ of the one-dimensional partial differential equation (PDE)</p> <p>$$-\\frac{d}{dx}\\bigg(a(x, \\omega) \\frac{d}{dx} u(x, \\omega) \\bigg) = f(x), \\quad 0 \\leq x \\leq 1,$$ $$u(0, \\cdot) = u_0,$$ $$u(1, \\cdot) = u_1.$$</p> <p>The notation $u(x, \\omega)$ is used to indicate that the solution depends on both the spatial variable $x$ and the uncertain parameter $\\omega$. This uncertainty is present because the diffusion coefficient, $a(x, \\omega)$, is given by a lognormal Gaussian random field with given covariance function. A common choice for the covariance function is the so-called Mat\u00e9rn covariance function</p> <p>$$c(x, y) = \\hat{c}\\bigg(\\frac{\\|x - y\\|}{\\lambda}\\bigg)\\quad \\quad \\hat{c}(r) = \\frac{2^{1-\\nu}}{\\Gamma(\\nu)} r^\\nu K_\\nu(r)$$</p> <p>with $\\Gamma$ the gamma function and $K_\\nu$ the Bessel function of the second kind. This covariance function has two parameters: $\\lambda$, the length scale, and $\\nu$, the smoothness parameter.</p> <p>We begin by defining the Mat\u00e9rn covariance function <code>Matern(x, y)</code>:</p> In\u00a0[4]: Copied! <pre>def Matern(x, y, smoothness=1, lengthscale=1):\n    distance = abs(x - y)\n    r = distance/lengthscale\n    prefactor = 2**(1-smoothness)/gamma(smoothness)\n    term1 = r**smoothness\n    term2 = kv(smoothness, r)\n    np.fill_diagonal(term2, 1)\n    cov = prefactor * term1 * term2\n    np.fill_diagonal(cov, 1)\n    return cov\n</pre> def Matern(x, y, smoothness=1, lengthscale=1):     distance = abs(x - y)     r = distance/lengthscale     prefactor = 2**(1-smoothness)/gamma(smoothness)     term1 = r**smoothness     term2 = kv(smoothness, r)     np.fill_diagonal(term2, 1)     cov = prefactor * term1 * term2     np.fill_diagonal(cov, 1)     return cov <p>Let's take a look at the covariance matrix obtained by evaluating the covariance function in <code>n=25</code> equidistant points in <code>[0, 1]</code>.</p> In\u00a0[5]: Copied! <pre>def get_covariance_matrix(pts, smoothness=1, lengthscale=1):\n    X, Y = np.meshgrid(pts, pts)\n    return Matern(X, Y, smoothness, lengthscale)\n</pre> def get_covariance_matrix(pts, smoothness=1, lengthscale=1):     X, Y = np.meshgrid(pts, pts)     return Matern(X, Y, smoothness, lengthscale) In\u00a0[6]: Copied! <pre>n = 25\npts = np.linspace(0, 1, num=n)\nfig, ax = plt.subplots(figsize=(6, 5))\nax.pcolor(get_covariance_matrix(pts).T)\nax.invert_yaxis()\nax.set_ylabel(r\"$x$\")\nax.set_xlabel(r\"$y$\")\nax.set_title(f\"Matern kernel\")\nplt.show();\n</pre> n = 25 pts = np.linspace(0, 1, num=n) fig, ax = plt.subplots(figsize=(6, 5)) ax.pcolor(get_covariance_matrix(pts).T) ax.invert_yaxis() ax.set_ylabel(r\"$x$\") ax.set_xlabel(r\"$y$\") ax.set_title(f\"Matern kernel\") plt.show(); <p>A lognormal Gaussian random field $a(x, \\omega)$ can be expressed as $a(x, \\omega) = \\exp(b(x, \\omega))$, where $b(x, \\omega)$ is a Gaussian random field. Samples of the Gaussian random field $b(x, \\omega)$ can be computed from a factorization of the covariance matrix. Specifically, suppose we have a spectral (eigenvalue) expansion of the covariance matrix $C$ as</p> <p>$$C = V W V^T,$$</p> <p>then samples of the Gaussian random field can be computed as</p> <p>$$\\boldsymbol{b} = S \\boldsymbol{x},$$</p> <p>where $S = V W^{1/2}$ and $\\boldsymbol{x}$ is a vector of standard normal independent and identically distributed random variables. This is easy to see, since</p> <p>$$\\mathbb{E}[\\boldsymbol{b}] = \\mathbb{E}[S \\boldsymbol{x}] = S\\mathbb{E}[\\boldsymbol{x}] = \\boldsymbol{0}$$ $$\\mathbb{E}[\\boldsymbol{b} \\boldsymbol{b}^T] =  \\mathbb{E}[S \\boldsymbol{x} \\boldsymbol{x}^T S^T] = S \\mathbb{E}[\\boldsymbol{x} \\boldsymbol{x}^T] S^T = SS^T = VWV^T = C.$$</p> <p>First, let's compute an eigenvalue decomposition of the covariance matrix.</p> In\u00a0[7]: Copied! <pre>def get_eigenpairs(n, smoothness=1, lengthscale=1):\n    h = 1/(n-1)\n    pts = np.linspace(h/2, 1 - h/2, num=n - 1)\n    cov = get_covariance_matrix(pts, smoothness, lengthscale)\n    w, v = np.linalg.eig(cov)\n    # ensure all eigenvectors are correctly oriented\n    for col in range(v.shape[1]):\n        if v[0, col] &lt; 0:\n            v[:, col] *= -1\n    return pts, w, v\n</pre> def get_eigenpairs(n, smoothness=1, lengthscale=1):     h = 1/(n-1)     pts = np.linspace(h/2, 1 - h/2, num=n - 1)     cov = get_covariance_matrix(pts, smoothness, lengthscale)     w, v = np.linalg.eig(cov)     # ensure all eigenvectors are correctly oriented     for col in range(v.shape[1]):         if v[0, col] &lt; 0:             v[:, col] *= -1     return pts, w, v <p>Next, we plot the eigenfunctions for different values of $n$, the number of grid points.</p> In\u00a0[8]: Copied! <pre>n = [8, 16, 32] # list of number of gridpoints to plot\nm = 5 # number of eigenfunctions to plot\nfig, axes = plt.subplots(m, len(n), figsize=(8, 6))\nfor j, k in enumerate(n):\n    x, w, v = get_eigenpairs(k)\n    for i in range(m):\n        _ = axes[i, j].plot(x, v[:, i])\n        _ = axes[i, j].set_xlim(0, 1)\n        _ = axes[i, j].get_yaxis().set_ticks([])\n        if i &lt; m - 1:\n            _ = axes[i, j].get_xaxis().set_ticks([])\n        if i == 0:\n            _ = axes[i, j].set_title(r\"$n = \" + repr(k) + r\"$\")\nplt.tight_layout()\n</pre> n = [8, 16, 32] # list of number of gridpoints to plot m = 5 # number of eigenfunctions to plot fig, axes = plt.subplots(m, len(n), figsize=(8, 6)) for j, k in enumerate(n):     x, w, v = get_eigenpairs(k)     for i in range(m):         _ = axes[i, j].plot(x, v[:, i])         _ = axes[i, j].set_xlim(0, 1)         _ = axes[i, j].get_yaxis().set_ticks([])         if i &lt; m - 1:             _ = axes[i, j].get_xaxis().set_ticks([])         if i == 0:             _ = axes[i, j].set_title(r\"$n = \" + repr(k) + r\"$\") plt.tight_layout() <p>With this eigenvalue decomposition, we can compute samples of the Gaussian random field $b(x, \\omega)$, and hence, also of the lognormal Gaussian random field $a(x, \\omega) = \\exp(b(x, \\omega))$, since $\\boldsymbol{b} = V W^{1/2} \\boldsymbol{x}$.</p> In\u00a0[9]: Copied! <pre>def evaluate(w, v, y=None):\n    if y is None:\n        y = np.random.randn(len(w) - 1)\n    m = len(y)\n    return v[:, :m] @ np.diag(np.sqrt(w[:m])) @ y\n</pre> def evaluate(w, v, y=None):     if y is None:         y = np.random.randn(len(w) - 1)     m = len(y)     return v[:, :m] @ np.diag(np.sqrt(w[:m])) @ y <p>Let's plot a couple of realizations of the Gaussian random field $b(x, \\omega)$.</p> In\u00a0[10]: Copied! <pre>n = 64\nx, w, v = get_eigenpairs(n)\nfig, ax = plt.subplots(figsize=(6, 4))\nfor _ in range(10):\n    ax.plot(x, evaluate(w, v))\nax.set_xlim(0, 1)\nax.set_xlabel(r\"$x$\")\nax.set_ylabel(r\"$\\log(a(x, \\cdot))$\")\nplt.show();\n</pre> n = 64 x, w, v = get_eigenpairs(n) fig, ax = plt.subplots(figsize=(6, 4)) for _ in range(10):     ax.plot(x, evaluate(w, v)) ax.set_xlim(0, 1) ax.set_xlabel(r\"$x$\") ax.set_ylabel(r\"$\\log(a(x, \\cdot))$\") plt.show(); <p>Now that we are able to compute realizations of the Gaussian random field, a next step is to compute a numerical solution of the PDE</p> <p>$$-\\frac{d}{dx}\\bigg(a(x, \\omega) \\frac{d}{dx} u(x, \\omega) \\bigg) = f(x), \\quad 0 \\leq x \\leq 1.$$</p> <p>Using a straightforward finite-difference approximation, it is easy to show that the numerical solution $\\boldsymbol{u}$ is the solution of a tridiagonal system. The solutions of such a tridiagonal system can be easily obtained in $O(n)$ (linear) time using the tridiagonal matrix algorithm (also known as the Thomas algorithm). More details can be found here.</p> In\u00a0[11]: Copied! <pre>def thomas(a, b, c, d):\n    n = len(b)\n    x = np.zeros(n)\n    for i in range(1, n):\n        w = a[i-1]/b[i-1]\n        b[i] -= w*c[i-1]\n        d[i] -= w*d[i-1]\n    x[n-1] = d[n-1]/b[n-1]\n    for i in reversed(range(n-1)):\n        x[i] = (d[i] - c[i]*x[i+1])/b[i]\n    return x\n</pre> def thomas(a, b, c, d):     n = len(b)     x = np.zeros(n)     for i in range(1, n):         w = a[i-1]/b[i-1]         b[i] -= w*c[i-1]         d[i] -= w*d[i-1]     x[n-1] = d[n-1]/b[n-1]     for i in reversed(range(n-1)):         x[i] = (d[i] - c[i]*x[i+1])/b[i]     return x <p>For the remainder of this notebook, we will assume that the source term $f(x)=1$ and Dirichlet boundary conditions $u(0) = u(1) = 0$.</p> In\u00a0[12]: Copied! <pre>def pde_solve(a):\n    n = len(a)\n    b = np.full(n-1, 1/n**2)  # Changed from (n-1, 1) to (n-1,)\n    x = thomas(-a[1:n-1], a[:n-1]+a[1:], -a[1:n-1], b)\n    return np.insert(x, [0, n-1], [0, 0])\n</pre> def pde_solve(a):     n = len(a)     b = np.full(n-1, 1/n**2)  # Changed from (n-1, 1) to (n-1,)     x = thomas(-a[1:n-1], a[:n-1]+a[1:], -a[1:n-1], b)     return np.insert(x, [0, n-1], [0, 0]) <p>Let's compute and plot a couple of solutions $u(x, \\omega)$.</p> In\u00a0[13]: Copied! <pre>n = 64\n_, w, v = get_eigenpairs(n)\nx = np.linspace(0, 1, num=n)\nfig, ax = plt.subplots(figsize=(6, 4))\nfor _ in range(10):\n    a = np.exp(evaluate(w, v))\n    u = pde_solve(a)\n    ax.plot(x, u)\nax.set_xlim(0, 1)\nax.set_xlabel(r\"$x$\")\nax.set_ylabel(r\"$u(x, \\cdot)$\")\nplt.show();\n</pre> n = 64 _, w, v = get_eigenpairs(n) x = np.linspace(0, 1, num=n) fig, ax = plt.subplots(figsize=(6, 4)) for _ in range(10):     a = np.exp(evaluate(w, v))     u = pde_solve(a)     ax.plot(x, u) ax.set_xlim(0, 1) ax.set_xlabel(r\"$x$\") ax.set_ylabel(r\"$u(x, \\cdot)$\") plt.show(); <p>In the multilevel Monte Carlo method, we will rely on the ability to generate \"correlated\" solutions of the PDE with varying mesh sizes. Such a correlated solutions can be used as efficient control variates to reduce the variance (or statistical error) in the approximation of the expected value $\\mathbb{E}[Q]$. Since we are using a factorization of the covariance matrix to generate realizations of the Gaussian random field, it is quite easy to obtain correlated samples: when sampling from the \"coarse\" solution level, use the same set of random numbers used to sample from the \"fine\" solution level, but truncated to the appropriate size. Since the eigenvalue decomposition will reveal the most important modes in the covariance matrix, that same eigenvalue decomposition on a \"coarse\" approximation level will contain the same eigenfunctions, represented on the coarse grid. Let's illustrate this property on an example using <code>n = 16</code> grid points for the fine solution level and <code>n = 8</code> grid points for the coarse solution level.</p> In\u00a0[14]: Copied! <pre>nf = 16\nnc = nf//2\n_, wf, vf = get_eigenpairs(nf)\n_, wc, vc = get_eigenpairs(nc)\nxf = np.linspace(0, 1, num=nf)\nxc = np.linspace(0, 1, num=nc)\nfig, ax = plt.subplots(figsize=(6, 4))\nfor _ in range(10):\n    yf = np.random.randn(nf - 1)\n    af = np.exp(evaluate(wf, vf, y=yf))\n    uf = pde_solve(af)\n    ax.plot(xf, uf)\n    yc = yf[:nc - 1]\n    ac = np.exp(evaluate(wc, vc, y=yc))\n    uc = pde_solve(ac)\n    ax.plot(xc, uc, color=ax.lines[-1].get_color(), linestyle=\"dashed\", dash_capstyle=\"round\")\nax.set_xlim(0, 1)\nax.set_ylim(bottom=0)\nax.set_xlabel(r\"$x$\")\nax.set_ylabel(r\"$u(x, \\cdot)$\")\nplt.show();\n</pre> nf = 16 nc = nf//2 _, wf, vf = get_eigenpairs(nf) _, wc, vc = get_eigenpairs(nc) xf = np.linspace(0, 1, num=nf) xc = np.linspace(0, 1, num=nc) fig, ax = plt.subplots(figsize=(6, 4)) for _ in range(10):     yf = np.random.randn(nf - 1)     af = np.exp(evaluate(wf, vf, y=yf))     uf = pde_solve(af)     ax.plot(xf, uf)     yc = yf[:nc - 1]     ac = np.exp(evaluate(wc, vc, y=yc))     uc = pde_solve(ac)     ax.plot(xc, uc, color=ax.lines[-1].get_color(), linestyle=\"dashed\", dash_capstyle=\"round\") ax.set_xlim(0, 1) ax.set_ylim(bottom=0) ax.set_xlabel(r\"$x$\") ax.set_ylabel(r\"$u(x, \\cdot)$\") plt.show(); <p>The better the coarse solution matches the fine grid solution, the more efficient the multilevel methods in Section 3 will perform.</p> <p>Let's begin by using the single-level Monte Carlo and quasi-Monte Carlo methods to compute the expected value $\\mathbb{E}[Q]$. As quantity of interest $Q$ we take the solution of the PDE at $x=1/2$, i.e., $Q = u(1/2, \\cdot)$.</p> <p>To integrate the elliptic PDE problem into <code>QMCPy</code>, we construct a simple class as follows:</p> In\u00a0[15]: Copied! <pre>class EllipticPDE(Integrand):\n    \n    def __init__(self, sampler, smoothness=1, lengthscale=1):\n        self.parameters = [\"smoothness\", \"lengthscale\", \"n\"]\n        self.smoothness = smoothness\n        self.lengthscale = lengthscale\n        self.n = sampler.gen_samples(n=1).shape[-1] + 1\n        self.compute_eigenpairs()\n        self.sampler = sampler\n        self.true_measure = qp.Gaussian(self.sampler)\n        super(EllipticPDE, self).__init__(dimension_indv=(),dimension_comb=(),parallel=False)\n        \n    def compute_eigenpairs(self):\n        _, w, v = get_eigenpairs(self.n)\n        self.eigenpairs = w, v\n        \n    def g(self, x):\n        y = np.zeros(x.shape[:-1])\n        for i in np.ndindex(y.shape):\n            y[i] = self.__g(x[i])\n        return y \n    \n    def __g(self, x):\n        w, v = self.eigenpairs\n        a = np.exp(evaluate(w, v, y=x))\n        u = pde_solve(a)\n        return u[len(u)//2]\n    \n    def _spawn(self, level, sampler):\n        return EllipticPDE(sampler, smoothness=self.smoothness, lengthscale=self.lengthscale)\n</pre> class EllipticPDE(Integrand):          def __init__(self, sampler, smoothness=1, lengthscale=1):         self.parameters = [\"smoothness\", \"lengthscale\", \"n\"]         self.smoothness = smoothness         self.lengthscale = lengthscale         self.n = sampler.gen_samples(n=1).shape[-1] + 1         self.compute_eigenpairs()         self.sampler = sampler         self.true_measure = qp.Gaussian(self.sampler)         super(EllipticPDE, self).__init__(dimension_indv=(),dimension_comb=(),parallel=False)              def compute_eigenpairs(self):         _, w, v = get_eigenpairs(self.n)         self.eigenpairs = w, v              def g(self, x):         y = np.zeros(x.shape[:-1])         for i in np.ndindex(y.shape):             y[i] = self.__g(x[i])         return y           def __g(self, x):         w, v = self.eigenpairs         a = np.exp(evaluate(w, v, y=x))         u = pde_solve(a)         return u[len(u)//2]          def _spawn(self, level, sampler):         return EllipticPDE(sampler, smoothness=self.smoothness, lengthscale=self.lengthscale) In\u00a0[16]: Copied! <pre># Custom print function\ndef print_data(data):\n    for key, val in vars(data).items():\n        kv = getattr(data, key)\n        if hasattr(kv, \"parameters\"):\n            print(f\"{key}: {type(val).__name__}\")\n            for param in kv.parameters:\n                print(f\"\\t{param}: {getattr(kv, param)}\")\n    for param in data.parameters:\n        print(f\"{param}: {getattr(data, param)}\")\n</pre> # Custom print function def print_data(data):     for key, val in vars(data).items():         kv = getattr(data, key)         if hasattr(kv, \"parameters\"):             print(f\"{key}: {type(val).__name__}\")             for param in kv.parameters:                 print(f\"\\t{param}: {getattr(kv, param)}\")     for param in data.parameters:         print(f\"{param}: {getattr(data, param)}\") In\u00a0[17]: Copied! <pre># Main function to test different methods\ndef test(problem, sampler, stopping_criterion, abs_tol=5e-3, verbose=True, **kwargs):\n    integrand = problem(sampler)\n    solution, data = stopping_criterion(integrand, abs_tol=abs_tol, **kwargs).integrate()\n    if verbose:\n        print(data)\n        print(\"\\nComputed solution %.3f in %.2f s\"%(solution, data.time_integrate))\n</pre> # Main function to test different methods def test(problem, sampler, stopping_criterion, abs_tol=5e-3, verbose=True, **kwargs):     integrand = problem(sampler)     solution, data = stopping_criterion(integrand, abs_tol=abs_tol, **kwargs).integrate()     if verbose:         print(data)         print(\"\\nComputed solution %.3f in %.2f s\"%(solution, data.time_integrate)) <p>Next, let's apply simple Monte Carlo to approximate the expected value $\\mathbb{E}[Q]$. The Monte Carlo estimator for $\\mathbb{E}[Q]$ is simply the sample average over a finite set of samples, i.e.,</p> <p>$$\\mathcal{Q}_N^\\text{MC} := \\frac{1}{N} \\sum_{n=0}^{N-1} Q^{(n)},$$</p> <p>where $Q^{(n)} := u(1/2, \\boldsymbol{x}^{(n)})$ and we explicitly denote the dependency of $Q$ on the standard normal random numbers $\\boldsymbol{x}$ used to sample from the Gaussian random field. We will continue to increase the number of samples $N$ until a certain error criterion is satisfied.</p> In\u00a0[18]: Copied! <pre># MC\ntest(EllipticPDE, qp.IIDStdUniform(32), qp.CubMCCLT)\n</pre> # MC test(EllipticPDE, qp.IIDStdUniform(32), qp.CubMCCLT) <pre>Data (Data)\n    solution        0.189\n    bound_low       0.184\n    bound_high      0.193\n    bound_diff      0.009\n    n_total         24229\n    time_integrate  0.845\nCubMCCLT (AbstractStoppingCriterion)\n    abs_tol         0.005\n    rel_tol         0\n    n_init          2^(10)\n    n_limit         2^(30)\n    inflate         1.200\n    alpha           0.010\nEllipticPDE (AbstractIntegrand)\n    smoothness      1\n    lengthscale     1\n    n               33\nGaussian (AbstractTrueMeasure)\n    mean            0\n    covariance      1\n    decomp_type     PCA\nIIDStdUniform (AbstractIIDDiscreteDistribution)\n    d               2^(5)\n    replications    1\n    entropy         184928893512601564896274248913732012064\n\nComputed solution 0.189 in 0.85 s\n</pre> <p>The solution should be $\\approx 0.189$.</p> <p>Similarly, the quasi-Monte Carlo estimator for $\\mathbb{E}[Q]$ is defined as</p> <p>$$\\mathcal{Q}_N^\\text{QMC} := \\frac{1}{N} \\sum_{n=0}^{N-1} Q^{(n)},$$</p> <p>where $Q^{(n)} := u(1/2, \\boldsymbol{t}^{(n)})$ with $\\boldsymbol{t}^{(n)}$ the $n$th low-discrepancy point transformed to the distribution of interest. For our elliptic PDE, this means that the quasi-Monte Carlo points, generated inside the unit cube $[0, 1)^d$, are mapped to $\\mathbb{R}^d$.</p> <p>Because the quasi-Monte Carlo estimator doesn't come with a reliable error estimator, we run $K$ different quasi-Monte Carlo estimators in parallel. The sample variance over these $K$ different estimators can then be used as an error estimator.</p> In\u00a0[19]: Copied! <pre># QMC\ntest(EllipticPDE, qp.Lattice(32, replications=8), qp.CubQMCCLT, n_init=32)\n</pre> # QMC test(EllipticPDE, qp.Lattice(32, replications=8), qp.CubQMCCLT, n_init=32) <pre>Data (Data)\n    solution        0.190\n    comb_bound_low  0.187\n    comb_bound_high 0.193\n    comb_bound_diff 0.006\n    comb_flags      1\n    n_total         4096\n    n               4096\n    n_rep           2^(9)\n    time_integrate  0.150\nCubQMCRepStudentT (AbstractStoppingCriterion)\n    inflate         1\n    alpha           0.010\n    abs_tol         0.005\n    rel_tol         0\n    n_init          2^(5)\n    n_limit         2^(30)\nEllipticPDE (AbstractIntegrand)\n    smoothness      1\n    lengthscale     1\n    n               33\nGaussian (AbstractTrueMeasure)\n    mean            0\n    covariance      1\n    decomp_type     PCA\nLattice (AbstractLDDiscreteDistribution)\n    d               2^(5)\n    replications    2^(3)\n    randomize       SHIFT\n    gen_vec_source  kuo.lattice-33002-1024-1048576.9125.txt\n    order           RADICAL INVERSE\n    n_limit         2^(20)\n    entropy         70356012850530284793779238667699207410\n\nComputed solution 0.190 in 0.15 s\n</pre> <p>Implicit to the Monte Carlo and quasi-Monte Carlo methods above is a discretization parameter used in the numerical solution of the PDE. Let's denote this parameter by $\\ell$, $0 \\leq \\ell \\leq L$. Multilevel methods are based on a telescopic sum expansion for the expected value $\\mathbb{E}[Q_L]$, as follows:</p> <p>$$\\mathbb{E}[Q_L] = \\mathbb{E}[Q_0] + \\mathbb{E}[Q_1 - Q_0] + ... + \\mathbb{E}[Q_L - Q_{L-1}].$$</p> <p>Using a Monte Carlo method for each of the terms on the right hand side yields a multilevel Monte Carlo method. Similarly, using a quasi-Monte Carlo method for each term on the right hand side yields a multilevel quasi-Monte Carlo method.</p> <p>Our class <code>EllipticPDE</code> needs some changes to be integrated with the multilevel methods in <code>QMCPy</code>.</p> In\u00a0[20]: Copied! <pre>class MLEllipticPDE(Integrand):\n    \n    def __init__(self, sampler, smoothness=1, lengthscale=1, _level=None):\n        self.l = _level\n        self.parameters = [\"smoothness\", \"lengthscale\", \"n\", \"nb_of_levels\"]\n        self.smoothness = smoothness\n        self.lengthscale = lengthscale\n        dim = sampler.d + 1\n        self.nb_of_levels = int(np.log2(dim + 1))\n        self.n = [2**(l+1) + 1 for l in range(self.nb_of_levels)]\n        self.compute_eigenpairs()\n        self.sampler = sampler\n        self.true_measure = qp.Gaussian(self.sampler)\n        self.cost = nf\n        d_out = () if _level is None else (2,)\n        super(MLEllipticPDE, self).__init__(dimension_indv=d_out,dimension_comb=d_out,parallel=False)\n    \n    def _spawn(self, level, sampler):\n        return MLEllipticPDE(sampler, smoothness=self.smoothness, lengthscale=self.lengthscale, _level=level)\n        \n    def compute_eigenpairs(self):\n        self.eigenpairs = {}\n        for l in range(self.nb_of_levels):\n            _, w, v = get_eigenpairs(self.n[l])\n            self.eigenpairs[l] = w, v\n        \n    def g(self, x): # This function is called by keyword reference for the level parameter \"l\"!\n        Qf = np.zeros(x.shape[:-1])\n        for i in np.ndindex(Qf.shape): \n            Qf[i] = self.__g(x[i], self.l)\n        if self.l==0:\n            Qc = np.zeros_like(Qf) \n        else:\n            Qc = np.zeros(x.shape[:-1])\n            for i in np.ndindex(Qf.shape): \n                Qc[i] = self.__g(x[i], self.l-1)\n        return np.stack([Qc,Qf],axis=0)\n    \n    def __g(self, x, l):\n        w, v = self.eigenpairs[l]\n        n = self.n[l]\n        a = np.exp(evaluate(w, v, y=x[:n-1]))\n        u = pde_solve(a)\n        return u[len(u)//2]\n  \n    def _dimension_at_level(self, l):\n        return self.n[l]\n</pre> class MLEllipticPDE(Integrand):          def __init__(self, sampler, smoothness=1, lengthscale=1, _level=None):         self.l = _level         self.parameters = [\"smoothness\", \"lengthscale\", \"n\", \"nb_of_levels\"]         self.smoothness = smoothness         self.lengthscale = lengthscale         dim = sampler.d + 1         self.nb_of_levels = int(np.log2(dim + 1))         self.n = [2**(l+1) + 1 for l in range(self.nb_of_levels)]         self.compute_eigenpairs()         self.sampler = sampler         self.true_measure = qp.Gaussian(self.sampler)         self.cost = nf         d_out = () if _level is None else (2,)         super(MLEllipticPDE, self).__init__(dimension_indv=d_out,dimension_comb=d_out,parallel=False)          def _spawn(self, level, sampler):         return MLEllipticPDE(sampler, smoothness=self.smoothness, lengthscale=self.lengthscale, _level=level)              def compute_eigenpairs(self):         self.eigenpairs = {}         for l in range(self.nb_of_levels):             _, w, v = get_eigenpairs(self.n[l])             self.eigenpairs[l] = w, v              def g(self, x): # This function is called by keyword reference for the level parameter \"l\"!         Qf = np.zeros(x.shape[:-1])         for i in np.ndindex(Qf.shape):              Qf[i] = self.__g(x[i], self.l)         if self.l==0:             Qc = np.zeros_like(Qf)          else:             Qc = np.zeros(x.shape[:-1])             for i in np.ndindex(Qf.shape):                  Qc[i] = self.__g(x[i], self.l-1)         return np.stack([Qc,Qf],axis=0)          def __g(self, x, l):         w, v = self.eigenpairs[l]         n = self.n[l]         a = np.exp(evaluate(w, v, y=x[:n-1]))         u = pde_solve(a)         return u[len(u)//2]        def _dimension_at_level(self, l):         return self.n[l] <p>Let's apply multilevel Monte Carlo to the elliptic PDE problem.</p> In\u00a0[21]: Copied! <pre>test(MLEllipticPDE, qp.IIDStdUniform(32), qp.CubMLMC)\n</pre> test(MLEllipticPDE, qp.IIDStdUniform(32), qp.CubMLMC) <pre>Data (Data)\n    solution        0.191\n    n_total         39412\n    levels          3\n    n_level         [34966  2510  1389]\n    mean_level      [1.906e-01 2.542e-04 4.468e-05]\n    var_level       [5.354e-02 3.192e-04 3.990e-05]\n    cost_per_sample [16. 16. 16.]\n    alpha           2.508\n    beta            3.000\n    gamma           2^(-1)\n    time_integrate  0.493\nCubMLMC (AbstractStoppingCriterion)\n    rmse_tol        0.002\n    n_init          2^(8)\n    levels_min      2^(1)\n    levels_max      10\n    theta           2^(-1)\nMLEllipticPDE (AbstractIntegrand)\n    smoothness      1\n    lengthscale     1\n    n               [ 3  5  9 17 33]\n    nb_of_levels    5\nGaussian (AbstractTrueMeasure)\n    mean            0\n    covariance      1\n    decomp_type     PCA\nIIDStdUniform (AbstractIIDDiscreteDistribution)\n    d               2^(5)\n    replications    1\n    entropy         262810463936254346141855410714432518731\n\nComputed solution 0.191 in 0.49 s\n</pre> <p>Now it's easy to switch to multilevel quasi-Monte Carlo. Just change the discrete distribution from <code>IIDStdUniform</code> to <code>Lattice</code>.</p> In\u00a0[22]: Copied! <pre>test(MLEllipticPDE, qp.Lattice(32,replications=8), qp.CubMLQMCCont, n_init=32)\n</pre> test(MLEllipticPDE, qp.Lattice(32,replications=8), qp.CubMLQMCCont, n_init=32) <pre>Data (Data)\n    solution        0.190\n    n_total         17664\n    levels          3\n    n_level         [2048  128   32]\n    mean_level      [ 1.900e-01  5.732e-05 -1.509e-04]\n    var_level       [1.631e-06 8.257e-07 1.649e-07]\n    bias_estimate   9.59e-04\n    time_integrate  0.198\nCubMLQMCCont (AbstractStoppingCriterion)\n    rmse_tol        0.002\n    n_init          2^(5)\n    n_limit         10000000000\n    replications    2^(3)\n    levels_min      2^(1)\n    levels_max      10\n    n_tols          10\n    inflate         1.668\n    theta_init      2^(-1)\n    theta           2^(-3)\nMLEllipticPDE (AbstractIntegrand)\n    smoothness      1\n    lengthscale     1\n    n               [ 3  5  9 17 33]\n    nb_of_levels    5\nGaussian (AbstractTrueMeasure)\n    mean            0\n    covariance      1\n    decomp_type     PCA\nLattice (AbstractLDDiscreteDistribution)\n    d               2^(5)\n    replications    2^(3)\n    randomize       SHIFT\n    gen_vec_source  kuo.lattice-33002-1024-1048576.9125.txt\n    order           RADICAL INVERSE\n    n_limit         2^(20)\n    entropy         211999726805994101373488928458251659661\n\nComputed solution 0.190 in 0.20 s\n</pre> <p>In the continuation multilevel (quasi-)Monte Carlo method, we run the standard multilevel (quasi-)Monte Carlo method for a sequence of larger tolerances to obtain better estimates of the algorithmic parameters. The continuation multilevel heuristic will generally compute the same solution just a bit faster.</p> In\u00a0[23]: Copied! <pre>test(MLEllipticPDE, qp.IIDStdUniform(32), qp.CubMCMLCont)\n</pre> test(MLEllipticPDE, qp.IIDStdUniform(32), qp.CubMCMLCont) <pre>Data (Data)\n    solution        0.185\n    n_total         24656\n    levels          3\n    n_level         [14682   941   256   256]\n    mean_level      [1.851e-01 3.890e-04 1.375e-04]\n    var_level       [4.678e-02 3.348e-04 8.258e-06]\n    cost_per_sample [16. 16. 16.]\n    alpha           1.500\n    beta            5.341\n    gamma           2^(-1)\n    time_integrate  0.192\nCubMLMCCont (AbstractStoppingCriterion)\n    rmse_tol        0.002\n    n_init          2^(8)\n    levels_min      2^(1)\n    levels_max      10\n    n_tols          10\n    inflate         1.668\n    theta_init      2^(-1)\n    theta           0.010\nMLEllipticPDE (AbstractIntegrand)\n    smoothness      1\n    lengthscale     1\n    n               [ 3  5  9 17 33]\n    nb_of_levels    5\nGaussian (AbstractTrueMeasure)\n    mean            0\n    covariance      1\n    decomp_type     PCA\nIIDStdUniform (AbstractIIDDiscreteDistribution)\n    d               2^(5)\n    replications    1\n    entropy         150146899559360985341942719648395449911\n\nComputed solution 0.185 in 0.19 s\n</pre> In\u00a0[24]: Copied! <pre>test(MLEllipticPDE, qp.Lattice(32,replications=8), qp.CubMLQMCCont, n_init=32)\n</pre> test(MLEllipticPDE, qp.Lattice(32,replications=8), qp.CubMLQMCCont, n_init=32) <pre>Data (Data)\n    solution        0.189\n    n_total         9472\n    levels          3\n    n_level         [1024  128   32]\n    mean_level      [ 1.893e-01 -3.314e-04  9.098e-05]\n    var_level       [1.941e-06 1.623e-06 1.425e-07]\n    bias_estimate   9.45e-06\n    time_integrate  0.116\nCubMLQMCCont (AbstractStoppingCriterion)\n    rmse_tol        0.002\n    n_init          2^(5)\n    n_limit         10000000000\n    replications    2^(3)\n    levels_min      2^(1)\n    levels_max      10\n    n_tols          10\n    inflate         1.668\n    theta_init      2^(-1)\n    theta           0.010\nMLEllipticPDE (AbstractIntegrand)\n    smoothness      1\n    lengthscale     1\n    n               [ 3  5  9 17 33]\n    nb_of_levels    5\nGaussian (AbstractTrueMeasure)\n    mean            0\n    covariance      1\n    decomp_type     PCA\nLattice (AbstractLDDiscreteDistribution)\n    d               2^(5)\n    replications    2^(3)\n    randomize       SHIFT\n    gen_vec_source  kuo.lattice-33002-1024-1048576.9125.txt\n    order           RADICAL INVERSE\n    n_limit         2^(20)\n    entropy         200817672418615218644638207846262968478\n\nComputed solution 0.189 in 0.12 s\n</pre> <p>Finally, we will run some convergence tests to see how these methods behave as a function of the error tolerance.</p> In\u00a0[25]: Copied! <pre># Main function to test convergence for given problem\ndef test_convergence(problem, sampler, stopping_criterion, abs_tol=1e-3, verbose=True, smoothness=1, lengthscale=1, **kwargs):\n    integrand = problem(sampler, smoothness=smoothness, lengthscale=lengthscale)\n    stopping_crit = stopping_criterion(integrand, abs_tol=abs_tol, **kwargs)\n    stopping_crit.data = stopping_crit._construct_data()\n    \n    # manually call \"integrate()\"\n    tol = []\n    n_samp = []\n    for t in range(stopping_crit.n_tols):\n        stopping_crit.rmse_tol = stopping_crit.inflate**(stopping_crit.n_tols-t-1)*stopping_crit.target_tol # update tol\n        stopping_crit._integrate(stopping_crit.data) # call _integrate()\n        tol.append(copy.copy(stopping_crit.rmse_tol))\n        n_samp.append(copy.copy(stopping_crit.data.n_level))\n\n        if verbose:\n            print(\"tol = {:5.3e}, number of samples = {}\".format(tol[-1], n_samp[-1]))\n            \n    return tol, n_samp\n</pre> # Main function to test convergence for given problem def test_convergence(problem, sampler, stopping_criterion, abs_tol=1e-3, verbose=True, smoothness=1, lengthscale=1, **kwargs):     integrand = problem(sampler, smoothness=smoothness, lengthscale=lengthscale)     stopping_crit = stopping_criterion(integrand, abs_tol=abs_tol, **kwargs)     stopping_crit.data = stopping_crit._construct_data()          # manually call \"integrate()\"     tol = []     n_samp = []     for t in range(stopping_crit.n_tols):         stopping_crit.rmse_tol = stopping_crit.inflate**(stopping_crit.n_tols-t-1)*stopping_crit.target_tol # update tol         stopping_crit._integrate(stopping_crit.data) # call _integrate()         tol.append(copy.copy(stopping_crit.rmse_tol))         n_samp.append(copy.copy(stopping_crit.data.n_level))          if verbose:             print(\"tol = {:5.3e}, number of samples = {}\".format(tol[-1], n_samp[-1]))                  return tol, n_samp In\u00a0[26]: Copied! <pre># Execute the convergence test\ndef execute_convergence_test(smoothness=1, lengthscale=1):\n    \n    # Convergence test for MLMC\n    tol_mlmc, n_samp_mlmc = test_convergence(MLEllipticPDE, qp.IIDStdUniform(32), qp.CubMLMCCont, verbose=False)\n    \n    # Convergence test for MLQMC\n    tol_mlqmc, n_samp_mlqmc = test_convergence(MLEllipticPDE, qp.Lattice(32,replications=8), qp.CubMLQMCCont, verbose=False, n_init=32)\n    \n    # Compute cost per level\n    max_levels = max(max([len(n_samp) for n_samp in n_samp_mlmc]), max([len(n_samp) for n_samp in n_samp_mlqmc]))\n    cost_per_level = np.array([2**level + int(2**(level-1)) for level in range(max_levels)])\n    cost_per_level = cost_per_level/cost_per_level[-1]\n    \n    # Compute total cost for each tolerance and store the result\n    cost = {}\n    cost[\"mc\"] = (tol_mlmc, [n_samp_mlmc[tol][0] for tol in range(len(tol_mlmc))]) # where we assume V[Q_0] = V[Q_L]\n    cost[\"qmc\"] = (tol_mlqmc, [n_samp_mlqmc[tol][0] for tol in range(len(tol_mlqmc))]) # where we assume V[Q_0] = V[Q_L]\n    cost[\"mlmc\"] = (tol_mlmc, [sum([n_samp*cost_per_level[j] for j, n_samp in enumerate(n_samp_mlmc[tol])]) for tol in range(len(tol_mlmc))])\n    cost[\"mlqmc\"] = (tol_mlqmc, [sum([n_samp*cost_per_level[j] for j, n_samp in enumerate(n_samp_mlqmc[tol])]) for tol in range(len(tol_mlqmc))])\n    \n    return cost\n</pre> # Execute the convergence test def execute_convergence_test(smoothness=1, lengthscale=1):          # Convergence test for MLMC     tol_mlmc, n_samp_mlmc = test_convergence(MLEllipticPDE, qp.IIDStdUniform(32), qp.CubMLMCCont, verbose=False)          # Convergence test for MLQMC     tol_mlqmc, n_samp_mlqmc = test_convergence(MLEllipticPDE, qp.Lattice(32,replications=8), qp.CubMLQMCCont, verbose=False, n_init=32)          # Compute cost per level     max_levels = max(max([len(n_samp) for n_samp in n_samp_mlmc]), max([len(n_samp) for n_samp in n_samp_mlqmc]))     cost_per_level = np.array([2**level + int(2**(level-1)) for level in range(max_levels)])     cost_per_level = cost_per_level/cost_per_level[-1]          # Compute total cost for each tolerance and store the result     cost = {}     cost[\"mc\"] = (tol_mlmc, [n_samp_mlmc[tol][0] for tol in range(len(tol_mlmc))]) # where we assume V[Q_0] = V[Q_L]     cost[\"qmc\"] = (tol_mlqmc, [n_samp_mlqmc[tol][0] for tol in range(len(tol_mlqmc))]) # where we assume V[Q_0] = V[Q_L]     cost[\"mlmc\"] = (tol_mlmc, [sum([n_samp*cost_per_level[j] for j, n_samp in enumerate(n_samp_mlmc[tol])]) for tol in range(len(tol_mlmc))])     cost[\"mlqmc\"] = (tol_mlqmc, [sum([n_samp*cost_per_level[j] for j, n_samp in enumerate(n_samp_mlqmc[tol])]) for tol in range(len(tol_mlqmc))])          return cost In\u00a0[27]: Copied! <pre># Plot the result\ndef plot_convergence(cost):\n    fig, ax = plt.subplots(figsize=(8, 6))\n    ax.plot(cost[\"mc\"][0], cost[\"mc\"][1], marker=\"o\", label=\"MC\")\n    ax.plot(cost[\"qmc\"][0], cost[\"qmc\"][1], marker=\"o\", label=\"QMC\")\n    ax.plot(cost[\"mlmc\"][0], cost[\"mlmc\"][1], marker=\"o\", label=\"MLMC\")\n    ax.plot(cost[\"mlqmc\"][0], cost[\"mlqmc\"][1], marker=\"o\", label=\"MLQMC\")\n    ax.legend(frameon=False)\n    ax.set_xscale(\"log\")\n    ax.set_yscale(\"log\")\n    ax.set_xlabel(r\"error tolerance $\\varepsilon$\")\n    ax.set_ylabel(r\"equivalent \\# model evaluations at finest level\")\n    plt.show();\n</pre> # Plot the result def plot_convergence(cost):     fig, ax = plt.subplots(figsize=(8, 6))     ax.plot(cost[\"mc\"][0], cost[\"mc\"][1], marker=\"o\", label=\"MC\")     ax.plot(cost[\"qmc\"][0], cost[\"qmc\"][1], marker=\"o\", label=\"QMC\")     ax.plot(cost[\"mlmc\"][0], cost[\"mlmc\"][1], marker=\"o\", label=\"MLMC\")     ax.plot(cost[\"mlqmc\"][0], cost[\"mlqmc\"][1], marker=\"o\", label=\"MLQMC\")     ax.legend(frameon=False)     ax.set_xscale(\"log\")     ax.set_yscale(\"log\")     ax.set_xlabel(r\"error tolerance $\\varepsilon$\")     ax.set_ylabel(r\"equivalent \\# model evaluations at finest level\")     plt.show(); <p>This command takes a while to execute (about 1 minute on my laptop):</p> In\u00a0[28]: Copied! <pre>plot_convergence(execute_convergence_test())\n</pre> plot_convergence(execute_convergence_test()) <p>The benefit of the low-discrepancy point set depends on the smoothness of the random field: the smoother the random field, the better. Here's an example for a Gaussian random field with a smaller smoothness $\\nu=1/2$ and smaller length scale $\\lambda=1/3$.</p> In\u00a0[29]: Copied! <pre>smoothness = 1/2\nlengthscale = 1/3\n</pre> smoothness = 1/2 lengthscale = 1/3 In\u00a0[30]: Copied! <pre>n = 256\nx, w, v = get_eigenpairs(n, smoothness=smoothness, lengthscale=lengthscale)\nfig, ax = plt.subplots(figsize=(6, 4))\nfor _ in range(10):\n    ax.plot(x, evaluate(w, v))\nax.set_xlim(0, 1)\nax.set_xlabel(r\"$x$\")\nax.set_ylabel(r\"$\\log(a(x, \\cdot))$\")\nplt.show();\n</pre> n = 256 x, w, v = get_eigenpairs(n, smoothness=smoothness, lengthscale=lengthscale) fig, ax = plt.subplots(figsize=(6, 4)) for _ in range(10):     ax.plot(x, evaluate(w, v)) ax.set_xlim(0, 1) ax.set_xlabel(r\"$x$\") ax.set_ylabel(r\"$\\log(a(x, \\cdot))$\") plt.show(); In\u00a0[31]: Copied! <pre>plot_convergence(execute_convergence_test(lengthscale=lengthscale, smoothness=smoothness))\n</pre> plot_convergence(execute_convergence_test(lengthscale=lengthscale, smoothness=smoothness)) <p>While the multilevel quasi-Monte Carlo method is still the fastest method, the asymptotic cost complexity of the QMC-based methods reduces to approximately the same rate as the MC-based methods.</p> <p>The benefits of the multilevel methods over single-level methods will be even larger for two- or three-dimensional PDE problems, since it will be even more computationally efficient to take samples on a coarse grid.</p>"},{"location":"demos/elliptic-pde/#elliptic-pde","title":"Elliptic PDE\u00b6","text":""},{"location":"demos/elliptic-pde/#1-problem-definition","title":"1. Problem definition\u00b6","text":""},{"location":"demos/elliptic-pde/#2-single-level-methods","title":"2. Single-level methods\u00b6","text":""},{"location":"demos/elliptic-pde/#3-multilevel-methods","title":"3. Multilevel methods\u00b6","text":""},{"location":"demos/elliptic-pde/#31-multilevel-quasi-monte-carlo","title":"3.1 Multilevel (quasi-)Monte Carlo\u00b6","text":""},{"location":"demos/elliptic-pde/#32-continuation-multilevel-quasi-monte-carlo","title":"3.2 Continuation multilevel (quasi-)Monte Carlo\u00b6","text":""},{"location":"demos/elliptic-pde/#4-convergence-tests","title":"4. Convergence tests\u00b6","text":""},{"location":"demos/iris/","title":"Sensitivity Analysis for the Iris Dataset","text":"In\u00a0[2]: Copied! <pre>from numpy import *\nfrom qmcpy import *\nimport pandas as pd\nfrom sklearn.datasets import load_iris\nfrom sklearn.tree import DecisionTreeClassifier,plot_tree\nfrom sklearn.model_selection import train_test_split\nfrom skopt import gp_minimize\nfrom matplotlib import pyplot\n</pre> from numpy import * from qmcpy import * import pandas as pd from sklearn.datasets import load_iris from sklearn.tree import DecisionTreeClassifier,plot_tree from sklearn.model_selection import train_test_split from skopt import gp_minimize from matplotlib import pyplot In\u00a0[3]: Copied! <pre>data = load_iris()\nprint(data['DESCR'])\n</pre> data = load_iris() print(data['DESCR']) <pre>.. _iris_dataset:\n\nIris plants dataset\n--------------------\n\n**Data Set Characteristics:**\n\n    :Number of Instances: 150 (50 in each of three classes)\n    :Number of Attributes: 4 numeric, predictive attributes and the class\n    :Attribute Information:\n        - sepal length in cm\n        - sepal width in cm\n        - petal length in cm\n        - petal width in cm\n        - class:\n                - Iris-Setosa\n                - Iris-Versicolour\n                - Iris-Virginica\n                \n    :Summary Statistics:\n\n    ============== ==== ==== ======= ===== ====================\n                    Min  Max   Mean    SD   Class Correlation\n    ============== ==== ==== ======= ===== ====================\n    sepal length:   4.3  7.9   5.84   0.83    0.7826\n    sepal width:    2.0  4.4   3.05   0.43   -0.4194\n    petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\n    petal width:    0.1  2.5   1.20   0.76    0.9565  (high!)\n    ============== ==== ==== ======= ===== ====================\n\n    :Missing Attribute Values: None\n    :Class Distribution: 33.3% for each of 3 classes.\n    :Creator: R.A. Fisher\n    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\n    :Date: July, 1988\n\nThe famous Iris database, first used by Sir R.A. Fisher. The dataset is taken\nfrom Fisher's paper. Note that it's the same as in R, but not as in the UCI\nMachine Learning Repository, which has two wrong data points.\n\nThis is perhaps the best known database to be found in the\npattern recognition literature.  Fisher's paper is a classic in the field and\nis referenced frequently to this day.  (See Duda &amp; Hart, for example.)  The\ndata set contains 3 classes of 50 instances each, where each class refers to a\ntype of iris plant.  One class is linearly separable from the other 2; the\nlatter are NOT linearly separable from each other.\n\n|details-start|\n**References**\n|details-split|\n\n- Fisher, R.A. \"The use of multiple measurements in taxonomic problems\"\n  Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\n  Mathematical Statistics\" (John Wiley, NY, 1950).\n- Duda, R.O., &amp; Hart, P.E. (1973) Pattern Classification and Scene Analysis.\n  (Q327.D83) John Wiley &amp; Sons.  ISBN 0-471-22361-1.  See page 218.\n- Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\n  Structure and Classification Rule for Recognition in Partially Exposed\n  Environments\".  IEEE Transactions on Pattern Analysis and Machine\n  Intelligence, Vol. PAMI-2, No. 1, 67-71.\n- Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\n  on Information Theory, May 1972, 431-433.\n- See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\n  conceptual clustering system finds 3 classes in the data.\n- Many, many more ...\n\n|details-end|\n</pre> In\u00a0[4]: Copied! <pre>x = data['data']\ny = data['target']\nfeature_names = data['feature_names']\ndf = pd.DataFrame(hstack((x,y[:,None])),columns=feature_names+['iris type'])\nprint('df shape:',df.shape)\ntarget_names = data['target_names']\niris_type_map = {i:target_names[i] for i in range(len(target_names))}\nprint('iris species map:',iris_type_map)\ndf.head()\n</pre> x = data['data'] y = data['target'] feature_names = data['feature_names'] df = pd.DataFrame(hstack((x,y[:,None])),columns=feature_names+['iris type']) print('df shape:',df.shape) target_names = data['target_names'] iris_type_map = {i:target_names[i] for i in range(len(target_names))} print('iris species map:',iris_type_map) df.head() <pre>df shape: (150, 5)\niris species map: {0: 'setosa', 1: 'versicolor', 2: 'virginica'}\n</pre> Out[4]: sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) iris type 0 5.1 3.5 1.4 0.2 0.0 1 4.9 3.0 1.4 0.2 0.0 2 4.7 3.2 1.3 0.2 0.0 3 4.6 3.1 1.5 0.2 0.0 4 5.0 3.6 1.4 0.2 0.0 In\u00a0[5]: Copied! <pre>xt,xv,yt,yv = train_test_split(x,y,test_size=1/3,random_state=7)\nprint('training data   (xt) shape: %s'%str(xt.shape))\nprint('training labels (yt) shape: %s'%str(yt.shape))\nprint('testing data    (xv) shape: %s'%str(xv.shape))\nprint('testing labels  (yv) shape: %s'%str(yv.shape))\n</pre> xt,xv,yt,yv = train_test_split(x,y,test_size=1/3,random_state=7) print('training data   (xt) shape: %s'%str(xt.shape)) print('training labels (yt) shape: %s'%str(yt.shape)) print('testing data    (xv) shape: %s'%str(xv.shape)) print('testing labels  (yv) shape: %s'%str(yv.shape)) <pre>training data   (xt) shape: (100, 4)\ntraining labels (yt) shape: (100,)\ntesting data    (xv) shape: (50, 4)\ntesting labels  (yv) shape: (50,)\n</pre> In\u00a0[6]: Copied! <pre>hp_domain = [\n    {'name':'max_depth', 'bounds':[1,8]},\n    {'name':'min_weight_fraction_leaf', 'bounds':[0,.5]}]\nhpnames = [param['name'] for param in hp_domain]\nhp_lb = array([param['bounds'][0] for param in hp_domain])\nhp_ub = array([param['bounds'][1] for param in hp_domain])\nd = len(hp_domain)\ndef get_dt_accuracy(hparams):\n    accuracies = zeros(len(hparams))\n    for i,hparam in enumerate(hparams):\n        kwargs = {hp_domain[j]['name']:hparam[j] for j in range(d)}\n        kwargs['max_depth'] = int(kwargs['max_depth'])\n        dt = DecisionTreeClassifier(random_state=7,**kwargs).fit(xt,yt)\n        yhat = dt.predict(xv)\n        accuracies[i] = mean(yhat==yv)\n    return accuracies\ncf = CustomFun(\n    true_measure = Uniform(DigitalNetB2(d,seed=7),lower_bound=hp_lb,upper_bound=hp_ub),\n    g = get_dt_accuracy,\n    parallel=False)\n</pre> hp_domain = [     {'name':'max_depth', 'bounds':[1,8]},     {'name':'min_weight_fraction_leaf', 'bounds':[0,.5]}] hpnames = [param['name'] for param in hp_domain] hp_lb = array([param['bounds'][0] for param in hp_domain]) hp_ub = array([param['bounds'][1] for param in hp_domain]) d = len(hp_domain) def get_dt_accuracy(hparams):     accuracies = zeros(len(hparams))     for i,hparam in enumerate(hparams):         kwargs = {hp_domain[j]['name']:hparam[j] for j in range(d)}         kwargs['max_depth'] = int(kwargs['max_depth'])         dt = DecisionTreeClassifier(random_state=7,**kwargs).fit(xt,yt)         yhat = dt.predict(xv)         accuracies[i] = mean(yhat==yv)     return accuracies cf = CustomFun(     true_measure = Uniform(DigitalNetB2(d,seed=7),lower_bound=hp_lb,upper_bound=hp_ub),     g = get_dt_accuracy,     parallel=False) In\u00a0[7]: Copied! <pre>avg_accuracy,data_avg_accuracy = CubQMCNetG(cf,abs_tol=1e-4).integrate()\ndata_avg_accuracy\n</pre> avg_accuracy,data_avg_accuracy = CubQMCNetG(cf,abs_tol=1e-4).integrate() data_avg_accuracy Out[7]: <pre>Data (Data)\n    solution        0.787\n    comb_bound_low  0.787\n    comb_bound_high 0.787\n    comb_bound_diff 1.63e-04\n    comb_flags      1\n    n_total         2^(14)\n    n               2^(14)\n    time_integrate  13.243\nCubQMCNetG (AbstractStoppingCriterion)\n    abs_tol         1.00e-04\n    rel_tol         0\n    n_init          2^(10)\n    n_limit         2^(35)\nCustomFun (AbstractIntegrand)\nUniform (AbstractTrueMeasure)\n    lower_bound     [1 0]\n    upper_bound     [8.  0.5]\nDigitalNetB2 (AbstractLDDiscreteDistribution)\n    d               2^(1)\n    replications    1\n    randomize       LMS_DS\n    gen_mats_source joe_kuo.6.21201.txt\n    order           NATURAL\n    t               63\n    alpha           1\n    n_limit         2^(32)\n    entropy         7</pre> <p>Here we find the average accuracy to be 78.7% using $2^{14}$ samples.</p> In\u00a0[8]: Copied! <pre>si = SensitivityIndices(cf)\nsolution_importance,data_importance = CubQMCNetG(si,abs_tol=2.5e-2).integrate()\ndata_importance\n</pre> si = SensitivityIndices(cf) solution_importance,data_importance = CubQMCNetG(si,abs_tol=2.5e-2).integrate() data_importance Out[8]: <pre>Data (Data)\n    solution        [[0.159 0.748]\n                     [0.253 0.837]]\n    comb_bound_low  [[0.144 0.726]\n                     [0.235 0.818]]\n    comb_bound_high [[0.174 0.769]\n                     [0.272 0.855]]\n    comb_bound_diff [[0.03  0.042]\n                     [0.037 0.037]]\n    comb_flags      [[ True  True]\n                     [ True  True]]\n    n_total         2^(13)\n    n               [[[4096 8192]\n                      [4096 8192]\n                      [4096 8192]]\n                    \n                     [[2048 8192]\n                      [2048 8192]\n                      [2048 8192]]]\n    time_integrate  23.404\nCubQMCNetG (AbstractStoppingCriterion)\n    abs_tol         0.025\n    rel_tol         0\n    n_init          2^(10)\n    n_limit         2^(35)\nSensitivityIndices (AbstractIntegrand)\n    indices         [[ True False]\n                     [False  True]]\nUniform (AbstractTrueMeasure)\n    lower_bound     [1 0]\n    upper_bound     [8.  0.5]\nDigitalNetB2 (AbstractLDDiscreteDistribution)\n    d               2^(1)\n    replications    1\n    randomize       LMS_DS\n    gen_mats_source joe_kuo.6.21201.txt\n    order           NATURAL\n    t               63\n    alpha           1\n    n_limit         2^(32)\n    entropy         7</pre> In\u00a0[9]: Copied! <pre>print('closed sensitivity indices: %s'%str(solution_importance[0].squeeze()))\nprint('total sensitivity indices: %s'%str(solution_importance[1].squeeze()))\n</pre> print('closed sensitivity indices: %s'%str(solution_importance[0].squeeze())) print('total sensitivity indices: %s'%str(solution_importance[1].squeeze())) <pre>closed sensitivity indices: [0.15864786 0.74759984]\ntotal sensitivity indices: [0.25307821 0.83668445]\n</pre> <p>Looking closer at the output, we see that the second hyperparameter (<code>min_weight_fraction_leaf</code>) is more important than the first one (<code>max_depth</code>). The closed sensitivity indices measure how much that hyperparameter contributes to testing accuracy variance. The total sensitivity indices measure how much that hyperparameter, or any subset of hyperparameters containing that one contributes to testing accuracy variance. For example, the first closed sensitivity index approximates the variability attributable to {<code>max_depth</code>} while the first total sensitivity index approximates the variability attributable to both {<code>max_depth</code>} and {<code>max_depth</code>,<code>min_weight_fraction_leaf</code>}.</p> In\u00a0[12]: Copied! <pre>def marginal(x,compute_flags,xpts,bools,not_bools):\n    n,_ = x.shape\n    x2 = zeros((n,d),dtype=float)\n    x2[:,bools] = x\n    y = zeros((n,len(xpts)),dtype=float)\n    for k,xpt in enumerate(xpts):\n        if not compute_flags[k]: continue\n        x2[:,not_bools] = xpt\n        y[:,k] = get_dt_accuracy(x2)\n    return y.T\n</pre> def marginal(x,compute_flags,xpts,bools,not_bools):     n,_ = x.shape     x2 = zeros((n,d),dtype=float)     x2[:,bools] = x     y = zeros((n,len(xpts)),dtype=float)     for k,xpt in enumerate(xpts):         if not compute_flags[k]: continue         x2[:,not_bools] = xpt         y[:,k] = get_dt_accuracy(x2)     return y.T In\u00a0[13]: Copied! <pre>fig,ax = pyplot.subplots(nrows=1,ncols=2,figsize=(8,4))\nnticks = 32\nxpts01 = linspace(0,1,nticks)\nfor i in range(2):\n    xpts = xpts01*(hp_ub[i]-hp_lb[i])+hp_lb[i]\n    bools = array([True if j not in [i] else False for j in range(d)])\n    def marginal_i(x,compute_flags): return marginal(x,compute_flags,xpts,bools,~bools)\n    cf = CustomFun(\n         true_measure = Uniform(DigitalNetB2(1,seed=7),lower_bound=hp_lb[bools],upper_bound=hp_ub[bools]),\n         g = marginal_i,\n         dimension_indv = len(xpts),\n         parallel=False)\n    sol,data = CubQMCNetG(cf,abs_tol=5e-2).integrate()\n    ax[i].plot(xpts,sol,'-o',color='m')\n    ax[i].fill_between(xpts,data.comb_bound_high,data.comb_bound_low,color='c',alpha=.5)\n    ax[i].set_xlabel(hpnames[i].replace('_',' '))\n    ax[i].set_ylabel('accuracy')\n</pre> fig,ax = pyplot.subplots(nrows=1,ncols=2,figsize=(8,4)) nticks = 32 xpts01 = linspace(0,1,nticks) for i in range(2):     xpts = xpts01*(hp_ub[i]-hp_lb[i])+hp_lb[i]     bools = array([True if j not in [i] else False for j in range(d)])     def marginal_i(x,compute_flags): return marginal(x,compute_flags,xpts,bools,~bools)     cf = CustomFun(          true_measure = Uniform(DigitalNetB2(1,seed=7),lower_bound=hp_lb[bools],upper_bound=hp_ub[bools]),          g = marginal_i,          dimension_indv = len(xpts),          parallel=False)     sol,data = CubQMCNetG(cf,abs_tol=5e-2).integrate()     ax[i].plot(xpts,sol,'-o',color='m')     ax[i].fill_between(xpts,data.comb_bound_high,data.comb_bound_low,color='c',alpha=.5)     ax[i].set_xlabel(hpnames[i].replace('_',' '))     ax[i].set_ylabel('accuracy') In\u00a0[14]: Copied! <pre>x0 = data_avg_accuracy.xfull*(hp_ub-hp_lb)+hp_lb\ny0 = -data_avg_accuracy.yfull.squeeze()\nprint('best result before BO is %d%% accuracy'%(-100*y0.min()))\nresult = gp_minimize(\n    func = lambda hparams: get_dt_accuracy(atleast_2d(hparams)).squeeze().item(),\n    dimensions = [(l,u) for l,u in zip(hp_lb,hp_ub)],\n    n_calls = 32,\n    n_initial_points = 0,\n    x0 = x0[:128].tolist(),\n    y0 = y0[:128].tolist(),\n    random_state = 7)\nxbo_best = result.x\nybo_best = -result.fun\nprint('best result from BO is %d%% accuracy'%(100*ybo_best))\nxbo = array(result.x_iters)\nybo = -array(result.func_vals)\n</pre> x0 = data_avg_accuracy.xfull*(hp_ub-hp_lb)+hp_lb y0 = -data_avg_accuracy.yfull.squeeze() print('best result before BO is %d%% accuracy'%(-100*y0.min())) result = gp_minimize(     func = lambda hparams: get_dt_accuracy(atleast_2d(hparams)).squeeze().item(),     dimensions = [(l,u) for l,u in zip(hp_lb,hp_ub)],     n_calls = 32,     n_initial_points = 0,     x0 = x0[:128].tolist(),     y0 = y0[:128].tolist(),     random_state = 7) xbo_best = result.x ybo_best = -result.fun print('best result from BO is %d%% accuracy'%(100*ybo_best)) xbo = array(result.x_iters) ybo = -array(result.func_vals) <pre>best result before BO is 94% accuracy\nbest result from BO is 94% accuracy\n</pre> In\u00a0[15]: Copied! <pre>best_kwargs = {name:val for name,val in zip(hpnames,xbo_best)}\nbest_kwargs['max_depth'] = int(best_kwargs['max_depth'])\nprint(best_kwargs)\ndt = DecisionTreeClassifier(random_state=7,**best_kwargs).fit(xt,yt)\nyhat = dt.predict(xv)\naccuracy = mean(yhat==yv)\nprint('best decision tree accuracy: %.1f%%'%(100*accuracy))\nfig = pyplot.figure(figsize=(10,15))\nplot_tree(dt,feature_names=feature_names,class_names=target_names,filled=True);\n</pre> best_kwargs = {name:val for name,val in zip(hpnames,xbo_best)} best_kwargs['max_depth'] = int(best_kwargs['max_depth']) print(best_kwargs) dt = DecisionTreeClassifier(random_state=7,**best_kwargs).fit(xt,yt) yhat = dt.predict(xv) accuracy = mean(yhat==yv) print('best decision tree accuracy: %.1f%%'%(100*accuracy)) fig = pyplot.figure(figsize=(10,15)) plot_tree(dt,feature_names=feature_names,class_names=target_names,filled=True); <pre>{'max_depth': 6, 'min_weight_fraction_leaf': 0.007127984848298143}\nbest decision tree accuracy: 94.0%\n</pre> In\u00a0[18]: Copied! <pre>xfeatures = df.to_numpy()\nxfeatures_low = xfeatures[:,:-1].min(0)\nxfeatures_high = xfeatures[:,:-1].max(0)\nd_features = len(xfeatures_low)\ndef dt_pp(t): return dt.predict_proba(t).T\ncf = CustomFun(\n    true_measure = Uniform(DigitalNetB2(d_features,seed=7),\n        lower_bound = xfeatures_low,\n        upper_bound = xfeatures_high),\n    g = dt_pp,\n    dimension_indv = 3,\n    parallel = False)\nsi_cf = SobolIndices(cf,indices=\"all\")\nsolution,data = CubQMCNetG(si_cf,abs_tol=1e-3,n_init=2**10).integrate()\ndata\n</pre> xfeatures = df.to_numpy() xfeatures_low = xfeatures[:,:-1].min(0) xfeatures_high = xfeatures[:,:-1].max(0) d_features = len(xfeatures_low) def dt_pp(t): return dt.predict_proba(t).T cf = CustomFun(     true_measure = Uniform(DigitalNetB2(d_features,seed=7),         lower_bound = xfeatures_low,         upper_bound = xfeatures_high),     g = dt_pp,     dimension_indv = 3,     parallel = False) si_cf = SobolIndices(cf,indices=\"all\") solution,data = CubQMCNetG(si_cf,abs_tol=1e-3,n_init=2**10).integrate() data Out[18]: <pre>Data (Data)\n    solution        [[[0.    0.049 0.05 ]\n                      [0.    0.    0.   ]\n                      [1.    0.301 0.279]\n                      ...\n                      [0.    0.396 0.409]\n                      [1.    0.999 0.999]\n                      [1.    0.775 0.768]]\n                    \n                     [[0.    0.225 0.232]\n                      [0.    0.    0.   ]\n                      [1.    0.605 0.592]\n                      ...\n                      [0.    0.699 0.721]\n                      [1.    0.999 0.999]\n                      [1.    0.951 0.95 ]]]\n    comb_bound_low  [[[0.    0.048 0.05 ]\n                      [0.    0.    0.   ]\n                      [0.999 0.301 0.278]\n                      ...\n                      [0.    0.395 0.408]\n                      [0.999 0.999 0.999]\n                      [0.999 0.775 0.767]]\n                    \n                     [[0.    0.224 0.231]\n                      [0.    0.    0.   ]\n                      [0.999 0.604 0.591]\n                      ...\n                      [0.    0.698 0.721]\n                      [0.999 0.999 0.999]\n                      [0.999 0.95  0.949]]]\n    comb_bound_high [[[0.    0.05  0.051]\n                      [0.    0.    0.   ]\n                      [1.    0.302 0.279]\n                      ...\n                      [0.    0.397 0.41 ]\n                      [1.    1.    1.   ]\n                      [1.    0.776 0.769]]\n                    \n                     [[0.    0.225 0.233]\n                      [0.    0.    0.   ]\n                      [1.    0.605 0.593]\n                      ...\n                      [0.    0.699 0.722]\n                      [1.    1.    1.   ]\n                      [1.    0.952 0.951]]]\n    comb_bound_diff [[[0.    0.001 0.001]\n                      [0.    0.    0.   ]\n                      [0.001 0.001 0.001]\n                      ...\n                      [0.    0.002 0.002]\n                      [0.001 0.001 0.001]\n                      [0.001 0.001 0.002]]\n                    \n                     [[0.    0.002 0.002]\n                      [0.    0.    0.   ]\n                      [0.001 0.001 0.002]\n                      ...\n                      [0.    0.001 0.001]\n                      [0.001 0.001 0.001]\n                      [0.001 0.002 0.002]]]\n    comb_flags      [[[ True  True  True]\n                      [ True  True  True]\n                      [ True  True  True]\n                      ...\n                      [ True  True  True]\n                      [ True  True  True]\n                      [ True  True  True]]\n                    \n                     [[ True  True  True]\n                      [ True  True  True]\n                      [ True  True  True]\n                      ...\n                      [ True  True  True]\n                      [ True  True  True]\n                      [ True  True  True]]]\n    n_total         2^(18)\n    n               [[[[  1024  65536  65536]\n                       [  1024   1024   1024]\n                       [  8192 131072 131072]\n                       ...\n                       [  1024 131072 131072]\n                       [  8192  65536  32768]\n                       [  8192 262144 131072]]\n                    \n                      [[  1024  65536  65536]\n                       [  1024   1024   1024]\n                       [  8192 131072 131072]\n                       ...\n                       [  1024 131072 131072]\n                       [  8192  65536  32768]\n                       [  8192 262144 131072]]\n                    \n                      [[  1024  65536  65536]\n                       [  1024   1024   1024]\n                       [  8192 131072 131072]\n                       ...\n                       [  1024 131072 131072]\n                       [  8192  65536  32768]\n                       [  8192 262144 131072]]]\n                    \n                    \n                     [[[  1024  32768  32768]\n                       [  1024   1024   1024]\n                       [  8192 131072  65536]\n                       ...\n                       [  1024 131072 131072]\n                       [  8192  65536  32768]\n                       [  8192 131072 131072]]\n                    \n                      [[  1024  32768  32768]\n                       [  1024   1024   1024]\n                       [  8192 131072  65536]\n                       ...\n                       [  1024 131072 131072]\n                       [  8192  65536  32768]\n                       [  8192 131072 131072]]\n                    \n                      [[  1024  32768  32768]\n                       [  1024   1024   1024]\n                       [  8192 131072  65536]\n                       ...\n                       [  1024 131072 131072]\n                       [  8192  65536  32768]\n                       [  8192 131072 131072]]]]\n    time_integrate  8.221\nCubQMCNetG (AbstractStoppingCriterion)\n    abs_tol         0.001\n    rel_tol         0\n    n_init          2^(10)\n    n_limit         2^(35)\nSensitivityIndices (AbstractIntegrand)\n    indices         [[ True False False False]\n                     [False  True False False]\n                     [False False  True False]\n                     ...\n                     [ True  True False  True]\n                     [ True False  True  True]\n                     [False  True  True  True]]\nUniform (AbstractTrueMeasure)\n    lower_bound     [4.3 2.  1.  0.1]\n    upper_bound     [7.9 4.4 6.9 2.5]\nDigitalNetB2 (AbstractLDDiscreteDistribution)\n    d               2^(2)\n    replications    1\n    randomize       LMS_DS\n    gen_mats_source joe_kuo.6.21201.txt\n    order           NATURAL\n    t               63\n    alpha           1\n    n_limit         2^(32)\n    entropy         7</pre> <p>While the solution looks unwieldy, it has quite a natural interpretation. The first axis determines whether we are looking at a closed (index 0) or total (index 1) sensitivity index as before. The second axis indexes the subset of features we are testing. The third and final axis is length 3 for the 3 class probabilities we are interested in. For example, <code>solution[0,2,2]</code> looks at the closed sensitivity index of our index 2 feature (petal length) for our index 2 probability (virginica) AKA how important is petal length alone to determining if an Iris is virginica.</p> <p>The results indicate that setosa Irises can be completely determined based on petal length while the versicolor and virginica Irises can be completely determined by looking at both petal length and petal width. Interestingly sepal length and sepal width do not contribute significantly to determining species.</p> <p>These insights are not surprising or especially insightful for a decision tree where the tree structure indicates importance and the scores may even be computed directly. However, for more complicated models and datasets, this analysis pipeline may provide advanced insight into both hyperparameter tuning and feature importance.</p> In\u00a0[19]: Copied! <pre>print('solution shape:',solution.shape,'\\n')\nsi_closed = solution[0]\nsi_total = solution[1]\nprint('SI Closed')\nprint(si_closed,'\\n')\nprint('SI Total')\nprint(si_total)\n</pre> print('solution shape:',solution.shape,'\\n') si_closed = solution[0] si_total = solution[1] print('SI Closed') print(si_closed,'\\n') print('SI Total') print(si_total) <pre>solution shape: (2, 14, 3) \n\nSI Closed\n[[0.         0.04908409 0.05049225]\n [0.         0.         0.        ]\n [0.99967739 0.30125221 0.27859386]\n [0.         0.32358608 0.33400856]\n [0.         0.04908409 0.05049225]\n [0.99967739 0.45170218 0.43419784]\n [0.         0.3959954  0.40918357]\n [0.99967739 0.30125221 0.27859386]\n [0.         0.32358608 0.33400856]\n [0.99967739 0.77513307 0.7682392 ]\n [0.99967739 0.45170218 0.43419784]\n [0.         0.3959954  0.40918357]\n [0.99967739 0.9993052  0.99928672]\n [0.99967739 0.77513307 0.7682392 ]] \n\nSI Total\n[[0.         0.22459549 0.23200145]\n [0.         0.         0.        ]\n [0.99957471 0.60450383 0.59188948]\n [0.         0.54868564 0.56664366]\n [0.         0.22459549 0.23200145]\n [0.99957471 0.67654515 0.6661002 ]\n [0.         0.69852711 0.72138276]\n [0.99957471 0.60450383 0.59188948]\n [0.         0.54868564 0.56664366]\n [0.99957471 0.95117562 0.9496174 ]\n [0.99957471 0.67654515 0.6661002 ]\n [0.         0.69852711 0.72138276]\n [0.99957471 0.99944024 0.99945703]\n [0.99957471 0.95117562 0.9496174 ]]\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"demos/iris/#ml-sensitivity-indices","title":"ML Sensitivity Indices\u00b6","text":"<p>This notebook demonstrates QMCPy's support for vectorized sensitivity index computation. We preview this functionality by performing classification of Iris species using a decision tree. The computed sensitivity indices provide insight into input subset importance for a classic machine learning problem.</p>"},{"location":"demos/iris/#load-data","title":"Load Data\u00b6","text":"<p>We begin by reading in the Iris dataset and providing some basic summary statistics. Our goal will be to predict the Iris class (Setosa, Versicolour, or Virginica) based on Iris attributes (sepal length, sepal width, petal length, and petal width).</p>"},{"location":"demos/iris/#importance-of-decision-tree-hyperparameters","title":"Importance of Decision Tree Hyperparameters\u00b6","text":"<p>We would like to predict Iris species using a Decision Tree (DT) classifier. When initializing a DT, we arrive at the question of how to set hyperparameters such as tree depth or the minimum weight fraction for each leaf. These hyperparameters can greatly effect classification accuracy, so it is worthwhile to consider their importance to determining classification performance.</p> <p>Note that while this notebook uses decision trees and the Iris dataset, the methodology is directly applicable to other datasets and models.</p> <p>We begin this exploration by setting up a hyperparameter domain in which to uniformly sample DT hyperparameter configurations. A helper function and its tie into QMCPy are also created.</p>"},{"location":"demos/iris/#average-accuracy","title":"Average Accuracy\u00b6","text":"<p>Our first goal will be to find the average DT accuracy across the hyperparameter domain. To do so, we perform quasi-Monte Carlo numerical integration to approximate the mean testing accuracy.</p>"},{"location":"demos/iris/#sensitivity-indices","title":"Sensitivity Indices\u00b6","text":"<p>Next, we wish to quantify how important individual hyperparameters are to determining testing accuracy. To do this, we compute the sensitivity indices of our hyperparameters. In QMCPy we use the <code>SensitivityIndices</code> class to compute these sensitivity indices.</p>"},{"location":"demos/iris/#marginals","title":"Marginals\u00b6","text":"<p>We may also use QMCPy's support for vectorized quasi-Monte Carlo to compute marginal distributions. This is relatively straightforward to do for the Uniform true measure used here, but caution should be taken when adapting these techniques to distributions without independent marginals.</p>"},{"location":"demos/iris/#bayesian-optimization-of-hyperparameters","title":"Bayesian Optimization of Hyperparameters\u00b6","text":"<p>Having explored DT hyperparameter importance, we are now ready to construct our optimal DT. We already have quite a bit of data relating hyperparameter settings to testing accuracy, so we may simply select the best configuration and call this an optimal DT. However, if we are looking to squeeze out even more performance, we may choose to perform Bayesian Optimization which incorporates our past metadata. Sample code is provided below despite not finding an improved configuration for this problem.</p>"},{"location":"demos/iris/#best-decision-tree-analysis","title":"Best Decision Tree Analysis\u00b6","text":"<p>Below we print the configuration that rested in the best DT. We also print the optimal accuracy achieved (at this configuration) and visualize the branches of this tree.</p>"},{"location":"demos/iris/#feature-importance","title":"Feature Importance\u00b6","text":"<p>With the optimal DT in hand, we may now question how important the Irises features are in determining the class/species. To answer this question, we again perform sensitivity analysis, but this time we select a uniform measure over the domain of Iris features. Our output which we wish to quantify the variance of is now a length 3 vector of class probabilities. How variable is each species classification as a function of each Iris feature?</p>"},{"location":"demos/lattice_random_generator/","title":"2023 Random Lattice Generating Vectors","text":"In\u00a0[16]: Copied! <pre>import qmcpy as qp\nimport numpy as np  #basic numerical routines in Python\nimport time  #timing routines\nfrom matplotlib import pyplot;  #plotting\n\npyplot.rc('font', size=16)  #set defaults so that the plots are readable\npyplot.rc('axes', titlesize=16)\npyplot.rc('axes', labelsize=16)\npyplot.rc('xtick', labelsize=16)\npyplot.rc('ytick', labelsize=16)\npyplot.rc('legend', fontsize=16)\npyplot.rc('figure', titlesize=16)\n\n#a helpful plotting method to show increasing numbers of points\ndef plot_successive_points(distrib,ld_name,first_n=64,n_cols=1,pt_clr='bgkcmy',\n                           xlim=[0,1],ylim=[0,1],coord1 = 0,coord2 = 1):\n  fig,ax = pyplot.subplots(nrows=1,ncols=n_cols,figsize=(5*n_cols,5.5))\n  if n_cols==1: ax = [ax]\n  last_n = first_n*(2**n_cols)\n  points = distrib.gen_samples(n=last_n)\n  for i in range(n_cols):\n    n = first_n\n    nstart = 0\n    for j in range(i+1):\n      n = first_n*(2**j)\n      ax[i].scatter(points[nstart:n,coord1],points[nstart:n,coord2],color=pt_clr[j])\n      nstart = n\n    ax[i].set_title('n = %d'%n)\n    ax[i].set_xlim(xlim); ax[i].set_xticks(xlim); ax[i].set_xlabel('$x_{i,%d}$'%(coord1+1))\n    ax[i].set_ylim(ylim); ax[i].set_yticks(ylim); ax[i].set_ylabel('$x_{i,%d}$'%(coord2+1))\n    ax[i].set_aspect((xlim[1]-xlim[0])/(ylim[1]-ylim[0]))\n  fig.suptitle('%s Points'%ld_name)\n</pre> import qmcpy as qp import numpy as np  #basic numerical routines in Python import time  #timing routines from matplotlib import pyplot;  #plotting  pyplot.rc('font', size=16)  #set defaults so that the plots are readable pyplot.rc('axes', titlesize=16) pyplot.rc('axes', labelsize=16) pyplot.rc('xtick', labelsize=16) pyplot.rc('ytick', labelsize=16) pyplot.rc('legend', fontsize=16) pyplot.rc('figure', titlesize=16)  #a helpful plotting method to show increasing numbers of points def plot_successive_points(distrib,ld_name,first_n=64,n_cols=1,pt_clr='bgkcmy',                            xlim=[0,1],ylim=[0,1],coord1 = 0,coord2 = 1):   fig,ax = pyplot.subplots(nrows=1,ncols=n_cols,figsize=(5*n_cols,5.5))   if n_cols==1: ax = [ax]   last_n = first_n*(2**n_cols)   points = distrib.gen_samples(n=last_n)   for i in range(n_cols):     n = first_n     nstart = 0     for j in range(i+1):       n = first_n*(2**j)       ax[i].scatter(points[nstart:n,coord1],points[nstart:n,coord2],color=pt_clr[j])       nstart = n     ax[i].set_title('n = %d'%n)     ax[i].set_xlim(xlim); ax[i].set_xticks(xlim); ax[i].set_xlabel('$x_{i,%d}$'%(coord1+1))     ax[i].set_ylim(ylim); ax[i].set_yticks(ylim); ax[i].set_ylabel('$x_{i,%d}$'%(coord2+1))     ax[i].set_aspect((xlim[1]-xlim[0])/(ylim[1]-ylim[0]))   fig.suptitle('%s Points'%ld_name) In\u00a0[17]: Copied! <pre>lat = qp.Lattice()\nhelp(lat.__init__)\n</pre> lat = qp.Lattice() help(lat.__init__) <pre>Help on method __init__ in module qmcpy.discrete_distribution.lattice.lattice:\n\n__init__(dimension=1, replications=None, seed=None, randomize='SHIFT', generating_vector='kuo.lattice-33002-1024-1048576.9125.txt', order='NATURAL', m_max=None) method of qmcpy.discrete_distribution.lattice.lattice.Lattice instance\n    Args:\n        dimension (Union[int,np.ndarray]): Dimension of the generator.\n    \n            - If an `int` is passed in, use generating vector components at indices 0,...,`dimension`-1.\n            - If an `np.ndarray` is passed in, use generating vector components at these indices.\n        \n        replications (int): Number of independent randomizations.\n        seed (Union[None,int,np.random.SeedSeq): Seed the random number generator for reproducibility.\n        randomize (str): Options are \n            \n            - `'SHIFT'`: Random shift.\n            - `'FALSE'`: No randomization. In this case the first point will be the origin. \n        \n        generating_vector (Union[str,np.ndarray,int]: Specify the generating vector.\n            \n            - A `str` should be the name (or path) of a file from the LDData repo at [https://github.com/QMCSoftware/LDData/tree/main/lattice](https://github.com/QMCSoftware/LDData/tree/main/lattice).\n            - A `np.ndarray` of integers with shape $(d,)$ or $(r,d)$ where $d$ is the number of dimensions and $r$ is the number of replications.  \n                Must supply `m_max` where $2^{m_\\mathrm{max}}$ is the max number of supported samples. \n            - An `int`, call it $M$, \n            gives the random generating vector $(1,v_1,\\dots,v_{d-1})^T$ \n            where $d$ is the dimension and $v_i$ are randomly selected from $\\{3,5,\\dots,2^M-1\\}$ uniformly and independently.  \n            We require require $1 &lt; M &lt; 27$. \n    \n        order (str): `'LINEAR'`, `'NATURAL'`, or `'GRAY'` ordering. See the doctest example above.\n        m_max (int): $2^{m_\\mathrm{max}}$ is the maximum number of supported samples.\n\n</pre> In\u00a0[18]: Copied! <pre>help(lat.gen_samples)\n</pre> help(lat.gen_samples) <pre>Help on method gen_samples in module qmcpy.discrete_distribution.abstract_discrete_distribution:\n\ngen_samples(n=None, n_min=None, n_max=None, return_binary=False, warn=True) method of qmcpy.discrete_distribution.lattice.lattice.Lattice instance\n\n</pre> In\u00a0[19]: Copied! <pre>lat = qp.Lattice(dimension = 2,randomize= True, generating_vector=21, seed = 120) \nprint(\"Basic information of the lattice:\")\nprint(lat)\n\nprint(\"\\nA sample lattice generated by the random generating vector: \")\n\nn = 16 #number of points in the sample\nprint(lat.gen_samples(n))\n</pre> lat = qp.Lattice(dimension = 2,randomize= True, generating_vector=21, seed = 120)  print(\"Basic information of the lattice:\") print(lat)  print(\"\\nA sample lattice generated by the random generating vector: \")  n = 16 #number of points in the sample print(lat.gen_samples(n)) <pre>Basic information of the lattice:\nLattice (AbstractLDDiscreteDistribution)\n    d               2^(1)\n    replications    1\n    randomize       SHIFT\n    gen_vec_source  random\n    order           NATURAL\n    n_limit         2^(21)\n    entropy         120\n\nA sample lattice generated by the random generating vector: \n[[0.34548142 0.46736834]\n [0.84548142 0.96736834]\n [0.59548142 0.21736834]\n [0.09548142 0.71736834]\n [0.47048142 0.84236834]\n [0.97048142 0.34236834]\n [0.72048142 0.59236834]\n [0.22048142 0.09236834]\n [0.40798142 0.15486834]\n [0.90798142 0.65486834]\n [0.65798142 0.90486834]\n [0.15798142 0.40486834]\n [0.53298142 0.52986834]\n [0.03298142 0.02986834]\n [0.78298142 0.27986834]\n [0.28298142 0.77986834]]\n</pre> In\u00a0[20]: Copied! <pre>#Here the sample sizes are prime numbers\nlat = qp.Lattice(dimension=2,generating_vector= 16,seed = 136, order=\"GRAY\")\n    \nprimes = [64,256,512,1024]\n\nfor i in range(len(primes)):\n    plot_successive_points(distrib = lat,ld_name = \"Lattice\",first_n=primes[i],pt_clr=\"bgcmr\"[i])\n</pre> #Here the sample sizes are prime numbers lat = qp.Lattice(dimension=2,generating_vector= 16,seed = 136, order=\"GRAY\")      primes = [64,256,512,1024]  for i in range(len(primes)):     plot_successive_points(distrib = lat,ld_name = \"Lattice\",first_n=primes[i],pt_clr=\"bgcmr\"[i])  In\u00a0[21]: Copied! <pre>import warnings\nwarnings.simplefilter('ignore')\n\nd = 5  #coded as parameters so that \ntol = 1E-3 #you can change here and propagate them through this example\n\ndata_random = qp.CubQMCLatticeG(qp.Keister(qp.Gaussian(qp.Lattice(d,generating_vector = 26), mean = 0, covariance = 1/2)), abs_tol = tol).integrate()[1]\ndata_default = qp.CubQMCLatticeG(qp.Keister(qp.Gaussian(qp.Lattice(d), mean = 0, covariance = 1/2)),  abs_tol = tol).integrate()[1]\nprint(\"Integration data from a random lattice generator:\")\nprint(data_random)\nprint(\"\\nIntegration data from the default lattice generator:\")\nprint(data_default)\n</pre> import warnings warnings.simplefilter('ignore')  d = 5  #coded as parameters so that  tol = 1E-3 #you can change here and propagate them through this example  data_random = qp.CubQMCLatticeG(qp.Keister(qp.Gaussian(qp.Lattice(d,generating_vector = 26), mean = 0, covariance = 1/2)), abs_tol = tol).integrate()[1] data_default = qp.CubQMCLatticeG(qp.Keister(qp.Gaussian(qp.Lattice(d), mean = 0, covariance = 1/2)),  abs_tol = tol).integrate()[1] print(\"Integration data from a random lattice generator:\") print(data_random) print(\"\\nIntegration data from the default lattice generator:\") print(data_default)   <pre>Integration data from a random lattice generator:\nData (Data)\n    solution        1.135\n    comb_bound_low  1.135\n    comb_bound_high 1.136\n    comb_bound_diff 0.001\n    comb_flags      1\n    n_total         2^(17)\n    n               2^(17)\n    time_integrate  0.153\nCubQMCLatticeG (AbstractStoppingCriterion)\n    abs_tol         0.001\n    rel_tol         0\n    n_init          2^(10)\n    n_limit         2^(30)\nKeister (AbstractIntegrand)\nGaussian (AbstractTrueMeasure)\n    mean            0\n    covariance      2^(-1)\n    decomp_type     PCA\n    transform       Gaussian (AbstractTrueMeasure)\n                        mean            0\n                        covariance      2^(-1)\n                        decomp_type     PCA\nLattice (AbstractLDDiscreteDistribution)\n    d               5\n    replications    1\n    randomize       SHIFT\n    gen_vec_source  random\n    order           NATURAL\n    n_limit         2^(26)\n    entropy         253523607185012067649885865695647709066\n\nIntegration data from the default lattice generator:\nData (Data)\n    solution        1.135\n    comb_bound_low  1.135\n    comb_bound_high 1.136\n    comb_bound_diff 0.001\n    comb_flags      1\n    n_total         2^(17)\n    n               2^(17)\n    time_integrate  0.147\nCubQMCLatticeG (AbstractStoppingCriterion)\n    abs_tol         0.001\n    rel_tol         0\n    n_init          2^(10)\n    n_limit         2^(30)\nKeister (AbstractIntegrand)\nGaussian (AbstractTrueMeasure)\n    mean            0\n    covariance      2^(-1)\n    decomp_type     PCA\n    transform       Gaussian (AbstractTrueMeasure)\n                        mean            0\n                        covariance      2^(-1)\n                        decomp_type     PCA\nLattice (AbstractLDDiscreteDistribution)\n    d               5\n    replications    1\n    randomize       SHIFT\n    gen_vec_source  kuo.lattice-33002-1024-1048576.9125.txt\n    order           NATURAL\n    n_limit         2^(20)\n    entropy         122490702083376388603057539026669171278\n</pre> In\u00a0[\u00a0]: Copied! <pre>#mean vs. median plot\nimport qmcpy as qp\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\n\nd = 2\nN_min = 6\nN_max = 18\nN_list = 2**np.arange(N_min,N_max)\nr = 11\nnum_trials = 25\n\n\nerror_median = np.zeros(N_max - N_min) \nerror_mean = np.zeros(N_max - N_min) \nerror_mean_onegen = np.zeros(N_max - N_min) \nfor i in range(num_trials):\n    y_median = []\n    y_mean = []\n    y_mean_one_gen = []\n    print(\"%d, \"%i,end='',flush=True)\n    list_of_keister_objects_random = []\n    list_of_keister_objects_default = []\n    y_randomized_list = []\n    y_default_list = []\n    for k in range(r):\n        lattice = qp.Lattice(generating_vector = 26,dimension=d)\n        keister = qp.Keister(lattice)\n        list_of_keister_objects_random.append(keister)\n        x = keister.discrete_distrib.gen_samples(N_list.max())\n        y = keister.f(x)\n        y_randomized_list.append(y)\n        keister = qp.Keister(qp.Lattice(d))\n        list_of_keister_objects_default.append(keister)\n        x = keister.discrete_distrib.gen_samples(N_list.max())\n        y = keister.f(x)    \n        y_default_list.append(y) \n            \n    for N in N_list:\n\n        y_median.append(np.median([np.mean(y[:N]) for y in y_randomized_list]))\n        y_mean_one_gen.append(np.mean([np.mean(y[:N]) for y in y_default_list]))\n        y_mean.append(np.mean([np.mean(y[:N]) for y in y_randomized_list]))\n\n    answer = keister.exact_integ(d)\n    error_median += abs(answer-y_median)\n    error_mean += abs(answer-y_mean)\n    error_mean_onegen += abs(answer-y_mean_one_gen)\nprint()\n\nerror_median /= num_trials\nerror_mean /= num_trials\nerror_mean_onegen /= num_trials\n\nplt.loglog(N_list,error_median,label = \"median of means\")\nplt.loglog(N_list,error_mean,label = \"mean of means\")\nplt.loglog(N_list,error_mean_onegen,label = \"mean of random shifts\")\nplt.xlabel(\"sample size\")\nplt.ylabel(\"error\")\nplt.title(\"Comparison of lattice generators\")\nplt.legend()\n# plt.savefig(\"./meanvsmedian.png\")\n</pre> #mean vs. median plot import qmcpy as qp import numpy as np import matplotlib.pyplot as plt    d = 2 N_min = 6 N_max = 18 N_list = 2**np.arange(N_min,N_max) r = 11 num_trials = 25   error_median = np.zeros(N_max - N_min)  error_mean = np.zeros(N_max - N_min)  error_mean_onegen = np.zeros(N_max - N_min)  for i in range(num_trials):     y_median = []     y_mean = []     y_mean_one_gen = []     print(\"%d, \"%i,end='',flush=True)     list_of_keister_objects_random = []     list_of_keister_objects_default = []     y_randomized_list = []     y_default_list = []     for k in range(r):         lattice = qp.Lattice(generating_vector = 26,dimension=d)         keister = qp.Keister(lattice)         list_of_keister_objects_random.append(keister)         x = keister.discrete_distrib.gen_samples(N_list.max())         y = keister.f(x)         y_randomized_list.append(y)         keister = qp.Keister(qp.Lattice(d))         list_of_keister_objects_default.append(keister)         x = keister.discrete_distrib.gen_samples(N_list.max())         y = keister.f(x)             y_default_list.append(y)                   for N in N_list:          y_median.append(np.median([np.mean(y[:N]) for y in y_randomized_list]))         y_mean_one_gen.append(np.mean([np.mean(y[:N]) for y in y_default_list]))         y_mean.append(np.mean([np.mean(y[:N]) for y in y_randomized_list]))      answer = keister.exact_integ(d)     error_median += abs(answer-y_median)     error_mean += abs(answer-y_mean)     error_mean_onegen += abs(answer-y_mean_one_gen) print()  error_median /= num_trials error_mean /= num_trials error_mean_onegen /= num_trials  plt.loglog(N_list,error_median,label = \"median of means\") plt.loglog(N_list,error_mean,label = \"mean of means\") plt.loglog(N_list,error_mean_onegen,label = \"mean of random shifts\") plt.xlabel(\"sample size\") plt.ylabel(\"error\") plt.title(\"Comparison of lattice generators\") plt.legend() # plt.savefig(\"./meanvsmedian.png\")  <pre>0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, \n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"demos/lattice_random_generator/#random-lattice-generators-are-not-bad","title":"Random Lattice Generators Are Not Bad\u00b6","text":""},{"location":"demos/lattice_random_generator/#lattice-declaration-and-the-gen_samples-function","title":"Lattice Declaration and the gen_samples function\u00b6","text":""},{"location":"demos/lattice_random_generator/#driver-code","title":"Driver code\u00b6","text":""},{"location":"demos/lattice_random_generator/#plots-of-lattices","title":"Plots of lattices\u00b6","text":""},{"location":"demos/lattice_random_generator/#integration","title":"Integration\u00b6","text":""},{"location":"demos/lattice_random_generator/#runtime-comparison-between-random-generator-and-hard-conded-generator","title":"Runtime comparison between random generator and hard-conded generator\u00b6","text":""},{"location":"demos/lattice_random_generator/#mean-vs-median-as-a-function-of-the-sample-size-plot","title":"Mean vs. Median as a function of the sample size plot\u00b6","text":""},{"location":"demos/lebesgue_integration/","title":"Lebesgue Integration","text":"In\u00a0[26]: Copied! <pre>from qmcpy import *\nfrom numpy import *\n</pre> from qmcpy import * from numpy import * In\u00a0[27]: Copied! <pre>abs_tol = .01\ndim = 1\na = 0\nb = 2\ntrue_value = 8./3\n</pre> abs_tol = .01 dim = 1 a = 0 b = 2 true_value = 8./3 In\u00a0[28]: Copied! <pre># Lebesgue Measure\nintegrand = CustomFun(\n    true_measure = Lebesgue(Uniform(Halton(dim, seed=7, replications=32),lower_bound=a, upper_bound=b)),\n    g = lambda x: (x**2).sum(-1))\nsolution,data = CubQMCCLT(integrand, abs_tol=abs_tol).integrate()\nprint('y = %.3f'%solution)\nerror = abs((solution-true_value))\nif error&gt;abs_tol:\n    raise Exception(\"Not within error tolerance\")\n</pre> # Lebesgue Measure integrand = CustomFun(     true_measure = Lebesgue(Uniform(Halton(dim, seed=7, replications=32),lower_bound=a, upper_bound=b)),     g = lambda x: (x**2).sum(-1)) solution,data = CubQMCCLT(integrand, abs_tol=abs_tol).integrate() print('y = %.3f'%solution) error = abs((solution-true_value)) if error&gt;abs_tol:     raise Exception(\"Not within error tolerance\") <pre>y = 2.667\n</pre> In\u00a0[29]: Copied! <pre># Uniform Measure\nintegrand = CustomFun(\n    true_measure = Uniform(Halton(dim, seed=7, replications=32), lower_bound=a, upper_bound=b),\n    g = lambda x: (2*(x**2)).sum(-1))\nsolution,data = CubQMCCLT(integrand, abs_tol=abs_tol).integrate()\nprint('y = %.3f'%solution)\nerror = abs((solution-true_value))\nif error&gt;abs_tol:\n    raise Exception(\"Not within error tolerance\")\n</pre> # Uniform Measure integrand = CustomFun(     true_measure = Uniform(Halton(dim, seed=7, replications=32), lower_bound=a, upper_bound=b),     g = lambda x: (2*(x**2)).sum(-1)) solution,data = CubQMCCLT(integrand, abs_tol=abs_tol).integrate() print('y = %.3f'%solution) error = abs((solution-true_value)) if error&gt;abs_tol:     raise Exception(\"Not within error tolerance\") <pre>y = 2.667\n</pre> In\u00a0[30]: Copied! <pre>abs_tol = .001\ndim = 2\na = array([1.,2.])\nb = array([2.,4.])\ntrue_value = ((a[0]**3-b[0]**3)*(a[1]-b[1])+(a[0]-b[0])*(a[1]**3-b[1]**3))/3\nprint('Answer = %.5f'%true_value)\n</pre> abs_tol = .001 dim = 2 a = array([1.,2.]) b = array([2.,4.]) true_value = ((a[0]**3-b[0]**3)*(a[1]-b[1])+(a[0]-b[0])*(a[1]**3-b[1]**3))/3 print('Answer = %.5f'%true_value) <pre>Answer = 23.33333\n</pre> In\u00a0[31]: Copied! <pre># Lebesgue Measure\nintegrand = CustomFun(\n    true_measure = Lebesgue(Uniform(DigitalNetB2(dim, seed=7, replications=32), lower_bound=a, upper_bound=b)), \n    g = lambda x: (x**2).sum(-1))\nsolution,data = CubQMCCLT(integrand, abs_tol=abs_tol).integrate()\nprint('y = %.5f'%solution)\nerror = abs((solution-true_value))\nif error&gt;abs_tol:\n    raise Exception(\"Not within error tolerance\")\n</pre> # Lebesgue Measure integrand = CustomFun(     true_measure = Lebesgue(Uniform(DigitalNetB2(dim, seed=7, replications=32), lower_bound=a, upper_bound=b)),      g = lambda x: (x**2).sum(-1)) solution,data = CubQMCCLT(integrand, abs_tol=abs_tol).integrate() print('y = %.5f'%solution) error = abs((solution-true_value)) if error&gt;abs_tol:     raise Exception(\"Not within error tolerance\") <pre>y = 23.33336\n</pre> In\u00a0[32]: Copied! <pre># Uniform Measure\nintegrand = CustomFun(\n    true_measure = Uniform(DigitalNetB2(dim, seed=17, replications=32), lower_bound=a, upper_bound=b),\n    g = lambda x: (b-a).prod()*(x**2).sum(-1))\nsolution,data = CubQMCCLT(integrand, abs_tol=abs_tol).integrate()\nprint('y = %.5f'%solution)\nerror = abs((solution-true_value))\nif error&gt;abs_tol:\n    raise Exception(\"Not within error tolerance\")\n</pre> # Uniform Measure integrand = CustomFun(     true_measure = Uniform(DigitalNetB2(dim, seed=17, replications=32), lower_bound=a, upper_bound=b),     g = lambda x: (b-a).prod()*(x**2).sum(-1)) solution,data = CubQMCCLT(integrand, abs_tol=abs_tol).integrate() print('y = %.5f'%solution) error = abs((solution-true_value)) if error&gt;abs_tol:     raise Exception(\"Not within error tolerance\") <pre>y = 23.33333\n</pre> In\u00a0[33]: Copied! <pre>abs_tol = .0001\ndim = 1\na = 3\nb = 5\ntrue_value = -0.87961 \n</pre> abs_tol = .0001 dim = 1 a = 3 b = 5 true_value = -0.87961  In\u00a0[34]: Copied! <pre># Lebesgue Measure\nintegrand = CustomFun(\n    true_measure = Lebesgue(Uniform(Lattice(dim, randomize=True, seed=7),a,b)), \n    g = lambda x: (sin(x)/log(x)).sum(-1))\nsolution,data = CubQMCLatticeG(integrand, abs_tol=abs_tol).integrate()\nprint('y = %.3f'%solution)\nerror = abs((solution-true_value))\nif error&gt;abs_tol:\n    raise Exception(\"Not within error tolerance\")\n</pre> # Lebesgue Measure integrand = CustomFun(     true_measure = Lebesgue(Uniform(Lattice(dim, randomize=True, seed=7),a,b)),      g = lambda x: (sin(x)/log(x)).sum(-1)) solution,data = CubQMCLatticeG(integrand, abs_tol=abs_tol).integrate() print('y = %.3f'%solution) error = abs((solution-true_value)) if error&gt;abs_tol:     raise Exception(\"Not within error tolerance\") <pre>y = -0.880\n</pre> In\u00a0[35]: Copied! <pre>abs_tol = .1\ndim = 2\ntrue_value = pi\n</pre> abs_tol = .1 dim = 2 true_value = pi In\u00a0[36]: Copied! <pre>integrand = CustomFun(\n    true_measure = Lebesgue(Gaussian(Lattice(dim,seed=7))),\n    g = lambda x: exp(-x**2).prod(1))\nsolution,data = CubQMCLatticeG(integrand,abs_tol=abs_tol).integrate()\nprint('y = %.3f'%solution)\nerror = abs((solution-true_value))\nif error&gt;abs_tol:\n    raise Exception(\"Not within error tolerance\")\n</pre> integrand = CustomFun(     true_measure = Lebesgue(Gaussian(Lattice(dim,seed=7))),     g = lambda x: exp(-x**2).prod(1)) solution,data = CubQMCLatticeG(integrand,abs_tol=abs_tol).integrate() print('y = %.3f'%solution) error = abs((solution-true_value)) if error&gt;abs_tol:     raise Exception(\"Not within error tolerance\") <pre>y = 3.142\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"demos/lebesgue_integration/#qmcpy-for-lebesgue-integration","title":"QMCPy for Lebesgue Integration\u00b6","text":"<p>This notebook will give examples of how to use QMCPy for integration problems that not are defined in terms of a standard measure. i.e. Uniform or Gaussian.</p>"},{"location":"demos/lebesgue_integration/#sample-problem-1","title":"Sample Problem 1\u00b6","text":"<p>$$y  = \\int_{[0,2]} x^2 dx = 2\\int_{[0,2]} \\frac{x^2}{2} dx$$</p> <p>where the middle expression may be viewed as WRT the Lebesgue measure and the right expression may be viewed as WRT the uniform measure</p>"},{"location":"demos/lebesgue_integration/#sample-problem-2","title":"Sample Problem 2\u00b6","text":"<p>$$y = \\int_{[a,b]^d} ||x||_2^2 dx = \\Pi_{i=1}^d (b_i-a_i)\\int_{[a,b]^d} ||x||_2^2 \\; [ \\Pi_{i=1}^d (b_i-a_i)]^{-1} dx$$</p> <p>where the middle expression may be viewed as WRT the Lebesgue measure and the right expression may be viewed as WRT the uniform measure</p>"},{"location":"demos/lebesgue_integration/#sample-problem-3","title":"Sample Problem 3\u00b6","text":"<p>Integral that cannot be done in terms of any standard mathematical functions</p> <p>$$y = \\int_{[a,b]} \\frac{\\sin{x}}{\\log{x}} dx$$</p> <p>where the middle expression may be viewed as WRT the Lebesgue measure and the right expression may be viewed as WRT the uniform measure</p> <p>Mathematica Code: <code>Integrate[Sin[x]/Log[x], {x,a,b}]</code></p>"},{"location":"demos/lebesgue_integration/#sample-problem-4","title":"Sample Problem 4\u00b6","text":"<p>Integral over $\\mathbb{R}^d$ $$y = \\int_{\\mathbb{R}^2} e^{-||x||_2^2} dx$$</p>"},{"location":"demos/linear-scrambled-halton/","title":"Halton Points and their Randomizations","text":"<p>In Halton, each dimension has a prime number associated to it: 2 is associated to dimension 1, 3 to dimension 2, 5 to dimension 3 and so on. These prime numbers are referred as bases.</p> <p>Based on the bases, a different scrambling matrix is generated for each dimension where the lower triangle is random between 0 and (base - 1), the diagonal is random between 1 and (base - 1), and the upper triangle has all zeros. For each dimension, we convert the indices to their log base representations, compute their dot product with the scrambling matrix, and convert them to decimal (base 10) to generate our samples.</p> <p>Based on the bases, a different vector is generated for each dimension which is random between 0 and (base - 1). For each dimension, we convert the indices to their log base representations, add them to the vector , and convert them to decimal (base 10) to generate our samples.</p> <p>In this option, we implement the Linear Matrix Scrambling of Halton but before converting to base 10, we apply the Digital Shift to the scrambled coefficients (the dot product of the log base representations of the indices with the scrambling matrix), and then convert to base 10.</p> In\u00a0[1]: Copied! <pre>import qmcpy as qp\n</pre> import qmcpy as qp In\u00a0[2]: Copied! <pre>help(qp.Halton().__init__)\n</pre> help(qp.Halton().__init__) <pre>Help on method __init__ in module qmcpy.discrete_distribution.halton:\n\n__init__(dimension=1, replications=None, seed=None, randomize='LMS_PERM', t=63, n_lim=4294967296, t_lms=None) method of qmcpy.discrete_distribution.halton.Halton instance\n    Args:\n        dimension (Union[int,np.ndarray]): Dimension of the generator.\n    \n            - If an `int` is passed in, use generating vector components at indices 0,...,`dimension`-1.\n            - If an `np.ndarray` is passed in, use generating vector components at these indices.\n        \n        replications (int): Number of independent randomizations of a pointset.\n        seed (Union[None,int,np.random.SeedSeq): Seed the random number generator for reproducibility.\n        randomize (str): Options are\n            \n            - `'LMS_PERM'`: Linear matrix scramble with digital shift.\n            - `'LMS_DS'`: Linear matrix scramble with permutation.\n            - `'LMS'`: Linear matrix scramble only.\n            - `'PERM'`: Permutation scramble only.\n            - `'DS'`: Digital shift only.\n            - `'NUS'`: Nested uniform scrambling.\n            - `'QRNG'`: Deterministic permutation scramble and random digital shift from QRNG [1] (with `generalize=True`). Does *not* support replications&gt;1.\n            - `'FALSE'`: No randomization. In this case the first point will be the origin. \n        t (int): Number of bits in integer represetation of points *after* randomization. The number of bits in the generating matrices is inferred based on the largest value.\n        n_lim (int): Maximum number of compatible points, determines the number of rows in the generating matrices.\n\n</pre> In\u00a0[3]: Copied! <pre>help(qp.Halton().gen_samples)\n</pre> help(qp.Halton().gen_samples) <pre>Help on method gen_samples in module qmcpy.discrete_distribution.abstract_discrete_distribution:\n\ngen_samples(n=None, n_min=None, n_max=None, return_binary=False, warn=True) method of qmcpy.discrete_distribution.halton.Halton instance\n\n</pre> In\u00a0[4]: Copied! <pre>dimension = 2\nlms_halton = qp.Halton(dimension, randomize= 'LMS') # Linear Matrix Scrambling\nds_halton = qp.Halton(dimension, randomize= 'DS') # Digital Shift\nlms_ds_halton = qp.Halton(dimension, randomize= 'LMS_DS') # Linear Matrix Scrambling Combined with Digital Shift\n</pre> dimension = 2 lms_halton = qp.Halton(dimension, randomize= 'LMS') # Linear Matrix Scrambling ds_halton = qp.Halton(dimension, randomize= 'DS') # Digital Shift lms_ds_halton = qp.Halton(dimension, randomize= 'LMS_DS') # Linear Matrix Scrambling Combined with Digital Shift <p>Here the difference between the three is shown by printing some samples:</p> In\u00a0[5]: Copied! <pre>print(\"Samples Generated by Linear Matrix Scrambling: \")\nprint(lms_halton.gen_samples(8,warn=False))\nprint(\"\\nSamples Generated by Digital Shift: \")\nprint(ds_halton.gen_samples(8))\nprint(\"\\nSamples Generated by Linear Matrix Scrambling Combined with Digital Shift: \")\nprint(lms_ds_halton.gen_samples(8))\n</pre> print(\"Samples Generated by Linear Matrix Scrambling: \") print(lms_halton.gen_samples(8,warn=False)) print(\"\\nSamples Generated by Digital Shift: \") print(ds_halton.gen_samples(8)) print(\"\\nSamples Generated by Linear Matrix Scrambling Combined with Digital Shift: \") print(lms_ds_halton.gen_samples(8)) <pre>Samples Generated by Linear Matrix Scrambling: \n[[0.         0.        ]\n [0.66488583 0.4953392 ]\n [0.25933582 0.9904689 ]\n [0.90756417 0.21615921]\n [0.16785788 0.5632913 ]\n [0.50313275 0.72102387]\n [0.40862172 0.28368836]\n [0.76047682 0.44152192]]\n\nSamples Generated by Digital Shift: \n[[0.06134401 0.4803077 ]\n [0.56134401 0.81364103]\n [0.31134401 0.14697436]\n [0.81134401 0.59141881]\n [0.18634401 0.92475214]\n [0.68634401 0.25808548]\n [0.43634401 0.36919659]\n [0.93634401 0.70252992]]\n\nSamples Generated by Linear Matrix Scrambling Combined with Digital Shift: \n[[0.43966325 0.00409093]\n [0.63714101 0.72179898]\n [0.08269944 0.44092915]\n [0.77633116 0.32543015]\n [0.35612274 0.90047472]\n [0.53412944 0.60725921]\n [0.24433632 0.17094181]\n [0.92618907 0.8776755 ]]\n</pre> <p>Here the difference between the three is shown by plotting with more samples:</p> In\u00a0[6]: Copied! <pre>fig1,ax1 = qp.plot_proj(lms_halton, n = 2**5)\nfig2,ax2 = qp.plot_proj(ds_halton,n = 2**5)\nfig3,ax3 = qp.plot_proj(lms_ds_halton,n = 2**5)\n</pre> fig1,ax1 = qp.plot_proj(lms_halton, n = 2**5) fig2,ax2 = qp.plot_proj(ds_halton,n = 2**5) fig3,ax3 = qp.plot_proj(lms_ds_halton,n = 2**5) <pre>/Users/alegresor/Desktop/QMCSoftware/qmcpy/discrete_distribution/halton.py:260: UserWarning: Without randomization, the first Halton point is the origin\n  warnings.warn(\"Without randomization, the first Halton point is the origin\")\n</pre> <p>Here we show a 3-dimensional scrambled Halton:</p> In\u00a0[7]: Copied! <pre>dimension = 3\nlms_halton = qp.Halton(dimension, randomize= 'LMS')\nfig,ax = qp.plot_proj(lms_halton, n = 2**5, d_horizontal = range(dimension), d_vertical = range(dimension),math_ind = False)\n</pre> dimension = 3 lms_halton = qp.Halton(dimension, randomize= 'LMS') fig,ax = qp.plot_proj(lms_halton, n = 2**5, d_horizontal = range(dimension), d_vertical = range(dimension),math_ind = False) <pre>/Users/alegresor/Desktop/QMCSoftware/qmcpy/discrete_distribution/halton.py:260: UserWarning: Without randomization, the first Halton point is the origin\n  warnings.warn(\"Without randomization, the first Halton point is the origin\")\n</pre> <p>Here we show a 4-dimensional scrambled Halton with successively increasing number of points. The initial points are in blue.  The next additional points are in orange. The final additional points are in green.</p> In\u00a0[8]: Copied! <pre>dimension = 4\nlms_halton = qp.Halton(dimension, randomize= 'LMS')\nfig,ax = qp.plot_proj(lms_halton, n = [2**5,2**6,2**7], d_horizontal = range(dimension), d_vertical = range(dimension),math_ind = False,marker_size = 15)\n</pre> dimension = 4 lms_halton = qp.Halton(dimension, randomize= 'LMS') fig,ax = qp.plot_proj(lms_halton, n = [2**5,2**6,2**7], d_horizontal = range(dimension), d_vertical = range(dimension),math_ind = False,marker_size = 15) <pre>/Users/alegresor/Desktop/QMCSoftware/qmcpy/discrete_distribution/halton.py:260: UserWarning: Without randomization, the first Halton point is the origin\n  warnings.warn(\"Without randomization, the first Halton point is the origin\")\n</pre> <p>Here we show a 3-dimensional Halton with Digital Shift:</p> In\u00a0[9]: Copied! <pre>dimension = 3\nds_halton = qp.Halton(dimension, randomize= 'DS')\nfig,ax = qp.plot_proj(ds_halton, n = 2**5, d_horizontal = range(dimension), d_vertical = range(dimension),math_ind = False)\n</pre> dimension = 3 ds_halton = qp.Halton(dimension, randomize= 'DS') fig,ax = qp.plot_proj(ds_halton, n = 2**5, d_horizontal = range(dimension), d_vertical = range(dimension),math_ind = False) <p>Here we show a 4-dimensional Halton with Digital Shift with successively increasing number of points. The initial points are in blue.  The next additional points are in orange. The final additional points are in green.</p> In\u00a0[10]: Copied! <pre>dimension = 4\nds_halton = qp.Halton(dimension, randomize= 'DS')\nfig,ax = qp.plot_proj(ds_halton, n = [2**5,2**6,2**7], d_horizontal = range(dimension), d_vertical = range(dimension),math_ind = False,marker_size = 15)\n</pre> dimension = 4 ds_halton = qp.Halton(dimension, randomize= 'DS') fig,ax = qp.plot_proj(ds_halton, n = [2**5,2**6,2**7], d_horizontal = range(dimension), d_vertical = range(dimension),math_ind = False,marker_size = 15) <p>Here we show a 3-dimensional Halton with Linear Matrix Scrambling Combined with Digital Shift:</p> In\u00a0[11]: Copied! <pre>dimension = 3\nlms_ds_halton = qp.Halton(dimension, randomize='LMS_DS')\nfig,ax = qp.plot_proj(lms_ds_halton, n = 2**5, d_horizontal = range(dimension), d_vertical = range(dimension),math_ind = False)\n</pre> dimension = 3 lms_ds_halton = qp.Halton(dimension, randomize='LMS_DS') fig,ax = qp.plot_proj(lms_ds_halton, n = 2**5, d_horizontal = range(dimension), d_vertical = range(dimension),math_ind = False) <p>Here we show a 4-dimensional Halton with Linear Matrix Scrambling combined with Digital Shift with successively increasing number of points. The initial points are in blue.  The next additional points are in orange. The final additional points are in green.</p> In\u00a0[12]: Copied! <pre>dimension = 4\nlms_ds_halton = qp.Halton(dimension, randomize='LMS_DS')\nfig,ax = qp.plot_proj(lms_ds_halton, n = [2**5,2**6,2**7], d_horizontal = range(dimension), d_vertical = range(dimension),math_ind = False,marker_size = 15)\n</pre> dimension = 4 lms_ds_halton = qp.Halton(dimension, randomize='LMS_DS') fig,ax = qp.plot_proj(lms_ds_halton, n = [2**5,2**6,2**7], d_horizontal = range(dimension), d_vertical = range(dimension),math_ind = False,marker_size = 15) In\u00a0[14]: Copied! <pre>from time import time\ndimension = 25\nsamples = 1000\nrand_options = ['QRNG','LMS', 'DS', 'LMS_DS', 'OWEN']\nfor i in range(len(rand_options)):\n    t_start = time()\n    rand_halton = qp.Halton(dimension,randomize = rand_options[i]).gen_samples(samples,warn=False)\n    t_end = time()\n    print(\"Time to generate samples for \" + rand_options[i] + \" = \" + str(t_end - t_start))\n</pre> from time import time dimension = 25 samples = 1000 rand_options = ['QRNG','LMS', 'DS', 'LMS_DS', 'OWEN'] for i in range(len(rand_options)):     t_start = time()     rand_halton = qp.Halton(dimension,randomize = rand_options[i]).gen_samples(samples,warn=False)     t_end = time()     print(\"Time to generate samples for \" + rand_options[i] + \" = \" + str(t_end - t_start)) <pre>Time to generate samples for QRNG = 0.01398015022277832\nTime to generate samples for LMS = 0.02975296974182129\nTime to generate samples for DS = 0.016762971878051758\nTime to generate samples for LMS_DS = 0.03149604797363281\nTime to generate samples for OWEN = 10.350087881088257\n</pre>"},{"location":"demos/linear-scrambled-halton/#linear-matrix-scrambling-and-digital-shift-for-halton","title":"Linear Matrix Scrambling and Digital Shift for Halton\u00b6","text":""},{"location":"demos/linear-scrambled-halton/#here-we-explain-linear-matrix-scrambling","title":"Here we explain Linear Matrix Scrambling:\u00b6","text":""},{"location":"demos/linear-scrambled-halton/#here-we-explain-digital-shift","title":"Here we explain Digital Shift:\u00b6","text":""},{"location":"demos/linear-scrambled-halton/#here-we-explain-linear-matrix-scrambling-combined-with-digital-shift","title":"Here we explain Linear Matrix Scrambling Combined with Digital Shift:\u00b6","text":""},{"location":"demos/linear-scrambled-halton/#here-we-set-up-the-qmcpy-environment","title":"Here we set up the QMCPY environment:\u00b6","text":""},{"location":"demos/linear-scrambled-halton/#here-we-explain-the-parameters-of-the-init-function","title":"Here we explain the parameters of the init function:\u00b6","text":""},{"location":"demos/linear-scrambled-halton/#here-we-explain-the-parameters-of-the-gen_samples-function","title":"Here we explain the parameters of the gen_samples function:\u00b6","text":""},{"location":"demos/linear-scrambled-halton/#comparison-between-lms-ds-and-lms_ds","title":"Comparison between LMS, DS, and LMS_DS:\u00b6","text":""},{"location":"demos/linear-scrambled-halton/#examples-of-linear-matrix-scrambling-with-plots","title":"Examples of Linear Matrix Scrambling with plots:\u00b6","text":""},{"location":"demos/linear-scrambled-halton/#examples-of-digital-shifts-with-plots","title":"Examples of Digital Shifts with plots:\u00b6","text":""},{"location":"demos/linear-scrambled-halton/#examples-of-linear-matrix-scrambling-combined-with-digital-shift-with-plots","title":"Examples of Linear Matrix Scrambling Combined with Digital Shift with plots:\u00b6","text":""},{"location":"demos/linear-scrambled-halton/#speed-comparison-between-different-halton-randomize-options","title":"Speed Comparison Between Different Halton Randomize Options:\u00b6","text":""},{"location":"demos/nei_demo/","title":"Noisy EI","text":"In\u00a0[1]: Copied! <pre>import numpy as np\nimport qmcpy as qp\nfrom scipy.linalg import solve_triangular, cho_solve, cho_factor\nfrom scipy.stats import norm\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nlw = 3\nms = 8\n</pre> import numpy as np import qmcpy as qp from scipy.linalg import solve_triangular, cho_solve, cho_factor from scipy.stats import norm import matplotlib.pyplot as plt %matplotlib inline  lw = 3 ms = 8 <p>We make some fake data and consider the sequential decision-making problem of trying to optimize the function depicted below.</p> In\u00a0[2]: Copied! <pre>def yf(x):\n    return np.cos(10 * x) * np.exp(.2 * x) + np.exp(-5 * (x - .4) ** 2)\n\nxplt = np.linspace(0, 1, 300)\nyplt = yf(xplt)\n\nx = np.array([.1, .2, .4, .7, .9])\ny = yf(x)\nv = np.array([.001, .05, .01, .1, .4])\n\nplt.plot(xplt, yplt, linewidth=lw)\nplt.plot(x, y, 'o', markersize=ms, color='orange')\nplt.errorbar(x, y, yerr=2 * np.sqrt(v), marker='', linestyle='', color='orange', linewidth=3)\nplt.title('Sample data with noise');\n</pre> def yf(x):     return np.cos(10 * x) * np.exp(.2 * x) + np.exp(-5 * (x - .4) ** 2)  xplt = np.linspace(0, 1, 300) yplt = yf(xplt)  x = np.array([.1, .2, .4, .7, .9]) y = yf(x) v = np.array([.001, .05, .01, .1, .4])  plt.plot(xplt, yplt, linewidth=lw) plt.plot(x, y, 'o', markersize=ms, color='orange') plt.errorbar(x, y, yerr=2 * np.sqrt(v), marker='', linestyle='', color='orange', linewidth=3) plt.title('Sample data with noise'); <p>We can build a zero mean Gaussian process model to this data, observed under noise.  Below are plots of the posterior distribution.  We use the Gaussian (square exponential) kernel as our prior covariance belief.</p> <p>This kernel has a shape parameter, the Gaussian process has a global variance, which are both chosen fixed for simplicity.  The <code>fudge_factor</code> which is added here to prevent ill-conditioning for a large matrix.</p> <p>Notice the higher uncertainty in the posterior in locations where the observed noise is greater.</p> In\u00a0[3]: Copied! <pre>def gaussian_kernel(x, z, e, pv):\n    return pv * np.exp(-e ** 2 * (x[:, None] - z[None, :]) ** 2)\n\nshape_parameter = 4.1\nprocess_variance = .9\nfudge_factor = 1e-10\n\nkernel_prior_data = gaussian_kernel(x, x, shape_parameter, process_variance)\nkernel_cross_matrix = gaussian_kernel(xplt, x, shape_parameter, process_variance)\nkernel_prior_plot = gaussian_kernel(xplt, xplt, shape_parameter, process_variance)\n\nprior_cholesky = np.linalg.cholesky(kernel_prior_data + np.diag(v))\npartial_cardinal_functions = solve_triangular(prior_cholesky, kernel_cross_matrix.T, lower=True)\nposterior_covariance = kernel_prior_plot - np.dot(partial_cardinal_functions.T, partial_cardinal_functions)\nposterior_cholesky = np.linalg.cholesky(posterior_covariance + fudge_factor * np.eye(len(xplt)))\n\nfull_cardinal_functions = solve_triangular(prior_cholesky.T, partial_cardinal_functions, lower=False)\nposterior_mean = np.dot(full_cardinal_functions.T, y)\n\nnum_posterior_draws = 123\nnormal_draws = np.random.normal(size=(num_posterior_draws, len(xplt)))\nposterior_draws = posterior_mean[:, None] + np.dot(posterior_cholesky, normal_draws.T)\n\nplt.plot(xplt, posterior_draws, alpha=.1, color='r')\nplt.plot(xplt, posterior_mean, color='k', linewidth=lw)\nplt.errorbar(x, y, yerr=2 * np.sqrt(v), marker='', linestyle='', color='orange', linewidth=3);\n</pre> def gaussian_kernel(x, z, e, pv):     return pv * np.exp(-e ** 2 * (x[:, None] - z[None, :]) ** 2)  shape_parameter = 4.1 process_variance = .9 fudge_factor = 1e-10  kernel_prior_data = gaussian_kernel(x, x, shape_parameter, process_variance) kernel_cross_matrix = gaussian_kernel(xplt, x, shape_parameter, process_variance) kernel_prior_plot = gaussian_kernel(xplt, xplt, shape_parameter, process_variance)  prior_cholesky = np.linalg.cholesky(kernel_prior_data + np.diag(v)) partial_cardinal_functions = solve_triangular(prior_cholesky, kernel_cross_matrix.T, lower=True) posterior_covariance = kernel_prior_plot - np.dot(partial_cardinal_functions.T, partial_cardinal_functions) posterior_cholesky = np.linalg.cholesky(posterior_covariance + fudge_factor * np.eye(len(xplt)))  full_cardinal_functions = solve_triangular(prior_cholesky.T, partial_cardinal_functions, lower=False) posterior_mean = np.dot(full_cardinal_functions.T, y)  num_posterior_draws = 123 normal_draws = np.random.normal(size=(num_posterior_draws, len(xplt))) posterior_draws = posterior_mean[:, None] + np.dot(posterior_cholesky, normal_draws.T)  plt.plot(xplt, posterior_draws, alpha=.1, color='r') plt.plot(xplt, posterior_mean, color='k', linewidth=lw) plt.errorbar(x, y, yerr=2 * np.sqrt(v), marker='', linestyle='', color='orange', linewidth=3); <p>First we take a look at the EI quantity by itself which, despite having a closed form, we will approximate using basic Monte Carlo below.  The closed form is very preferable, but not applicable in all situations.</p> <p>Expected improvement is just the expectation (under the posterior distribution) of the improvement beyond the current best value.  If we were trying to maximize this function that we are studying then improvement would be defined as</p> <p>$$I(x) = (Y_x|\\mathcal{D} - y^*)_+,$$</p> <p>the positive part of the gap between the model $Y_x|\\mathcal{D}$ and the current highest value $y^*=\\max\\{y_1,\\ldots,y_N\\}$.  Since $Y_x|\\mathcal{D}$ is a random variable (normally distributed because we have a Gaussian process model), we generally study the expected value of this, which is plotted below.  Written as an integral, this would look like</p> <p>$$\\mathrm{EI}(x) = \\int_{-\\infty}^\\infty (y - y^*)_+\\, p_{Y_x|\\mathcal{D}}(y)\\; \\text{d}y$$</p> <p>NOTE: This quantity is written for maximization here, but most of the literature is concerned with minimization.  We can rewrite this if needed, but the math is essentially the same.</p> <p>This $EI$ quantity is referred to as an acquisition function, a function which defines the utility associated with sampling at a given point.  For each acquisition function, there is a balance between exploration and exploitation (as is the focus of most topics involving sequential decision-making under uncertainty).</p> In\u00a0[4]: Copied! <pre>improvement_draws = np.fmax(posterior_draws - max(y), 0)\nplt.plot(xplt, improvement_draws, alpha=.1, color='#96CA4F', linewidth=lw)\nplt.ylabel('improvement draws')\nax2 = plt.gca().twinx()\nax2.plot(xplt, np.mean(improvement_draws, axis=1), color='#A23D97', linewidth=lw)\nax2.set_ylabel('expected improvement');\n</pre> improvement_draws = np.fmax(posterior_draws - max(y), 0) plt.plot(xplt, improvement_draws, alpha=.1, color='#96CA4F', linewidth=lw) plt.ylabel('improvement draws') ax2 = plt.gca().twinx() ax2.plot(xplt, np.mean(improvement_draws, axis=1), color='#A23D97', linewidth=lw) ax2.set_ylabel('expected improvement'); <p>The NEI quantity is then computed using multiple EI computations (each using a different posterior GP draw) computed without noise.  In this computation below, we will use the closed form of EI, to speed up the computation -- it is possible to execute the same strategy as above, though.</p> <p>This computation is vectorized so as to compute for multiple $x$ locations at the same time. The algorithm from the Facebook paper is written for only a single location.  We are omitting the constraints aspect of their paper because the problem can be considered without that.  To define the integral, though, we need some more definitions/notation.</p> <p>First, we need to define $\\mathrm{EI}(x;\\mathbf{y}, \\mathcal{X}, \\boldsymbol{\\epsilon})$ to be the expected improvement at a location $x$, given the $N$ values stored in the vector $\\mathbf{y}$ having been evaluated with noise $\\boldsymbol{\\epsilon}$ at the points $\\mathcal{X}$,</p> <p>$$ \\mathbf{y}=\\begin{pmatrix}y_1\\\\\\vdots\\\\y_N\\end{pmatrix},\\qquad \\mathcal{X}=\\{\\mathbf{x}_1,\\ldots,\\mathbf{x}_N\\},\\qquad \\boldsymbol{\\epsilon}=\\begin{pmatrix}\\epsilon_1\\\\\\vdots\\\\\\epsilon_N\\end{pmatrix}. $$</p> <p>The noise is assumed to be $\\epsilon_i\\sim\\mathcal{N}(0, \\sigma^2)$ for some fixed $\\sigma^2$.  The noise need not actually be homoscedastic, but it is a standard assumption. We encapsulate this information in $\\mathcal{D}=\\{\\mathbf{y},\\mathcal{X},\\boldsymbol{\\epsilon}\\}$.  This is omitted from the earlier notation, because the data would be fixed.</p> <p>The point of NEI though is to deal with noisy observed values (EI, itself, is notorious for not dealing with noisy data very well).  It does this by considering a variety of posterior draws at the locations in $\\mathcal{X}$.  These have distribution</p> <p>$$ Y_{\\mathcal{X}}|\\mathcal{D}=Y_{\\mathcal{X}}|\\mathbf{y}, \\mathcal{X}, \\boldsymbol{\\epsilon}\\sim \\mathcal{N}\\left(\\mathsf{K}(\\mathsf{K}+\\mathsf{E})^{-1}\\mathbf{y}, \\mathsf{K} - \\mathsf{K}(\\mathsf{K}+\\mathsf{E})^{-1}\\mathsf{K}\\right), $$</p> <p>where</p> <p>$$ \\mathbf{k}(x)=\\begin{pmatrix}K(x,x_1)\\\\\\vdots\\\\K(x,x_N)\\end{pmatrix},\\qquad \\mathsf{K}=\\begin{pmatrix} K(x_1,x_1)&amp;\\cdots&amp;K(x_1, x_N)\\\\&amp;\\vdots&amp;\\\\K(x_N,x_1)&amp;\\cdots&amp;K(x_N, x_N) \\end{pmatrix}=\\begin{pmatrix}\\mathbf{k}(x_1)^T\\\\\\vdots\\\\\\mathbf{k}(x_N)^T\\end{pmatrix},\\qquad \\mathsf{E}=\\begin{pmatrix}\\epsilon_1&amp;&amp;\\\\&amp;\\ddots&amp;\\\\&amp;&amp;\\epsilon_N\\end{pmatrix} $$</p> <p>In practice, unless noise has actually been measured at each point, it would be common to simply plug in $\\epsilon_1=\\ldots=\\epsilon_N=\\sigma^2$.  The term <code>noisy_predictions_at_data</code> below is drawn from this distribution (though in a standard iid fashion, not a more awesome QMC fashion).</p> <p>The EI integral, although approximated earlier using Monte Carlo, can actually be written in closed form.  We do so below to also solidify our newer notation:</p> <p>$$ \\mathrm{EI}(x;\\mathbf{y}, \\mathcal{X}, \\boldsymbol{\\epsilon}) = \\int_{-\\infty}^\\infty (y - y^*)_+\\, p_{Y_x|\\mathbf{y}, \\mathcal{X}, \\boldsymbol{\\epsilon}}(y)\\; \\text{d}y = s(z\\Phi(z)+\\phi(z)) $$</p> <p>where $\\phi$ and $\\Phi$ are the standard normal pdf and cdf, and</p> <p>$$ \\mu=\\mathbf{k}(x)^T(\\mathsf{K}+\\mathsf{E})^{-1}\\mathbf{y},\\qquad s^2 = K(x, x)-\\mathbf{k}(x)^T(\\mathsf{K}+\\mathsf{E})^{-1}\\mathbf{k}(x),\\qquad z=(\\mu - y^*)/s. $$</p> <p>It is very important to remember that these quantities are functions of $\\mathbf{y},\\mathcal{X},\\boldsymbol{\\epsilon}$ despite the absence of those quantities in the notation.</p> <p>The goal of the NEI integral is to simulate many possible random realizations of what could actually be the truth at the locations $\\mathcal{X}$ and then run a noiseless EI computation over each of those realizations.  The average of these outcomes is the NEI quantity.  This would look like:</p> <p>$$ \\mathrm{NEI}(x) = \\int_{\\mathbf{f}\\in\\mathbb{R}^N} \\mathrm{EI}(x;\\mathbf{f}, \\mathcal{X}, 0)\\, p_{Y_{\\mathcal{X}}|\\mathbf{y},\\mathcal{X},\\boldsymbol{\\epsilon}}(\\mathbf{f})\\;\\text{d}\\mathbf{f} $$</p> <p>NOTE: There are ways to do this computation in a more vectorized fashion, so it would more likely be a loop involving chunks of MC elements at a time.  Just so you know.</p> In\u00a0[5]: Copied! <pre>num_draws_at_data = 109\n# These draws are done through QMC in the FB paper\nnormal_draws_at_data = np.random.normal(size=(num_draws_at_data, len(x)))\n\npartial_cardinal_functions_at_data = solve_triangular(prior_cholesky, kernel_prior_data.T, lower=True)\nposterior_covariance_at_data = kernel_prior_data - np.dot(partial_cardinal_functions_at_data.T, partial_cardinal_functions_at_data)\nposterior_cholesky_at_data = np.linalg.cholesky(posterior_covariance_at_data + fudge_factor * np.eye(len(x)))\n\nnoisy_predictions_at_data = y[:, None] + np.dot(posterior_cholesky_at_data, normal_draws_at_data.T)\n\nprior_cholesky_noiseless = np.linalg.cholesky(kernel_prior_data)\npartial_cardinal_functions = solve_triangular(prior_cholesky_noiseless, kernel_cross_matrix.T, lower=True)\nfull_cardinal_functions = solve_triangular(prior_cholesky.T, partial_cardinal_functions, lower=False)\npointwise_sd = np.sqrt(np.fmax(process_variance - np.sum(partial_cardinal_functions ** 2, axis=0), 1e-100))\n\nall_noiseless_eis = []\nfor draw in noisy_predictions_at_data.T:\n    posterior_mean = np.dot(full_cardinal_functions.T, draw)\n    \n    z = (posterior_mean - max(y)) / pointwise_sd\n    ei = pointwise_sd * (z * norm.cdf(z) + norm.pdf(z))\n    \n    all_noiseless_eis.append(ei)\n\nall_noiseless_eis = np.array(all_noiseless_eis)\n\nplt.plot(xplt, all_noiseless_eis.T, alpha=.1, color='#96CA4F', linewidth=lw)\nplt.ylabel('expected improvement draws', color='#96CA4F')\nax2 = plt.gca().twinx()\nax2.plot(xplt, np.mean(all_noiseless_eis, axis=0), color='#A23D97', linewidth=lw)\nax2.set_ylabel('noisy expected improvement', color='#A23D97');\n</pre> num_draws_at_data = 109 # These draws are done through QMC in the FB paper normal_draws_at_data = np.random.normal(size=(num_draws_at_data, len(x)))  partial_cardinal_functions_at_data = solve_triangular(prior_cholesky, kernel_prior_data.T, lower=True) posterior_covariance_at_data = kernel_prior_data - np.dot(partial_cardinal_functions_at_data.T, partial_cardinal_functions_at_data) posterior_cholesky_at_data = np.linalg.cholesky(posterior_covariance_at_data + fudge_factor * np.eye(len(x)))  noisy_predictions_at_data = y[:, None] + np.dot(posterior_cholesky_at_data, normal_draws_at_data.T)  prior_cholesky_noiseless = np.linalg.cholesky(kernel_prior_data) partial_cardinal_functions = solve_triangular(prior_cholesky_noiseless, kernel_cross_matrix.T, lower=True) full_cardinal_functions = solve_triangular(prior_cholesky.T, partial_cardinal_functions, lower=False) pointwise_sd = np.sqrt(np.fmax(process_variance - np.sum(partial_cardinal_functions ** 2, axis=0), 1e-100))  all_noiseless_eis = [] for draw in noisy_predictions_at_data.T:     posterior_mean = np.dot(full_cardinal_functions.T, draw)          z = (posterior_mean - max(y)) / pointwise_sd     ei = pointwise_sd * (z * norm.cdf(z) + norm.pdf(z))          all_noiseless_eis.append(ei)  all_noiseless_eis = np.array(all_noiseless_eis)  plt.plot(xplt, all_noiseless_eis.T, alpha=.1, color='#96CA4F', linewidth=lw) plt.ylabel('expected improvement draws', color='#96CA4F') ax2 = plt.gca().twinx() ax2.plot(xplt, np.mean(all_noiseless_eis, axis=0), color='#A23D97', linewidth=lw) ax2.set_ylabel('noisy expected improvement', color='#A23D97'); <p>Even the EI integral, which does have a closed form, might better be considered in a QMC fashion because of interesting use cases.  We are going to reconsider the same problem from above, but here we am not looking to maximize the function -- we want to find the \"level set\" associated with the value $y=1$.  Below you can see how the different outcome might look.</p> <p>In this case, the quantity of relevance is not exactly an integral, but it is a function of this posterior mean and standard deviation, which might need to be estimated through an integral (rather than the closed form, which we do have for a GP situation).</p> In\u00a0[6]: Copied! <pre>fig, axes = plt.subplots(1, 3, figsize=(14, 4))\n\nax = axes[0]\nax.plot(xplt, yplt, linewidth=lw)\nax.plot(x, y, 'o', markersize=ms, color='orange')\nax.errorbar(x, y, yerr=2 * np.sqrt(v), marker='', linestyle='', color='orange', linewidth=3)\nax.set_title('Sample data with noise')\nax.set_ylim(-2.4, 2.4)\n\nax = axes[1]\nax.plot(xplt, posterior_draws, alpha=.1, color='r')\nax.plot(xplt, posterior_mean, color='k', linewidth=lw)\nax.set_title('Posterior draws')\nax.set_ylim(-2.4, 2.4)\n\nax = axes[2]\nposterior_mean_distance_from_1 = np.mean(np.abs(posterior_draws - 1), axis=1)\nposterior_standard_deviation = np.std(posterior_draws, axis=1)\nlevel_set_expected_improvement = norm.cdf(-posterior_mean_distance_from_1 / posterior_standard_deviation)\nax.plot(xplt, level_set_expected_improvement, color='#A23D97', linewidth=lw)\nax.set_title('level set expected improvement')\n\nplt.tight_layout();\n</pre> fig, axes = plt.subplots(1, 3, figsize=(14, 4))  ax = axes[0] ax.plot(xplt, yplt, linewidth=lw) ax.plot(x, y, 'o', markersize=ms, color='orange') ax.errorbar(x, y, yerr=2 * np.sqrt(v), marker='', linestyle='', color='orange', linewidth=3) ax.set_title('Sample data with noise') ax.set_ylim(-2.4, 2.4)  ax = axes[1] ax.plot(xplt, posterior_draws, alpha=.1, color='r') ax.plot(xplt, posterior_mean, color='k', linewidth=lw) ax.set_title('Posterior draws') ax.set_ylim(-2.4, 2.4)  ax = axes[2] posterior_mean_distance_from_1 = np.mean(np.abs(posterior_draws - 1), axis=1) posterior_standard_deviation = np.std(posterior_draws, axis=1) level_set_expected_improvement = norm.cdf(-posterior_mean_distance_from_1 / posterior_standard_deviation) ax.plot(xplt, level_set_expected_improvement, color='#A23D97', linewidth=lw) ax.set_title('level set expected improvement')  plt.tight_layout(); In\u00a0[7]: Copied! <pre>q = 5  # number of \"next points\" to be considered simultaneously\nnext_x = np.array([0.158,  0.416,  0.718,  0.935,  0.465])\n\ndef compute_qei(next_x, mc_strat, num_posterior_draws):\n    q = len(next_x)\n    \n    kernel_prior_data = gaussian_kernel(x, x, shape_parameter, process_variance)\n    kernel_cross_matrix = gaussian_kernel(next_x, x, shape_parameter, process_variance)\n    kernel_prior_plot = gaussian_kernel(next_x, next_x, shape_parameter, process_variance)\n    prior_cholesky = np.linalg.cholesky(kernel_prior_data + np.diag(v))\n    \n    partial_cardinal_functions = solve_triangular(prior_cholesky, kernel_cross_matrix.T, lower=True)\n    posterior_covariance = kernel_prior_plot - np.dot(partial_cardinal_functions.T, partial_cardinal_functions)\n    posterior_cholesky = np.linalg.cholesky(posterior_covariance + fudge_factor * np.eye(q))\n    \n    full_cardinal_functions = solve_triangular(prior_cholesky.T, partial_cardinal_functions, lower=False)\n    posterior_mean = np.dot(full_cardinal_functions.T, y)\n        \n    if mc_strat == 'numpy':\n        normal_draws = np.random.normal(size=(num_posterior_draws, q))\n    elif mc_strat == 'lattice':\n        g =  qp.Gaussian(qp.Lattice(dimension=q, randomize=True))\n        normal_draws = g.gen_samples(n=num_posterior_draws)\n    else:\n        g =  qp.Gaussian(qp.IIDStdUniform(dimension=q))\n        normal_draws = g.gen_samples(n = num_posterior_draws)\n    posterior_draws = posterior_mean[:, None] + np.dot(posterior_cholesky, normal_draws.T)\n    \n    return np.mean(np.fmax(np.max(posterior_draws[:, :num_posterior_draws] - max(y), axis=0), 0))\n</pre> q = 5  # number of \"next points\" to be considered simultaneously next_x = np.array([0.158,  0.416,  0.718,  0.935,  0.465])  def compute_qei(next_x, mc_strat, num_posterior_draws):     q = len(next_x)          kernel_prior_data = gaussian_kernel(x, x, shape_parameter, process_variance)     kernel_cross_matrix = gaussian_kernel(next_x, x, shape_parameter, process_variance)     kernel_prior_plot = gaussian_kernel(next_x, next_x, shape_parameter, process_variance)     prior_cholesky = np.linalg.cholesky(kernel_prior_data + np.diag(v))          partial_cardinal_functions = solve_triangular(prior_cholesky, kernel_cross_matrix.T, lower=True)     posterior_covariance = kernel_prior_plot - np.dot(partial_cardinal_functions.T, partial_cardinal_functions)     posterior_cholesky = np.linalg.cholesky(posterior_covariance + fudge_factor * np.eye(q))          full_cardinal_functions = solve_triangular(prior_cholesky.T, partial_cardinal_functions, lower=False)     posterior_mean = np.dot(full_cardinal_functions.T, y)              if mc_strat == 'numpy':         normal_draws = np.random.normal(size=(num_posterior_draws, q))     elif mc_strat == 'lattice':         g =  qp.Gaussian(qp.Lattice(dimension=q, randomize=True))         normal_draws = g.gen_samples(n=num_posterior_draws)     else:         g =  qp.Gaussian(qp.IIDStdUniform(dimension=q))         normal_draws = g.gen_samples(n = num_posterior_draws)     posterior_draws = posterior_mean[:, None] + np.dot(posterior_cholesky, normal_draws.T)          return np.mean(np.fmax(np.max(posterior_draws[:, :num_posterior_draws] - max(y), axis=0), 0)) In\u00a0[8]: Copied! <pre>num_posterior_draws_to_test = 2 ** np.arange(4, 17)\ntrials = 10\n\nvals = {}\nfor mc_strat in ('numpy', 'iid', 'lattice'):\n    vals[mc_strat] = []\n\n    for num_posterior_draws in num_posterior_draws_to_test:\n        qei_estimate = 0.\n        for trial in range(trials):\n            qei_estimate += compute_qei(next_x, mc_strat, num_posterior_draws)\n        avg_qei_estimate = qei_estimate/float(trials)\n        vals[mc_strat].append(avg_qei_estimate)\n\n    vals[mc_strat] = np.array(vals[mc_strat])\n#reference_answer = compute_qei(next_x, 'lattice', 2 ** 7 * max(num_posterior_draws_to_test))\nreference_answer = compute_qei(next_x, 'lattice', 2 ** 20)\n</pre> num_posterior_draws_to_test = 2 ** np.arange(4, 17) trials = 10  vals = {} for mc_strat in ('numpy', 'iid', 'lattice'):     vals[mc_strat] = []      for num_posterior_draws in num_posterior_draws_to_test:         qei_estimate = 0.         for trial in range(trials):             qei_estimate += compute_qei(next_x, mc_strat, num_posterior_draws)         avg_qei_estimate = qei_estimate/float(trials)         vals[mc_strat].append(avg_qei_estimate)      vals[mc_strat] = np.array(vals[mc_strat]) #reference_answer = compute_qei(next_x, 'lattice', 2 ** 7 * max(num_posterior_draws_to_test)) reference_answer = compute_qei(next_x, 'lattice', 2 ** 20) In\u00a0[9]: Copied! <pre>for name, results in vals.items():\n    plt.loglog(num_posterior_draws_to_test, abs(results - reference_answer), label=name)\nplt.loglog(num_posterior_draws_to_test, .05 * num_posterior_draws_to_test ** -.5, '--k', label='$O(N^{-1/2})$')\nplt.loglog(num_posterior_draws_to_test, .3 * num_posterior_draws_to_test ** -1.0, '-.k', label='$O(N^{-1})$')\nplt.xlabel('N - number of points')\nplt.ylabel('Accuracy')\nplt.legend(loc='lower left');\n</pre> for name, results in vals.items():     plt.loglog(num_posterior_draws_to_test, abs(results - reference_answer), label=name) plt.loglog(num_posterior_draws_to_test, .05 * num_posterior_draws_to_test ** -.5, '--k', label='$O(N^{-1/2})$') plt.loglog(num_posterior_draws_to_test, .3 * num_posterior_draws_to_test ** -1.0, '-.k', label='$O(N^{-1})$') plt.xlabel('N - number of points') plt.ylabel('Accuracy') plt.legend(loc='lower left'); <p>This is very similar to what the FB paper talked about, and we think exactly the kind of thing we should be emphasizing in our discussions in a potential blog post which talks about BO applications of QMC.</p> <p>Such a blog post is something that we would be happy to write up, by the way.</p> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"demos/nei_demo/#nei-noisy-expected-improvement-demo","title":"NEI (Noisy Expected Improvement) Demo\u00b6","text":"<p>You can also look at the Botorch implementation, but that requires a lot more understanding of code which involves Pytorch.  So we tried to put a simple example together here.</p>"},{"location":"demos/nei_demo/#goal","title":"Goal\u00b6","text":"<p>What would be really great would be if we could compute integrals like the EI integral or the NEI integral using QMC. If there are opportunities to use the latest research to adaptively study tolerance and truncate, that would be absolutely amazing.</p> <p>We put the NEI example up first because the FB crew has already done a great job showing how QMC can play a role.  But, as you can see, NEI is more complicated than EI, and also not yet as popular in the community (though that may change).</p>"},{"location":"demos/nei_demo/#bonus-stuff","title":"Bonus stuff\u00b6","text":""},{"location":"demos/nei_demo/#computation-of-the-qei-quantity-using-qmcpy","title":"Computation of the QEI quantity using <code>qmcpy</code>\u00b6","text":"<p>NEI is an important quantity, but there are other quantities as well which could be considered relevant demonstrations of higher dimensional integrals.</p> <p>One such quantity is a computation involving $q$ \"next points\" to sample in a BO process; in the standard formulation this quantity might involve just $q=1$, but $q&gt;1$ is also of interest for batched evaluation in parallel.</p> <p>This quantity is defined as</p> <p>$$ \\mathrm{EI}_q(x_1, \\ldots, x_q;\\mathbf{y}, \\mathcal{X}, \\boldsymbol{\\epsilon}) = \\int_{\\mathbb{R}^q} \\max_{1\\leq i\\leq q}\\left[{(y_i - y^*)_+}\\right]\\, p_{Y_{x_1,\\ldots, x_q}|\\mathbf{y}, \\mathcal{X}, \\boldsymbol{\\epsilon}}(y_1, \\ldots, y_q)\\; \\text{d}y_1\\cdots\\text{d}y_q $$</p> <p>The example we are considering here is with $q=5$ but this quantity could be made larger.  Each of these QEI computations (done in a vectorized fashion in production) would be needed in an optimization loop (likely powered by CMAES or some other high dimensional non-convex optimization tool).  This optimization problem would take place in a $qd$ dimensional space, which is one aspect which usually prevents $q$ from being too large.</p> <p>Note that some of this will look much more confusing in $d&gt;1$, but it is written here in a simplified version.</p>"},{"location":"demos/plot_proj_function/","title":"Plotting Points Automatically","text":"<p>This notebook demonstrates the different usages of the plot_proj function for Discrete Distribution and True Measure</p> <p>A Discrete Distribution or True Measure object with d dimensions has a maximum of $d\\times(d-1)$ dimensional pairings (for e.g: [2,3] and [3,2] are being considered seperate parings). The plot_proj function plots all or a subset of all the possible dimension pairings using the parameters d_vertical and d_horizontal and can also display extensibility based on the parameter n. Extensibility occurs when we want to plot the Discrete Distribution or True Measure Object with successively increasing numbers of points. Each set of points is displayed in a different color. This extensibility helps us see how the space fills up.</p> In\u00a0[1]: Copied! <pre>import qmcpy as qp\n</pre> import qmcpy as qp In\u00a0[2]: Copied! <pre>help(qp.plot_proj)\n</pre> help(qp.plot_proj) <pre>Help on function plot_proj in module qmcpy.util.plot_functions:\n\nplot_proj(sampler, n=64, d_horizontal=1, d_vertical=2, math_ind=True, marker_size=5, figfac=5, fig_title='Projection of Samples', axis_pad=0, want_grid=True, font_family='sans-serif', where_title=1, **kwargs)\n    Args:\n        sampler (DiscreteDistribution,TrueMeasure): The generator of samples to be plotted.\n        n (Union[int,list]): The number of samples or a list of samples(used for extensibility) to be plotted.\n        d_horizontal (Union[int,list]): The dimension or list of dimensions to be plotted on the horizontal axes.\n        d_vertical (Union[int,list]): The dimension or list of dimensions to be plotted on the vertical axes.\n        math_ind (bool): Setting to `True` will enable user to pass in math indices.\n        marker_size (float): The marker size (typographic points are 1/72 in.).\n        figfac (float): The figure size factor.\n        fig_title (str): The title of the figure.\n        axis_pad (float): The padding of the axis so that points on the boundaries can be seen.\n        want_grid (bool): Setting to `True` will enable grid on the plot.\n        font_family (str): The font family of the plot.\n        where_title (float): the position of the title on the plot. Default value is 1.\n        **kwargs (dict): Additional keyword arguments passed to `matplotlib.pyplot.scatter`.\n\n</pre> <p>Here we show a two dimensional projection of an IID Object</p> <p>We have given a more descriptive title.</p> In\u00a0[3]: Copied! <pre>d = 2\niid = qp.IIDStdUniform(d)\nfig,ax = qp.plot_proj(iid, n = 2**7, fig_title =\"IID samples\")\n</pre> d = 2 iid = qp.IIDStdUniform(d) fig,ax = qp.plot_proj(iid, n = 2**7, fig_title =\"IID samples\") <p>Here we show a two dimensional projection of an LD Halton object with successively increasing numbers of points.  The initial points are in blue. The next additional points are in orange. The final additional points are in green :</p> <p>If we want to adjust the title more tightly, we can with the where_title parameter</p> In\u00a0[4]: Copied! <pre>d = 4\nhalton = qp.Halton(d)\nfig,ax = qp.plot_proj(halton, n = [2**5, 2**6, 2**7],fig_title =\"Halton samples\",where_title = 0.95)\n</pre> d = 4 halton = qp.Halton(d) fig,ax = qp.plot_proj(halton, n = [2**5, 2**6, 2**7],fig_title =\"Halton samples\",where_title = 0.95) <p>Here we show a four dimensional projection of a LD Digital Net Object : We need to adjust the placement of the title.</p> <p>We also turned off the grid</p> In\u00a0[5]: Copied! <pre>d = 4\nnet = qp.DigitalNetB2(d)\nfig, ax = qp.plot_proj(net, n = 2**7, d_horizontal = range(d), d_vertical = range(d), math_ind = False, marker_size = 15, want_grid = False)\n</pre> d = 4 net = qp.DigitalNetB2(d) fig, ax = qp.plot_proj(net, n = 2**7, d_horizontal = range(d), d_vertical = range(d), math_ind = False, marker_size = 15, want_grid = False) <p>Here we show certain specified dimensional projections (dimensions 1 and 3 on the x axis, dimensions 2 and 4 on the y axis) of a LD Digital Net Object:</p> In\u00a0[6]: Copied! <pre>d = 4\nnet = qp.DigitalNetB2(d)\nfig, ax = qp.plot_proj(net, n = 2**7, d_horizontal = [1,3], d_vertical = [2,4])\n</pre> d = 4 net = qp.DigitalNetB2(d) fig, ax = qp.plot_proj(net, n = 2**7, d_horizontal = [1,3], d_vertical = [2,4]) <p>Here we show a five dimensional projection of a LD Lattice object with successively increasing numbers of points.  The initial points are in blue. The next additional points are in orange. The final additional points are in green :</p> <p>So that points near the boundary can be seen, we add some padding</p> In\u00a0[7]: Copied! <pre>d = 5\nlattice = qp.Lattice(d,generating_vector=\"https://raw.githubusercontent.com/QMCSoftware/LDData/refs/heads/main/lattice/kuo.lattice-33002-1024-1048576.9125.txt\")\nfig, ax = qp.plot_proj(lattice, n = [2**6, 2**7, 2**8], d_horizontal = range(d), d_vertical = range(d), math_ind = False, marker_size = 15, axis_pad = 0.05)\n</pre> d = 5 lattice = qp.Lattice(d,generating_vector=\"https://raw.githubusercontent.com/QMCSoftware/LDData/refs/heads/main/lattice/kuo.lattice-33002-1024-1048576.9125.txt\") fig, ax = qp.plot_proj(lattice, n = [2**6, 2**7, 2**8], d_horizontal = range(d), d_vertical = range(d), math_ind = False, marker_size = 15, axis_pad = 0.05) <p>Here we show a two dimensional projection of a Gaussian Object and how the axes returned by the plot_proj function can be manipulated by adding a horizontal and vertical line to denote the x and y axis respectively:</p> In\u00a0[8]: Copied! <pre>d = 2\niid = qp.IIDStdUniform(d)\niid_gaussian = qp.Gaussian(iid,mean =[2,4],covariance=[[9,4],[4,5]])\nfig,ax = qp.plot_proj(iid_gaussian, n = 2**7)\nax[0,0].axvline(x=0,color= 'k',alpha=.25); #adding vertical line\nax[0,0].axhline(y=0,color= 'k',alpha=.25); #adding horizontal line\n</pre> d = 2 iid = qp.IIDStdUniform(d) iid_gaussian = qp.Gaussian(iid,mean =[2,4],covariance=[[9,4],[4,5]]) fig,ax = qp.plot_proj(iid_gaussian, n = 2**7) ax[0,0].axvline(x=0,color= 'k',alpha=.25); #adding vertical line ax[0,0].axhline(y=0,color= 'k',alpha=.25); #adding horizontal line <p>Here we show a two dimensional projection of a Gaussian Object with successively increasing numbers of points.  The initial points are in blue. The next additional points are in orange. The final additional points are in green :</p> In\u00a0[9]: Copied! <pre>d = 2\niid = qp.IIDStdUniform(d)\niid_gaussian = qp.Gaussian(iid,mean =[2,4],covariance=[[9,4],[4,5]])\nfig,ax = qp.plot_proj(iid_gaussian, n = [2**5,2**6,2**7], fig_title = \"Gaussian Distribution\")\n</pre> d = 2 iid = qp.IIDStdUniform(d) iid_gaussian = qp.Gaussian(iid,mean =[2,4],covariance=[[9,4],[4,5]]) fig,ax = qp.plot_proj(iid_gaussian, n = [2**5,2**6,2**7], fig_title = \"Gaussian Distribution\") <p>Here we show a four dimensional projection of a SciPyWrapper Object:</p> In\u00a0[10]: Copied! <pre>import scipy.stats\nd = 4\nnet = qp.DigitalNetB2(d)\nnet_sci_py_wrapper = qp.SciPyWrapper(net,[scipy.stats.beta(a=1,b=5),scipy.stats.gamma(a=1),scipy.stats.uniform(loc = 1,scale = 2),scipy.stats.norm(loc = 3, scale = 4)])\nfig, ax = qp.plot_proj(net_sci_py_wrapper, n = 2**7, d_horizontal = range(d), d_vertical = range(d), math_ind= False, marker_size = 15)\n</pre> import scipy.stats d = 4 net = qp.DigitalNetB2(d) net_sci_py_wrapper = qp.SciPyWrapper(net,[scipy.stats.beta(a=1,b=5),scipy.stats.gamma(a=1),scipy.stats.uniform(loc = 1,scale = 2),scipy.stats.norm(loc = 3, scale = 4)]) fig, ax = qp.plot_proj(net_sci_py_wrapper, n = 2**7, d_horizontal = range(d), d_vertical = range(d), math_ind= False, marker_size = 15) <p>Here we show certain specified dimensional projections (dimensions 1 and 3 on the x axis, dimensions 2 and 4 on the y axis) of a SciPyWrapper Object:</p> In\u00a0[11]: Copied! <pre>import scipy.stats\nd = 4\nnet = qp.DigitalNetB2(d)\nnet_sci_py_wrapper = qp.SciPyWrapper(net,[scipy.stats.beta(a=1,b=5),scipy.stats.gamma(a=1),scipy.stats.uniform(loc = 1,scale = 2),scipy.stats.norm(loc = 3, scale = 4)])\nfig, ax = qp.plot_proj(net_sci_py_wrapper, n = 2**7, d_horizontal = [1,3], d_vertical = [2,4])\n</pre> import scipy.stats d = 4 net = qp.DigitalNetB2(d) net_sci_py_wrapper = qp.SciPyWrapper(net,[scipy.stats.beta(a=1,b=5),scipy.stats.gamma(a=1),scipy.stats.uniform(loc = 1,scale = 2),scipy.stats.norm(loc = 3, scale = 4)]) fig, ax = qp.plot_proj(net_sci_py_wrapper, n = 2**7, d_horizontal = [1,3], d_vertical = [2,4]) <p>Here we show a four dimensional projection of a Uniform object with successively increasing numbers of points.  The initial points are in blue. The next additional points are in orange. The final additional points are in green :</p> In\u00a0[12]: Copied! <pre>d = 4\nhalton = qp.Halton(d)\nhalton_uniform = qp.Uniform(halton,lower_bound=[1,2,3,4],upper_bound=[5,7,9,11])\nfig, ax = qp.plot_proj(halton_uniform, n = [2**6, 2**7, 2**8], d_horizontal = range(d), d_vertical = range(d), math_ind = False, marker_size = 15)\n</pre> d = 4 halton = qp.Halton(d) halton_uniform = qp.Uniform(halton,lower_bound=[1,2,3,4],upper_bound=[5,7,9,11]) fig, ax = qp.plot_proj(halton_uniform, n = [2**6, 2**7, 2**8], d_horizontal = range(d), d_vertical = range(d), math_ind = False, marker_size = 15)"},{"location":"demos/plot_proj_function/#the-qmcpy-plot-projection-function-for-discrete-distribution-and-true-measure","title":"The QMCPY Plot Projection Function for Discrete Distribution and True Measure\u00b6","text":""},{"location":"demos/plot_proj_function/#here-we-set-up-the-qmcpy-environment-enabling-us-to-utilize-this-function","title":"Here we set up the QMCPY environment enabling us to utilize this function:\u00b6","text":""},{"location":"demos/plot_proj_function/#here-we-explain-the-parameters-of-the-plot_proj-function","title":"Here we explain the parameters of the plot_proj function:\u00b6","text":""},{"location":"demos/plot_proj_function/#the-following-examples-show-plotting-of-different-discrete-distribution-objects","title":"The following examples show plotting of different Discrete Distribution Objects:\u00b6","text":""},{"location":"demos/plot_proj_function/#the-following-examples-show-plotting-of-different-true-measure-objects","title":"The following examples show plotting of different True Measure Objects:\u00b6","text":""},{"location":"demos/pricing_options/","title":"Pricing Options","text":"In\u00a0[36]: Copied! <pre># Import necessary packages\nimport qmcpy as qp\nimport numpy as np\nfrom scipy import stats\nimport matplotlib.pyplot as plt\nimport time\n</pre> # Import necessary packages import qmcpy as qp import numpy as np from scipy import stats import matplotlib.pyplot as plt import time In\u00a0[37]: Copied! <pre>initPrice = 120 # initial stock price\ninterest = 0.02 # risk-free interest rate\nvol = 0.5 # volatility\nstrike = 130 # strike price\ntfinal = 1/4 # mature time\nd = 12 # number of observations\nabsTol = 0.05 # absolute tolerance of a nickel\nrelTol = 0 # zero relative tolerance\nsampleSize = 10**6 # number of samples\n</pre> initPrice = 120 # initial stock price interest = 0.02 # risk-free interest rate vol = 0.5 # volatility strike = 130 # strike price tfinal = 1/4 # mature time d = 12 # number of observations absTol = 0.05 # absolute tolerance of a nickel relTol = 0 # zero relative tolerance sampleSize = 10**6 # number of samples  In\u00a0[38]: Copied! <pre>EuroCall = qp.FinancialOption(\n    qp.IIDStdUniform(dimension=d,seed=7),\n    option=\"EUROPEAN\",\n    volatility=vol,\n    start_price=initPrice,\n    strike_price=strike,\n    interest_rate=interest,\n    t_final=tfinal,\n    call_put=\"call\")\ny = EuroCall(sampleSize)\nprint(\"The exact price of this European Call Option is \",f\"{EuroCall.get_exact_value():.4f}\")\nprint(\"After generate \", sampleSize,\"iid points, the price of estimation of the fair price is\",f\"{y.mean():.4f}\")\n</pre> EuroCall = qp.FinancialOption(     qp.IIDStdUniform(dimension=d,seed=7),     option=\"EUROPEAN\",     volatility=vol,     start_price=initPrice,     strike_price=strike,     interest_rate=interest,     t_final=tfinal,     call_put=\"call\") y = EuroCall(sampleSize) print(\"The exact price of this European Call Option is \",f\"{EuroCall.get_exact_value():.4f}\") print(\"After generate \", sampleSize,\"iid points, the price of estimation of the fair price is\",f\"{y.mean():.4f}\") <pre>The exact price of this European Call Option is  8.2779\nAfter generate  1000000 iid points, the price of estimation of the fair price is 8.2718\n</pre> In\u00a0[39]: Copied! <pre>ArithMeanCall = qp.FinancialOption(\n    qp.IIDStdUniform(dimension=d,seed=7),\n    option=\"ASIAN\",\n    volatility=vol,\n    start_price=initPrice,\n    strike_price=strike,\n    interest_rate=interest,\n    t_final=tfinal,\n    call_put=\"call\",\n    asian_mean = 'arithmetic')\ny = ArithMeanCall(sampleSize)\nprint(\"After generate \", sampleSize,\"iid points, the price of this Arithmetic Mean Call Option is\",f\"{y.mean():.4f}\")\n</pre> ArithMeanCall = qp.FinancialOption(     qp.IIDStdUniform(dimension=d,seed=7),     option=\"ASIAN\",     volatility=vol,     start_price=initPrice,     strike_price=strike,     interest_rate=interest,     t_final=tfinal,     call_put=\"call\",     asian_mean = 'arithmetic') y = ArithMeanCall(sampleSize) print(\"After generate \", sampleSize,\"iid points, the price of this Arithmetic Mean Call Option is\",f\"{y.mean():.4f}\") <pre>After generate  1000000 iid points, the price of this Arithmetic Mean Call Option is 3.3856\n</pre> <p>The price of the Asian arithmetic mean call option is smaller than the price of the European call option.</p> <p>We may also price the Asian arithmetic mean put option as follows:</p> In\u00a0[40]: Copied! <pre>ArithMeanPut = qp.FinancialOption(\n    qp.IIDStdUniform(dimension=d,seed=7),\n    option=\"ASIAN\",\n    volatility=vol,\n    start_price=initPrice,\n    strike_price=strike,\n    interest_rate=interest,\n    t_final=tfinal,\n    call_put=\"put\",\n    asian_mean = 'arithmetic')\ny = ArithMeanPut(sampleSize)\nprint(\"After generate \", sampleSize,\"iid points, the price of this Arithmetic Mean Put Option is \",f\"{y.mean():.4f}\")\n</pre> ArithMeanPut = qp.FinancialOption(     qp.IIDStdUniform(dimension=d,seed=7),     option=\"ASIAN\",     volatility=vol,     start_price=initPrice,     strike_price=strike,     interest_rate=interest,     t_final=tfinal,     call_put=\"put\",     asian_mean = 'arithmetic') y = ArithMeanPut(sampleSize) print(\"After generate \", sampleSize,\"iid points, the price of this Arithmetic Mean Put Option is \",f\"{y.mean():.4f}\") <pre>After generate  1000000 iid points, the price of this Arithmetic Mean Put Option is  13.0233\n</pre> <p>Note that the price is greater.  This is because one strike price is above the initial price, making the expected payoff greater.</p> <p>In the limit of continuous monitoring $d \\to \\infty$, the payoff is</p> <p>$$ \\begin{array}{rcc} &amp; \\textbf{call} &amp; \\textbf{put} \\\\ \\hline \\textbf{payoff} &amp;  \\displaystyle \\max\\biggl(\\frac 1T \\int_{0}^T S(t) \\, {\\rm d} t - K,0 \\biggr)\\mathsf{e}^{-rT} &amp;  \\displaystyle \\max\\biggl(K - \\frac 1T \\int_{0}^T S(t) \\, {\\rm d} t,0 \\biggr)\\mathsf{e}^{-rT}  \\end{array}  $$</p> <p>Such an option can be approximated by taking smaller time steps:</p> In\u00a0[41]: Copied! <pre>ArithMeanPut = qp.FinancialOption(\n    qp.IIDStdUniform(dimension=62,seed=7), # weekly monitoring for 3 months\n    option=\"ASIAN\",\n    volatility=vol,\n    start_price=initPrice,\n    strike_price=strike,\n    interest_rate=interest,\n    t_final=tfinal,\n    call_put=\"call\",\n    asian_mean = 'arithmetic')\ny = ArithMeanCall(sampleSize)\nprint(\"After generate \", sampleSize,\"iid points, the price of this Arithmetic Mean Call Option is\",f\"{y.mean():.4f}\")\n</pre> ArithMeanPut = qp.FinancialOption(     qp.IIDStdUniform(dimension=62,seed=7), # weekly monitoring for 3 months     option=\"ASIAN\",     volatility=vol,     start_price=initPrice,     strike_price=strike,     interest_rate=interest,     t_final=tfinal,     call_put=\"call\",     asian_mean = 'arithmetic') y = ArithMeanCall(sampleSize) print(\"After generate \", sampleSize,\"iid points, the price of this Arithmetic Mean Call Option is\",f\"{y.mean():.4f}\") <pre>After generate  1000000 iid points, the price of this Arithmetic Mean Call Option is 3.3858\n</pre> <p>The price is a bit lower, and the time is longer because more time steps are needed, which means more random variables are needed.</p> In\u00a0[42]: Copied! <pre>GeoMeanPut = qp.FinancialOption(\n    qp.IIDStdUniform(dimension=12,seed=7), #Weekly Monitoring for three months\n    option=\"ASIAN\",\n    volatility=vol,\n    start_price=initPrice,\n    strike_price=strike,\n    interest_rate=interest,\n    t_final=tfinal,\n    call_put=\"put\",\n    asian_mean='geometric',\n    asian_mean_quadrature_rule=\"RIGHT\")\ny = GeoMeanPut(sampleSize)\nprint(\"The exact price of this Geometric Asian Put Option is \",f\"{GeoMeanPut.get_exact_value():.4f}\")\nprint(\"After generate \", sampleSize,\"iid points, the price of this Geometric Mean Put Option is \",f\"{y.mean():.4f}\")\n</pre> GeoMeanPut = qp.FinancialOption(     qp.IIDStdUniform(dimension=12,seed=7), #Weekly Monitoring for three months     option=\"ASIAN\",     volatility=vol,     start_price=initPrice,     strike_price=strike,     interest_rate=interest,     t_final=tfinal,     call_put=\"put\",     asian_mean='geometric',     asian_mean_quadrature_rule=\"RIGHT\") y = GeoMeanPut(sampleSize) print(\"The exact price of this Geometric Asian Put Option is \",f\"{GeoMeanPut.get_exact_value():.4f}\") print(\"After generate \", sampleSize,\"iid points, the price of this Geometric Mean Put Option is \",f\"{y.mean():.4f}\") <pre>The exact price of this Geometric Asian Put Option is  13.7841\nAfter generate  1000000 iid points, the price of this Geometric Mean Put Option is  13.7663\n</pre> In\u00a0[43]: Copied! <pre>GeoMeanCall = qp.FinancialOption(\n    qp.IIDStdUniform(dimension=12,seed=7), #Weekly Monitoring for three months\n    option=\"ASIAN\",\n    volatility=vol,\n    start_price=initPrice,\n    strike_price=strike,\n    interest_rate=interest,\n    t_final=tfinal,\n    call_put=\"call\",\n    asian_mean='geometric',\n    asian_mean_quadrature_rule=\"RIGHT\")\ny = GeoMeanCall(sampleSize)\nprint(\"The exact price of this Geometric Asian Put Option is \",f\"{GeoMeanCall.get_exact_value():.4f}\")\nprint(\"After generate \", sampleSize,\"iid points, the price of this Geometric Mean Call Option is \",f\"{y.mean():.4f}\")\n</pre> GeoMeanCall = qp.FinancialOption(     qp.IIDStdUniform(dimension=12,seed=7), #Weekly Monitoring for three months     option=\"ASIAN\",     volatility=vol,     start_price=initPrice,     strike_price=strike,     interest_rate=interest,     t_final=tfinal,     call_put=\"call\",     asian_mean='geometric',     asian_mean_quadrature_rule=\"RIGHT\") y = GeoMeanCall(sampleSize) print(\"The exact price of this Geometric Asian Put Option is \",f\"{GeoMeanCall.get_exact_value():.4f}\") print(\"After generate \", sampleSize,\"iid points, the price of this Geometric Mean Call Option is \",f\"{y.mean():.4f}\") <pre>The exact price of this Geometric Asian Put Option is  3.5401\nAfter generate  1000000 iid points, the price of this Geometric Mean Call Option is  3.5359\n</pre> In\u00a0[44]: Copied! <pre>BarrierUpInCall = qp.FinancialOption(\n    qp.IIDStdUniform(dimension=12,seed=7), #Weekly Monitoring for three months\n    option=\"BARRIER\",\n    volatility=vol,\n    start_price=initPrice,\n    strike_price=strike,\n    interest_rate=interest,\n    t_final=tfinal,\n    call_put=\"call\",\n    barrier_in_out=\"in\",\n    barrier_price=150)\ny = BarrierUpInCall(sampleSize)\nprint(\"After generate \", sampleSize,\"iid points, the price of this Barrier UpIn Call Option is \",f\"{y.mean():.4f}\")\n</pre> BarrierUpInCall = qp.FinancialOption(     qp.IIDStdUniform(dimension=12,seed=7), #Weekly Monitoring for three months     option=\"BARRIER\",     volatility=vol,     start_price=initPrice,     strike_price=strike,     interest_rate=interest,     t_final=tfinal,     call_put=\"call\",     barrier_in_out=\"in\",     barrier_price=150) y = BarrierUpInCall(sampleSize) print(\"After generate \", sampleSize,\"iid points, the price of this Barrier UpIn Call Option is \",f\"{y.mean():.4f}\") <pre>After generate  1000000 iid points, the price of this Barrier UpIn Call Option is  7.4022\n</pre> <p>Note that this price is less than the European call option because the asset price must cross the barrier for the option to become active.</p> In\u00a0[\u00a0]: Copied! <pre>LookCall = qp.FinancialOption(\n    qp.IIDStdUniform(dimension=12,seed=7), #Weekly Monitoring for three months\n    option=\"LOOKBACK\",\n    volatility=vol,\n    start_price=initPrice,\n    strike_price=strike,\n    interest_rate=interest,\n    t_final=tfinal,\n    call_put=\"call\")\ny = LookCall(sampleSize)\nprint(\"After generate \", sampleSize,\"iid points, the price of this Lookback Call Option is \",f\"{y.mean():.4f}\")\n</pre> LookCall = qp.FinancialOption(     qp.IIDStdUniform(dimension=12,seed=7), #Weekly Monitoring for three months     option=\"LOOKBACK\",     volatility=vol,     start_price=initPrice,     strike_price=strike,     interest_rate=interest,     t_final=tfinal,     call_put=\"call\") y = LookCall(sampleSize) print(\"After generate \", sampleSize,\"iid points, the price of this Lookback Call Option is \",f\"{y.mean():.4f}\") <pre>After generate  1000000 iid points, the price of this Lookback Call Option is  17.7080\n</pre>"},{"location":"demos/pricing_options/#pricing-asian-style-options","title":"Pricing Asian Style Options\u00b6","text":"<p>In this script we show how to use classes in QMCPy for Monte Carlo option pricing of options with Asian style payoffs and European exercise.</p> <ul> <li>The payoff depends on the whole asset price path, not only on the terminal asset price.</li> <li>The option is only exercised at expiry, unlike American options, which can be exercised at any time before expiry.</li> </ul>"},{"location":"demos/pricing_options/#european-options","title":"European Options\u00b6","text":""},{"location":"demos/pricing_options/#arithmetic-mean-options","title":"Arithmetic Mean Options\u00b6","text":"<p>The payoff of the arithmetic mean option depends on the average of the stock price, not the final stock price.  Here are the discounted payoffs:</p> <p>$$\\begin{array}{rcc}  &amp; \\textbf{call} &amp; \\textbf{put} \\\\ \\hline \\textbf{payoff} &amp;  \\displaystyle \\max\\biggl(\\frac 1d \\sum_{j=1}^d S(jT/d) - K,0 \\biggr)\\mathsf{e}^{-rT} &amp;  \\displaystyle \\max\\biggl(K - \\frac 1d \\sum_{j=1}^d S(jT/d),0 \\biggr)\\mathsf{e}^{-rT}  \\end{array} $$</p>"},{"location":"demos/pricing_options/#geometric-mean-options","title":"Geometric Mean Options\u00b6","text":"<p>One can also base the payoff on a geometric mean rather than an arithmetic mean.  Such options have a closed form solution.</p> <p>The price of a geometric mean $ \\begin{Bmatrix}  \\text{call} \\\\  \\text{put} \\end{Bmatrix}$ option is $\\begin{Bmatrix} \\le \\\\ \\ge \\end{Bmatrix}$ the price of an arithmetic mean $\\begin{Bmatrix} \\text{call} \\\\\\text{put} \\end{Bmatrix}$ option because a geometric mean is smaller than an arithmetic mean.</p>"},{"location":"demos/pricing_options/#barrier-option","title":"Barrier Option\u00b6","text":"<p>In barrier options the payoff only occurs if the asset price crosses or fails to cross a barrier, $b$</p> <p>$$ \\begin{array}{rcc}  &amp; \\textbf{up} (S(0) &lt; b) &amp; \\textbf{down} (S(0) &gt; b) \\\\ \\hline  \\textbf{in} &amp; \\text{active if } S(t) \\ge b &amp; \\text{active if } S(t) \\le  b \\\\  \\textbf{out} &amp; \\text{inactive if } S(t) \\ge b &amp; \\text{inactive if } S(t) \\le  b   \\end{array} $$</p> <p>For the barrier option with a European call type payoff, this corresponds to</p> <p>$$  \\begin{array}{rcc}  \\textbf{payoff} &amp; \\textbf{up} (S(0) &lt; b) &amp; \\textbf{down} (S(0) &gt; b) \\\\ \\hline  \\textbf{in} &amp;   1_{[b,\\infty)}(\\max_{0 \\le t \\le T} S(t)) \\max(S(T)-K,0)\\mathsf{e}^{-rT} &amp;   1_{[0,b]}(\\min_{0 \\le t \\le T} S(t)) \\max(S(T)-K,0)\\mathsf{e}^{-rT} \\\\  \\textbf{out} &amp; 1_{[0,b)}(\\max_{0 \\le t \\le T} S(t)) \\max(S(T)-K,0)\\mathsf{e}^{-rT} &amp;   1_{[b,\\infty)}(\\min_{0 \\le t \\le T} S(t)) \\max(S(T)-K,0)\\mathsf{e}^{-rT}  \\end{array} $$</p>"},{"location":"demos/pricing_options/#lookback-options","title":"Lookback Options\u00b6","text":"<p>Lookback options do not use a strike price but use the minimum or maximum asset price as their strike.  The discounted payoffs are</p> <p>$$ \\begin{array}{rcc} &amp; \\textbf{call} &amp; \\textbf{put} \\\\ \\hline  \\textbf{payoff} &amp;   \\displaystyle \\Bigl(S(T) - \\min_{0 \\le t \\le T} S(t),0 \\Bigr)\\mathsf{e}^{-rT} &amp;   \\displaystyle \\Bigl(\\max_{0 \\le t \\le T} S(t) - S(T),0 \\Bigr)\\mathsf{e}^{-rT}   \\end{array} $$</p> <p>where the values of $t$ considered for the minimum or maximum are either discrete, $0, T/d, \\dots, T$, or continuous.  Note that we would expect the prices of these options to be greater than their out of the money European counterparts.</p>"},{"location":"demos/qei-demo-for-blog/","title":"qEI","text":"In\u00a0[1]: Copied! <pre>from qmcpy import *\nimport numpy as np\nfrom scipy.linalg import solve_triangular, cho_solve, cho_factor\nfrom scipy.stats import norm\nimport matplotlib.pyplot as pyplot\n%matplotlib inline\n\nlw = 3\nms = 8\n</pre> from qmcpy import * import numpy as np from scipy.linalg import solve_triangular, cho_solve, cho_factor from scipy.stats import norm import matplotlib.pyplot as pyplot %matplotlib inline  lw = 3 ms = 8 In\u00a0[2]: Copied! <pre>def yf(x):\n    return np.cos(10 * x) * np.exp(.2 * x) + np.exp(-5 * (x - .4) ** 2)\n\nxplt = np.linspace(0, 1, 300)\nyplt = yf(xplt)\n\nx = np.array([.1, .2, .4, .7, .9])\ny = yf(x)\nv = np.array([.001, .05, .01, .1, .4])\n\npyplot.plot(xplt, yplt, linewidth=lw)\npyplot.plot(x, y, 'o', markersize=ms, color='orange')\npyplot.errorbar(x, y, yerr=2 * np.sqrt(v), marker='', linestyle='', color='orange', linewidth=lw)\npyplot.title('Sample data with noise');\n</pre> def yf(x):     return np.cos(10 * x) * np.exp(.2 * x) + np.exp(-5 * (x - .4) ** 2)  xplt = np.linspace(0, 1, 300) yplt = yf(xplt)  x = np.array([.1, .2, .4, .7, .9]) y = yf(x) v = np.array([.001, .05, .01, .1, .4])  pyplot.plot(xplt, yplt, linewidth=lw) pyplot.plot(x, y, 'o', markersize=ms, color='orange') pyplot.errorbar(x, y, yerr=2 * np.sqrt(v), marker='', linestyle='', color='orange', linewidth=lw) pyplot.title('Sample data with noise'); In\u00a0[3]: Copied! <pre>shape_parameter = 4.1\nprocess_variance = .9\nfudge_factor = 1e-10\n\ndef gaussian_kernel(x, z):\n    return process_variance * np.exp(-shape_parameter ** 2 * (x[:, None] - z[None, :]) ** 2)\n\ndef gp_posterior_params(x_to_draw):\n    n = len(x_to_draw)\n    \n    kernel_prior_data = gaussian_kernel(x, x)\n    kernel_cross_matrix = gaussian_kernel(x_to_draw, x)\n    kernel_prior_plot = gaussian_kernel(x_to_draw, x_to_draw)\n    prior_cholesky = np.linalg.cholesky(kernel_prior_data + np.diag(v))\n    \n    partial_cardinal_functions = solve_triangular(prior_cholesky, kernel_cross_matrix.T, lower=True)\n    posterior_covariance = kernel_prior_plot - np.dot(partial_cardinal_functions.T, partial_cardinal_functions) + fudge_factor * np.eye(n)\n    \n    full_cardinal_functions = solve_triangular(prior_cholesky.T, partial_cardinal_functions, lower=False)\n    posterior_mean = np.dot(full_cardinal_functions.T, y)\n    return posterior_mean,posterior_covariance\n\ndef gp_posterior_draws(x_to_draw, mc_strat, num_posterior_draws, posterior_mean, posterior_covariance):\n    q = len(x_to_draw)\n    if mc_strat == 'iid':\n        dd = IIDStdUniform(q)\n    elif mc_strat == 'lattice':\n        dd = Lattice(q)\n    elif mc_strat == 'sobol':\n        dd = Sobol(q)\n    g = Gaussian(dd,posterior_mean,posterior_covariance)\n    posterior_draws = g.gen_samples(num_posterior_draws)\n    return posterior_draws\n\ndef compute_qei(posterior_draws):\n    y_gp = np.fmax(np.max(posterior_draws.T - max(y), axis=0), 0)\n    return y_gp\n</pre> shape_parameter = 4.1 process_variance = .9 fudge_factor = 1e-10  def gaussian_kernel(x, z):     return process_variance * np.exp(-shape_parameter ** 2 * (x[:, None] - z[None, :]) ** 2)  def gp_posterior_params(x_to_draw):     n = len(x_to_draw)          kernel_prior_data = gaussian_kernel(x, x)     kernel_cross_matrix = gaussian_kernel(x_to_draw, x)     kernel_prior_plot = gaussian_kernel(x_to_draw, x_to_draw)     prior_cholesky = np.linalg.cholesky(kernel_prior_data + np.diag(v))          partial_cardinal_functions = solve_triangular(prior_cholesky, kernel_cross_matrix.T, lower=True)     posterior_covariance = kernel_prior_plot - np.dot(partial_cardinal_functions.T, partial_cardinal_functions) + fudge_factor * np.eye(n)          full_cardinal_functions = solve_triangular(prior_cholesky.T, partial_cardinal_functions, lower=False)     posterior_mean = np.dot(full_cardinal_functions.T, y)     return posterior_mean,posterior_covariance  def gp_posterior_draws(x_to_draw, mc_strat, num_posterior_draws, posterior_mean, posterior_covariance):     q = len(x_to_draw)     if mc_strat == 'iid':         dd = IIDStdUniform(q)     elif mc_strat == 'lattice':         dd = Lattice(q)     elif mc_strat == 'sobol':         dd = Sobol(q)     g = Gaussian(dd,posterior_mean,posterior_covariance)     posterior_draws = g.gen_samples(num_posterior_draws)     return posterior_draws  def compute_qei(posterior_draws):     y_gp = np.fmax(np.max(posterior_draws.T - max(y), axis=0), 0)     return y_gp In\u00a0[4]: Copied! <pre>num_posterior_draws = 2 ** 7\nNp = (25, 24)\nX, Y = np.meshgrid(np.linspace(0, 1, Np[1]), np.linspace(0, 1, Np[0]))\nxp = np.array([X.reshape(-1), Y.reshape(-1)]).T\nmu_post,sigma_cov = gp_posterior_params(xplt)\ny_draws = gp_posterior_draws(xplt, 'lattice', num_posterior_draws,mu_post,sigma_cov).T\nqei_vals = np.empty(len(xp))\nfor k, next_x in enumerate(xp):\n    mu_post,sigma_cov = gp_posterior_params(next_x)\n    gp_draws = gp_posterior_draws(next_x, 'sobol', num_posterior_draws,mu_post,sigma_cov)\n    qei_vals[k] = compute_qei(gp_draws).mean()\nZ = qei_vals.reshape(Np)\n\nfig, axes = pyplot.subplots(1, 3, figsize=(14, 4))\n\nax = axes[0]\nax.plot(xplt, yplt, linewidth=lw)\nax.plot(x, y, 'o', markersize=ms, color='orange')\nax.errorbar(x, y, yerr=2 * np.sqrt(v), marker='', linestyle='', color='orange', linewidth=lw)\nax.set_title('Sample data with noise')\nax.set_ylim((-2.3, 2.6))\n\nax = axes[1]\nax.plot(xplt, y_draws, linewidth=lw, color='b', alpha=.05)\nax.plot(x, y, 'o', markersize=ms, color='orange')\nax.errorbar(x, y, yerr=2 * np.sqrt(v), marker='', linestyle='', color='orange', linewidth=lw)\nax.set_title(f'{num_posterior_draws} GP posterior draws')\nax.set_ylim((-2.3, 2.6))\n\nax = axes[2]\nh = ax.contourf(X, Y, Z)\nax.set_xlabel('First next point')\nax.set_ylabel('Second next point')\nax.set_title('qEI for q=2 next points')\ncax = fig.colorbar(h, ax=ax)\ncax.set_label('qEI')\n\nfig.tight_layout()\n</pre> num_posterior_draws = 2 ** 7 Np = (25, 24) X, Y = np.meshgrid(np.linspace(0, 1, Np[1]), np.linspace(0, 1, Np[0])) xp = np.array([X.reshape(-1), Y.reshape(-1)]).T mu_post,sigma_cov = gp_posterior_params(xplt) y_draws = gp_posterior_draws(xplt, 'lattice', num_posterior_draws,mu_post,sigma_cov).T qei_vals = np.empty(len(xp)) for k, next_x in enumerate(xp):     mu_post,sigma_cov = gp_posterior_params(next_x)     gp_draws = gp_posterior_draws(next_x, 'sobol', num_posterior_draws,mu_post,sigma_cov)     qei_vals[k] = compute_qei(gp_draws).mean() Z = qei_vals.reshape(Np)  fig, axes = pyplot.subplots(1, 3, figsize=(14, 4))  ax = axes[0] ax.plot(xplt, yplt, linewidth=lw) ax.plot(x, y, 'o', markersize=ms, color='orange') ax.errorbar(x, y, yerr=2 * np.sqrt(v), marker='', linestyle='', color='orange', linewidth=lw) ax.set_title('Sample data with noise') ax.set_ylim((-2.3, 2.6))  ax = axes[1] ax.plot(xplt, y_draws, linewidth=lw, color='b', alpha=.05) ax.plot(x, y, 'o', markersize=ms, color='orange') ax.errorbar(x, y, yerr=2 * np.sqrt(v), marker='', linestyle='', color='orange', linewidth=lw) ax.set_title(f'{num_posterior_draws} GP posterior draws') ax.set_ylim((-2.3, 2.6))  ax = axes[2] h = ax.contourf(X, Y, Z) ax.set_xlabel('First next point') ax.set_ylabel('Second next point') ax.set_title('qEI for q=2 next points') cax = fig.colorbar(h, ax=ax) cax.set_label('qEI')  fig.tight_layout() In\u00a0[5]: Copied! <pre># parameters\nnext_x = np.array([0.158,  0.416,  0.718,  0.935,  0.465])\nnum_posterior_draws_to_test = 2 ** np.arange(4, 20)\nd = len(next_x)\nmu_post,sigma_cov = gp_posterior_params(next_x)\n</pre> # parameters next_x = np.array([0.158,  0.416,  0.718,  0.935,  0.465]) num_posterior_draws_to_test = 2 ** np.arange(4, 20) d = len(next_x) mu_post,sigma_cov = gp_posterior_params(next_x) In\u00a0[6]: Copied! <pre># get reference answer with qmcpy\nintegrand = CustomFun(\n    true_measure = Gaussian(Sobol(d),mu_post,sigma_cov),\n    g = compute_qei)\nstopping_criterion = CubQMCSobolG(integrand, abs_tol=5e-7)\nreference_answer,data = stopping_criterion.integrate()\nprint(data)\n</pre> # get reference answer with qmcpy integrand = CustomFun(     true_measure = Gaussian(Sobol(d),mu_post,sigma_cov),     g = compute_qei) stopping_criterion = CubQMCSobolG(integrand, abs_tol=5e-7) reference_answer,data = stopping_criterion.integrate() print(data) <pre>Data (Data)\n    solution        0.024\n    comb_bound_low  0.024\n    comb_bound_high 0.024\n    comb_bound_diff 8.33e-07\n    comb_flags      1\n    n_total         2^(22)\n    n               2^(22)\n    time_integrate  4.020\nCubQMCNetG (AbstractStoppingCriterion)\n    abs_tol         5.00e-07\n    rel_tol         0\n    n_init          2^(10)\n    n_limit         2^(35)\nCustomFun (AbstractIntegrand)\nGaussian (AbstractTrueMeasure)\n    mean            [ 0.807  0.371  1.204 -0.455  0.67 ]\n    covariance      [[ 1.845e-02 -2.039e-03  1.150e-04  1.219e-04 -5.985e-03]\n                     [-2.039e-03  1.355e-02  6.999e-04 -1.967e-03  2.302e-02]\n                     [ 1.150e-04  6.999e-04  8.871e-02  2.043e-02  5.757e-03]\n                     [ 1.219e-04 -1.967e-03  2.043e-02  2.995e-01 -9.482e-03]\n                     [-5.985e-03  2.302e-02  5.757e-03 -9.482e-03  6.296e-02]]\n    decomp_type     PCA\nDigitalNetB2 (AbstractLDDiscreteDistribution)\n    d               5\n    replications    1\n    randomize       LMS_DS\n    gen_mats_source joe_kuo.6.21201.txt\n    order           NATURAL\n    t               63\n    alpha           1\n    n_limit         2^(32)\n    entropy         1084764658139426821181530528540663833\n</pre> In\u00a0[7]: Copied! <pre># generate data\nnum_posterior_draws_to_test = 2 ** np.arange(4, 20)\nvals = {}\nnum_repeats = 50\nmc_strats = ('iid', 'lattice', 'sobol')\nfor mc_strat in mc_strats:\n    vals[mc_strat] = []\n    for num_posterior_draws in num_posterior_draws_to_test:\n        all_estimates = []\n        for _ in range(num_repeats):\n            y_draws = gp_posterior_draws(next_x, mc_strat, num_posterior_draws,mu_post,sigma_cov)\n            all_estimates.append(compute_qei(y_draws).mean())\n        vals[mc_strat].append(all_estimates)\n    vals[mc_strat] = np.array(vals[mc_strat])\n</pre> # generate data num_posterior_draws_to_test = 2 ** np.arange(4, 20) vals = {} num_repeats = 50 mc_strats = ('iid', 'lattice', 'sobol') for mc_strat in mc_strats:     vals[mc_strat] = []     for num_posterior_draws in num_posterior_draws_to_test:         all_estimates = []         for _ in range(num_repeats):             y_draws = gp_posterior_draws(next_x, mc_strat, num_posterior_draws,mu_post,sigma_cov)             all_estimates.append(compute_qei(y_draws).mean())         vals[mc_strat].append(all_estimates)     vals[mc_strat] = np.array(vals[mc_strat]) In\u00a0[8]: Copied! <pre>fig, ax = pyplot.subplots(1, 1, figsize=(6, 4))\n\ncolors = ('#F5811F', '#A23D97', '#00B253')\nalpha = .3\n\nfor (name, results), color in zip(vals.items(), colors):\n    bot = np.percentile(abs(results - reference_answer), 25, axis=1)\n    med = np.percentile(abs(results - reference_answer), 50, axis=1)\n    top = np.percentile(abs(results - reference_answer), 75, axis=1)\n    ax.loglog(num_posterior_draws_to_test, med, label=name, color=color)\n    ax.fill_between(num_posterior_draws_to_test, bot, top, color=color, alpha=alpha)\nax.loglog(num_posterior_draws_to_test, .1 * num_posterior_draws_to_test ** -.5, '--k', label='$O(N^{-1/2})$')\nax.loglog(num_posterior_draws_to_test, .25 * num_posterior_draws_to_test ** -1.0, '-.k', label='$O(N^{-1})$')\nax.set_xlabel('N - number of points')\nax.set_ylabel('Accuracy')\nax.legend(loc='lower left')\nax.set_title(f'Statistics from {num_repeats} runs');\n\n# plt.savefig('qei_convergence.png');\n</pre> fig, ax = pyplot.subplots(1, 1, figsize=(6, 4))  colors = ('#F5811F', '#A23D97', '#00B253') alpha = .3  for (name, results), color in zip(vals.items(), colors):     bot = np.percentile(abs(results - reference_answer), 25, axis=1)     med = np.percentile(abs(results - reference_answer), 50, axis=1)     top = np.percentile(abs(results - reference_answer), 75, axis=1)     ax.loglog(num_posterior_draws_to_test, med, label=name, color=color)     ax.fill_between(num_posterior_draws_to_test, bot, top, color=color, alpha=alpha) ax.loglog(num_posterior_draws_to_test, .1 * num_posterior_draws_to_test ** -.5, '--k', label='$O(N^{-1/2})$') ax.loglog(num_posterior_draws_to_test, .25 * num_posterior_draws_to_test ** -1.0, '-.k', label='$O(N^{-1})$') ax.set_xlabel('N - number of points') ax.set_ylabel('Accuracy') ax.legend(loc='lower left') ax.set_title(f'Statistics from {num_repeats} runs');  # plt.savefig('qei_convergence.png'); In\u00a0[9]: Copied! <pre># parameters\nnames = ['IID','Lattice','Sobol']\nepsilons = [\n      [2e-4, 5e-4, 1e-3, 2e-3, 5e-3, 1e-2], # iid nodes\n      [5e-6, 1e-5, 2e-5, 5e-5, 1e-4, 2e-4, 5e-4, 1e-3, 2e-3, 5e-3, 1e-2], # lattice\n      [5e-6, 1e-5, 2e-5, 5e-5, 1e-4, 2e-4, 5e-4, 1e-3, 2e-3, 5e-3, 1e-2]] # sobol\ntrials = 25\n# initialize time data\ntimes = {names[j]:np.zeros((len(epsilons[j]),trials),dtype=float) for j in range(len(names))}\nn_needed = {names[j]:np.zeros((len(epsilons[j]),trials),dtype=float) for j in range(len(names))}\n# run tests\nfor t in range(trials):\n  print(t)\n  for j in range(len(names)):\n    for i in range(len(epsilons[j])): \n      if j == 0:\n        sc = CubMCG(CustomFun(Gaussian(IIDStdUniform(d),mu_post,sigma_cov),compute_qei),abs_tol=epsilons[j][i],rel_tol=0)\n      elif j == 1:\n        sc = CubQMCLatticeG(CustomFun(Gaussian(Lattice(d),mu_post,sigma_cov),compute_qei),abs_tol=epsilons[j][i],rel_tol=0)\n      else:\n        sc = CubQMCSobolG(CustomFun(Gaussian(Sobol(d),mu_post,sigma_cov),compute_qei),abs_tol=epsilons[j][i],rel_tol=0)\n      solution,data = sc.integrate()\n      times[names[j]][i,t] = data.time_integrate\n      n_needed[names[j]][i,t] = data.n_total\n</pre> # parameters names = ['IID','Lattice','Sobol'] epsilons = [       [2e-4, 5e-4, 1e-3, 2e-3, 5e-3, 1e-2], # iid nodes       [5e-6, 1e-5, 2e-5, 5e-5, 1e-4, 2e-4, 5e-4, 1e-3, 2e-3, 5e-3, 1e-2], # lattice       [5e-6, 1e-5, 2e-5, 5e-5, 1e-4, 2e-4, 5e-4, 1e-3, 2e-3, 5e-3, 1e-2]] # sobol trials = 25 # initialize time data times = {names[j]:np.zeros((len(epsilons[j]),trials),dtype=float) for j in range(len(names))} n_needed = {names[j]:np.zeros((len(epsilons[j]),trials),dtype=float) for j in range(len(names))} # run tests for t in range(trials):   print(t)   for j in range(len(names)):     for i in range(len(epsilons[j])):        if j == 0:         sc = CubMCG(CustomFun(Gaussian(IIDStdUniform(d),mu_post,sigma_cov),compute_qei),abs_tol=epsilons[j][i],rel_tol=0)       elif j == 1:         sc = CubQMCLatticeG(CustomFun(Gaussian(Lattice(d),mu_post,sigma_cov),compute_qei),abs_tol=epsilons[j][i],rel_tol=0)       else:         sc = CubQMCSobolG(CustomFun(Gaussian(Sobol(d),mu_post,sigma_cov),compute_qei),abs_tol=epsilons[j][i],rel_tol=0)       solution,data = sc.integrate()       times[names[j]][i,t] = data.time_integrate       n_needed[names[j]][i,t] = data.n_total <pre>0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n</pre> In\u00a0[11]: Copied! <pre>fig,axs = pyplot.subplots(1, 3, figsize=(22, 6))\ncolors = ('#245EAB', '#A23D97', '#00B253')\nlight_colors = ('#A3DDFF', '#FFBCFF', '#4DFFA0')\nalpha = .3\ndef plot_fills(eps,data,name,color,light_color):\n    bot = np.percentile(data, 5, axis=1)\n    med = np.percentile(data, 50, axis=1)\n    top = np.percentile(data, 95, axis=1)\n    ax.loglog(eps, med, label=name, color=color)\n    ax.fill_between(eps, bot, top, color=light_color)\n    return med\nfor i,(nt_data,label) in enumerate(zip([times,n_needed],['time','n'])):\n  ax = axs[i+1]\n  # iid plot\n  eps_iid = np.array(epsilons[0])\n  data = nt_data['IID']\n  med_iid = plot_fills(eps_iid,data,'IID',colors[0],light_colors[0])\n  # lattice plot\n  eps = np.array(epsilons[1])\n  data = nt_data['Lattice']\n  med_lattice = plot_fills(eps,data,'Lattice',colors[1],light_colors[1])\n  # sobol plot\n  eps = np.array(epsilons[2])\n  data = nt_data['Sobol']\n  med_sobol = plot_fills(eps,data,'Sobol',colors[2],light_colors[2])\n  # iid bigO\n  ax.loglog(eps_iid, (med_iid[0]*eps_iid[0]**2)/(eps_iid**2), '--k', label=r'$\\mathcal{O}(1/\\epsilon^2)$')\n  # ld bigO\n  ax.loglog(eps, ((med_lattice[0]*med_sobol[0])**.5 *eps[0]) / eps , '-.k', label=r'$\\mathcal{O}(1/\\epsilon)$')\n  # metas\n  ax.set_xlabel(r'$\\epsilon$')\n  ax.set_ylabel(label)\n  ax.spines['right'].set_visible(False)\n  ax.spines['top'].set_visible(False)\n  ax.legend(loc='lower left',frameon=False)\n  ax.set_title(f'Statistics from {trials} runs')\n# plot sample data\nax = axs[0]\nax.plot(xplt, yplt, linewidth=lw)\nax.plot(x, y, 'o', markersize=ms, color='orange')\nax.errorbar(x, y, yerr=2 * np.sqrt(v), marker='', linestyle='', color='orange', linewidth=lw)\nax.set_title('Sample data with noise')\nax.set_xlim([0,1])\nax.set_xticks([0,1])\nax.set_ylim([-3, 3])\nax.set_yticks([-3,3]);\n</pre> fig,axs = pyplot.subplots(1, 3, figsize=(22, 6)) colors = ('#245EAB', '#A23D97', '#00B253') light_colors = ('#A3DDFF', '#FFBCFF', '#4DFFA0') alpha = .3 def plot_fills(eps,data,name,color,light_color):     bot = np.percentile(data, 5, axis=1)     med = np.percentile(data, 50, axis=1)     top = np.percentile(data, 95, axis=1)     ax.loglog(eps, med, label=name, color=color)     ax.fill_between(eps, bot, top, color=light_color)     return med for i,(nt_data,label) in enumerate(zip([times,n_needed],['time','n'])):   ax = axs[i+1]   # iid plot   eps_iid = np.array(epsilons[0])   data = nt_data['IID']   med_iid = plot_fills(eps_iid,data,'IID',colors[0],light_colors[0])   # lattice plot   eps = np.array(epsilons[1])   data = nt_data['Lattice']   med_lattice = plot_fills(eps,data,'Lattice',colors[1],light_colors[1])   # sobol plot   eps = np.array(epsilons[2])   data = nt_data['Sobol']   med_sobol = plot_fills(eps,data,'Sobol',colors[2],light_colors[2])   # iid bigO   ax.loglog(eps_iid, (med_iid[0]*eps_iid[0]**2)/(eps_iid**2), '--k', label=r'$\\mathcal{O}(1/\\epsilon^2)$')   # ld bigO   ax.loglog(eps, ((med_lattice[0]*med_sobol[0])**.5 *eps[0]) / eps , '-.k', label=r'$\\mathcal{O}(1/\\epsilon)$')   # metas   ax.set_xlabel(r'$\\epsilon$')   ax.set_ylabel(label)   ax.spines['right'].set_visible(False)   ax.spines['top'].set_visible(False)   ax.legend(loc='lower left',frameon=False)   ax.set_title(f'Statistics from {trials} runs') # plot sample data ax = axs[0] ax.plot(xplt, yplt, linewidth=lw) ax.plot(x, y, 'o', markersize=ms, color='orange') ax.errorbar(x, y, yerr=2 * np.sqrt(v), marker='', linestyle='', color='orange', linewidth=lw) ax.set_title('Sample data with noise') ax.set_xlim([0,1]) ax.set_xticks([0,1]) ax.set_ylim([-3, 3]) ax.set_yticks([-3,3]); In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"demos/qei-demo-for-blog/#qei-q-noisy-expected-improvement-demo-for-blog","title":"QEI (Q-Noisy Expected Improvement) Demo for Blog\u00b6","text":""},{"location":"demos/qei-demo-for-blog/#problem-setup","title":"Problem setup\u00b6","text":"<p>Here is the current data ($x$ and $y$ values with noise) from which we want to build a GP and run a Bayesian optimization.</p>"},{"location":"demos/qei-demo-for-blog/#computation-of-the-qei-quantity-using-qmcpy","title":"Computation of the qEI quantity using <code>qmcpy</code>\u00b6","text":"<p>One quantity which can appear often during BO is a computation involving $q$ \"next points\" to sample in a BO process; in the standard formulation this quantity might involve just $q=1$, but $q&gt;1$ is also of interest for batched evaluation in parallel.</p> <p>This quantity is defined as $$ \\mathrm{EI}_q(x_1, \\ldots, x_q;\\mathbf{y}, \\mathcal{X}, \\boldsymbol{\\epsilon}) = \\int_{\\mathbb{R}^q} \\max_{1\\leq i\\leq q}\\left[{(y_i - y^*)_+}\\right]\\, p_{Y_{x_1,\\ldots, x_q}|\\mathbf{y}, \\mathcal{X}, \\boldsymbol{\\epsilon}}(y_1, \\ldots, y_q)\\; \\text{d}y_1\\cdots\\text{d}y_q $$</p> <p>The example we are considering here is with $q=5$ but this quantity could be made larger.  Each of these QEI computations (done in a vectorized fashion in production) would be needed in an optimization loop (likely powered by CMAES or some other high dimensional non-convex optimization tool).  This optimization problem would take place in a $qd$ dimensional space, which is one aspect which usually prevents $q$ from being too large.</p> <p>Note that some of this will look much more confusing in $d&gt;1$, but it is written here in a simplified version.</p>"},{"location":"demos/qei-demo-for-blog/#gp-model-definition-kernel-information-and-qei-definition","title":"GP model definition (kernel information) and qEI definition\u00b6","text":""},{"location":"demos/qei-demo-for-blog/#demonstrate-the-concept-of-qei-on-2-points","title":"Demonstrate the concept of qEI on 2 points\u00b6","text":""},{"location":"demos/qei-demo-for-blog/#choose-some-set-of-next-points-against-which-to-test-the-computation","title":"Choose some set of next points against which to test the computation\u00b6","text":"<p>Here, we consider $q=5$, which is much more costly to compute than the $q=2$ demonstration above.</p> <p>Note This will take some time to run.  Use fewer <code>num_repeats</code> to reduce the cost.</p>"},{"location":"demos/qmcpy_intro/","title":"Introduction","text":"<p>Here we show three different ways to import QMCPy in a Python environment. First, we can import the package <code>qmcpy</code> under the alias <code>qp</code>.</p> In\u00a0[1]: Copied! <pre>import qmcpy as qp\nprint(qp.name, qp.__version__)\n</pre> import qmcpy as qp print(qp.name, qp.__version__) <pre>qmcpy 1.6.3c\n</pre> <p>Alternatively, we can import individual objects from 'qmcpy' as shown below.</p> In\u00a0[2]: Copied! <pre>from qmcpy.integrand import *\nfrom qmcpy.true_measure import *\nfrom qmcpy.discrete_distribution import *\nfrom qmcpy.stopping_criterion import *\n</pre> from qmcpy.integrand import * from qmcpy.true_measure import * from qmcpy.discrete_distribution import * from qmcpy.stopping_criterion import * <p>Lastly, we can import all objects from the package using an asterisk.</p> In\u00a0[3]: Copied! <pre>from qmcpy import *\n</pre> from qmcpy import * In\u00a0[4]: Copied! <pre>distribution = Lattice(dimension=2, randomize=True, seed=7)\ndistribution.gen_samples(n_min=0,n_max=4)\n</pre> distribution = Lattice(dimension=2, randomize=True, seed=7) distribution.gen_samples(n_min=0,n_max=4) Out[4]: <pre>array([[0.04386058, 0.58727432],\n       [0.54386058, 0.08727432],\n       [0.29386058, 0.33727432],\n       [0.79386058, 0.83727432]])</pre> In\u00a0[5]: Copied! <pre>from numpy.linalg import norm as norm\nfrom numpy import sqrt, array\n</pre> from numpy.linalg import norm as norm from numpy import sqrt, array <p>Our first attempt maybe to create the integrand as a Python function as follows:</p> In\u00a0[6]: Copied! <pre>def f(x): return norm(x) ** sqrt(norm(x))\n</pre> def f(x): return norm(x) ** sqrt(norm(x)) <p>It looks reasonable except that maybe the Numpy function norm is executed twice. It's okay for now. Let us quickly test if the function behaves as expected at a point value:</p> In\u00a0[7]: Copied! <pre>x = 0.01\nf(x)\n</pre> x = 0.01 f(x) Out[7]: <pre>0.6309573444801932</pre> <p>What about an array that represents $n=3$ sampling points in a two-dimensional domain, i.e., $d=2$?</p> In\u00a0[8]: Copied! <pre>x = array([[1., 0.], \n           [0., 0.01],\n           [0.04, 0.04]])\nf(x)\n</pre> x = array([[1., 0.],             [0., 0.01],            [0.04, 0.04]]) f(x) Out[8]: <pre>1.001650000560437</pre> <p>Now, the function should have returned $n=3$ real values that corresponding to each of the sampling points. Let's debug our Python function.</p> In\u00a0[9]: Copied! <pre>norm(x)\n</pre> norm(x) Out[9]: <pre>1.0016486409914407</pre> <p>Numpy's <code>norm(x)</code> is obviously a matrix norm, but we want it to be vector 2-norm that acts on each row of <code>x</code>. To that end, let's add an axis argument to the function:</p> In\u00a0[10]: Copied! <pre>norm(x, axis = 1)\n</pre> norm(x, axis = 1) Out[10]: <pre>array([1.        , 0.01      , 0.05656854])</pre> <p>Now it's working! Let's make sure that the <code>sqrt</code> function is acting on each element of the vector norm results:</p> In\u00a0[11]: Copied! <pre>sqrt(norm(x, axis = 1))\n</pre> sqrt(norm(x, axis = 1)) Out[11]: <pre>array([1.        , 0.1       , 0.23784142])</pre> <p>It is. Putting everything together, we have:</p> In\u00a0[12]: Copied! <pre>norm(x, axis = 1) ** sqrt(norm(x, axis = 1))\n</pre> norm(x, axis = 1) ** sqrt(norm(x, axis = 1)) Out[12]: <pre>array([1.        , 0.63095734, 0.50502242])</pre> <p>We have got our proper function definition now.</p> In\u00a0[13]: Copied! <pre>def myfunc(x):\n    x_norms = norm(x, axis = 1)\n    return x_norms ** sqrt(x_norms)\n</pre> def myfunc(x):     x_norms = norm(x, axis = 1)     return x_norms ** sqrt(x_norms) <p>We can now create an <code>integrand</code> instance with our <code>QuickConstruct</code> class in QMCPy and then invoke QMCPy's <code>integrate</code> function:</p> In\u00a0[14]: Copied! <pre>dim = 1\nabs_tol = .01\nintegrand = CustomFun(\n    true_measure = Uniform(IIDStdUniform(dimension=dim, seed=7)),\n    g=myfunc)\nsolution,data = CubMCCLT(integrand,abs_tol=abs_tol,rel_tol=0).integrate()\nprint(data)\n</pre> dim = 1 abs_tol = .01 integrand = CustomFun(     true_measure = Uniform(IIDStdUniform(dimension=dim, seed=7)),     g=myfunc) solution,data = CubMCCLT(integrand,abs_tol=abs_tol,rel_tol=0).integrate() print(data) <pre>Data (Data)\n    solution        0.657\n    bound_low       0.648\n    bound_high      0.667\n    bound_diff      0.019\n    n_total         3378\n    time_integrate  0.002\nCubMCCLT (AbstractStoppingCriterion)\n    abs_tol         0.010\n    rel_tol         0\n    n_init          2^(10)\n    n_limit         2^(30)\n    inflate         1.200\n    alpha           0.010\nCustomFun (AbstractIntegrand)\nUniform (AbstractTrueMeasure)\n    lower_bound     0\n    upper_bound     1\nIIDStdUniform (AbstractIIDDiscreteDistribution)\n    d               1\n    replications    1\n    entropy         7\n</pre> <p>For our integral, we know the true value. Let's check if QMCPy's solution is accurate enough:</p> In\u00a0[15]: Copied! <pre>true_sol = 0.658582  # In WolframAlpha: Integral[x**Sqrt[x], {x,0,1}]\nabs_tol = data.stopping_crit.abs_tol\nqmcpy_error = abs(true_sol - solution)\nif qmcpy_error &gt; abs_tol: raise Exception(\"Error not within bounds\")\n</pre> true_sol = 0.658582  # In WolframAlpha: Integral[x**Sqrt[x], {x,0,1}] abs_tol = data.stopping_crit.abs_tol qmcpy_error = abs(true_sol - solution) if qmcpy_error &gt; abs_tol: raise Exception(\"Error not within bounds\") <p>It's good. Shall we test the function with $d=2$ by simply changing the input parameter value of dimension for QuickConstruct?</p> In\u00a0[16]: Copied! <pre>dim = 2\nintegrand = CustomFun(\n    true_measure = Uniform(IIDStdUniform(dimension=dim, seed=7)),\n    g = myfunc)\nsolution2,data2 = CubMCCLT(integrand,abs_tol=abs_tol,rel_tol=0).integrate()\nprint(data2)\n</pre> dim = 2 integrand = CustomFun(     true_measure = Uniform(IIDStdUniform(dimension=dim, seed=7)),     g = myfunc) solution2,data2 = CubMCCLT(integrand,abs_tol=abs_tol,rel_tol=0).integrate() print(data2) <pre>Data (Data)\n    solution        0.830\n    bound_low       0.820\n    bound_high      0.840\n    bound_diff      0.020\n    n_total         5640\n    time_integrate  0.002\nCubMCCLT (AbstractStoppingCriterion)\n    abs_tol         0.010\n    rel_tol         0\n    n_init          2^(10)\n    n_limit         2^(30)\n    inflate         1.200\n    alpha           0.010\nCustomFun (AbstractIntegrand)\nUniform (AbstractTrueMeasure)\n    lower_bound     0\n    upper_bound     1\nIIDStdUniform (AbstractIIDDiscreteDistribution)\n    d               2^(1)\n    replications    1\n    entropy         7\n</pre> <p>Once again, we could test for accuracy of QMCPy with respect to the true value:</p> In\u00a0[17]: Copied! <pre>true_sol2 = 0.827606  # In WolframAlpha: Integral[Sqrt[x**2+y**2])**Sqrt[Sqrt[x**2+y**2]], {x,0,1}, {y,0,1}]\nabs_tol2 = data2.stopping_crit.abs_tol\nqmcpy_error2 = abs(true_sol2 - solution2)\nif qmcpy_error2 &gt; abs_tol2: raise Exception(\"Error not within bounds\")\n</pre> true_sol2 = 0.827606  # In WolframAlpha: Integral[Sqrt[x**2+y**2])**Sqrt[Sqrt[x**2+y**2]], {x,0,1}, {y,0,1}] abs_tol2 = data2.stopping_crit.abs_tol qmcpy_error2 = abs(true_sol2 - solution2) if qmcpy_error2 &gt; abs_tol2: raise Exception(\"Error not within bounds\") In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"demos/qmcpy_intro/#welcome-to-qmcpy","title":"Welcome to QMCPy\u00b6","text":""},{"location":"demos/qmcpy_intro/#importing-qmcpy","title":"Importing QMCPy\u00b6","text":""},{"location":"demos/qmcpy_intro/#important-notes","title":"Important Notes\u00b6","text":""},{"location":"demos/qmcpy_intro/#iid-vs-lds","title":"IID vs LDS\u00b6","text":"<p>Low discrepancy (LD) sequences such as lattice and Sobol' are not independent like IID (independent identically distributed) points.</p> <p>The code below generates 4 Sobol samples of 2 dimensions.</p>"},{"location":"demos/qmcpy_intro/#multi-dimensional-inputs","title":"Multi-Dimensional Inputs\u00b6","text":"<p>Suppose we want to create an integrand in QMCPy for evaluating the following integral:</p> <p>$$\\int_{[0,1]^d} \\|x\\|_2^{\\|x\\|_2^{1/2}} dx,$$</p> <p>where $[0,1]^d$ is the unit hypercube in $\\mathbb{R}^d$.</p> <p>The integrand is defined everywhere except at $x=0$ and hence the definite integral is also defined.</p> <p>The key in defining a Python function of an integrand in the QMCPy framework is that not only  it should be able to take one point $x \\in \\mathbb{R}^d$ and return a real value, but also that it would be able to take a set of $n$ sampling points as rows in a Numpy array of size $n \\times d$ and return an array with $n$ values evaluated at each sampling point. The following examples illustrate this point.</p>"},{"location":"demos/quickstart/","title":"Quickstart","text":"<p>Consider the problem of integrating the Keister function [2] with respect to a $d$-dimensional Gaussian measure:</p> <p>$$f(\\boldsymbol{x}) = \\pi^{d/2} \\cos(||\\boldsymbol{x}||), \\qquad \\boldsymbol{x} \\in \\mathbb{R}^d, \\qquad \\boldsymbol{X} \\sim \\mathcal{N}(\\boldsymbol{0}_d,\\mathsf{I}_d/2),   \\\\ \\mu  =  \\mathbb{E}[f(\\boldsymbol{X})] := \\int_{\\mathbb{R}^d} f(\\boldsymbol{x}) \\, \\pi^{-d/2} \\exp( - ||\\boldsymbol{x}||^2) \\,  \\rm d \\boldsymbol{x}  \\\\     =  \\int_{[0,1]^d} \\pi^{d/2}  \\cos\\left(\\sqrt{ \\frac 12 \\sum_{j=1}^d\\Phi^{-1}(x_j)}\\right)  \\, \\rm d \\boldsymbol{x},$$ where $||\\boldsymbol{x}||$ is the Euclidean norm, $\\mathsf{I}_d$ is the $d$-dimensional identity matrix, and $\\Phi$ denotes the standard normal cumulative distribution function. When $d=2$, $\\mu \\approx 1.80819$.</p> <p>The Keister function is implemented below with help from NumPy [3] in the following code snippet:</p> In\u00a0[4]: Copied! <pre>import numpy as np\ndef keister(x):\n    \"\"\"\n    x: nxd numpy ndarray\n       n samples\n       d dimensions\n\n    returns n-vector of the Keister function\n    evaluated at the n input samples\n    \"\"\"\n    d = x.shape[-1]\n    norm_x = np.sqrt((x**2).sum(-1))\n    k = np.pi**(d/2) * np.cos(norm_x)\n    return k # size n vector\n</pre> import numpy as np def keister(x):     \"\"\"     x: nxd numpy ndarray        n samples        d dimensions      returns n-vector of the Keister function     evaluated at the n input samples     \"\"\"     d = x.shape[-1]     norm_x = np.sqrt((x**2).sum(-1))     k = np.pi**(d/2) * np.cos(norm_x)     return k # size n vector <p>In addition to our Keister integrand and Gaussian true measure, we must select a discrete distribution, and a stopping criterion [4]. The stopping criterion determines the number of points at which to evaluate the integrand in order for the mean approximation to be accurate within a user-specified error tolerance, $\\varepsilon$. The discrete distribution determines the sites at which the integrand is evaluated.</p> <p>For this Keister example, we select the lattice sequence as the discrete distribution and corresponding cubature-based stopping criterion [5]. The discrete distribution, true measure, integrand, and stopping criterion are then constructed within the QMCPy framework below.</p> In\u00a0[5]: Copied! <pre>import qmcpy\nd = 2\ndiscrete_distrib = qmcpy.Lattice(dimension = d)\ntrue_measure = qmcpy.Gaussian(discrete_distrib, mean = 0, covariance = 1/2)\nintegrand = qmcpy.CustomFun(true_measure,keister)\nstopping_criterion = qmcpy.CubQMCLatticeG(integrand = integrand, abs_tol = 1e-3)\n</pre> import qmcpy d = 2 discrete_distrib = qmcpy.Lattice(dimension = d) true_measure = qmcpy.Gaussian(discrete_distrib, mean = 0, covariance = 1/2) integrand = qmcpy.CustomFun(true_measure,keister) stopping_criterion = qmcpy.CubQMCLatticeG(integrand = integrand, abs_tol = 1e-3) <p>Calling integrate on the stopping_criterion instance returns the numerical solution and a data object. Printing the data object will provide a neat summary of the integration problem. For details of the output fields, refer to the online, searchable QMCPy Documentation at https://qmcpy.readthedocs.io/.</p> In\u00a0[6]: Copied! <pre>solution, data = stopping_criterion.integrate()\nprint(data)\n</pre> solution, data = stopping_criterion.integrate() print(data) <pre>Data (Data)\n    solution        1.808\n    comb_bound_low  1.808\n    comb_bound_high 1.809\n    comb_bound_diff 0.001\n    comb_flags      1\n    n_total         2^(13)\n    n               2^(13)\n    time_integrate  0.013\nCubQMCLatticeG (AbstractStoppingCriterion)\n    abs_tol         0.001\n    rel_tol         0\n    n_init          2^(10)\n    n_limit         2^(30)\nCustomFun (AbstractIntegrand)\nGaussian (AbstractTrueMeasure)\n    mean            0\n    covariance      2^(-1)\n    decomp_type     PCA\nLattice (AbstractLDDiscreteDistribution)\n    d               2^(1)\n    replications    1\n    randomize       SHIFT\n    gen_vec_source  kuo.lattice-33002-1024-1048576.9125.txt\n    order           NATURAL\n    n_limit         2^(20)\n    entropy         331737392988868539347762820736945757498\n</pre> <p>This guide is not meant to be exhaustive but rather a quick introduction to the QMCPy framework and syntax. In an upcoming blog, we will take a closer look at low-discrepancy sequences such as the lattice sequence from the above example.</p>"},{"location":"demos/quickstart/#a-qmcpy-quick-start","title":"A QMCPy Quick Start\u00b6","text":"<p>In this tutorial, we introduce QMCPy [1]  by an example. QMCPy can be installed with pip install qmcpy or cloned from the  QMCSoftware GitHub repository.</p>"},{"location":"demos/quickstart/#references","title":"References\u00b6","text":"<ol> <li>Choi,  S.-C.  T.,  Hickernell,  F.,  McCourt,  M., Rathinavel J., &amp;  Sorokin,  A. QMCPy:  A quasi-Monte  Carlo  Python  Library. https://qmcsoftware.github.io/QMCSoftware/. 2020.</li> <li>Keister, B. D. Multidimensional Quadrature Algorithms. Computers in Physics 10, 119\u2013122 (1996).</li> <li>Oliphant,  T., Guide  to  NumPy https://ecs.wgtn.ac.nz/foswiki/pub/Support/ManualPagesAndDocumentation/numpybook.pdf (Trelgol Publishing USA, 2006).</li> <li>Hickernell, F., Choi, S.-C. T., Jiang, L. &amp; Jimenez Rugama, L. A. in WileyStatsRef-Statistics Reference Online (eds Davidian, M.et al.) (John Wiley &amp; Sons Ltd., 2018).</li> <li>Jimenez Rugama, L. A. &amp; Hickernell, F. Adaptive  Multidimensional  Integration  Based  on  Rank-1  Lattices in Monte  Carlo  and  Quasi-Monte  Carlo Methods: MCQMC, Leuven, Belgium, April 2014 (eds Cools, R. &amp; Nuyens, D.) 163.arXiv:1411.1966 (Springer-Verlag, Berlin, 2016), 407\u2013422.</li> </ol>"},{"location":"demos/ray_tracing/","title":"Ray Tracing","text":"In\u00a0[9]: Copied! <pre>from qmcpy import *\nfrom numpy import *\nfrom time import time\nfrom PIL import Image\nfrom matplotlib import pyplot\nfrom threading import Thread\n%matplotlib inline\n</pre> from qmcpy import * from numpy import * from time import time from PIL import Image from matplotlib import pyplot from threading import Thread %matplotlib inline In\u00a0[\u00a0]: Copied! <pre># constants\nEPS = 1e-8 # numerical precision error tolerance\nnorm = lambda v: sqrt(dot(v,v))\ne = array([0,0,0],dtype=float) # eye location at the origin\n</pre> # constants EPS = 1e-8 # numerical precision error tolerance norm = lambda v: sqrt(dot(v,v)) e = array([0,0,0],dtype=float) # eye location at the origin In\u00a0[11]: Copied! <pre>class Camera(object):\n    \"\"\" An object to render the scene. \"\"\"\n    def __init__(self, ax, scene, px=8, parallel_x_blocks=2, parallel_y_blocks=2, image_angle=90, image_dist=1):\n        \"\"\"\n        Args:\n            ax (axes): matplotlib ax to plot image on\n            scene (int): object with scene.render_px(p) method. See examples later in the notebook.\n            px (int): number of pixels in height and width. \n                Resolution = px*px.\n            parallel_x_blocks (int): number of cuts along the x axis in which to make parallel.\n            parallel_y_blocks (int): number of cuts along the y axis in which to make parallel. \n                parallel_x/y_blocks must be divisors of px. \n                Number of threads = parallel_y_blocks * parallel_x_blocks. \n        \"\"\"\n        self.ax = ax\n        self.scene = scene\n        self.px = px\n        self.fl = image_dist # distance from eye to image\n        self.i_hw = tan(image_angle/2)*self.fl # half width of the image\n        self.px_hw = self.i_hw/px # half width of a pixel\n        # set parallelization constants\n        self.p_xb = parallel_x_blocks; self.p_yb = parallel_y_blocks\n        bs_x = px/self.p_xb; bs_y = px/self.p_yb # block size for x,y\n        if bs_x%1!=0 or bs_y%1!=0: raise Exception('parallel_x/y_blocks must divide px')\n        self.bs_x = int(bs_x); self.bs_y = int(bs_y)\n    def render(self):\n        \"\"\" Render the image. \"\"\"\n        t0 = time() # start a timer\n        img = Image.new('RGB',(self.px,self.px),(0,0,0))\n        if self.p_xb==1 and self.p_yb==1:\n            # use non-parallel processing (helpful for debugging)\n            self.block_render(img,0,self.px,0,self.px)\n        else:\n            # parallel processing\n            threads = [None]*(self.p_xb*self.p_yb)\n            i_t = 0 # thread index\n            for xb in range(0,self.px,self.bs_x):\n                for yb in range(0,self.px,self.bs_y):\n                    threads[i_t] = Thread(target=self.block_render, args=(img,xb,xb+self.bs_x,yb,yb+self.bs_y))\n                    threads[i_t].start() # start threads\n                    i_t += 1\n            for i in range(len(threads)): threads[i].join() # wait for all threads to complete\n        self.ax.axis('off')\n        self.ax.imshow(asarray(img))\n        print('Render took %.1f seconds'%(time()-t0))\n    def block_render(self, img, px_x_start, px_x_end, px_y_start, px_y_end):\n        \"\"\"\n        Render a block of the image. \n        \n        Args:\n            img (PIL.Image): the image to color pixels of. \n            px_x_start (int): x index of pixel to start rendering at. \n            px_x_end (int): x index of pixel to end rendering at. \n            px_y_start (int): y index of pixel to start rendering at. \n            px_y_end (int): y index of pixel to start rendering at.\n        \"\"\"\n        for p_x in range(px_x_start,px_x_end):\n            for p_y in range(px_y_start,px_y_end):\n                p = array([-self.i_hw+2*self.i_hw*p_x/self.px,-self.i_hw+2*self.i_hw*p_y/self.px,self.fl])\n                color = self.scene.render_px(p)\n                img.putpixel((p_x,self.px-p_y-1),color)    \n</pre> class Camera(object):     \"\"\" An object to render the scene. \"\"\"     def __init__(self, ax, scene, px=8, parallel_x_blocks=2, parallel_y_blocks=2, image_angle=90, image_dist=1):         \"\"\"         Args:             ax (axes): matplotlib ax to plot image on             scene (int): object with scene.render_px(p) method. See examples later in the notebook.             px (int): number of pixels in height and width.                  Resolution = px*px.             parallel_x_blocks (int): number of cuts along the x axis in which to make parallel.             parallel_y_blocks (int): number of cuts along the y axis in which to make parallel.                  parallel_x/y_blocks must be divisors of px.                  Number of threads = parallel_y_blocks * parallel_x_blocks.          \"\"\"         self.ax = ax         self.scene = scene         self.px = px         self.fl = image_dist # distance from eye to image         self.i_hw = tan(image_angle/2)*self.fl # half width of the image         self.px_hw = self.i_hw/px # half width of a pixel         # set parallelization constants         self.p_xb = parallel_x_blocks; self.p_yb = parallel_y_blocks         bs_x = px/self.p_xb; bs_y = px/self.p_yb # block size for x,y         if bs_x%1!=0 or bs_y%1!=0: raise Exception('parallel_x/y_blocks must divide px')         self.bs_x = int(bs_x); self.bs_y = int(bs_y)     def render(self):         \"\"\" Render the image. \"\"\"         t0 = time() # start a timer         img = Image.new('RGB',(self.px,self.px),(0,0,0))         if self.p_xb==1 and self.p_yb==1:             # use non-parallel processing (helpful for debugging)             self.block_render(img,0,self.px,0,self.px)         else:             # parallel processing             threads = [None]*(self.p_xb*self.p_yb)             i_t = 0 # thread index             for xb in range(0,self.px,self.bs_x):                 for yb in range(0,self.px,self.bs_y):                     threads[i_t] = Thread(target=self.block_render, args=(img,xb,xb+self.bs_x,yb,yb+self.bs_y))                     threads[i_t].start() # start threads                     i_t += 1             for i in range(len(threads)): threads[i].join() # wait for all threads to complete         self.ax.axis('off')         self.ax.imshow(asarray(img))         print('Render took %.1f seconds'%(time()-t0))     def block_render(self, img, px_x_start, px_x_end, px_y_start, px_y_end):         \"\"\"         Render a block of the image.                   Args:             img (PIL.Image): the image to color pixels of.              px_x_start (int): x index of pixel to start rendering at.              px_x_end (int): x index of pixel to end rendering at.              px_y_start (int): y index of pixel to start rendering at.              px_y_end (int): y index of pixel to start rendering at.         \"\"\"         for p_x in range(px_x_start,px_x_end):             for p_y in range(px_y_start,px_y_end):                 p = array([-self.i_hw+2*self.i_hw*p_x/self.px,-self.i_hw+2*self.i_hw*p_y/self.px,self.fl])                 color = self.scene.render_px(p)                 img.putpixel((p_x,self.px-p_y-1),color)     In\u00a0[12]: Copied! <pre>class Plane(object):\n    def __init__(self, norm_axis, position, color):\n        \"\"\"\n        Args:\n            norm_axis (str): either 'x', 'y', or 'z'.\n            position (str): constant position of plane along the norm_axis.\n            color (tuple): length 3 tuple of rgb values.\n        \"\"\"\n        dim_dict = {'x':0,'y':1,'z':2}\n        self.d = dim_dict[norm_axis]\n        self.pos = position # self.norm_axis coordinate of the floor \n        self.color = color\n    def hit(self, o, u):\n        \"\"\"\n        Test if the beam o+tu hits the plane.\n        \n        Args:\n            o (ndarray): length 3 origin point.\n            u (ndarray): length 3 unit vector.\n        \n        Returns:\n            tuple: \n                - hit (bool): was an object hit?\n                - hit_p (ndarray): point where beam intersects object.\n                - color (tuple): length 3 tuple rgb value.\n        \"\"\"\n        k = u[self.d]\n        if k != 0:\n            t = (self.pos - o[self.d]) / u[self.d]\n            if t &gt; EPS:\n                return True, o+t*u # ray intersects the plane\n        return False, None # ray misses the plane\n    def normal(self, o, u):\n        \"\"\"\n        Get the unit normal vector to the plane at this point.\n        \n        Args:\n            o (ndarray): length 3 origin point.\n            u (ndarray): length 3 unit vector in direction of light.\n        \n        \n        Returns:\n            ndarray: length three unit normal vector.\n        \"\"\"\n        v = array([0,0,0])\n        v[self.d] = 1\n        if dot(v,u)&lt;0: \n            v[self.d] = -1\n        return v\n</pre> class Plane(object):     def __init__(self, norm_axis, position, color):         \"\"\"         Args:             norm_axis (str): either 'x', 'y', or 'z'.             position (str): constant position of plane along the norm_axis.             color (tuple): length 3 tuple of rgb values.         \"\"\"         dim_dict = {'x':0,'y':1,'z':2}         self.d = dim_dict[norm_axis]         self.pos = position # self.norm_axis coordinate of the floor          self.color = color     def hit(self, o, u):         \"\"\"         Test if the beam o+tu hits the plane.                  Args:             o (ndarray): length 3 origin point.             u (ndarray): length 3 unit vector.                  Returns:             tuple:                  - hit (bool): was an object hit?                 - hit_p (ndarray): point where beam intersects object.                 - color (tuple): length 3 tuple rgb value.         \"\"\"         k = u[self.d]         if k != 0:             t = (self.pos - o[self.d]) / u[self.d]             if t &gt; EPS:                 return True, o+t*u # ray intersects the plane         return False, None # ray misses the plane     def normal(self, o, u):         \"\"\"         Get the unit normal vector to the plane at this point.                  Args:             o (ndarray): length 3 origin point.             u (ndarray): length 3 unit vector in direction of light.                           Returns:             ndarray: length three unit normal vector.         \"\"\"         v = array([0,0,0])         v[self.d] = 1         if dot(v,u)&lt;0:              v[self.d] = -1         return v In\u00a0[13]: Copied! <pre>class Ball(object):\n    def __init__(self, center, radius, color):\n        \"\"\"\n        Args:\n            center (ndarray): length 3 center position of the ball.\n            radius (float): radius of the ball.\n            color (tuple): length 3 tuple of rgb values.\n        \"\"\"\n        self.c = center\n        self.r = radius\n        self.color = color\n    def hit(self, o, u):\n        \"\"\"\n        Test if the beam o+tu hits the ball.\n        \n        Args:\n            o (ndarray): length 3 origin point.\n            u (ndarray): length 3 unit vector.\n        \n        Returns:\n            tuple: \n                - hit (bool): was an object was hit?\n                - hit_p (ndarray): point where beam intersects object.\n                - color (tuple): length 3 tuple rgb value.\n        \"\"\"\n        q = o - self.c\n        a = dot(u,u)\n        b = 2*dot(u,q)\n        c = dot(q,q) - self.r**2\n        d = b**2 - 4*a*c\n        if d &gt; 0: # ray intersects sphere\n            tt = (-b + array([1,-1],dtype=float)*sqrt(d)) / (2**a)\n            tt = tt[tt&gt;EPS] # only want intersection from rays moving in positive direction\n            if len(tt) &gt;= 1: # at least one positive intersection\n                # beam going forward intersects ball\n                t = min(tt)\n                return True, o+t*u\n        return False, None # ray does not intersect sphere or only intersects in opposite direction\n    def normal(self, o, u):\n        \"\"\"\n        Get the unit normal vector to the sphere at thi point.\n        \n        Args:\n            o (ndarray): length 3 origin point.\n            u (ndarray): length 3 unit vector in direction of light.\n        \n        Returns:\n            ndarray: length three unit normal vector.\n        \"\"\"\n        v = (o-self.c)\n        v_u = v/norm(v)\n        return v_u\n</pre> class Ball(object):     def __init__(self, center, radius, color):         \"\"\"         Args:             center (ndarray): length 3 center position of the ball.             radius (float): radius of the ball.             color (tuple): length 3 tuple of rgb values.         \"\"\"         self.c = center         self.r = radius         self.color = color     def hit(self, o, u):         \"\"\"         Test if the beam o+tu hits the ball.                  Args:             o (ndarray): length 3 origin point.             u (ndarray): length 3 unit vector.                  Returns:             tuple:                  - hit (bool): was an object was hit?                 - hit_p (ndarray): point where beam intersects object.                 - color (tuple): length 3 tuple rgb value.         \"\"\"         q = o - self.c         a = dot(u,u)         b = 2*dot(u,q)         c = dot(q,q) - self.r**2         d = b**2 - 4*a*c         if d &gt; 0: # ray intersects sphere             tt = (-b + array([1,-1],dtype=float)*sqrt(d)) / (2**a)             tt = tt[tt&gt;EPS] # only want intersection from rays moving in positive direction             if len(tt) &gt;= 1: # at least one positive intersection                 # beam going forward intersects ball                 t = min(tt)                 return True, o+t*u         return False, None # ray does not intersect sphere or only intersects in opposite direction     def normal(self, o, u):         \"\"\"         Get the unit normal vector to the sphere at thi point.                  Args:             o (ndarray): length 3 origin point.             u (ndarray): length 3 unit vector in direction of light.                  Returns:             ndarray: length three unit normal vector.         \"\"\"         v = (o-self.c)         v_u = v/norm(v)         return v_u In\u00a0[14]: Copied! <pre>class PointLight(object):\n    \"\"\" A lamp that is a point and emits. light in all directions. \"\"\"\n    def __init__(self, position, intensity):\n        \"\"\"\n        Args:\n            position (ndarray): length 3 coordinate of light position. \n            intensity (float): intensity of the light, between 0 and 1. \n        \"\"\"\n        self.p = position\n        self.i = intensity\n</pre> class PointLight(object):     \"\"\" A lamp that is a point and emits. light in all directions. \"\"\"     def __init__(self, position, intensity):         \"\"\"         Args:             position (ndarray): length 3 coordinate of light position.              intensity (float): intensity of the light, between 0 and 1.          \"\"\"         self.p = position         self.i = intensity In\u00a0[15]: Copied! <pre>class CustomScene(object):\n    def __init__(self, objs, light, n, d, mc_type):\n        self.objs = objs\n        self.light = light\n        self.black = array([0,0,0],dtype=float) # the color black \n        self.n = n\n        self.d = d\n        self.mc_type = mc_type\n        # generate constant samples to be used for every ray tracing event\n        if self.mc_type == 'IID':\n            self.pts = IIDStdUniform(2*self.d).gen_samples(self.n)\n        elif self.mc_type == 'SOBOL':\n            self.pts = Sobol(2*self.d,order=\"GRAY\").gen_samples(self.n) \n        else: \n            raise Exception(\"mc_type must be IID or Sobol\")\n    def find_closest_obj(self,o,v):\n        \"\"\"\n        Find the closest object to point o heading in direction v\n\n        Args:\n            o (ndarray): length 3 coordinate of point we will try and find closest object to\n\n        Returns:\n            tuple: \n                hit (bool): weather any objects were hit. \n                hit_p (ndarray): length 3 coordinates of where obj was hit. \n                hit_dist (float): distance from hit_p to o. \n                hit_obj (object): the object that was hit.\n        \"\"\"\n        hit,hit_p,hit_dist,hit_obj = False,None,inf,None\n        for obj in self.objs:\n            obj_hit,obj_p = obj.hit(o,v)\n            if obj_hit:\n                v2 = obj_p-o # vector from o to object position\n                obj_dist = sqrt(dot(v2,v2))\n                if obj_dist &lt; hit_dist:\n                    hit,hit_p,hit_dist,hit_obj = True,obj_p,obj_dist,obj\n        return hit,hit_p,hit_dist,hit_obj\n    def get_obj_color(self,obj,p,l):\n        \"\"\"\n        Get the objects color at point p with light in direction l.\n\n        Args:\n            obj (object): object on which p lies\n            p (ndarray): length 3 coordinate of point on the object\n            l (ndarray): length 3 vector of direction from p to light source\n\n        Returns:\n            ndarray: length 3 RGB color\n        \"\"\"\n        n_v = obj.normal(p,l) # normal vector to obj at point p\n        color = obj.color*self.light.i*dot(n_v,l) / (norm(n_v)*norm(l))\n        return color\n    def beam_r(self,o,v,n,d,pts,nidx,didx):\n        \"\"\"\n        Recursive (Quasi-)Monte Carlo simulation of a light beam\n\n        Args:\n            o (ndarray): length 3 coordinate of current light beam position\n            v (ndarray): length 3 vector of light beam direction\n            n (ndarray): number of rays to cast when it cannot find light directly \n            d (int): remaining bounces before beam gives up\n            pts (ndarray): n samples x d dimension ndarray of samples generated by QMCPy\n            nidx (int): 2*(index of the beam)\n            didx (int): index of the sample\n        \"\"\"\n        hit,hit_p,hit_dist,hit_obj = self.find_closest_obj(o,v)\n        if hit: # an object was hit\n            l = self.light.p-hit_p # vector from position where beam hit to the lamp\n            l_dist = norm(l) # distance from hit location to lamp\n            l_u = l/l_dist # unit vector of l\n            itw,itw_p,itw_dist,itw_obj = self.find_closest_obj(hit_p,l_u) # find any object in the way\n            if itw and itw_dist&lt;= l_dist: # object between hit object and the lamp\n                if d==0:\n                    # no remaining bounces --&gt; return black (give up)\n                    return self.black\n                else:\n                    # beam has remaining bounces\n                    color_total = self.black.copy()\n                    for i in range(n):\n                        theta_0 = 2*pi*pts[nidx+i,didx]\n                        theta_1 = 2*pi*pts[nidx+i,didx+1]\n                        x = sin(theta_0)*sin(theta_1)\n                        y = sin(theta_0)*cos(theta_0)\n                        z = sin(theta_1)\n                        v_rand = array([x,y,z],dtype=float) # random direction\n                        ho_n = hit_obj.normal(hit_p,l_u)\n                        if dot(v_rand,ho_n) &lt; 0: v_rand = -v_rand # flip direction to correct hemisphere\n                        obj_color = self.get_obj_color(hit_obj,hit_p,l_u)\n                        color_total += obj_color*self.beam_r(hit_p,v_rand,n=1,d=d-1,pts=pts,nidx=nidx+i,didx=didx+2)\n                    return color_total/n # take the average of many simulations\n            else: # nothin between the object and the light source\n                # get the color based on point, normal to obj, and direction to light\n                return self.get_obj_color(hit_obj,hit_p,l_u)\n        return self.black # nothing hit --&gt; return black '\n    def render_px(self,p):\n        \"\"\"\n        Get pixel value for ball-lamp-floor scene\n\n        Args:\n            p (ndarray): length 3 array coordinates of center of pixel to render\n        \"\"\"\n        u = (p-e)/norm(p-e) # unit vector in direction of eye to pixel\n        color_0_1 = self.beam_r(e,u,n=self.n,d=self.d,pts=self.pts,nidx=0,didx=0)\n        color = (color_0_1*256).astype(int)\n        return color[0],color[1],color[2]\n</pre> class CustomScene(object):     def __init__(self, objs, light, n, d, mc_type):         self.objs = objs         self.light = light         self.black = array([0,0,0],dtype=float) # the color black          self.n = n         self.d = d         self.mc_type = mc_type         # generate constant samples to be used for every ray tracing event         if self.mc_type == 'IID':             self.pts = IIDStdUniform(2*self.d).gen_samples(self.n)         elif self.mc_type == 'SOBOL':             self.pts = Sobol(2*self.d,order=\"GRAY\").gen_samples(self.n)          else:              raise Exception(\"mc_type must be IID or Sobol\")     def find_closest_obj(self,o,v):         \"\"\"         Find the closest object to point o heading in direction v          Args:             o (ndarray): length 3 coordinate of point we will try and find closest object to          Returns:             tuple:                  hit (bool): weather any objects were hit.                  hit_p (ndarray): length 3 coordinates of where obj was hit.                  hit_dist (float): distance from hit_p to o.                  hit_obj (object): the object that was hit.         \"\"\"         hit,hit_p,hit_dist,hit_obj = False,None,inf,None         for obj in self.objs:             obj_hit,obj_p = obj.hit(o,v)             if obj_hit:                 v2 = obj_p-o # vector from o to object position                 obj_dist = sqrt(dot(v2,v2))                 if obj_dist &lt; hit_dist:                     hit,hit_p,hit_dist,hit_obj = True,obj_p,obj_dist,obj         return hit,hit_p,hit_dist,hit_obj     def get_obj_color(self,obj,p,l):         \"\"\"         Get the objects color at point p with light in direction l.          Args:             obj (object): object on which p lies             p (ndarray): length 3 coordinate of point on the object             l (ndarray): length 3 vector of direction from p to light source          Returns:             ndarray: length 3 RGB color         \"\"\"         n_v = obj.normal(p,l) # normal vector to obj at point p         color = obj.color*self.light.i*dot(n_v,l) / (norm(n_v)*norm(l))         return color     def beam_r(self,o,v,n,d,pts,nidx,didx):         \"\"\"         Recursive (Quasi-)Monte Carlo simulation of a light beam          Args:             o (ndarray): length 3 coordinate of current light beam position             v (ndarray): length 3 vector of light beam direction             n (ndarray): number of rays to cast when it cannot find light directly              d (int): remaining bounces before beam gives up             pts (ndarray): n samples x d dimension ndarray of samples generated by QMCPy             nidx (int): 2*(index of the beam)             didx (int): index of the sample         \"\"\"         hit,hit_p,hit_dist,hit_obj = self.find_closest_obj(o,v)         if hit: # an object was hit             l = self.light.p-hit_p # vector from position where beam hit to the lamp             l_dist = norm(l) # distance from hit location to lamp             l_u = l/l_dist # unit vector of l             itw,itw_p,itw_dist,itw_obj = self.find_closest_obj(hit_p,l_u) # find any object in the way             if itw and itw_dist&lt;= l_dist: # object between hit object and the lamp                 if d==0:                     # no remaining bounces --&gt; return black (give up)                     return self.black                 else:                     # beam has remaining bounces                     color_total = self.black.copy()                     for i in range(n):                         theta_0 = 2*pi*pts[nidx+i,didx]                         theta_1 = 2*pi*pts[nidx+i,didx+1]                         x = sin(theta_0)*sin(theta_1)                         y = sin(theta_0)*cos(theta_0)                         z = sin(theta_1)                         v_rand = array([x,y,z],dtype=float) # random direction                         ho_n = hit_obj.normal(hit_p,l_u)                         if dot(v_rand,ho_n) &lt; 0: v_rand = -v_rand # flip direction to correct hemisphere                         obj_color = self.get_obj_color(hit_obj,hit_p,l_u)                         color_total += obj_color*self.beam_r(hit_p,v_rand,n=1,d=d-1,pts=pts,nidx=nidx+i,didx=didx+2)                     return color_total/n # take the average of many simulations             else: # nothin between the object and the light source                 # get the color based on point, normal to obj, and direction to light                 return self.get_obj_color(hit_obj,hit_p,l_u)         return self.black # nothing hit --&gt; return black '     def render_px(self,p):         \"\"\"         Get pixel value for ball-lamp-floor scene          Args:             p (ndarray): length 3 array coordinates of center of pixel to render         \"\"\"         u = (p-e)/norm(p-e) # unit vector in direction of eye to pixel         color_0_1 = self.beam_r(e,u,n=self.n,d=self.d,pts=self.pts,nidx=0,didx=0)         color = (color_0_1*256).astype(int)         return color[0],color[1],color[2] In\u00a0[18]: Copied! <pre># create a scene\nobjs = [\n    Plane(norm_axis='y', position=-50, color=array([.75,.75,.75],dtype=float)), # floor\n    Plane(norm_axis='y', position=50,  color=array([.75,.75,.75],dtype=float)), # ceiling\n    Plane(norm_axis='x', position=50,  color=array([.75,.75,.75],dtype=float)), # right wall\n    Plane(norm_axis='x', position=-50, color=array([.75,.75,.75],dtype=float)), # left wall\n    Plane(norm_axis='z', position=150,  color=array([.75,.75,.75],dtype=float)), # back wall\n    Ball(center=array([-25,25,75],dtype=float), radius=20, color=array([1,0,0],dtype=float)), # ball\n    Ball(center=array([25,25,75],dtype=float),  radius=20, color=array([0,1,0],dtype=float)), # ball\n    Ball(center=array([0,-25,75],dtype=float),  radius=20, color=array([0,0,1],dtype=float)), # ball\n]\nlight = PointLight(position=array([0,25,0],dtype=float), intensity=1)\n# parameters\nn = 16 # number of beams\nd = 16 # max bounces of any given beam\npx = 256\n# render image\nfig,ax = pyplot.subplots(ncols=2,nrows=1,figsize=(20,10))# render scene\n# IID (MC)\nscene = CustomScene(objs,light,n,d,mc_type='IID')\ncamera = Camera(ax[0], scene, px=px, parallel_x_blocks=1, parallel_y_blocks=1, image_angle=pi/2, image_dist=1)\ncamera.render()\n# Sobol (QMC)\nscene = CustomScene(objs,light,n,d,mc_type='SOBOL')\ncamera = Camera(ax[1], scene, px=px, parallel_x_blocks=1, parallel_y_blocks=1, image_angle=pi/2, image_dist=1)\ncamera.render()\n</pre> # create a scene objs = [     Plane(norm_axis='y', position=-50, color=array([.75,.75,.75],dtype=float)), # floor     Plane(norm_axis='y', position=50,  color=array([.75,.75,.75],dtype=float)), # ceiling     Plane(norm_axis='x', position=50,  color=array([.75,.75,.75],dtype=float)), # right wall     Plane(norm_axis='x', position=-50, color=array([.75,.75,.75],dtype=float)), # left wall     Plane(norm_axis='z', position=150,  color=array([.75,.75,.75],dtype=float)), # back wall     Ball(center=array([-25,25,75],dtype=float), radius=20, color=array([1,0,0],dtype=float)), # ball     Ball(center=array([25,25,75],dtype=float),  radius=20, color=array([0,1,0],dtype=float)), # ball     Ball(center=array([0,-25,75],dtype=float),  radius=20, color=array([0,0,1],dtype=float)), # ball ] light = PointLight(position=array([0,25,0],dtype=float), intensity=1) # parameters n = 16 # number of beams d = 16 # max bounces of any given beam px = 256 # render image fig,ax = pyplot.subplots(ncols=2,nrows=1,figsize=(20,10))# render scene # IID (MC) scene = CustomScene(objs,light,n,d,mc_type='IID') camera = Camera(ax[0], scene, px=px, parallel_x_blocks=1, parallel_y_blocks=1, image_angle=pi/2, image_dist=1) camera.render() # Sobol (QMC) scene = CustomScene(objs,light,n,d,mc_type='SOBOL') camera = Camera(ax[1], scene, px=px, parallel_x_blocks=1, parallel_y_blocks=1, image_angle=pi/2, image_dist=1) camera.render() <pre>Render took 27.2 seconds\nRender took 28.4 seconds\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"demos/ray_tracing/#basic-ray-tracing","title":"Basic Ray Tracing\u00b6","text":"<ul> <li>Introduction to Ray Tracing: a Simple Method for Creating 3D Images by Scratchapixel 2.0</li> <li>Computer Graphics from scratch by Gabriel Gambetta</li> <li>Ray Tracing: Graphics for the Masses by Paul Rademacher</li> </ul>"},{"location":"demos/sample_scatter_plots/","title":"Plotting Points Manually","text":"In\u00a0[14]: Copied! <pre>from qmcpy import *\nfrom copy import deepcopy\nfrom numpy import ceil, linspace, meshgrid, zeros, array, arange, random\nfrom mpl_toolkits.mplot3d.axes3d import Axes3D\n\nimport matplotlib\n%matplotlib inline\nimport matplotlib.pyplot as plt\n\nplt.rc('font', size=16)          # controls default text sizes\nplt.rc('axes', titlesize=16)     # fontsize of the axes title\nplt.rc('axes', labelsize=16)    # fontsize of the x and y labels\nplt.rc('xtick', labelsize=16)    # fontsize of the tick labels\nplt.rc('ytick', labelsize=16)    # fontsize of the tick labels\nplt.rc('legend', fontsize=16)    # legend fontsize\nplt.rc('figure', titlesize=16)  # fontsize of the figure title\n</pre> from qmcpy import * from copy import deepcopy from numpy import ceil, linspace, meshgrid, zeros, array, arange, random from mpl_toolkits.mplot3d.axes3d import Axes3D  import matplotlib %matplotlib inline import matplotlib.pyplot as plt  plt.rc('font', size=16)          # controls default text sizes plt.rc('axes', titlesize=16)     # fontsize of the axes title plt.rc('axes', labelsize=16)    # fontsize of the x and y labels plt.rc('xtick', labelsize=16)    # fontsize of the tick labels plt.rc('ytick', labelsize=16)    # fontsize of the tick labels plt.rc('legend', fontsize=16)    # legend fontsize plt.rc('figure', titlesize=16)  # fontsize of the figure title In\u00a0[15]: Copied! <pre>n = 128\n</pre> n = 128 In\u00a0[16]: Copied! <pre>random.seed(7)\ndiscrete_distribs = [\n    IIDStdUniform(dimension=2, seed=7),\n    #IIDStdGaussian(dimension=2, seed=7),\n    #CustomIIDDistribution(lambda n: random.exponential(scale=2./3,size=(n,2)))\n]\ndd_names = [\"$\\\\mathcal{U}_2\\\\,(0,1)$\", \"$\\\\mathcal{N}_2\\\\,(0,1)$\", \"Exp(1.5)\"]\ncolors = [\"b\", \"r\", \"g\"]\nlims = [[0, 1], [-2.5, 2.5],[0,4]]\nfig, ax = plt.subplots(nrows=1, ncols=1, figsize=(16, 6))\nfor i, (dd_obj, color, lim, dd_name) in enumerate(zip(discrete_distribs, colors, lims, dd_names)):\n    samples = dd_obj.gen_samples(n)\n    ax.scatter(samples[:, 0], samples[:, 1], color=color)\n    ax.set_xlabel(\"$x_1$\")\n    ax.set_ylabel(\"$x_2$\")\n    ax.set_xlim(lim)\n    ax.set_ylim(lim)\n    ax.set_aspect(\"equal\")\n    ax.set_title(dd_name)\nfig.suptitle(\"IID Discrete Distributions\");\n</pre> random.seed(7) discrete_distribs = [     IIDStdUniform(dimension=2, seed=7),     #IIDStdGaussian(dimension=2, seed=7),     #CustomIIDDistribution(lambda n: random.exponential(scale=2./3,size=(n,2))) ] dd_names = [\"$\\\\mathcal{U}_2\\\\,(0,1)$\", \"$\\\\mathcal{N}_2\\\\,(0,1)$\", \"Exp(1.5)\"] colors = [\"b\", \"r\", \"g\"] lims = [[0, 1], [-2.5, 2.5],[0,4]] fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(16, 6)) for i, (dd_obj, color, lim, dd_name) in enumerate(zip(discrete_distribs, colors, lims, dd_names)):     samples = dd_obj.gen_samples(n)     ax.scatter(samples[:, 0], samples[:, 1], color=color)     ax.set_xlabel(\"$x_1$\")     ax.set_ylabel(\"$x_2$\")     ax.set_xlim(lim)     ax.set_ylim(lim)     ax.set_aspect(\"equal\")     ax.set_title(dd_name) fig.suptitle(\"IID Discrete Distributions\"); In\u00a0[17]: Copied! <pre>discrete_distribs = [\n    Lattice(dimension=2, randomize=True, seed=7),\n    Sobol(dimension=2, randomize=True, seed=7),\n    Halton(dimension=2,seed=7)]\ndd_names = [\"Shifted Lattice\", \"Scrambled Sobol\", \"Generalized Halton\", \"Randomized Korobov\"]\ncolors = [\"g\", \"c\", \"m\", \"r\"]\nfig, ax = plt.subplots(nrows=1, ncols=3, figsize=(16, 6))\nfor i, (dd_obj, color, dd_name) in \\\n        enumerate(zip(discrete_distribs, colors, dd_names)):\n    samples = dd_obj.gen_samples(n)\n    ax[i].scatter(samples[:, 0], samples[:, 1], color=color)\n    ax[i].set_xlabel(\"$x_1$\")\n    ax[i].set_ylabel(\"$x_2$\")\n    ax[i].set_xlim([0, 1])\n    ax[i].set_ylim([0, 1])\n    ax[i].set_aspect(\"equal\")\n    ax[i].set_title(dd_name)\nfig.suptitle(\"Low Discrepancy Discrete Distributions\")\nplt.tight_layout();\n</pre> discrete_distribs = [     Lattice(dimension=2, randomize=True, seed=7),     Sobol(dimension=2, randomize=True, seed=7),     Halton(dimension=2,seed=7)] dd_names = [\"Shifted Lattice\", \"Scrambled Sobol\", \"Generalized Halton\", \"Randomized Korobov\"] colors = [\"g\", \"c\", \"m\", \"r\"] fig, ax = plt.subplots(nrows=1, ncols=3, figsize=(16, 6)) for i, (dd_obj, color, dd_name) in \\         enumerate(zip(discrete_distribs, colors, dd_names)):     samples = dd_obj.gen_samples(n)     ax[i].scatter(samples[:, 0], samples[:, 1], color=color)     ax[i].set_xlabel(\"$x_1$\")     ax[i].set_ylabel(\"$x_2$\")     ax[i].set_xlim([0, 1])     ax[i].set_ylim([0, 1])     ax[i].set_aspect(\"equal\")     ax[i].set_title(dd_name) fig.suptitle(\"Low Discrepancy Discrete Distributions\") plt.tight_layout(); In\u00a0[18]: Copied! <pre>def plot_tm_transformed(tm_name, color, lim, measure, **kwargs):\n    fig, ax = plt.subplots(nrows=1, ncols=4, figsize=(20, 6))\n    i = 0\n    # IID Distributions\n    iid_distribs = [\n        IIDStdUniform(dimension=2, seed=7),\n        #IIDStdGaussian(dimension=2, seed=7)\n    ]\n    iid_names = [\n        \"IID $\\\\mathcal{U}\\\\,(0,1)^2$\",\n        \"IID $\\\\mathcal{N}\\\\,(0,1)^2$\"]\n    for distrib, distrib_name in zip(iid_distribs, iid_names):\n        measure_obj = measure(distrib, **kwargs)\n        samples = measure_obj.gen_samples(n)\n        ax[i].scatter(samples[:, 0], samples[:, 1], color=color)\n        i += 1\n    # Quasi Random Distributions\n    qrng_distribs = [\n        Lattice(dimension=2, randomize=True, seed=7),\n        Sobol(dimension=2, randomize=True, seed=7),\n        Halton(dimension=2, randomize=True, seed=7)]\n    qrng_names = [\"Shifted Lattice\",\n                  \"Scrambled Sobol\",\n                  \"Randomized Halton\"]\n    for distrib, distrib_name in zip(qrng_distribs, qrng_names):\n        measure_obj = measure(distrib, **kwargs)\n        samples = measure_obj.gen_samples(n_min=0,n_max=n)\n        ax[i].scatter(samples[:, 0], samples[:, 1], color=color)\n        i += 1\n    # Plot Metas\n    for i,distrib in enumerate(iid_distribs+qrng_distribs):\n        ax[i].set_xlabel(\"$x_1$\")\n        if i==0:\n            ax[i].set_ylabel(\"$x_2$\")\n        else:\n            ax[i].set_yticks([])\n        ax[i].set_xlim(lim)\n        ax[i].set_ylim(lim)\n        ax[i].set_aspect(\"equal\")\n        ax[i].set_title(type(distrib).__name__)\n    fig.suptitle(\"Transformed to %s from...\" % tm_name)\n    plt.tight_layout()\n    prefix = type(measure_obj).__name__;\n</pre> def plot_tm_transformed(tm_name, color, lim, measure, **kwargs):     fig, ax = plt.subplots(nrows=1, ncols=4, figsize=(20, 6))     i = 0     # IID Distributions     iid_distribs = [         IIDStdUniform(dimension=2, seed=7),         #IIDStdGaussian(dimension=2, seed=7)     ]     iid_names = [         \"IID $\\\\mathcal{U}\\\\,(0,1)^2$\",         \"IID $\\\\mathcal{N}\\\\,(0,1)^2$\"]     for distrib, distrib_name in zip(iid_distribs, iid_names):         measure_obj = measure(distrib, **kwargs)         samples = measure_obj.gen_samples(n)         ax[i].scatter(samples[:, 0], samples[:, 1], color=color)         i += 1     # Quasi Random Distributions     qrng_distribs = [         Lattice(dimension=2, randomize=True, seed=7),         Sobol(dimension=2, randomize=True, seed=7),         Halton(dimension=2, randomize=True, seed=7)]     qrng_names = [\"Shifted Lattice\",                   \"Scrambled Sobol\",                   \"Randomized Halton\"]     for distrib, distrib_name in zip(qrng_distribs, qrng_names):         measure_obj = measure(distrib, **kwargs)         samples = measure_obj.gen_samples(n_min=0,n_max=n)         ax[i].scatter(samples[:, 0], samples[:, 1], color=color)         i += 1     # Plot Metas     for i,distrib in enumerate(iid_distribs+qrng_distribs):         ax[i].set_xlabel(\"$x_1$\")         if i==0:             ax[i].set_ylabel(\"$x_2$\")         else:             ax[i].set_yticks([])         ax[i].set_xlim(lim)         ax[i].set_ylim(lim)         ax[i].set_aspect(\"equal\")         ax[i].set_title(type(distrib).__name__)     fig.suptitle(\"Transformed to %s from...\" % tm_name)     plt.tight_layout()     prefix = type(measure_obj).__name__; In\u00a0[19]: Copied! <pre>plot_tm_transformed(\"$\\\\mathcal{U}\\\\,(0,1)^2$\",\"b\",[0, 1],Uniform)\n</pre> plot_tm_transformed(\"$\\\\mathcal{U}\\\\,(0,1)^2$\",\"b\",[0, 1],Uniform) In\u00a0[20]: Copied! <pre>plot_tm_transformed(\"$\\\\mathcal{N}\\\\,(0,1)^2$\",\"r\",[-2.5, 2.5],Gaussian)\n</pre> plot_tm_transformed(\"$\\\\mathcal{N}\\\\,(0,1)^2$\",\"r\",[-2.5, 2.5],Gaussian) In\u00a0[21]: Copied! <pre>plot_tm_transformed(\"Discretized BrownianMotion with time_vector = [.5 , 1]\",\n                   \"g\",[-2.5, 2.5],BrownianMotion)\n</pre> plot_tm_transformed(\"Discretized BrownianMotion with time_vector = [.5 , 1]\",                    \"g\",[-2.5, 2.5],BrownianMotion) In\u00a0[22]: Copied! <pre>fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(12, 7))\nu1_a, u1_b = 2, 4\nu2_a, u2_b = 6, 8\ng1_mu, g1_var = 3, 9\ng2_mu, g2_var = 7, 9\ng_cov = 5\ndistribution = Sobol(dimension=2, randomize=True, seed=7)\nuniform_measure = Uniform(distribution,lower_bound=[u1_a, u2_a],upper_bound=[u1_b, u2_b])\ngaussian_measure = Gaussian(distribution,mean=[g1_mu, g2_mu],covariance=[[g1_var, g_cov],[g_cov,g2_var]])\n# Generate Samples and Create Scatter Plots\nfor i, (measure, color) in enumerate(zip([uniform_measure, gaussian_measure], [\"m\", \"y\"])):\n    samples = measure.gen_samples(n)\n    ax[i].scatter(samples[:, 0], samples[:, 1], color=color)\n# Plot Metas\nfor i in range(2):\n    ax[i].set_xlabel(\"$x_1$\")\n    ax[i].set_ylabel(\"$x_2$\")\n    ax[i].set_aspect(\"equal\")\nax[0].set_xlim([u1_a, u1_b])\nax[0].set_ylim([u2_a, u2_b])\nspread_g1 = ceil(3 * g1_var**.5)\nspread_g2 = ceil(3 * g2_var**.5)\nax[1].set_xlim([g1_mu - spread_g1, g1_mu + spread_g1])\nax[1].set_ylim([g2_mu - spread_g2, g2_mu + spread_g2])\nfig.suptitle(\"Shifted and Stretched Sobol Samples\")\nplt.tight_layout();\n</pre> fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(12, 7)) u1_a, u1_b = 2, 4 u2_a, u2_b = 6, 8 g1_mu, g1_var = 3, 9 g2_mu, g2_var = 7, 9 g_cov = 5 distribution = Sobol(dimension=2, randomize=True, seed=7) uniform_measure = Uniform(distribution,lower_bound=[u1_a, u2_a],upper_bound=[u1_b, u2_b]) gaussian_measure = Gaussian(distribution,mean=[g1_mu, g2_mu],covariance=[[g1_var, g_cov],[g_cov,g2_var]]) # Generate Samples and Create Scatter Plots for i, (measure, color) in enumerate(zip([uniform_measure, gaussian_measure], [\"m\", \"y\"])):     samples = measure.gen_samples(n)     ax[i].scatter(samples[:, 0], samples[:, 1], color=color) # Plot Metas for i in range(2):     ax[i].set_xlabel(\"$x_1$\")     ax[i].set_ylabel(\"$x_2$\")     ax[i].set_aspect(\"equal\") ax[0].set_xlim([u1_a, u1_b]) ax[0].set_ylim([u2_a, u2_b]) spread_g1 = ceil(3 * g1_var**.5) spread_g2 = ceil(3 * g2_var**.5) ax[1].set_xlim([g1_mu - spread_g1, g1_mu + spread_g1]) ax[1].set_ylim([g2_mu - spread_g2, g2_mu + spread_g2]) fig.suptitle(\"Shifted and Stretched Sobol Samples\") plt.tight_layout(); In\u00a0[23]: Copied! <pre>abs_tol = .3\nintegrand = Keister(IIDStdUniform(dimension=2, seed=7))\nsolution,data = CubMCCLT(integrand,abs_tol=abs_tol,rel_tol=0,n_init=16).integrate()\nprint(data)\n</pre> abs_tol = .3 integrand = Keister(IIDStdUniform(dimension=2, seed=7)) solution,data = CubMCCLT(integrand,abs_tol=abs_tol,rel_tol=0,n_init=16).integrate() print(data) <pre>Data (Data)\n    solution        1.680\n    bound_low       1.222\n    bound_high      2.137\n    bound_diff      0.915\n    n_total         63\n    time_integrate  0.001\nCubMCCLT (AbstractStoppingCriterion)\n    abs_tol         0.300\n    rel_tol         0\n    n_init          2^(4)\n    n_limit         2^(30)\n    inflate         1.200\n    alpha           0.010\nKeister (AbstractIntegrand)\nGaussian (AbstractTrueMeasure)\n    mean            0\n    covariance      2^(-1)\n    decomp_type     PCA\nIIDStdUniform (AbstractIIDDiscreteDistribution)\n    d               2^(1)\n    replications    1\n    entropy         7\n</pre> In\u00a0[24]: Copied! <pre># Constants based on running the above CLT Example\neps_list = [.5, .4, .3]\nn_list = [66, 84, 125]\nmu_hat_list = [1.8757, 1.8057, 1.8829]\n# Function Points\nnx, ny = (99, 99)\npoints_fun = zeros((nx * ny, 3))\nx = arange(.01, 1, .01)\ny = arange(.01, 1, .01)\nx_2d, y_2d = meshgrid(x, y)\npoints_fun[:, 0] = x_2d.flatten()\npoints_fun[:, 1] = y_2d.flatten()\npoints_fun[:, 2] = integrand.f(points_fun[:, :2]).squeeze()\nx_surf = points_fun[:, 0].reshape((nx, ny))\ny_surf = points_fun[:, 1].reshape((nx, ny))\nz_surf = points_fun[:, 2].reshape((nx, ny))\n# 3D Plot\nfig = plt.figure(figsize=(25, 8))\nax1 = fig.add_subplot(131, projection=\"3d\")\nax2 = fig.add_subplot(132, projection=\"3d\")\nax3 = fig.add_subplot(133, projection=\"3d\")\nfor idx, ax in enumerate([ax1, ax2, ax3]):\n    n = n_list[idx]\n    epsilon = eps_list[idx]\n    mu = mu_hat_list[idx]\n    # Surface\n    ax.plot_surface(x_surf, y_surf, z_surf, cmap=\"winter\", alpha=.2)\n    # Scatters\n    points = zeros((n, 3))\n    points[:, :2] = integrand.discrete_distrib.gen_samples(n)\n    points[:, 2] = integrand.f(points[:, :2]).squeeze()\n    ax.scatter(points[:, 0], points[:, 1], points[:, 2], color=\"r\", s=5)\n    ax.scatter(points[:, 0], points[:, 1], points[:, 2], color=\"r\", s=5)\n    ax.set_title(\"\\t$\\\\epsilon$ = %-7.1f $n$ = %-7d $\\\\hat{\\\\mu}$ = %-7.2f \"\n                 % (epsilon, n, mu), fontdict={\"fontsize\": 16})\n    # axis metas\n    n *= 2\n    ax.grid(False)\n    ax.xaxis.pane.set_edgecolor(\"black\")\n    ax.yaxis.pane.set_edgecolor(\"black\")\n    ax.set_xlabel(\"$x_1$\", fontdict={\"fontsize\": 16})\n    ax.set_ylabel(\"$x_2$\", fontdict={\"fontsize\": 16})\n    ax.set_zlabel(\"$f\\\\:(x_1,x_2)$\", fontdict={\"fontsize\": 16})\n    ax.view_init(20, 45);\n</pre> # Constants based on running the above CLT Example eps_list = [.5, .4, .3] n_list = [66, 84, 125] mu_hat_list = [1.8757, 1.8057, 1.8829] # Function Points nx, ny = (99, 99) points_fun = zeros((nx * ny, 3)) x = arange(.01, 1, .01) y = arange(.01, 1, .01) x_2d, y_2d = meshgrid(x, y) points_fun[:, 0] = x_2d.flatten() points_fun[:, 1] = y_2d.flatten() points_fun[:, 2] = integrand.f(points_fun[:, :2]).squeeze() x_surf = points_fun[:, 0].reshape((nx, ny)) y_surf = points_fun[:, 1].reshape((nx, ny)) z_surf = points_fun[:, 2].reshape((nx, ny)) # 3D Plot fig = plt.figure(figsize=(25, 8)) ax1 = fig.add_subplot(131, projection=\"3d\") ax2 = fig.add_subplot(132, projection=\"3d\") ax3 = fig.add_subplot(133, projection=\"3d\") for idx, ax in enumerate([ax1, ax2, ax3]):     n = n_list[idx]     epsilon = eps_list[idx]     mu = mu_hat_list[idx]     # Surface     ax.plot_surface(x_surf, y_surf, z_surf, cmap=\"winter\", alpha=.2)     # Scatters     points = zeros((n, 3))     points[:, :2] = integrand.discrete_distrib.gen_samples(n)     points[:, 2] = integrand.f(points[:, :2]).squeeze()     ax.scatter(points[:, 0], points[:, 1], points[:, 2], color=\"r\", s=5)     ax.scatter(points[:, 0], points[:, 1], points[:, 2], color=\"r\", s=5)     ax.set_title(\"\\t$\\\\epsilon$ = %-7.1f $n$ = %-7d $\\\\hat{\\\\mu}$ = %-7.2f \"                  % (epsilon, n, mu), fontdict={\"fontsize\": 16})     # axis metas     n *= 2     ax.grid(False)     ax.xaxis.pane.set_edgecolor(\"black\")     ax.yaxis.pane.set_edgecolor(\"black\")     ax.set_xlabel(\"$x_1$\", fontdict={\"fontsize\": 16})     ax.set_ylabel(\"$x_2$\", fontdict={\"fontsize\": 16})     ax.set_zlabel(\"$f\\\\:(x_1,x_2)$\", fontdict={\"fontsize\": 16})     ax.view_init(20, 45); In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"demos/sample_scatter_plots/#scatter-plots-of-samples","title":"Scatter Plots of Samples\u00b6","text":""},{"location":"demos/sample_scatter_plots/#iid-samples","title":"IID Samples\u00b6","text":""},{"location":"demos/sample_scatter_plots/#ld-samples","title":"LD Samples\u00b6","text":""},{"location":"demos/sample_scatter_plots/#transform-to-the-true-distribution","title":"Transform to the True Distribution\u00b6","text":"<p>Transform samples from a few discrete distributions to mimic various measures</p>"},{"location":"demos/sample_scatter_plots/#shift-and-stretch-the-true-distribution","title":"Shift and Stretch the True Distribution\u00b6","text":"<p>Transform Sobol sequences to mimic non-standard Uniform and Gaussian measures</p>"},{"location":"demos/sample_scatter_plots/#plots-samples-on-a-2d-keister-function","title":"Plots samples on a 2D Keister function\u00b6","text":""},{"location":"demos/some_true_measures/","title":"Some True Measures","text":"In\u00a0[1]: Copied! <pre>from qmcpy import *\nfrom scipy.special import gamma\nfrom numpy import *\n</pre> from qmcpy import * from scipy.special import gamma from numpy import * In\u00a0[2]: Copied! <pre>import matplotlib\nfrom matplotlib import pyplot\n%matplotlib inline\npyplot.rc('font', size=16)          # controls default text sizes\npyplot.rc('axes', titlesize=16)     # fontsize of the axes title\npyplot.rc('axes', labelsize=16)     # fontsize of the x and y labels\npyplot.rc('xtick', labelsize=16)    # fontsize of the tick labels\npyplot.rc('ytick', labelsize=16)    # fontsize of the tick labels\npyplot.rc('legend', fontsize=16)    # legend fontsize\npyplot.rc('figure', titlesize=16)   # fontsize of the figure title\n</pre> import matplotlib from matplotlib import pyplot %matplotlib inline pyplot.rc('font', size=16)          # controls default text sizes pyplot.rc('axes', titlesize=16)     # fontsize of the axes title pyplot.rc('axes', labelsize=16)     # fontsize of the x and y labels pyplot.rc('xtick', labelsize=16)    # fontsize of the tick labels pyplot.rc('ytick', labelsize=16)    # fontsize of the tick labels pyplot.rc('legend', fontsize=16)    # legend fontsize pyplot.rc('figure', titlesize=16)   # fontsize of the figure title In\u00a0[3]: Copied! <pre>def plt_1d(tm,lim,ax):\n    print(tm)\n    n_mesh = 100\n    ticks = linspace(*lim,n_mesh)\n    y = tm._weight(ticks[:,None])\n    ax.plot(ticks,y)\n    ax.set_xlim(lim)\n    ax.set_xlabel(\"$T$\")\n    ax.set_xticks(lim)    \n    ax.set_title(type(tm).__name__)\n</pre> def plt_1d(tm,lim,ax):     print(tm)     n_mesh = 100     ticks = linspace(*lim,n_mesh)     y = tm._weight(ticks[:,None])     ax.plot(ticks,y)     ax.set_xlim(lim)     ax.set_xlabel(\"$T$\")     ax.set_xticks(lim)         ax.set_title(type(tm).__name__) In\u00a0[4]: Copied! <pre>fig,ax = pyplot.subplots(figsize=(23,6),nrows=1,ncols=3)\nsobol = Sobol(1)\nkumaraswamy = Kumaraswamy(sobol,a=2,b=4)\nplt_1d(kumaraswamy,lim=[0,1],ax=ax[0])\nbern = BernoulliCont(sobol,lam=.75)\nplt_1d(bern,lim=[0,1],ax=ax[1])\njsu = JohnsonsSU(sobol,gamma=1,xi=2,delta=1,lam=2)\nplt_1d(jsu,lim=[-2,2],ax=ax[2])\nax[0].set_ylabel(\"Density\");\n</pre> fig,ax = pyplot.subplots(figsize=(23,6),nrows=1,ncols=3) sobol = Sobol(1) kumaraswamy = Kumaraswamy(sobol,a=2,b=4) plt_1d(kumaraswamy,lim=[0,1],ax=ax[0]) bern = BernoulliCont(sobol,lam=.75) plt_1d(bern,lim=[0,1],ax=ax[1]) jsu = JohnsonsSU(sobol,gamma=1,xi=2,delta=1,lam=2) plt_1d(jsu,lim=[-2,2],ax=ax[2]) ax[0].set_ylabel(\"Density\"); <pre>Kumaraswamy (AbstractTrueMeasure)\n    a               2^(1)\n    b               2^(2)\nBernoulliCont (AbstractTrueMeasure)\n    lam             0.750\nJohnsonsSU (AbstractTrueMeasure)\n    gamma           1\n    xi              2^(1)\n    delta           1\n    lam             2^(1)\n</pre> In\u00a0[5]: Copied! <pre>def plt_2d(tm,n,lim,ax):\n    print(tm)\n    n_mesh = 502\n    # Points\n    t = tm.gen_samples(n)\n    # PDF\n    mesh = zeros(((n_mesh)**2,3),dtype=float)\n    grid_tics = linspace(*lim,n_mesh)\n    x_mesh,y_mesh = meshgrid(grid_tics,grid_tics)\n    mesh[:,0] = x_mesh.flatten()\n    mesh[:,1] = y_mesh.flatten()\n    mesh[:,2] = tm._weight(mesh[:,:2])\n    z_mesh = mesh[:,2].reshape((n_mesh,n_mesh))\n    #   colors \n    clevel = arange(mesh[:,2].min(),mesh[:,2].max(),.025)\n    cmap = matplotlib.colors.LinearSegmentedColormap.from_list(\"\", [(.95,.95,.95),(0,0,1)]) \n    # cmap = pyplot.get_cmap('Blues')\n    #   contour + scatter plot\n    ax.contourf(x_mesh,y_mesh,z_mesh,clevel,cmap=cmap,extend='both')\n    ax.scatter(t[:,0],t[:,1],s=5,color='w')\n    #   axis\n    for nsew in ['top','bottom','left','right']: ax.spines[nsew].set_visible(False)\n    ax.xaxis.set_ticks_position('none') \n    ax.yaxis.set_ticks_position('none') \n    ax.set_aspect(1)\n    ax.set_xlim(lim)\n    ax.set_xticks(lim)\n    ax.set_ylim(lim)\n    ax.set_yticks(lim)\n    #   labels\n    ax.set_xlabel('$T_1$')\n    ax.set_ylabel('$T_2$')\n    ax.set_title('%s PDF and Random Samples'%type(tm).__name__)\n    #   metas\n    fig.tight_layout()\n</pre> def plt_2d(tm,n,lim,ax):     print(tm)     n_mesh = 502     # Points     t = tm.gen_samples(n)     # PDF     mesh = zeros(((n_mesh)**2,3),dtype=float)     grid_tics = linspace(*lim,n_mesh)     x_mesh,y_mesh = meshgrid(grid_tics,grid_tics)     mesh[:,0] = x_mesh.flatten()     mesh[:,1] = y_mesh.flatten()     mesh[:,2] = tm._weight(mesh[:,:2])     z_mesh = mesh[:,2].reshape((n_mesh,n_mesh))     #   colors      clevel = arange(mesh[:,2].min(),mesh[:,2].max(),.025)     cmap = matplotlib.colors.LinearSegmentedColormap.from_list(\"\", [(.95,.95,.95),(0,0,1)])      # cmap = pyplot.get_cmap('Blues')     #   contour + scatter plot     ax.contourf(x_mesh,y_mesh,z_mesh,clevel,cmap=cmap,extend='both')     ax.scatter(t[:,0],t[:,1],s=5,color='w')     #   axis     for nsew in ['top','bottom','left','right']: ax.spines[nsew].set_visible(False)     ax.xaxis.set_ticks_position('none')      ax.yaxis.set_ticks_position('none')      ax.set_aspect(1)     ax.set_xlim(lim)     ax.set_xticks(lim)     ax.set_ylim(lim)     ax.set_yticks(lim)     #   labels     ax.set_xlabel('$T_1$')     ax.set_ylabel('$T_2$')     ax.set_title('%s PDF and Random Samples'%type(tm).__name__)     #   metas     fig.tight_layout() In\u00a0[6]: Copied! <pre>fig,ax = pyplot.subplots(figsize=(18,6),nrows=1,ncols=3)\nsobol = Sobol(2)\nkumaraswamy = Kumaraswamy(sobol,a=[2,3],b=[3,5])\nplt_2d(kumaraswamy,n=2**8,lim=[0,1],ax=ax[0])\nbern = BernoulliCont(sobol,lam=[.25,.75])\nplt_2d(bern,n=2**8,lim=[0,1],ax=ax[1])\njsu = JohnsonsSU(sobol,gamma=[1,1],xi=[1,1],delta=[1,1],lam=[1,1])\nplt_2d(jsu,n=2**8,lim=[-2,2],ax=ax[2])\n</pre> fig,ax = pyplot.subplots(figsize=(18,6),nrows=1,ncols=3) sobol = Sobol(2) kumaraswamy = Kumaraswamy(sobol,a=[2,3],b=[3,5]) plt_2d(kumaraswamy,n=2**8,lim=[0,1],ax=ax[0]) bern = BernoulliCont(sobol,lam=[.25,.75]) plt_2d(bern,n=2**8,lim=[0,1],ax=ax[1]) jsu = JohnsonsSU(sobol,gamma=[1,1],xi=[1,1],delta=[1,1],lam=[1,1]) plt_2d(jsu,n=2**8,lim=[-2,2],ax=ax[2]) <pre>Kumaraswamy (AbstractTrueMeasure)\n    a               [2 3]\n    b               [3 5]\nBernoulliCont (AbstractTrueMeasure)\n    lam             [0.25 0.75]\nJohnsonsSU (AbstractTrueMeasure)\n    gamma           [1 1]\n    xi              [1 1]\n    delta           [1 1]\n    lam             [1 1]\n</pre> In\u00a0[9]: Copied! <pre>def compute_expected_val(tm,true_value,abs_tol):\n    if tm.d!=1: raise Exception(\"tm must be 1 dimensional for this test\")\n    cf = CustomFun(tm, g=lambda x: x[...,0])\n    sol,data = CubQMCSobolG(cf,abs_tol=abs_tol).integrate()\n    error = abs(true_value-sol)\n    if error&gt;abs_tol: \n        raise Exception(\"QMC error %.3f not within tolerance.\"%error)\n    print(\"%s integration within tolerance\"%type(tm).__name__)\n    print(\"\\tEstimated mean: %.4f\"%sol)\n    print(\"\\t%s\"%str(tm).replace('\\n','\\n\\t'))\n</pre> def compute_expected_val(tm,true_value,abs_tol):     if tm.d!=1: raise Exception(\"tm must be 1 dimensional for this test\")     cf = CustomFun(tm, g=lambda x: x[...,0])     sol,data = CubQMCSobolG(cf,abs_tol=abs_tol).integrate()     error = abs(true_value-sol)     if error&gt;abs_tol:          raise Exception(\"QMC error %.3f not within tolerance.\"%error)     print(\"%s integration within tolerance\"%type(tm).__name__)     print(\"\\tEstimated mean: %.4f\"%sol)     print(\"\\t%s\"%str(tm).replace('\\n','\\n\\t')) In\u00a0[10]: Copied! <pre>abs_tol = 1e-5\n# kumaraswamy \na,b = 2,6\nkuma = Kumaraswamy(Sobol(1),a,b)\nkuma_tv = b*gamma(1+1/a)*gamma(b)/gamma(1+1/a+b)\ncompute_expected_val(kuma,kuma_tv,abs_tol)\n# Continuous Bernoulli\nlam = .75\nbern = BernoulliCont(Sobol(1),lam=lam)\nbern_tv = 1/2 if lam==1/2 else lam/(2*lam-1)+1/(2*arctanh(1-2*lam))\ncompute_expected_val(bern,bern_tv,abs_tol)\n# Johnson's SU\n_gamma,xi,delta,lam = 1,2,3,4\njsu = JohnsonsSU(Sobol(1),gamma=_gamma,xi=xi,delta=delta,lam=lam)\njsu_tv = xi-lam*exp(1/(2*delta**2))*sinh(_gamma/delta)\ncompute_expected_val(jsu,jsu_tv,abs_tol)\n</pre> abs_tol = 1e-5 # kumaraswamy  a,b = 2,6 kuma = Kumaraswamy(Sobol(1),a,b) kuma_tv = b*gamma(1+1/a)*gamma(b)/gamma(1+1/a+b) compute_expected_val(kuma,kuma_tv,abs_tol) # Continuous Bernoulli lam = .75 bern = BernoulliCont(Sobol(1),lam=lam) bern_tv = 1/2 if lam==1/2 else lam/(2*lam-1)+1/(2*arctanh(1-2*lam)) compute_expected_val(bern,bern_tv,abs_tol) # Johnson's SU _gamma,xi,delta,lam = 1,2,3,4 jsu = JohnsonsSU(Sobol(1),gamma=_gamma,xi=xi,delta=delta,lam=lam) jsu_tv = xi-lam*exp(1/(2*delta**2))*sinh(_gamma/delta) compute_expected_val(jsu,jsu_tv,abs_tol) <pre>Kumaraswamy integration within tolerance\n\tEstimated mean: 0.3410\n\tKumaraswamy (AbstractTrueMeasure)\n\t    a               2^(1)\n\t    b               6\nBernoulliCont integration within tolerance\n\tEstimated mean: 0.5898\n\tBernoulliCont (AbstractTrueMeasure)\n\t    lam             0.750\nJohnsonsSU integration within tolerance\n\tEstimated mean: 0.5642\n\tJohnsonsSU (AbstractTrueMeasure)\n\t    gamma           1\n\t    xi              2^(1)\n\t    delta           3\n\t    lam             2^(2)\n</pre> In\u00a0[12]: Copied! <pre># compose with a Kumaraswamy transformation\nabs_tol = 1e-4\ncf = CustomFun(Uniform(Kumaraswamy(Sobol(1,seed=7),a=.5,b=.5)), g=lambda x: x[...,0])\nsol,data = CubQMCSobolG(cf,abs_tol=abs_tol).integrate() \nprint(data)\ntrue_val = .5 # expected value of standard uniform\nerror = abs(true_val-sol)\nif error&gt;abs_tol: raise Exception(\"QMC error %.3f not within tolerance.\"%error)\n</pre> # compose with a Kumaraswamy transformation abs_tol = 1e-4 cf = CustomFun(Uniform(Kumaraswamy(Sobol(1,seed=7),a=.5,b=.5)), g=lambda x: x[...,0]) sol,data = CubQMCSobolG(cf,abs_tol=abs_tol).integrate()  print(data) true_val = .5 # expected value of standard uniform error = abs(true_val-sol) if error&gt;abs_tol: raise Exception(\"QMC error %.3f not within tolerance.\"%error) <pre>Data (Data)\n    solution        0.500\n    comb_bound_low  0.500\n    comb_bound_high 0.500\n    comb_bound_diff 7.60e-05\n    comb_flags      1\n    n_total         2^(11)\n    n               2^(11)\n    time_integrate  0.004\nCubQMCNetG (AbstractStoppingCriterion)\n    abs_tol         1.00e-04\n    rel_tol         0\n    n_init          2^(10)\n    n_limit         2^(35)\nCustomFun (AbstractIntegrand)\nUniform (AbstractTrueMeasure)\n    lower_bound     0\n    upper_bound     1\n    transform       Kumaraswamy (AbstractTrueMeasure)\n                        a               2^(-1)\n                        b               2^(-1)\nDigitalNetB2 (AbstractLDDiscreteDistribution)\n    d               1\n    replications    1\n    randomize       LMS_DS\n    gen_mats_source joe_kuo.6.21201.txt\n    order           NATURAL\n    t               63\n    alpha           1\n    n_limit         2^(32)\n    entropy         7\n</pre> In\u00a0[14]: Copied! <pre># plot the above functions\nx = linspace(0,1,1000).reshape(-1,1)\nx = x[1:-1] # plotting locations\n# plot\nfig,ax = pyplot.subplots(ncols=2,figsize=(15,5))\n#    density\nrho = cf.true_measure.transform._weight(x)\ntfvals = cf.true_measure.transform._jacobian_transform_r(x,return_weights=False)\nax[0].hist(tfvals,density=True,bins=50,color='c')\nax[0].plot(x,rho,color='g',linewidth=2)\nax[0].set_title('Sampling density')\n#    functions\ngs = cf.g(x)\nfs = cf.f(x)\nax[1].plot(x,gs,color='r',label='g')\nax[1].plot(x,fs,color='b',label='f')\nax[1].legend()\nax[1].set_title('functions');\n# metas\nfor i in range(2):\n    ax[i].set_xlim([0,1])\n    ax[i].set_xticks([0,1])\n</pre> # plot the above functions x = linspace(0,1,1000).reshape(-1,1) x = x[1:-1] # plotting locations # plot fig,ax = pyplot.subplots(ncols=2,figsize=(15,5)) #    density rho = cf.true_measure.transform._weight(x) tfvals = cf.true_measure.transform._jacobian_transform_r(x,return_weights=False) ax[0].hist(tfvals,density=True,bins=50,color='c') ax[0].plot(x,rho,color='g',linewidth=2) ax[0].set_title('Sampling density') #    functions gs = cf.g(x) fs = cf.f(x) ax[1].plot(x,gs,color='r',label='g') ax[1].plot(x,fs,color='b',label='f') ax[1].legend() ax[1].set_title('functions'); # metas for i in range(2):     ax[i].set_xlim([0,1])     ax[i].set_xticks([0,1]) In\u00a0[15]: Copied! <pre># compose multiple Kumaraswamy distributions\nabs_tol = 1e-3\ndd = Sobol(1,seed=7)\nt1 = Kumaraswamy(dd,a=.5,b=.5) # first transformation\nt2 = Kumaraswamy(t1,a=5,b=2) # second transformation\ncf = CustomFun(Uniform(t2), g=lambda x:x[...,0])\nsol,data = CubQMCSobolG(cf,abs_tol=abs_tol).integrate() # expected value of standard uniform\nprint(data)\ntrue_val = .5\nerror = abs(true_val-sol)\nif error&gt;abs_tol: raise Exception(\"QMC error %.3f not within tolerance.\"%error)\n</pre> # compose multiple Kumaraswamy distributions abs_tol = 1e-3 dd = Sobol(1,seed=7) t1 = Kumaraswamy(dd,a=.5,b=.5) # first transformation t2 = Kumaraswamy(t1,a=5,b=2) # second transformation cf = CustomFun(Uniform(t2), g=lambda x:x[...,0]) sol,data = CubQMCSobolG(cf,abs_tol=abs_tol).integrate() # expected value of standard uniform print(data) true_val = .5 error = abs(true_val-sol) if error&gt;abs_tol: raise Exception(\"QMC error %.3f not within tolerance.\"%error) <pre>Data (Data)\n    solution        0.500\n    comb_bound_low  0.499\n    comb_bound_high 0.501\n    comb_bound_diff 0.002\n    comb_flags      1\n    n_total         2^(10)\n    n               2^(10)\n    time_integrate  0.002\nCubQMCNetG (AbstractStoppingCriterion)\n    abs_tol         0.001\n    rel_tol         0\n    n_init          2^(10)\n    n_limit         2^(35)\nCustomFun (AbstractIntegrand)\nUniform (AbstractTrueMeasure)\n    lower_bound     0\n    upper_bound     1\n    transform       Kumaraswamy (AbstractTrueMeasure)\n                        a               5\n                        b               2^(1)\n                        transform       Kumaraswamy (AbstractTrueMeasure)\n                                            a               2^(-1)\n                                            b               2^(-1)\nDigitalNetB2 (AbstractLDDiscreteDistribution)\n    d               1\n    replications    1\n    randomize       LMS_DS\n    gen_mats_source joe_kuo.6.21201.txt\n    order           NATURAL\n    t               63\n    alpha           1\n    n_limit         2^(32)\n    entropy         7\n</pre> In\u00a0[17]: Copied! <pre>x = linspace(0,1,1000).reshape(-1,1)\nx = x[1:-1] # plotting locations\n# plot\nfig,ax = pyplot.subplots(ncols=2,figsize=(15,5))\n#    density\ntfvals = cf.true_measure.transform._jacobian_transform_r(x,return_weights=False)\nax[0].hist(tfvals,density=True,bins=50,color='c')\nax[0].set_title('Sampling density')\n#    functions\ngs = cf.g(x)\nfs = cf.f(x)\nax[1].plot(x,gs,color='r',label='g')\nax[1].plot(x,fs,color='b',label='f')\nax[1].legend()\nfor i in range(2):\n    ax[i].set_xlim([0,1])\n    ax[i].set_xticks([0,1])\nax[1].set_title('functions');\n</pre> x = linspace(0,1,1000).reshape(-1,1) x = x[1:-1] # plotting locations # plot fig,ax = pyplot.subplots(ncols=2,figsize=(15,5)) #    density tfvals = cf.true_measure.transform._jacobian_transform_r(x,return_weights=False) ax[0].hist(tfvals,density=True,bins=50,color='c') ax[0].set_title('Sampling density') #    functions gs = cf.g(x) fs = cf.f(x) ax[1].plot(x,gs,color='r',label='g') ax[1].plot(x,fs,color='b',label='f') ax[1].legend() for i in range(2):     ax[i].set_xlim([0,1])     ax[i].set_xticks([0,1]) ax[1].set_title('functions'); In\u00a0[18]: Copied! <pre>abs_tol = 1e-4\nd = 1\n</pre> abs_tol = 1e-4 d = 1 In\u00a0[19]: Copied! <pre># standard method\nkeister_std = Keister(Sobol(d))\nsol_std,data_std = CubQMCSobolG(keister_std,abs_tol=abs_tol).integrate()\nprint(\"Standard method estimate of %.4f took %.2e seconds and %.2e samples\"%\\\n      (sol_std,data_std.time_integrate,data_std.n_total))\nprint(keister_std.true_measure)\n</pre> # standard method keister_std = Keister(Sobol(d)) sol_std,data_std = CubQMCSobolG(keister_std,abs_tol=abs_tol).integrate() print(\"Standard method estimate of %.4f took %.2e seconds and %.2e samples\"%\\       (sol_std,data_std.time_integrate,data_std.n_total)) print(keister_std.true_measure) <pre>Standard method estimate of 1.3804 took 1.71e-02 seconds and 1.64e+04 samples\nGaussian (AbstractTrueMeasure)\n    mean            0\n    covariance      2^(-1)\n    decomp_type     PCA\n</pre> In\u00a0[20]: Copied! <pre># Kumaraswamy importance sampling\ndd = Sobol(d,seed=7)\nt1 = Kumaraswamy(dd,a=.8,b=.8) # first transformation\nt2 = Gaussian(t1)\nkeister_kuma = Keister(t2)\nsol_kuma,data_kuma = CubQMCSobolG(keister_kuma,abs_tol=abs_tol).integrate() \nprint(\"Kumaraswamy IS estimate of %.4f took %.2e seconds and %.2e samples\"%\\\n      (sol_kuma,data_kuma.time_integrate,data_kuma.n_total))\nt_frac = data_kuma.time_integrate/data_std.time_integrate\nn_frac = data_kuma.n_total/data_std.n_total\nprint('That is %.1f%% of the time and %.1f%% of the samples compared to default keister.'%(t_frac*100,n_frac*100))\nprint(keister_kuma.true_measure)\n</pre> # Kumaraswamy importance sampling dd = Sobol(d,seed=7) t1 = Kumaraswamy(dd,a=.8,b=.8) # first transformation t2 = Gaussian(t1) keister_kuma = Keister(t2) sol_kuma,data_kuma = CubQMCSobolG(keister_kuma,abs_tol=abs_tol).integrate()  print(\"Kumaraswamy IS estimate of %.4f took %.2e seconds and %.2e samples\"%\\       (sol_kuma,data_kuma.time_integrate,data_kuma.n_total)) t_frac = data_kuma.time_integrate/data_std.time_integrate n_frac = data_kuma.n_total/data_std.n_total print('That is %.1f%% of the time and %.1f%% of the samples compared to default keister.'%(t_frac*100,n_frac*100)) print(keister_kuma.true_measure) <pre>Kumaraswamy IS estimate of 1.3804 took 4.00e-03 seconds and 2.05e+03 samples\nThat is 23.4% of the time and 12.5% of the samples compared to default keister.\nGaussian (AbstractTrueMeasure)\n    mean            0\n    covariance      2^(-1)\n    decomp_type     PCA\n    transform       Gaussian (AbstractTrueMeasure)\n                        mean            0\n                        covariance      1\n                        decomp_type     PCA\n                        transform       Kumaraswamy (AbstractTrueMeasure)\n                                            a               0.800\n                                            b               0.800\n</pre> In\u00a0[21]: Copied! <pre># Continuous Bernoulli importance sampling\ndd = Sobol(d,seed=7)\nt1 = BernoulliCont(dd,lam=.25) # first transformation\n#t2 = BernoulliCont(t1,lam=.75) # first transformation\nt3 = Gaussian(t1)\nkeister_cb = Keister(t3)\nsol_cb,data_cb = CubQMCSobolG(keister_cb,abs_tol=abs_tol).integrate() \nprint(\"Continuous Bernoulli IS estimate of %.4f took %.2e seconds and %.2e samples\"%\\\n      (sol_cb,data_cb.time_integrate,data_cb.n_total))\nt_frac = data_cb.time_integrate/data_std.time_integrate\nn_frac = data_cb.n_total/data_std.n_total\nprint('That is %.1f%% of the time and %.1f%% of the samples compared to default keister.'%(t_frac*100,n_frac*100))\nprint(keister_cb.true_measure)\n</pre> # Continuous Bernoulli importance sampling dd = Sobol(d,seed=7) t1 = BernoulliCont(dd,lam=.25) # first transformation #t2 = BernoulliCont(t1,lam=.75) # first transformation t3 = Gaussian(t1) keister_cb = Keister(t3) sol_cb,data_cb = CubQMCSobolG(keister_cb,abs_tol=abs_tol).integrate()  print(\"Continuous Bernoulli IS estimate of %.4f took %.2e seconds and %.2e samples\"%\\       (sol_cb,data_cb.time_integrate,data_cb.n_total)) t_frac = data_cb.time_integrate/data_std.time_integrate n_frac = data_cb.n_total/data_std.n_total print('That is %.1f%% of the time and %.1f%% of the samples compared to default keister.'%(t_frac*100,n_frac*100)) print(keister_cb.true_measure) <pre>Continuous Bernoulli IS estimate of 1.3804 took 1.28e-02 seconds and 8.19e+03 samples\nThat is 74.8% of the time and 50.0% of the samples compared to default keister.\nGaussian (AbstractTrueMeasure)\n    mean            0\n    covariance      2^(-1)\n    decomp_type     PCA\n    transform       Gaussian (AbstractTrueMeasure)\n                        mean            0\n                        covariance      1\n                        decomp_type     PCA\n                        transform       BernoulliCont (AbstractTrueMeasure)\n                                            lam             2^(-2)\n</pre> In\u00a0[22]: Copied! <pre>#  Kumaraswamy importance sampling\ndd = Sobol(d,seed=7)\nt1 = JohnsonsSU(dd,xi=2,delta=1,gamma=2,lam=1) # first transformation\nkeister_jsu = Keister(t1)\nsol_jsu,data_jsu = CubQMCSobolG(keister_jsu,abs_tol=abs_tol).integrate() \nprint(\"Kumaraswamy IS estimate of %.4f took %.2e seconds and %.2e samples\"%\\\n      (sol_jsu,data_jsu.time_integrate,data_jsu.n_total))\nt_frac = data_jsu.time_integrate/data_std.time_integrate\nn_frac = data_jsu.n_total/data_std.n_total\nprint('That is %.1f%% of the time and %.1f%% of the samples compared to default keister.'%(t_frac*100,n_frac*100))\nprint(keister_jsu.true_measure)\n</pre> #  Kumaraswamy importance sampling dd = Sobol(d,seed=7) t1 = JohnsonsSU(dd,xi=2,delta=1,gamma=2,lam=1) # first transformation keister_jsu = Keister(t1) sol_jsu,data_jsu = CubQMCSobolG(keister_jsu,abs_tol=abs_tol).integrate()  print(\"Kumaraswamy IS estimate of %.4f took %.2e seconds and %.2e samples\"%\\       (sol_jsu,data_jsu.time_integrate,data_jsu.n_total)) t_frac = data_jsu.time_integrate/data_std.time_integrate n_frac = data_jsu.n_total/data_std.n_total print('That is %.1f%% of the time and %.1f%% of the samples compared to default keister.'%(t_frac*100,n_frac*100)) print(keister_jsu.true_measure) <pre>Kumaraswamy IS estimate of 1.3804 took 9.99e-03 seconds and 8.19e+03 samples\nThat is 58.4% of the time and 50.0% of the samples compared to default keister.\nGaussian (AbstractTrueMeasure)\n    mean            0\n    covariance      2^(-1)\n    decomp_type     PCA\n    transform       JohnsonsSU (AbstractTrueMeasure)\n                        gamma           2^(1)\n                        xi              2^(1)\n                        delta           1\n                        lam             1\n</pre> In\u00a0[23]: Copied! <pre>x = linspace(0,1,1000).reshape(-1,1)\nx = x[1:-1] # plotting locations\n# plot\nfig,ax = pyplot.subplots(figsize=(10,8))\n#    functions\nfs = [keister_std.f,keister_kuma.f,keister_cb.f,keister_jsu.f]\nlabels = ['Default Keister','Kuma Keister','Cont. Bernoulli Keister',\"Johnson's SU Keister\"]\ncolors = ['m','c','r','g']\nfor f,label,color in zip(fs,labels,colors): ax.plot(x,f(x),color=color,label=label)\nax.legend()\nax.set_xlim([0,1])\nax.set_xticks([0,1])\nax.set_title('functions');\n</pre> x = linspace(0,1,1000).reshape(-1,1) x = x[1:-1] # plotting locations # plot fig,ax = pyplot.subplots(figsize=(10,8)) #    functions fs = [keister_std.f,keister_kuma.f,keister_cb.f,keister_jsu.f] labels = ['Default Keister','Kuma Keister','Cont. Bernoulli Keister',\"Johnson's SU Keister\"] colors = ['m','c','r','g'] for f,label,color in zip(fs,labels,colors): ax.plot(x,f(x),color=color,label=label) ax.legend() ax.set_xlim([0,1]) ax.set_xticks([0,1]) ax.set_title('functions'); In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"demos/some_true_measures/#some-true-measures","title":"Some True Measures\u00b6","text":"<p>In this notebook we explore some of the new, lesser-known, <code>TrueMeasure</code> instances in QMCPy. Specifically, we look at the Kumaraswamy, Continuous Bernoulli, and Johnson's $S_U$ measures.</p>"},{"location":"demos/some_true_measures/#mathematics","title":"Mathematics\u00b6","text":"<p>Denote by $f$ the one-dimensional PDF, $F$ the one-dimensional CDF, and $\\Psi=F^{-1}$ the inverse CDF transform that takes samples mimicking $\\mathcal{U}[0,1]$ to mimic the desired one-dimensional true measure. For each of these true measures we assume the dimensions are independent, so the density and CDF and computed by taking the product across dimensions and the inverse transform is applied elementwise.</p> <p>Kumaraswamy</p> <p>Parameters $a,b&gt;0$ $$f(x) = a b x^{a-1}(1-x^{a})^{b-1}$$ $$F(x) = 1-(1-x^{a})^{b}$$ $$\\Psi(x) = (1-(1-x)^{1/b})^{1/a}$$ $$\\Psi'(x) = \\frac{\\left(1-\\left(1-x\\right)^{1/b}\\right)^{1/a-1}(1-x)^{1/b-1}}{ab}$$</p> <p>Continuous Bernoulli</p> <p>Parameter $\\lambda \\in (0,1)$</p> <p>If $\\lambda=1/2$, then $$f(x) = 2\\lambda^x(1-\\lambda)^{(1-x)}$$ $$F(x) = x$$ $$\\Psi(x) = x$$ $$\\Psi'(x) = 1$$</p> <p>If $\\lambda \\neq 1/2$, then $$f(x) = \\frac{2\\tanh^{-1}(1-2\\lambda)}{1-2\\lambda} \\lambda^x(1-\\lambda)^{(1-x)}$$ $$F(x) = \\frac{\\lambda^x(1-\\lambda)^{(1-x)}+\\lambda-1}{2\\lambda-1}$$ $$\\Psi(x) = \\log\\left(\\frac{(2\\lambda-1)x-\\lambda+1}{1-\\lambda}\\right) \\,/\\, \\log\\left(\\frac{\\lambda}{1-\\lambda}\\right)$$ $$\\Psi'(x) = \\frac{1}{\\log(\\lambda/(1-\\lambda))} \\cdot \\frac{2\\lambda-1}{(2\\lambda-1)x-\\lambda+1}$$</p> <p>Johnson's $S_U$</p> <p>Parameters $\\gamma,\\xi,\\delta&gt;0,\\lambda&gt;0$ $$f(x) = \\frac{\\delta\\exp\\left(-\\frac{1}{2}\\left(\\gamma+\\delta\\sinh^{-1}\\left(\\frac{x-\\xi}{\\lambda}\\right)\\right)^2\\right)}{\\lambda\\sqrt{2\\pi\\left(1+\\left(\\frac{x-\\xi}{\\lambda}\\right)^2\\right)}}$$ $$F(x) = \\Phi\\left(\\gamma+\\delta\\sinh^{-1}\\left(\\frac{x-\\xi}{\\lambda}\\right)\\right)$$ $$\\Psi(x) = \\lambda \\sinh\\left(\\frac{\\Phi^{-1}(x)-\\gamma}{\\delta}\\right) + \\xi$$ $$\\Psi'(x) = \\frac{\\lambda}{\\delta}\\cosh\\left(\\frac{\\Phi^{-1}(x)-\\gamma}{\\delta}\\right) \\,/\\, \\phi\\left(\\Phi^{-1}(x)\\right)$$ where $\\phi$ is the standard normal PDF and $\\Phi$ is the standard normal CDF.</p>"},{"location":"demos/some_true_measures/#imports","title":"Imports\u00b6","text":""},{"location":"demos/some_true_measures/#1d-density-plot","title":"1D Density Plot\u00b6","text":""},{"location":"demos/some_true_measures/#2d-density-plot","title":"2D Density Plot\u00b6","text":""},{"location":"demos/some_true_measures/#1d-expected-values","title":"1D Expected Values\u00b6","text":""},{"location":"demos/some_true_measures/#importance-sampling-with-a-single-kumaraswamy","title":"Importance Sampling with a Single Kumaraswamy\u00b6","text":""},{"location":"demos/some_true_measures/#importance-sampling-with-2-composed-kumaraswamys","title":"Importance Sampling with 2 (Composed) Kumaraswamys\u00b6","text":""},{"location":"demos/some_true_measures/#can-we-improve-the-keister-function","title":"Can we Improve the Keister function?\u00b6","text":""},{"location":"demos/umbridge/","title":"UM-Bridge","text":"In\u00a0[1]: Copied! <pre>import umbridge\nimport qmcpy as qp\n</pre> import umbridge import qmcpy as qp In\u00a0[2]: Copied! <pre>!docker run --name muqbp -d -it -p 4243:4243 linusseelinger/benchmark-muq-beam-propagation:latest\n</pre> !docker run --name muqbp -d -it -p 4243:4243 linusseelinger/benchmark-muq-beam-propagation:latest <pre>8192780429eeace8439d0007df81db12a43215530634f5b5737bfc705135ccc5\n</pre> <p>Initialize a QMCPy sampler and distribution.</p> In\u00a0[3]: Copied! <pre>sampler = qp.DigitalNetB2(dimension=3,seed=7) # DISCRETE DISTRIBUTION\ndistribution = qp.Uniform(sampler,lower_bound=1,upper_bound=1.05) # TRUE MEASURE\n</pre> sampler = qp.DigitalNetB2(dimension=3,seed=7) # DISCRETE DISTRIBUTION distribution = qp.Uniform(sampler,lower_bound=1,upper_bound=1.05) # TRUE MEASURE <p>Initialize a UM-Bridge model and wrap it into a QMCPy compatible Integrand</p> In\u00a0[4]: Copied! <pre>model = umbridge.HTTPModel('http://localhost:4243','forward')\numbridge_config = {\"d\": sampler.d}\nintegrand = qp.UMBridgeWrapper(distribution,model,umbridge_config,parallel=False) # INTEGRAND\n</pre> model = umbridge.HTTPModel('http://localhost:4243','forward') umbridge_config = {\"d\": sampler.d} integrand = qp.UMBridgeWrapper(distribution,model,umbridge_config,parallel=False) # INTEGRAND In\u00a0[5]: Copied! <pre>x = sampler(16) # same as sampler.gen_samples(16)\ny = integrand.f(x)\nprint(y.shape)\nprint(type(y))\nprint(y.dtype)\n</pre> x = sampler(16) # same as sampler.gen_samples(16) y = integrand.f(x) print(y.shape) print(type(y)) print(y.dtype) <pre>(31, 16)\n&lt;class 'numpy.ndarray'&gt;\nfloat64\n</pre> In\u00a0[6]: Copied! <pre>qmc_stop_crit = qp.CubQMCNetG(integrand,abs_tol=2.5e-2) # QMC STOPPING CRITERION\nsolution,data = qmc_stop_crit.integrate()\nprint(data)\n</pre> qmc_stop_crit = qp.CubQMCNetG(integrand,abs_tol=2.5e-2) # QMC STOPPING CRITERION solution,data = qmc_stop_crit.integrate() print(data) <pre>Data (Data)\n    solution        [  0.      3.855  14.69  ... 898.921 935.383 971.884]\n    comb_bound_low  [  0.      3.854  14.688 ... 898.901 935.362 971.862]\n    comb_bound_high [  0.      3.855  14.691 ... 898.941 935.404 971.906]\n    comb_bound_diff [0.    0.001 0.003 ... 0.041 0.042 0.044]\n    comb_flags      [ True  True  True ...  True  True  True]\n    n_total         2^(11)\n    n               [1024 1024 1024 ... 2048 2048 2048]\n    time_integrate  9.566\nCubQMCNetG (AbstractStoppingCriterion)\n    abs_tol         0.025\n    rel_tol         0\n    n_init          2^(10)\n    n_limit         2^(35)\nUMBridgeWrapper (AbstractIntegrand)\nUniform (AbstractTrueMeasure)\n    lower_bound     1\n    upper_bound     1.050\nDigitalNetB2 (AbstractLDDiscreteDistribution)\n    d               3\n    replications    1\n    randomize       LMS_DS\n    gen_mats_source joe_kuo.6.21201.txt\n    order           NATURAL\n    t               63\n    alpha           1\n    n_limit         2^(32)\n    entropy         7\n</pre> In\u00a0[7]: Copied! <pre>from matplotlib import pyplot\nfig,ax = pyplot.subplots(figsize=(6,3))\nax.plot(solution,'-o')\nax.set_xlim([0,len(solution)-1]); ax.set_xlabel(r'$x$')\nax.set_ylim([1000,-10]);  ax.set_ylabel(r'$u(x)$');\n</pre> from matplotlib import pyplot fig,ax = pyplot.subplots(figsize=(6,3)) ax.plot(solution,'-o') ax.set_xlim([0,len(solution)-1]); ax.set_xlabel(r'$x$') ax.set_ylim([1000,-10]);  ax.set_ylabel(r'$u(x)$'); <p>QMCPy can automatically multi-threaded requests to the model by setting <code>parallel=p</code> where <code>p</code> is the number of processors used by multiprocessing.pool.ThreadPool. Setting <code>parallel=True</code> is equivalent to setting <code>parallel=os.cpu_count()</code>.</p> In\u00a0[8]: Copied! <pre>import os\nprint('Available CPUs: %d'%os.cpu_count())\n</pre> import os print('Available CPUs: %d'%os.cpu_count()) <pre>Available CPUs: 12\n</pre> In\u00a0[10]: Copied! <pre>integrand = qp.UMBridgeWrapper(distribution,model,umbridge_config,parallel=8)\nsolution,data = qp.CubQMCNetG(integrand,abs_tol=2.5e-2).integrate()\ndata\n</pre> integrand = qp.UMBridgeWrapper(distribution,model,umbridge_config,parallel=8) solution,data = qp.CubQMCNetG(integrand,abs_tol=2.5e-2).integrate() data Out[10]: <pre>Data (Data)\n    solution        [  0.      3.855  14.69  ... 898.921 935.383 971.884]\n    comb_bound_low  [  0.      3.854  14.688 ... 898.901 935.362 971.862]\n    comb_bound_high [  0.      3.855  14.691 ... 898.941 935.404 971.906]\n    comb_bound_diff [0.    0.001 0.003 ... 0.041 0.042 0.044]\n    comb_flags      [ True  True  True ...  True  True  True]\n    n_total         2^(11)\n    n               [1024 1024 1024 ... 2048 2048 2048]\n    time_integrate  4.719\nCubQMCNetG (AbstractStoppingCriterion)\n    abs_tol         0.025\n    rel_tol         0\n    n_init          2^(10)\n    n_limit         2^(35)\nUMBridgeWrapper (AbstractIntegrand)\nUniform (AbstractTrueMeasure)\n    lower_bound     1\n    upper_bound     1.050\nDigitalNetB2 (AbstractLDDiscreteDistribution)\n    d               3\n    replications    1\n    randomize       LMS_DS\n    gen_mats_source joe_kuo.6.21201.txt\n    order           NATURAL\n    t               63\n    alpha           1\n    n_limit         2^(32)\n    entropy         7</pre> In\u00a0[11]: Copied! <pre>!docker rm -f muqbp\n</pre> !docker rm -f muqbp <pre>muqbp\n</pre>"},{"location":"demos/umbridge/#um-bridge-with-qmcpy","title":"UM-Bridge with QMCPy\u00b6","text":"<p>Using QMCPy to evaluate the UM-Bridge Cantilever Beam Function and approximate the expectation with respect to a uniform random variable.</p>"},{"location":"demos/umbridge/#imports","title":"Imports\u00b6","text":""},{"location":"demos/umbridge/#start-docker-container","title":"Start Docker Container\u00b6","text":"<p>See the UM-Bridge Documentation for image options.</p>"},{"location":"demos/umbridge/#problem-setup","title":"Problem Setup\u00b6","text":""},{"location":"demos/umbridge/#model-evaluation","title":"Model Evaluation\u00b6","text":""},{"location":"demos/umbridge/#automatically-approximate-the-expectation","title":"Automatically Approximate the Expectation\u00b6","text":""},{"location":"demos/umbridge/#parallel-evaluation","title":"Parallel Evaluation\u00b6","text":""},{"location":"demos/umbridge/#shut-down-docker-image","title":"Shut Down Docker Image\u00b6","text":""},{"location":"demos/vectorized_qmc/","title":"Vectorized QMC Algorithms Tracking Fourier Coefficient Decay","text":"In\u00a0[\u00a0]: Copied! <pre>%%capture\n# @title Execute this cell to install dependancies\ntry:\n  import google.colab\n  import os\n  !pip install -q qmcpy &gt;&gt; /dev/null\n  !apt-get update &amp;&amp; apt-get install -y --no-install-recommends texlive-latex-base texlive-fonts-recommended texlive-latex-extra cm-super dvipng\nexcept:\n  pass\n\nimport matplotlib.pyplot as plt\n\nplt.rcParams.update({\n\"text.usetex\": True,\n\"font.family\": \"serif\",\n\"text.latex.preamble\": r\"\\usepackage{amsmath}\\usepackage{amssymb}\\newcommand{\\bx}{\\boldsymbol{x}}\"\n})\n</pre> %%capture # @title Execute this cell to install dependancies try:   import google.colab   import os   !pip install -q qmcpy &gt;&gt; /dev/null   !apt-get update &amp;&amp; apt-get install -y --no-install-recommends texlive-latex-base texlive-fonts-recommended texlive-latex-extra cm-super dvipng except:   pass  import matplotlib.pyplot as plt  plt.rcParams.update({ \"text.usetex\": True, \"font.family\": \"serif\", \"text.latex.preamble\": r\"\\usepackage{amsmath}\\usepackage{amssymb}\\newcommand{\\bx}{\\boldsymbol{x}}\" }) In\u00a0[2]: Copied! <pre>import qmcpy as qp\nimport numpy as np\n</pre> import qmcpy as qp import numpy as np In\u00a0[3]: Copied! <pre>from matplotlib import pyplot\n%matplotlib inline\nroot = None#'./'\n</pre> from matplotlib import pyplot %matplotlib inline root = None#'./' In\u00a0[4]: Copied! <pre>n = 2**6\ns = 10\nfig,ax = pyplot.subplots(figsize=(8,4),nrows=1,ncols=3)\nfor i,(dd,name) in enumerate(zip(\n    [qp.IIDStdUniform(2,seed=7),qp.DigitalNetB2(2,seed=7),qp.Lattice(2,seed=7)],\n    ['IID','Randomized Digital Net (LD)','Randomized Lattice (LD)'])):\n    pts = dd.gen_samples(n)\n    ax[i].scatter(pts[0:n//4,0],pts[0:n//4,1],color='k',marker='s',s=s)\n    ax[i].scatter(pts[n//4:n//2,0],pts[n//4:n//2,1],color='k',marker='o',s=s)\n    ax[i].scatter(pts[n//2:n,0],pts[n//2:n,1],color='k',marker='^',s=s)\n    ax[i].set_aspect(1)\n    ax[i].set_xlabel(r'$x_{1}$')\n    ax[i].set_ylabel(r'$x_{2}$')\n    ax[i].set_xlim([0,1])\n    ax[i].set_ylim([0,1])\n    ax[i].set_xticks([0,.25,.5,.75,1])\n    ax[i].set_xticklabels([r'$0$',r'$1/4$',r'$1/2$',r'$3/4$',r'$1$'])\n    ax[i].set_yticks([0,.25,.5,.75,1])\n    ax[i].set_yticklabels([r'$0$',r'$1/4$',r'$1/2$',r'$3/4$',r'$1$'])\n    ax[i].set_title(name)\nif root: fig.savefig(root+'ld_seqs.pdf',transparent=True)\n</pre> n = 2**6 s = 10 fig,ax = pyplot.subplots(figsize=(8,4),nrows=1,ncols=3) for i,(dd,name) in enumerate(zip(     [qp.IIDStdUniform(2,seed=7),qp.DigitalNetB2(2,seed=7),qp.Lattice(2,seed=7)],     ['IID','Randomized Digital Net (LD)','Randomized Lattice (LD)'])):     pts = dd.gen_samples(n)     ax[i].scatter(pts[0:n//4,0],pts[0:n//4,1],color='k',marker='s',s=s)     ax[i].scatter(pts[n//4:n//2,0],pts[n//4:n//2,1],color='k',marker='o',s=s)     ax[i].scatter(pts[n//2:n,0],pts[n//2:n,1],color='k',marker='^',s=s)     ax[i].set_aspect(1)     ax[i].set_xlabel(r'$x_{1}$')     ax[i].set_ylabel(r'$x_{2}$')     ax[i].set_xlim([0,1])     ax[i].set_ylim([0,1])     ax[i].set_xticks([0,.25,.5,.75,1])     ax[i].set_xticklabels([r'$0$',r'$1/4$',r'$1/2$',r'$3/4$',r'$1$'])     ax[i].set_yticks([0,.25,.5,.75,1])     ax[i].set_yticklabels([r'$0$',r'$1/4$',r'$1/2$',r'$3/4$',r'$1$'])     ax[i].set_title(name) if root: fig.savefig(root+'ld_seqs.pdf',transparent=True) In\u00a0[\u00a0]: Copied! <pre>def cantilever_beam_function(T,compute_flags): # T is (n x 3)\n    Y = np.zeros((2,len(T)),dtype=float) # (n x 2)\n    l,w,t = 100,4,2\n    T1,T2,T3 = T[:,0],T[:,1],T[:,2] # Python is indexed from 0\n    if compute_flags[0]: # compute D. x^2 is \"x**2\" in Python\n        Y[0] = 4*l**3/(T1*w*t)*np.sqrt(T2**2/t**4+T3**2/w**4)\n    if compute_flags[1]: # compute S\n        Y[1] = 600*(T2/(w*t**2)+T3/(w**2*t))\n    return Y\ntrue_measure = qp.Gaussian(\n    sampler = qp.DigitalNetB2(dimension=3,seed=7),\n    mean = [2.9e7,500,1000],\n    covariance = np.diag([(1.45e6)**2,(100)**2,(100)**2]))\nintegrand = qp.CustomFun(true_measure,\n    g = cantilever_beam_function,\n    dimension_indv = 2)\nqmc_stop_crit = qp.CubQMCNetG(integrand,\n    abs_tol = 1e-3,\n    rel_tol = 1e-6)\nsolution,data = qmc_stop_crit.integrate()\nprint(data)\n\"\"\"\nData (Data)\n    solution        [2.426e+00 3.750e+04]\n    comb_bound_low  [2.425e+00 3.750e+04]\n    comb_bound_high [2.427e+00 3.750e+04]\n    comb_bound_diff [0.002 0.041]\n    comb_flags      [ True  True]\n    n_total         2^(18)\n    n               [  1024 262144]\n    time_integrate  0.060\n\"\"\"\n</pre> def cantilever_beam_function(T,compute_flags): # T is (n x 3)     Y = np.zeros((2,len(T)),dtype=float) # (n x 2)     l,w,t = 100,4,2     T1,T2,T3 = T[:,0],T[:,1],T[:,2] # Python is indexed from 0     if compute_flags[0]: # compute D. x^2 is \"x**2\" in Python         Y[0] = 4*l**3/(T1*w*t)*np.sqrt(T2**2/t**4+T3**2/w**4)     if compute_flags[1]: # compute S         Y[1] = 600*(T2/(w*t**2)+T3/(w**2*t))     return Y true_measure = qp.Gaussian(     sampler = qp.DigitalNetB2(dimension=3,seed=7),     mean = [2.9e7,500,1000],     covariance = np.diag([(1.45e6)**2,(100)**2,(100)**2])) integrand = qp.CustomFun(true_measure,     g = cantilever_beam_function,     dimension_indv = 2) qmc_stop_crit = qp.CubQMCNetG(integrand,     abs_tol = 1e-3,     rel_tol = 1e-6) solution,data = qmc_stop_crit.integrate() print(data) \"\"\" Data (Data)     solution        [2.426e+00 3.750e+04]     comb_bound_low  [2.425e+00 3.750e+04]     comb_bound_high [2.427e+00 3.750e+04]     comb_bound_diff [0.002 0.041]     comb_flags      [ True  True]     n_total         2^(18)     n               [  1024 262144]     time_integrate  0.060 \"\"\" <pre>Data (Data)\n    solution        [2.426e+00 3.750e+04]\n    comb_bound_low  [2.425e+00 3.750e+04]\n    comb_bound_high [2.427e+00 3.750e+04]\n    comb_bound_diff [0.002 0.041]\n    comb_flags      [ True  True]\n    n_total         2^(18)\n    n               [  1024 262144]\n    time_integrate  0.060\nCubQMCNetG (AbstractStoppingCriterion)\n    abs_tol         0.001\n    rel_tol         1.00e-06\n    n_init          2^(10)\n    n_limit         2^(35)\nCustomFun (AbstractIntegrand)\nGaussian (AbstractTrueMeasure)\n    mean            [2.9e+07 5.0e+02 1.0e+03]\n    covariance      [[2.102e+12 0.000e+00 0.000e+00]\n                     [0.000e+00 1.000e+04 0.000e+00]\n                     [0.000e+00 0.000e+00 1.000e+04]]\n    decomp_type     PCA\nDigitalNetB2 (AbstractLDDiscreteDistribution)\n    d               3\n    replications    1\n    randomize       LMS DS\n    gen_mats_source joe_kuo.6.21201.txt\n    order           RADICAL INVERSE\n    t               63\n    alpha           1\n    n_limit         2^(32)\n    entropy         7\n</pre> In\u00a0[6]: Copied! <pre>import scipy\nfrom sklearn.gaussian_process import GaussianProcessRegressor,kernels\n\nf = lambda x: np.cos(10*x)*np.exp(.2*x)+np.exp(-5*(x-.4)**2)\nxplt = np.linspace(0,1,100)\nyplt = f(xplt)\nx = np.array([.1, .2, .4, .7, .9])\ny = f(x)\nymax = y.max()\n\ngp = GaussianProcessRegressor(kernel=kernels.RBF(length_scale=1.0,length_scale_bounds=(1e-2, 1e2)),\n    n_restarts_optimizer = 16).fit(x[:,None],y)\nyhatplt,stdhatplt = gp.predict(xplt[:,None],return_std=True)\n\ntpax = 32\nx0mesh,x1mesh = np.meshgrid(np.linspace(0,1,tpax),np.linspace(0,1,tpax))\npost_mus = np.zeros((tpax,tpax,2),dtype=float)\npost_sqrtcovs = np.zeros((tpax,tpax,2,2),dtype=float)\nfor j0 in range(tpax):\n    for j1 in range(tpax):\n        candidate = np.array([[x0mesh[j0,j1]],[x1mesh[j0,j1]]])\n        post_mus[j0,j1],post_cov = gp.predict(candidate,return_cov=True)\n        evals,evecs = scipy.linalg.eig(post_cov)\n        post_sqrtcovs[j0,j1] = np.sqrt(np.maximum(evals.real,0))*evecs\n\ndef qei_acq_vec(x,compute_flags):\n    xgauss = scipy.stats.norm.ppf(x)\n    n = len(x)\n    qei_vals = np.zeros((tpax,tpax,n),dtype=float)\n    for j0 in range(tpax):\n        for j1 in range(tpax):\n            if compute_flags[j0,j1]==False: continue\n            sqrt_cov = post_sqrtcovs[j0,j1]\n            mu_post = post_mus[j0,j1]\n            for i in range(len(x)):\n                yij = sqrt_cov@xgauss[i]+mu_post\n                qei_vals[j0,j1,i] = max((yij-ymax).max(),0)\n    return qei_vals\n\nqei_acq_vec_qmcpy = qp.CustomFun(\n    true_measure = qp.Uniform(qp.DigitalNetB2(2,seed=7)),\n    g = qei_acq_vec,\n    dimension_indv = (tpax,tpax),\n    parallel=False)\nqei_vals,qei_data = qp.CubQMCNetG(qei_acq_vec_qmcpy,abs_tol=.025,rel_tol=0).integrate() # .0005\nprint(qei_data)\n\na = np.unravel_index(np.argmax(qei_vals,axis=None),qei_vals.shape)\nxnext = np.array([x0mesh[a[0],a[1]],x1mesh[a[0],a[1]]])\nfnext = f(xnext)\n</pre> import scipy from sklearn.gaussian_process import GaussianProcessRegressor,kernels  f = lambda x: np.cos(10*x)*np.exp(.2*x)+np.exp(-5*(x-.4)**2) xplt = np.linspace(0,1,100) yplt = f(xplt) x = np.array([.1, .2, .4, .7, .9]) y = f(x) ymax = y.max()  gp = GaussianProcessRegressor(kernel=kernels.RBF(length_scale=1.0,length_scale_bounds=(1e-2, 1e2)),     n_restarts_optimizer = 16).fit(x[:,None],y) yhatplt,stdhatplt = gp.predict(xplt[:,None],return_std=True)  tpax = 32 x0mesh,x1mesh = np.meshgrid(np.linspace(0,1,tpax),np.linspace(0,1,tpax)) post_mus = np.zeros((tpax,tpax,2),dtype=float) post_sqrtcovs = np.zeros((tpax,tpax,2,2),dtype=float) for j0 in range(tpax):     for j1 in range(tpax):         candidate = np.array([[x0mesh[j0,j1]],[x1mesh[j0,j1]]])         post_mus[j0,j1],post_cov = gp.predict(candidate,return_cov=True)         evals,evecs = scipy.linalg.eig(post_cov)         post_sqrtcovs[j0,j1] = np.sqrt(np.maximum(evals.real,0))*evecs  def qei_acq_vec(x,compute_flags):     xgauss = scipy.stats.norm.ppf(x)     n = len(x)     qei_vals = np.zeros((tpax,tpax,n),dtype=float)     for j0 in range(tpax):         for j1 in range(tpax):             if compute_flags[j0,j1]==False: continue             sqrt_cov = post_sqrtcovs[j0,j1]             mu_post = post_mus[j0,j1]             for i in range(len(x)):                 yij = sqrt_cov@xgauss[i]+mu_post                 qei_vals[j0,j1,i] = max((yij-ymax).max(),0)     return qei_vals  qei_acq_vec_qmcpy = qp.CustomFun(     true_measure = qp.Uniform(qp.DigitalNetB2(2,seed=7)),     g = qei_acq_vec,     dimension_indv = (tpax,tpax),     parallel=False) qei_vals,qei_data = qp.CubQMCNetG(qei_acq_vec_qmcpy,abs_tol=.025,rel_tol=0).integrate() # .0005 print(qei_data)  a = np.unravel_index(np.argmax(qei_vals,axis=None),qei_vals.shape) xnext = np.array([x0mesh[a[0],a[1]],x1mesh[a[0],a[1]]]) fnext = f(xnext) <pre>Data (Data)\n    solution        [[0.06  0.079 0.072 ... 0.06  0.061 0.066]\n                     [0.079 0.064 0.067 ... 0.064 0.065 0.071]\n                     [0.072 0.067 0.032 ... 0.032 0.033 0.039]\n                     ...\n                     [0.06  0.064 0.032 ... 0.    0.001 0.007]\n                     [0.061 0.065 0.033 ... 0.001 0.001 0.007]\n                     [0.067 0.071 0.039 ... 0.007 0.007 0.007]]\n    comb_bound_low  [[0.059 0.079 0.071 ... 0.059 0.06  0.065]\n                     [0.078 0.063 0.066 ... 0.064 0.064 0.07 ]\n                     [0.071 0.066 0.032 ... 0.032 0.032 0.038]\n                     ...\n                     [0.059 0.063 0.032 ... 0.    0.001 0.006]\n                     [0.059 0.064 0.032 ... 0.001 0.001 0.006]\n                     [0.065 0.07  0.038 ... 0.006 0.006 0.006]]\n    comb_bound_high [[0.061 0.08  0.073 ... 0.061 0.062 0.068]\n                     [0.08  0.065 0.068 ... 0.065 0.066 0.072]\n                     [0.073 0.068 0.033 ... 0.033 0.033 0.04 ]\n                     ...\n                     [0.061 0.065 0.033 ... 0.    0.001 0.008]\n                     [0.062 0.066 0.034 ... 0.001 0.001 0.008]\n                     [0.068 0.072 0.04  ... 0.008 0.008 0.008]]\n    comb_bound_diff [[0.002 0.002 0.002 ... 0.002 0.002 0.002]\n                     [0.002 0.002 0.002 ... 0.002 0.002 0.002]\n                     [0.002 0.002 0.001 ... 0.001 0.001 0.002]\n                     ...\n                     [0.002 0.002 0.001 ... 0.    0.    0.002]\n                     [0.002 0.002 0.001 ... 0.    0.    0.002]\n                     [0.003 0.002 0.002 ... 0.002 0.002 0.002]]\n    comb_flags      [[ True  True  True ...  True  True  True]\n                     [ True  True  True ...  True  True  True]\n                     [ True  True  True ...  True  True  True]\n                     ...\n                     [ True  True  True ...  True  True  True]\n                     [ True  True  True ...  True  True  True]\n                     [ True  True  True ...  True  True  True]]\n    n_total         2^(10)\n    n               [[1024 1024 1024 ... 1024 1024 1024]\n                     [1024 1024 1024 ... 1024 1024 1024]\n                     [1024 1024 1024 ... 1024 1024 1024]\n                     ...\n                     [1024 1024 1024 ... 1024 1024 1024]\n                     [1024 1024 1024 ... 1024 1024 1024]\n                     [1024 1024 1024 ... 1024 1024 1024]]\n    time_integrate  2.242\nCubQMCNetG (AbstractStoppingCriterion)\n    abs_tol         0.025\n    rel_tol         0\n    n_init          2^(10)\n    n_limit         2^(35)\nCustomFun (AbstractIntegrand)\nUniform (AbstractTrueMeasure)\n    lower_bound     0\n    upper_bound     1\nDigitalNetB2 (AbstractLDDiscreteDistribution)\n    d               2^(1)\n    replications    1\n    randomize       LMS DS\n    gen_mats_source joe_kuo.6.21201.txt\n    order           RADICAL INVERSE\n    t               63\n    alpha           1\n    n_limit         2^(32)\n    entropy         7\n</pre> In\u00a0[7]: Copied! <pre>from matplotlib import cm\nfig,ax = pyplot.subplots(nrows=1,ncols=2,figsize=(8,3.25))\nax[0].scatter(x,y,color='k',label='Query Points')\nax[0].plot(xplt,yplt,color='k',linestyle='--',label='True function',linewidth=1)\nax[0].plot(xplt,yhatplt,color='k',label='GP Mean',linewidth=1)\nax[0].fill_between(xplt,yhatplt-1.96*stdhatplt,yhatplt+1.96*stdhatplt,color='k',alpha=.25,label='95% CI')\nax[0].scatter(xnext,fnext,color='k',marker='*',s=200,zorder=10)\nax[0].set_xlim([0,1])\nax[0].set_xticks([0,1])\nax[0].set_xlabel(r'$x$')\nax[0].set_ylabel(r'$y$')\nfig.legend(labels=['data','true function','posterior mean',r'95\\% CI','next points by qEI'],loc='lower center',bbox_to_anchor=(.5,-.05),ncol=5)\ncontour = ax[1].contourf(x0mesh,x1mesh,qei_vals,cmap=cm.Greys_r)\nax[1].scatter([xnext[0]],[xnext[1]],color='k',marker='*',s=200)\nfig.colorbar(contour,ax=None,shrink=1,aspect=5)\nax[1].scatter(x0mesh.flatten(),x1mesh.flatten(),color='w',s=1)\nax[1].set_xlim([0,1])\nax[1].set_xticks([0,1])\nax[1].set_ylim([0,1])\nax[1].set_yticks([0,1])\nax[1].set_xlabel(r'$x_1$')\nax[1].set_ylabel(r'$x_2$')\nif root: fig.savefig(root+'gp.pdf',transparent=True)\n</pre> from matplotlib import cm fig,ax = pyplot.subplots(nrows=1,ncols=2,figsize=(8,3.25)) ax[0].scatter(x,y,color='k',label='Query Points') ax[0].plot(xplt,yplt,color='k',linestyle='--',label='True function',linewidth=1) ax[0].plot(xplt,yhatplt,color='k',label='GP Mean',linewidth=1) ax[0].fill_between(xplt,yhatplt-1.96*stdhatplt,yhatplt+1.96*stdhatplt,color='k',alpha=.25,label='95% CI') ax[0].scatter(xnext,fnext,color='k',marker='*',s=200,zorder=10) ax[0].set_xlim([0,1]) ax[0].set_xticks([0,1]) ax[0].set_xlabel(r'$x$') ax[0].set_ylabel(r'$y$') fig.legend(labels=['data','true function','posterior mean',r'95\\% CI','next points by qEI'],loc='lower center',bbox_to_anchor=(.5,-.05),ncol=5) contour = ax[1].contourf(x0mesh,x1mesh,qei_vals,cmap=cm.Greys_r) ax[1].scatter([xnext[0]],[xnext[1]],color='k',marker='*',s=200) fig.colorbar(contour,ax=None,shrink=1,aspect=5) ax[1].scatter(x0mesh.flatten(),x1mesh.flatten(),color='w',s=1) ax[1].set_xlim([0,1]) ax[1].set_xticks([0,1]) ax[1].set_ylim([0,1]) ax[1].set_yticks([0,1]) ax[1].set_xlabel(r'$x_1$') ax[1].set_ylabel(r'$x_2$') if root: fig.savefig(root+'gp.pdf',transparent=True) In\u00a0[8]: Copied! <pre>import pandas as pd\nfrom sklearn.model_selection import train_test_split\ndf = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/haberman/haberman.data',header=None)\ndf.columns = ['Age','1900 Year','Axillary Nodes','Survival Status']\ndf.loc[df['Survival Status']==2,'Survival Status'] = 0\nx,y = df[['Age','1900 Year','Axillary Nodes']],df['Survival Status']\nxt,xv,yt,yv = train_test_split(x,y,test_size=.33,random_state=7)\n</pre> import pandas as pd from sklearn.model_selection import train_test_split df = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/haberman/haberman.data',header=None) df.columns = ['Age','1900 Year','Axillary Nodes','Survival Status'] df.loc[df['Survival Status']==2,'Survival Status'] = 0 x,y = df[['Age','1900 Year','Axillary Nodes']],df['Survival Status'] xt,xv,yt,yv = train_test_split(x,y,test_size=.33,random_state=7) In\u00a0[9]: Copied! <pre>print(df.head(),'\\n')\nprint(df[['Age','1900 Year','Axillary Nodes']].describe(),'\\n')\nprint(df['Survival Status'].astype(str).describe())\nprint('\\ntrain samples: %d test samples: %d\\n'%(len(xt),len(xv)))\nprint('train positives %d   train negatives: %d'%(np.sum(yt==1),np.sum(yt==0)))\nprint(' test positives %d    test negatives: %d'%(np.sum(yv==1),np.sum(yv==0)))\nxt.head()\n</pre> print(df.head(),'\\n') print(df[['Age','1900 Year','Axillary Nodes']].describe(),'\\n') print(df['Survival Status'].astype(str).describe()) print('\\ntrain samples: %d test samples: %d\\n'%(len(xt),len(xv))) print('train positives %d   train negatives: %d'%(np.sum(yt==1),np.sum(yt==0))) print(' test positives %d    test negatives: %d'%(np.sum(yv==1),np.sum(yv==0))) xt.head() <pre>   Age  1900 Year  Axillary Nodes  Survival Status\n0   30         64               1                1\n1   30         62               3                1\n2   30         65               0                1\n3   31         59               2                1\n4   31         65               4                1 \n\n              Age   1900 Year  Axillary Nodes\ncount  306.000000  306.000000      306.000000\nmean    52.457516   62.852941        4.026144\nstd     10.803452    3.249405        7.189654\nmin     30.000000   58.000000        0.000000\n25%     44.000000   60.000000        0.000000\n50%     52.000000   63.000000        1.000000\n75%     60.750000   65.750000        4.000000\nmax     83.000000   69.000000       52.000000 \n\ncount     306\nunique      2\ntop         1\nfreq      225\nName: Survival Status, dtype: object\n\ntrain samples: 205 test samples: 101\n\ntrain positives 151   train negatives: 54\n test positives 74    test negatives: 27\n</pre> Out[9]: Age 1900 Year Axillary Nodes 46 41 58 0 199 57 64 1 115 49 64 10 128 50 61 0 249 63 63 0 In\u00a0[\u00a0]: Copied! <pre>blr = qp.BayesianLRCoeffs(\n    sampler = qp.DigitalNetB2(4,seed=1),\n    feature_array = xt, # np.ndarray of shape (n,d-1)\n    response_vector = yt, # np.ndarray of shape (n,)\n    prior_mean = 0, # normal prior mean = (0,0,...,0)\n    prior_covariance = 5) # normal prior covariance = 5I\nqmc_sc = qp.CubQMCNetG(blr,\n    abs_tol = .1,\n    rel_tol = .5,\n    error_fun = \"BOTH\",\n    n_limit=2**18)\nblr_coefs,blr_data = qmc_sc.integrate()\nprint(blr_data)\n\"\"\"\nData (Data)\nsolution        [-0.128  0.084 -0.091  0.09 ]\ncomb_bound_low  [-0.198  0.069 -0.14   0.074]\ncomb_bound_high [-0.105  0.129 -0.075  0.138]\ncomb_bound_diff [0.092 0.06  0.065 0.065]\ncomb_flags      [ True  True  True  True]\nn_total         2^(17)\nn               [[  1024   1024   2048 131072]\n                    [  1024   1024   2048 131072]]\ntime_integrate  0.351\n\"\"\"\n</pre> blr = qp.BayesianLRCoeffs(     sampler = qp.DigitalNetB2(4,seed=1),     feature_array = xt, # np.ndarray of shape (n,d-1)     response_vector = yt, # np.ndarray of shape (n,)     prior_mean = 0, # normal prior mean = (0,0,...,0)     prior_covariance = 5) # normal prior covariance = 5I qmc_sc = qp.CubQMCNetG(blr,     abs_tol = .1,     rel_tol = .5,     error_fun = \"BOTH\",     n_limit=2**18) blr_coefs,blr_data = qmc_sc.integrate() print(blr_data) \"\"\" Data (Data) solution        [-0.128  0.084 -0.091  0.09 ] comb_bound_low  [-0.198  0.069 -0.14   0.074] comb_bound_high [-0.105  0.129 -0.075  0.138] comb_bound_diff [0.092 0.06  0.065 0.065] comb_flags      [ True  True  True  True] n_total         2^(17) n               [[  1024   1024   2048 131072]                     [  1024   1024   2048 131072]] time_integrate  0.351 \"\"\" <pre>Data (Data)\n    solution        [-0.128  0.084 -0.091  0.09 ]\n    comb_bound_low  [-0.198  0.069 -0.14   0.074]\n    comb_bound_high [-0.105  0.129 -0.075  0.138]\n    comb_bound_diff [0.092 0.06  0.065 0.065]\n    comb_flags      [ True  True  True  True]\n    n_total         2^(17)\n    n               [[  1024   1024   2048 131072]\n                     [  1024   1024   2048 131072]]\n    time_integrate  0.351\nCubQMCNetG (AbstractStoppingCriterion)\n    abs_tol         0.100\n    rel_tol         2^(-1)\n    n_init          2^(10)\n    n_limit         2^(18)\nBayesianLRCoeffs (AbstractIntegrand)\nGaussian (AbstractTrueMeasure)\n    mean            0\n    covariance      5\n    decomp_type     PCA\nDigitalNetB2 (AbstractLDDiscreteDistribution)\n    d               2^(2)\n    replications    1\n    randomize       LMS DS\n    gen_mats_source joe_kuo.6.21201.txt\n    order           RADICAL INVERSE\n    t               63\n    alpha           1\n    n_limit         2^(32)\n    entropy         1\n</pre> In\u00a0[13]: Copied! <pre>from sklearn.linear_model import LogisticRegression\ndef metrics(y,yhat):\n    y,yhat = np.array(y),np.array(yhat)\n    tp = np.sum((y==1)*(yhat==1))\n    tn = np.sum((y==0)*(yhat==0))\n    fp = np.sum((y==0)*(yhat==1))\n    fn = np.sum((y==1)*(yhat==0))\n    accuracy = (tp+tn)/(len(y))\n    precision = tp/(tp+fp)\n    recall = tp/(tp+fn)\n    return [accuracy,precision,recall]\n\nresults = pd.DataFrame({name:[] for name in ['method','Age','1900 Year','Axillary Nodes','Intercept','Accuracy','Precision','Recall']})\nfor i,l1_ratio in enumerate([0,.5,1]):\n    lr = LogisticRegression(random_state=7,penalty=\"elasticnet\",solver='saga',l1_ratio=l1_ratio).fit(xt,yt)\n    results.loc[i] = [r'Elastic-Net \\lambda=%.1f'%l1_ratio]+lr.coef_.squeeze().tolist()+[lr.intercept_.item()]+metrics(yv,lr.predict(xv))\n\nblr_predict = lambda x: 1/(1+np.exp(-np.array(x)@blr_coefs[:-1]-blr_coefs[-1]))&gt;=.5\nblr_train_accuracy = np.mean(blr_predict(xt)==yt)\nblr_test_accuracy = np.mean(blr_predict(xv)==yv)\nresults.loc[len(results)] = ['Bayesian']+blr_coefs.squeeze().tolist()+metrics(yv,blr_predict(xv))\n\nimport warnings\nwarnings.simplefilter('ignore',FutureWarning)\nresults.set_index('method',inplace=True)\nprint(results.head())\n#root: results.to_latex(root+'lr_table.tex',formatters={'%s'%tt:lambda v:'%.1f'%(100*v) for tt in ['accuracy','precision','recall']},float_format=\"%.2e\")\n</pre> from sklearn.linear_model import LogisticRegression def metrics(y,yhat):     y,yhat = np.array(y),np.array(yhat)     tp = np.sum((y==1)*(yhat==1))     tn = np.sum((y==0)*(yhat==0))     fp = np.sum((y==0)*(yhat==1))     fn = np.sum((y==1)*(yhat==0))     accuracy = (tp+tn)/(len(y))     precision = tp/(tp+fp)     recall = tp/(tp+fn)     return [accuracy,precision,recall]  results = pd.DataFrame({name:[] for name in ['method','Age','1900 Year','Axillary Nodes','Intercept','Accuracy','Precision','Recall']}) for i,l1_ratio in enumerate([0,.5,1]):     lr = LogisticRegression(random_state=7,penalty=\"elasticnet\",solver='saga',l1_ratio=l1_ratio).fit(xt,yt)     results.loc[i] = [r'Elastic-Net \\lambda=%.1f'%l1_ratio]+lr.coef_.squeeze().tolist()+[lr.intercept_.item()]+metrics(yv,lr.predict(xv))  blr_predict = lambda x: 1/(1+np.exp(-np.array(x)@blr_coefs[:-1]-blr_coefs[-1]))&gt;=.5 blr_train_accuracy = np.mean(blr_predict(xt)==yt) blr_test_accuracy = np.mean(blr_predict(xv)==yv) results.loc[len(results)] = ['Bayesian']+blr_coefs.squeeze().tolist()+metrics(yv,blr_predict(xv))  import warnings warnings.simplefilter('ignore',FutureWarning) results.set_index('method',inplace=True) print(results.head()) #root: results.to_latex(root+'lr_table.tex',formatters={'%s'%tt:lambda v:'%.1f'%(100*v) for tt in ['accuracy','precision','recall']},float_format=\"%.2e\") <pre>                              Age  1900 Year  Axillary Nodes  Intercept  \\\nmethod                                                                    \nElastic-Net \\lambda=0.0 -0.012279   0.034401       -0.115153   0.001990   \nElastic-Net \\lambda=0.5 -0.012041   0.034170       -0.114770   0.002025   \nElastic-Net \\lambda=1.0 -0.011803   0.033940       -0.114387   0.002061   \nBayesian                -0.128270   0.083826       -0.090906   0.089903   \n\n                         Accuracy  Precision    Recall  \nmethod                                                  \nElastic-Net \\lambda=0.0  0.742574   0.766667  0.932432  \nElastic-Net \\lambda=0.5  0.742574   0.766667  0.932432  \nElastic-Net \\lambda=1.0  0.742574   0.766667  0.932432  \nBayesian                 0.326733   1.000000  0.081081  \n</pre> In\u00a0[14]: Copied! <pre>a,b = 7,0.1\ndnb2 = qp.DigitalNetB2(3,seed=7)\nishigami = qp.Ishigami(dnb2,a,b)\nidxs = np.array([\n    [True,False,False],\n    [False,True,False],\n    [False,False,True],\n    [True,True,False],\n    [True,False,True],\n    [False,True,True]],dtype=bool)\nishigami_si = qp.SensitivityIndices(ishigami,idxs)\nqmc_algo = qp.CubQMCNetG(ishigami_si,abs_tol=.05)\nsolution,data = qmc_algo.integrate()\nprint(data)\nsi_closed = solution[0].squeeze()\nsi_total = solution[1].squeeze()\nci_comb_low_closed = data.comb_bound_low[0].squeeze()\nci_comb_high_closed = data.comb_bound_high[0].squeeze()\nci_comb_low_total = data.comb_bound_low[1].squeeze()\nci_comb_high_total = data.comb_bound_high[1].squeeze()\nprint(\"\\nApprox took %.1f sec and n = 2^(%d)\"%\n    (data.time_integrate,np.log2(data.n_total)))\nprint('\\t si_closed:',si_closed)\nprint('\\t si_total:',si_total)\nprint('\\t ci_comb_low_closed:',ci_comb_low_closed)\nprint('\\t ci_comb_high_closed:',ci_comb_high_closed)\nprint('\\t ci_comb_low_total:',ci_comb_low_total)\nprint('\\t ci_comb_high_total:',ci_comb_high_total)\n\ntrue_indices = qp.Ishigami._exact_sensitivity_indices(idxs,a,b)\nsi_closed_true = true_indices[0]\nsi_total_true = true_indices[1]\n</pre> a,b = 7,0.1 dnb2 = qp.DigitalNetB2(3,seed=7) ishigami = qp.Ishigami(dnb2,a,b) idxs = np.array([     [True,False,False],     [False,True,False],     [False,False,True],     [True,True,False],     [True,False,True],     [False,True,True]],dtype=bool) ishigami_si = qp.SensitivityIndices(ishigami,idxs) qmc_algo = qp.CubQMCNetG(ishigami_si,abs_tol=.05) solution,data = qmc_algo.integrate() print(data) si_closed = solution[0].squeeze() si_total = solution[1].squeeze() ci_comb_low_closed = data.comb_bound_low[0].squeeze() ci_comb_high_closed = data.comb_bound_high[0].squeeze() ci_comb_low_total = data.comb_bound_low[1].squeeze() ci_comb_high_total = data.comb_bound_high[1].squeeze() print(\"\\nApprox took %.1f sec and n = 2^(%d)\"%     (data.time_integrate,np.log2(data.n_total))) print('\\t si_closed:',si_closed) print('\\t si_total:',si_total) print('\\t ci_comb_low_closed:',ci_comb_low_closed) print('\\t ci_comb_high_closed:',ci_comb_high_closed) print('\\t ci_comb_low_total:',ci_comb_low_total) print('\\t ci_comb_high_total:',ci_comb_high_total)  true_indices = qp.Ishigami._exact_sensitivity_indices(idxs,a,b) si_closed_true = true_indices[0] si_total_true = true_indices[1] <pre>Data (Data)\n    solution        [[0.315 0.421 0.004 0.735 0.558 0.419]\n                     [0.565 0.444 0.244 0.987 0.558 0.688]]\n    comb_bound_low  [[0.293 0.409 0.    0.706 0.534 0.399]\n                     [0.541 0.435 0.234 0.974 0.539 0.667]]\n    comb_bound_high [[0.336 0.432 0.008 0.765 0.582 0.438]\n                     [0.59  0.452 0.253 1.    0.577 0.708]]\n    comb_bound_diff [[0.043 0.023 0.008 0.059 0.047 0.039]\n                     [0.049 0.017 0.02  0.026 0.038 0.042]]\n    comb_flags      [[ True  True  True  True  True  True]\n                     [ True  True  True  True  True  True]]\n    n_total         2^(10)\n    n               [[[1024 1024 1024 1024 1024 1024]\n                      [1024 1024 1024 1024 1024 1024]\n                      [1024 1024 1024 1024 1024 1024]]\n                    \n                     [[1024 1024 1024 1024 1024 1024]\n                      [1024 1024 1024 1024 1024 1024]\n                      [1024 1024 1024 1024 1024 1024]]]\n    time_integrate  0.008\nCubQMCNetG (AbstractStoppingCriterion)\n    abs_tol         0.050\n    rel_tol         0\n    n_init          2^(10)\n    n_limit         2^(35)\nSensitivityIndices (AbstractIntegrand)\n    indices         [[ True False False]\n                     [False  True False]\n                     [False False  True]\n                     [ True  True False]\n                     [ True False  True]\n                     [False  True  True]]\nUniform (AbstractTrueMeasure)\n    lower_bound     -3.142\n    upper_bound     3.142\nDigitalNetB2 (AbstractLDDiscreteDistribution)\n    d               3\n    replications    1\n    randomize       LMS DS\n    gen_mats_source joe_kuo.6.21201.txt\n    order           RADICAL INVERSE\n    t               63\n    alpha           1\n    n_limit         2^(32)\n    entropy         7\n\nApprox took 0.0 sec and n = 2^(10)\n\t si_closed: [0.31495301 0.42054985 0.00403061 0.73545156 0.5580266  0.41868287]\n\t si_total: [0.56524058 0.4435145  0.24364672 0.98722305 0.557734   0.68753131]\n\t ci_comb_low_closed: [0.2934895  0.4092381  0.         0.70577578 0.53435058 0.39922204]\n\t ci_comb_high_closed: [0.33641652 0.4318616  0.00806122 0.76512734 0.58170261 0.4381437 ]\n\t ci_comb_low_total: [0.54066449 0.43508219 0.23389506 0.9744461  0.53864679 0.66669245]\n\t ci_comb_high_total: [0.58981667 0.45194681 0.25339839 1.         0.57682122 0.70837017]\n</pre> In\u00a0[15]: Copied! <pre>fig,ax = pyplot.subplots(figsize=(6,3))\nax.grid(False)\nfor spine in ['top','left','right','bottom']: ax.spines[spine].set_visible(False)\nwidth = .75\nax.errorbar(fmt='none',color='k',\n    x = 1-si_total_true,\n    y = np.arange(len(si_closed)),\n    xerr = 0,\n    yerr = width/2,\n    alpha = 1)\nbar_closed = ax.barh(np.arange(len(si_closed)),np.flip(si_closed),width,label='Closed SI',color='w',edgecolor='k',alpha=.75,linestyle='--')\nax.errorbar(fmt='none',color='k',\n    x = si_closed,\n    y = np.flip(np.arange(len(si_closed)))+width/4,\n    xerr = np.vstack((si_closed-ci_comb_low_closed,ci_comb_high_closed-si_closed)),\n    yerr = 0,\n    #elinewidth = 5,\n    alpha = .75)\nbar_total = ax.barh(np.arange(len(si_closed)),si_total,width,label='Total SI',color='w',alpha=.25,edgecolor='k',left=1-si_total,zorder=10,linestyle='-.')\nax.errorbar(fmt='none',color='k',\n    x = 1-si_total,\n    y = np.arange(len(si_closed))-width/4,\n    xerr = np.vstack((si_total-ci_comb_low_total,ci_comb_high_total-si_total)),\n    yerr = 0,\n    #elinewidth = 5,\n    alpha = .25)\nclosed_labels = [r'$\\underline{s}_{\\{%s\\}} = %.2f$'%(','.join([str(i+1) for i in idx]),c) for idx,c in zip(idxs[::-1],np.flip(si_closed))]\nclosed_labels[3] = ''\ntotal_labels = [r'$\\overline{s}_{\\{%s\\}} = %.2f$'%(','.join([str(i+1) for i in idx]),t) for idx,t in zip(idxs,si_total)]\nax.bar_label(bar_closed,label_type='center',labels=closed_labels)\nax.bar_label(bar_total,label_type='center',labels=total_labels)\nax.set_xlim([-.001,1.001])\nax.axvline(x=0,ymin=0,ymax=len(si_closed),color='k',alpha=.25)\nax.axvline(x=1,ymin=0,ymax=len(si_closed),color='k',alpha=.25)\nax.set_yticklabels([])\nif root: fig.savefig(root+'ishigami.pdf',transparent=True)\n</pre> fig,ax = pyplot.subplots(figsize=(6,3)) ax.grid(False) for spine in ['top','left','right','bottom']: ax.spines[spine].set_visible(False) width = .75 ax.errorbar(fmt='none',color='k',     x = 1-si_total_true,     y = np.arange(len(si_closed)),     xerr = 0,     yerr = width/2,     alpha = 1) bar_closed = ax.barh(np.arange(len(si_closed)),np.flip(si_closed),width,label='Closed SI',color='w',edgecolor='k',alpha=.75,linestyle='--') ax.errorbar(fmt='none',color='k',     x = si_closed,     y = np.flip(np.arange(len(si_closed)))+width/4,     xerr = np.vstack((si_closed-ci_comb_low_closed,ci_comb_high_closed-si_closed)),     yerr = 0,     #elinewidth = 5,     alpha = .75) bar_total = ax.barh(np.arange(len(si_closed)),si_total,width,label='Total SI',color='w',alpha=.25,edgecolor='k',left=1-si_total,zorder=10,linestyle='-.') ax.errorbar(fmt='none',color='k',     x = 1-si_total,     y = np.arange(len(si_closed))-width/4,     xerr = np.vstack((si_total-ci_comb_low_total,ci_comb_high_total-si_total)),     yerr = 0,     #elinewidth = 5,     alpha = .25) closed_labels = [r'$\\underline{s}_{\\{%s\\}} = %.2f$'%(','.join([str(i+1) for i in idx]),c) for idx,c in zip(idxs[::-1],np.flip(si_closed))] closed_labels[3] = '' total_labels = [r'$\\overline{s}_{\\{%s\\}} = %.2f$'%(','.join([str(i+1) for i in idx]),t) for idx,t in zip(idxs,si_total)] ax.bar_label(bar_closed,label_type='center',labels=closed_labels) ax.bar_label(bar_total,label_type='center',labels=total_labels) ax.set_xlim([-.001,1.001]) ax.axvline(x=0,ymin=0,ymax=len(si_closed),color='k',alpha=.25) ax.axvline(x=1,ymin=0,ymax=len(si_closed),color='k',alpha=.25) ax.set_yticklabels([]) if root: fig.savefig(root+'ishigami.pdf',transparent=True) In\u00a0[14]: Copied! <pre>fig,ax = pyplot.subplots(nrows=1,ncols=3,figsize=(8,3))\nx_1d = np.linspace(0,1,num=128)\nx_1d_mat = np.tile(x_1d,(3,1)).T\ny_1d = qp.Ishigami._exact_fu_functions(x_1d_mat,indices=[[0],[1],[2]],a=a,b=b)\nfor i in range(2):\n    ax[i].plot(x_1d,y_1d[:,i],color='k')\n    ax[i].set_xlim([0,1])\n    ax[i].set_xticks([0,1])\n    ax[i].set_xlabel(r'$x_{%d}$'%(i+1))\n    ax[i].set_title(r'$f_{\\{%d\\}} \\in [%.1f,%.1f]$'%(i+1,y_1d[:,i].min(),y_1d[:,i].max()))\nx_mesh,y_mesh = np.meshgrid(x_1d,x_1d)\nxquery = np.zeros((x_mesh.size,3))\nfor i,idx in enumerate([[1,2]]): # [[0,1],[0,2],[1,2]]\n    xquery[:,idx[0]] = x_mesh.flatten()\n    xquery[:,idx[1]] = y_mesh.flatten()\n    zquery = qp.Ishigami._exact_fu_functions(xquery,indices=[idx],a=a,b=b)\n    z_mesh = zquery.reshape(x_mesh.shape)\n    ax[2+i].contourf(x_mesh,y_mesh,z_mesh,cmap=cm.Greys_r)\n    ax[2+i].set_xlabel(r'$x_{%d}$'%(idx[0]+1))\n    ax[2+i].set_ylabel(r'$x_{%d}$'%(idx[1]+1))\n    ax[2+i].set_title(r'$f_{\\{%d,%d\\}} \\in [%.1f,%.1f]$'%(tuple([i+1 for i in idx])+(z_mesh.min(),z_mesh.max())))\n    ax[2+i].set_xlim([0,1])\n    ax[2+i].set_ylim([0,1])\n    ax[2+i].set_xticks([0,1])\n    ax[2+i].set_yticks([0,1])\nif root: fig.savefig(root+'ishigami_fu.pdf')\n</pre> fig,ax = pyplot.subplots(nrows=1,ncols=3,figsize=(8,3)) x_1d = np.linspace(0,1,num=128) x_1d_mat = np.tile(x_1d,(3,1)).T y_1d = qp.Ishigami._exact_fu_functions(x_1d_mat,indices=[[0],[1],[2]],a=a,b=b) for i in range(2):     ax[i].plot(x_1d,y_1d[:,i],color='k')     ax[i].set_xlim([0,1])     ax[i].set_xticks([0,1])     ax[i].set_xlabel(r'$x_{%d}$'%(i+1))     ax[i].set_title(r'$f_{\\{%d\\}} \\in [%.1f,%.1f]$'%(i+1,y_1d[:,i].min(),y_1d[:,i].max())) x_mesh,y_mesh = np.meshgrid(x_1d,x_1d) xquery = np.zeros((x_mesh.size,3)) for i,idx in enumerate([[1,2]]): # [[0,1],[0,2],[1,2]]     xquery[:,idx[0]] = x_mesh.flatten()     xquery[:,idx[1]] = y_mesh.flatten()     zquery = qp.Ishigami._exact_fu_functions(xquery,indices=[idx],a=a,b=b)     z_mesh = zquery.reshape(x_mesh.shape)     ax[2+i].contourf(x_mesh,y_mesh,z_mesh,cmap=cm.Greys_r)     ax[2+i].set_xlabel(r'$x_{%d}$'%(idx[0]+1))     ax[2+i].set_ylabel(r'$x_{%d}$'%(idx[1]+1))     ax[2+i].set_title(r'$f_{\\{%d,%d\\}} \\in [%.1f,%.1f]$'%(tuple([i+1 for i in idx])+(z_mesh.min(),z_mesh.max())))     ax[2+i].set_xlim([0,1])     ax[2+i].set_ylim([0,1])     ax[2+i].set_xticks([0,1])     ax[2+i].set_yticks([0,1]) if root: fig.savefig(root+'ishigami_fu.pdf') In\u00a0[17]: Copied! <pre>from sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neural_network import MLPClassifier\ndata = load_iris()\nfeature_names = data[\"feature_names\"]\nfeature_names = [fn.replace('sepal ','S')\\\n    .replace('length ','L')\\\n    .replace('petal ','P')\\\n    .replace('width ','W')\\\n    .replace('(cm)','') for fn in feature_names]\ntarget_names = data[\"target_names\"]\nxt,xv,yt,yv = train_test_split(data[\"data\"],data[\"target\"],\n    test_size = 1/3,\n    random_state = 7)\n</pre> from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split from sklearn.neural_network import MLPClassifier data = load_iris() feature_names = data[\"feature_names\"] feature_names = [fn.replace('sepal ','S')\\     .replace('length ','L')\\     .replace('petal ','P')\\     .replace('width ','W')\\     .replace('(cm)','') for fn in feature_names] target_names = data[\"target_names\"] xt,xv,yt,yv = train_test_split(data[\"data\"],data[\"target\"],     test_size = 1/3,     random_state = 7) In\u00a0[\u00a0]: Copied! <pre>mlpc = MLPClassifier(random_state=7,max_iter=1024).fit(xt,yt)\nyhat = mlpc.predict(xv)\nprint(\"accuracy: %.1f%%\"%(100*(yv==yhat).mean()))\n# accuracy: 98.0%\nsampler = qp.DigitalNetB2(4,seed=7)\ntrue_measure =  qp.Uniform(sampler,\n    lower_bound = xt.min(0),\n    upper_bound = xt.max(0))\nfun = qp.CustomFun(\n    true_measure = true_measure,\n    g = lambda x: mlpc.predict_proba(x).T,\n    dimension_indv = 3)\nsi_fun = qp.SensitivityIndices(fun,indices=\"all\")\nqmc_algo = qp.CubQMCNetG(si_fun,abs_tol=.005)\nnn_sis,nn_sis_data = qmc_algo.integrate()\nnn_sis.shape\n\"\"\"\n(2, 14, 3)\n\"\"\"\n</pre> mlpc = MLPClassifier(random_state=7,max_iter=1024).fit(xt,yt) yhat = mlpc.predict(xv) print(\"accuracy: %.1f%%\"%(100*(yv==yhat).mean())) # accuracy: 98.0% sampler = qp.DigitalNetB2(4,seed=7) true_measure =  qp.Uniform(sampler,     lower_bound = xt.min(0),     upper_bound = xt.max(0)) fun = qp.CustomFun(     true_measure = true_measure,     g = lambda x: mlpc.predict_proba(x).T,     dimension_indv = 3) si_fun = qp.SensitivityIndices(fun,indices=\"all\") qmc_algo = qp.CubQMCNetG(si_fun,abs_tol=.005) nn_sis,nn_sis_data = qmc_algo.integrate() nn_sis.shape \"\"\" (2, 14, 3) \"\"\" <pre>accuracy: 98.0%\n</pre> Out[\u00a0]: <pre>(2, 14, 3)</pre> In\u00a0[20]: Copied! <pre>#print(nn_sis_data.flags_indv.shape)\n#print(nn_sis_data.flags_comb.shape)\nprint('samples: 2^(%d)'%np.log2(nn_sis_data.n_total))\nprint('time: %.1e'%nn_sis_data.time_integrate)\nprint('indices:\\n%s'%nn_sis_data.integrand.indices)\n\nimport pandas as pd\n\ndf_closed = pd.DataFrame(nn_sis[0],columns=target_names,index=[str(np.where(idx)[0]) for idx in nn_sis_data.integrand.indices])\nprint('\\nClosed Indices')\nprint(df_closed)\ndf_total = pd.DataFrame(nn_sis[1],columns=target_names,index=[str(np.where(idx)[0]) for idx in nn_sis_data.integrand.indices])\nprint('\\nTotal Indices')\ndf_closed_singletons = df_closed.loc[['[%d]'%i for i in range(4)]].T\ndf_closed_singletons['sum singletons'] = df_closed_singletons[['[%d]'%i for i in range(4)]].sum(1)\ndf_closed_singletons.columns = data['feature_names']+['sum']\ndf_closed_singletons = df_closed_singletons*100\ndf_closed_singletons\n</pre> #print(nn_sis_data.flags_indv.shape) #print(nn_sis_data.flags_comb.shape) print('samples: 2^(%d)'%np.log2(nn_sis_data.n_total)) print('time: %.1e'%nn_sis_data.time_integrate) print('indices:\\n%s'%nn_sis_data.integrand.indices)  import pandas as pd  df_closed = pd.DataFrame(nn_sis[0],columns=target_names,index=[str(np.where(idx)[0]) for idx in nn_sis_data.integrand.indices]) print('\\nClosed Indices') print(df_closed) df_total = pd.DataFrame(nn_sis[1],columns=target_names,index=[str(np.where(idx)[0]) for idx in nn_sis_data.integrand.indices]) print('\\nTotal Indices') df_closed_singletons = df_closed.loc[['[%d]'%i for i in range(4)]].T df_closed_singletons['sum singletons'] = df_closed_singletons[['[%d]'%i for i in range(4)]].sum(1) df_closed_singletons.columns = data['feature_names']+['sum'] df_closed_singletons = df_closed_singletons*100 df_closed_singletons <pre>samples: 2^(15)\ntime: 6.6e-01\nindices:\n[[ True False False False]\n [False  True False False]\n [False False  True False]\n [False False False  True]\n [ True  True False False]\n [ True False  True False]\n [ True False False  True]\n [False  True  True False]\n [False  True False  True]\n [False False  True  True]\n [ True  True  True False]\n [ True  True False  True]\n [ True False  True  True]\n [False  True  True  True]]\n\nClosed Indices\n           setosa  versicolor  virginica\n[0]      0.001323    0.068645   0.077325\n[1]      0.063749    0.026565   0.004784\n[2]      0.713825    0.325072   0.497800\n[3]      0.052967    0.025579   0.120317\n[0 1]    0.063925    0.091300   0.085151\n[0 2]    0.715316    0.460314   0.637738\n[0 3]    0.053469    0.092601   0.205639\n[1 2]    0.841655    0.431035   0.513277\n[1 3]    0.110739    0.039410   0.131264\n[2 3]    0.822910    0.583282   0.703142\n[0 1 2]  0.843726    0.570076   0.658272\n[0 1 3]  0.112798    0.104804   0.215817\n[0 2 3]  0.825330    0.815263   0.945267\n[1 2 3]  0.995864    0.739588   0.728499\n\nTotal Indices\n</pre> Out[20]: sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) sum setosa 0.132343 6.374860 71.382498 5.296674 83.186375 versicolor 6.864536 2.656530 32.507230 2.557911 44.586208 virginica 7.732521 0.478364 49.779978 12.031725 70.022587 In\u00a0[21]: Copied! <pre>nindices = len(nn_sis_data.integrand.indices)\nfig,ax = pyplot.subplots(figsize=(9,5))\nticks = np.arange(nindices)\nwidth = .25\nfor i,(alpha,species) in enumerate(zip([.25,.5,.75],data['target_names'])):\n    cvals = df_closed[species].to_numpy()\n    tvals = df_total[species].to_numpy()\n    ticks_i = ticks+i*width\n    ax.bar(ticks_i,cvals,width=width,align='edge',color='k',alpha=alpha,label=species)\n    #ax.bar(ticks_i,np.flip(tvals),width=width,align='edge',bottom=1-np.flip(tvals),color=color,alpha=.1)\nax.set_xlim([0,13+3*width])\nax.set_xticks(ticks+1.5*width)\n\n# closed_labels = [r'$\\underline{s}_{\\{%s\\}}$'%(','.join([r'\\text{%s}'%feature_names[i] for i in idx])) for idx in nn_sis_data.integrand.indices]\nclosed_labels = ['\\n'.join([feature_names[i] for i in np.where(idx)[0]]) for idx in nn_sis_data.integrand.indices]\nax.set_xticklabels(closed_labels,rotation=0)\nax.set_ylim([0,1]); ax.set_yticks([0,1])\nax.grid(False)\nfor spine in ['top','right','bottom']: ax.spines[spine].set_visible(False)\nax.legend(frameon=False,loc='lower center',bbox_to_anchor=(.5,-.2),ncol=3);\nfig.tight_layout()\nif root: fig.savefig(root+'nn_si.pdf')\n</pre> nindices = len(nn_sis_data.integrand.indices) fig,ax = pyplot.subplots(figsize=(9,5)) ticks = np.arange(nindices) width = .25 for i,(alpha,species) in enumerate(zip([.25,.5,.75],data['target_names'])):     cvals = df_closed[species].to_numpy()     tvals = df_total[species].to_numpy()     ticks_i = ticks+i*width     ax.bar(ticks_i,cvals,width=width,align='edge',color='k',alpha=alpha,label=species)     #ax.bar(ticks_i,np.flip(tvals),width=width,align='edge',bottom=1-np.flip(tvals),color=color,alpha=.1) ax.set_xlim([0,13+3*width]) ax.set_xticks(ticks+1.5*width)  # closed_labels = [r'$\\underline{s}_{\\{%s\\}}$'%(','.join([r'\\text{%s}'%feature_names[i] for i in idx])) for idx in nn_sis_data.integrand.indices] closed_labels = ['\\n'.join([feature_names[i] for i in np.where(idx)[0]]) for idx in nn_sis_data.integrand.indices] ax.set_xticklabels(closed_labels,rotation=0) ax.set_ylim([0,1]); ax.set_yticks([0,1]) ax.grid(False) for spine in ['top','right','bottom']: ax.spines[spine].set_visible(False) ax.legend(frameon=False,loc='lower center',bbox_to_anchor=(.5,-.2),ncol=3); fig.tight_layout() if root: fig.savefig(root+'nn_si.pdf') In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"demos/vectorized_qmc/#vectorized-qmc","title":"Vectorized QMC\u00b6","text":""},{"location":"demos/vectorized_qmc/#ld-sequence","title":"LD Sequence\u00b6","text":""},{"location":"demos/vectorized_qmc/#simple-example","title":"Simple Example\u00b6","text":""},{"location":"demos/vectorized_qmc/#bo-qei","title":"BO QEI\u00b6","text":"<p>See the QEI Demo in QMCPy or the BoTorch Acquisition documentation for details on Bayesian Optimization using q-Expected Improvement.</p>"},{"location":"demos/vectorized_qmc/#bayesian-logistic-regression","title":"Bayesian Logistic Regression\u00b6","text":""},{"location":"demos/vectorized_qmc/#sensitivity-indices","title":"Sensitivity Indices\u00b6","text":""},{"location":"demos/vectorized_qmc/#ishigami-function","title":"Ishigami Function\u00b6","text":""},{"location":"demos/vectorized_qmc/#neural-network","title":"Neural Network\u00b6","text":""},{"location":"demos/vectorized_qmc_bayes/","title":"Vectorized QMC Bayesian Cubature","text":"In\u00a0[1]: Copied! <pre>%%capture\n# @title Execute this cell to install dependancies\ntry:\n  import google.colab\n  import os\n  !pip install -q qmcpy &gt;&gt; /dev/null\n  !apt-get update &amp;&amp; apt-get install -y --no-install-recommends texlive-latex-base texlive-fonts-recommended texlive-latex-extra cm-super dvipng\nexcept:\n  pass\n\nimport matplotlib.pyplot as plt\n\nplt.rcParams.update({\n\"text.usetex\": True,\n\"font.family\": \"serif\",\n\"text.latex.preamble\": r\"\\usepackage{amsmath}\\usepackage{amssymb}\\newcommand{\\bx}{\\boldsymbol{x}}\"\n})\n!pip install scikit-learn\n</pre> %%capture # @title Execute this cell to install dependancies try:   import google.colab   import os   !pip install -q qmcpy &gt;&gt; /dev/null   !apt-get update &amp;&amp; apt-get install -y --no-install-recommends texlive-latex-base texlive-fonts-recommended texlive-latex-extra cm-super dvipng except:   pass  import matplotlib.pyplot as plt  plt.rcParams.update({ \"text.usetex\": True, \"font.family\": \"serif\", \"text.latex.preamble\": r\"\\usepackage{amsmath}\\usepackage{amssymb}\\newcommand{\\bx}{\\boldsymbol{x}}\" }) !pip install scikit-learn  In\u00a0[2]: Copied! <pre>import qmcpy as qp\nimport numpy as np\n</pre> import qmcpy as qp import numpy as np In\u00a0[7]: Copied! <pre>from matplotlib import pyplot\n%matplotlib inline\nroot = None#'.'\n</pre> from matplotlib import pyplot %matplotlib inline root = None#'.' In\u00a0[8]: Copied! <pre>n = 2**6\ns = 10\nfig,ax = pyplot.subplots(figsize=(8,4),nrows=1,ncols=3)\nfor i,(dd,name) in enumerate(zip(\n    [qp.IIDStdUniform(2,seed=7),qp.DigitalNetB2(2,seed=7),qp.Lattice(2,seed=7)],\n    ['IID','Randomized Digital Net (LD)','Randomized Lattice (LD)'])):\n    pts = dd.gen_samples(n)\n    ax[i].scatter(pts[0:n//4,0],pts[0:n//4,1],color='k',marker='s',s=s)\n    ax[i].scatter(pts[n//4:n//2,0],pts[n//4:n//2,1],color='k',marker='o',s=s)\n    ax[i].scatter(pts[n//2:n,0],pts[n//2:n,1],color='k',marker='^',s=s)\n    ax[i].set_aspect(1)\n    ax[i].set_xlabel(r'$x_{1}$')\n    ax[i].set_ylabel(r'$x_{2}$')\n    ax[i].set_xlim([0,1])\n    ax[i].set_ylim([0,1])\n    ax[i].set_xticks([0,.25,.5,.75,1])\n    ax[i].set_xticklabels([r'$0$',r'$1/4$',r'$1/2$',r'$3/4$',r'$1$'])\n    ax[i].set_yticks([0,.25,.5,.75,1])\n    ax[i].set_yticklabels([r'$0$',r'$1/4$',r'$1/2$',r'$3/4$',r'$1$'])\n    ax[i].set_title(name)\nif root: fig.savefig(root+'ld_seqs.pdf',transparent=True)\n</pre> n = 2**6 s = 10 fig,ax = pyplot.subplots(figsize=(8,4),nrows=1,ncols=3) for i,(dd,name) in enumerate(zip(     [qp.IIDStdUniform(2,seed=7),qp.DigitalNetB2(2,seed=7),qp.Lattice(2,seed=7)],     ['IID','Randomized Digital Net (LD)','Randomized Lattice (LD)'])):     pts = dd.gen_samples(n)     ax[i].scatter(pts[0:n//4,0],pts[0:n//4,1],color='k',marker='s',s=s)     ax[i].scatter(pts[n//4:n//2,0],pts[n//4:n//2,1],color='k',marker='o',s=s)     ax[i].scatter(pts[n//2:n,0],pts[n//2:n,1],color='k',marker='^',s=s)     ax[i].set_aspect(1)     ax[i].set_xlabel(r'$x_{1}$')     ax[i].set_ylabel(r'$x_{2}$')     ax[i].set_xlim([0,1])     ax[i].set_ylim([0,1])     ax[i].set_xticks([0,.25,.5,.75,1])     ax[i].set_xticklabels([r'$0$',r'$1/4$',r'$1/2$',r'$3/4$',r'$1$'])     ax[i].set_yticks([0,.25,.5,.75,1])     ax[i].set_yticklabels([r'$0$',r'$1/4$',r'$1/2$',r'$3/4$',r'$1$'])     ax[i].set_title(name) if root: fig.savefig(root+'ld_seqs.pdf',transparent=True) In\u00a0[11]: Copied! <pre>def cantilever_beam_function(T,compute_flags): # T is (n x 3)\n    Y = np.zeros((2,len(T)),dtype=float) # (n x 2)\n    l,w,t = 100,4,2\n    T1,T2,T3 = T[:,0],T[:,1],T[:,2] # Python is indexed from 0\n    if compute_flags[0]: # compute D. x^2 is \"x**2\" in Python\n        Y[0] = 4*l**3/(T1*w*t)*np.sqrt(T2**2/t**4+T3**2/w**4)\n    if compute_flags[1]: # compute S\n        Y[1] = 600*(T2/(w*t**2)+T3/(w**2*t))\n    return Y\ntrue_measure = qp.Gaussian(\n    sampler = qp.DigitalNetB2(dimension=3,seed=7),\n    mean = [2.9e7,500,1000],\n    covariance = np.diag([(1.45e6)**2,(100)**2,(100)**2]))\nintegrand = qp.CustomFun(true_measure,\n    g = cantilever_beam_function,\n    dimension_indv = 2)\nqmc_stop_crit = qp.CubBayesNetG(integrand,\n    abs_tol = 1e-3,\n    rel_tol = 1e-6)\nsolution,data = qmc_stop_crit.integrate()\nprint(solution)\n# [2.42575885e+00 3.74999973e+04]\nprint(data)\n</pre> def cantilever_beam_function(T,compute_flags): # T is (n x 3)     Y = np.zeros((2,len(T)),dtype=float) # (n x 2)     l,w,t = 100,4,2     T1,T2,T3 = T[:,0],T[:,1],T[:,2] # Python is indexed from 0     if compute_flags[0]: # compute D. x^2 is \"x**2\" in Python         Y[0] = 4*l**3/(T1*w*t)*np.sqrt(T2**2/t**4+T3**2/w**4)     if compute_flags[1]: # compute S         Y[1] = 600*(T2/(w*t**2)+T3/(w**2*t))     return Y true_measure = qp.Gaussian(     sampler = qp.DigitalNetB2(dimension=3,seed=7),     mean = [2.9e7,500,1000],     covariance = np.diag([(1.45e6)**2,(100)**2,(100)**2])) integrand = qp.CustomFun(true_measure,     g = cantilever_beam_function,     dimension_indv = 2) qmc_stop_crit = qp.CubBayesNetG(integrand,     abs_tol = 1e-3,     rel_tol = 1e-6) solution,data = qmc_stop_crit.integrate() print(solution) # [2.42575885e+00 3.74999973e+04] print(data) <pre>[2.42553968e+00 3.75000056e+04]\nData (Data)\n    solution        [2.426e+00 3.750e+04]\n    comb_bound_low  [2.425e+00 3.750e+04]\n    comb_bound_high [2.426e+00 3.750e+04]\n    comb_bound_diff [0.001 0.028]\n    comb_flags      [ True  True]\n    n_total         2^(17)\n    n               [  1024 131072]\n    time_integrate  1.515\nCubQMCBayesNetG (AbstractStoppingCriterion)\n    abs_tol         0.001\n    rel_tol         1.00e-06\n    n_init          2^(8)\n    n_limit         2^(22)\n    order           1\nCustomFun (AbstractIntegrand)\nGaussian (AbstractTrueMeasure)\n    mean            [2.9e+07 5.0e+02 1.0e+03]\n    covariance      [[2.102e+12 0.000e+00 0.000e+00]\n                     [0.000e+00 1.000e+04 0.000e+00]\n                     [0.000e+00 0.000e+00 1.000e+04]]\n    decomp_type     PCA\nDigitalNetB2 (AbstractLDDiscreteDistribution)\n    d               3\n    replications    1\n    randomize       LMS_DS\n    gen_mats_source joe_kuo.6.21201.txt\n    order           NATURAL\n    t               63\n    alpha           1\n    n_limit         2^(32)\n    entropy         7\n</pre> In\u00a0[12]: Copied! <pre>import scipy\nfrom sklearn.gaussian_process import GaussianProcessRegressor,kernels\n\nf = lambda x: np.cos(10*x)*np.exp(.2*x)+np.exp(-5*(x-.4)**2)\nxplt = np.linspace(0,1,100)\nyplt = f(xplt)\nx = np.array([.1, .2, .4, .7, .9])\ny = f(x)\nymax = y.max()\n\ngp = GaussianProcessRegressor(kernel=kernels.RBF(length_scale=1.0,length_scale_bounds=(1e-2, 1e2)),\n    n_restarts_optimizer = 16).fit(x[:,None],y)\nyhatplt,stdhatplt = gp.predict(xplt[:,None],return_std=True)\n\ntpax = 32\nx0mesh,x1mesh = np.meshgrid(np.linspace(0,1,tpax),np.linspace(0,1,tpax))\npost_mus = np.zeros((tpax,tpax,2),dtype=float)\npost_sqrtcovs = np.zeros((tpax,tpax,2,2),dtype=float)\nfor j0 in range(tpax):\n    for j1 in range(tpax):\n        candidate = np.array([[x0mesh[j0,j1]],[x1mesh[j0,j1]]])\n        post_mus[j0,j1],post_cov = gp.predict(candidate,return_cov=True)\n        evals,evecs = scipy.linalg.eig(post_cov)\n        post_sqrtcovs[j0,j1] = np.sqrt(np.maximum(evals.real,0))*evecs\n\ndef qei_acq_vec(x,compute_flags):\n    xgauss = scipy.stats.norm.ppf(x)\n    n = len(x)\n    qei_vals = np.zeros((tpax,tpax,n),dtype=float)\n    for j0 in range(tpax):\n        for j1 in range(tpax):\n            if compute_flags[j0,j1]==False: continue\n            sqrt_cov = post_sqrtcovs[j0,j1]\n            mu_post = post_mus[j0,j1]\n            for i in range(len(x)):\n                yij = sqrt_cov@xgauss[i]+mu_post\n                qei_vals[j0,j1,i] = max((yij-ymax).max(),0)\n    return qei_vals\n\nqei_acq_vec_qmcpy = qp.CustomFun(\n    true_measure = qp.Uniform(qp.DigitalNetB2(2,seed=7)),\n    g = qei_acq_vec,\n    dimension_indv = (tpax,tpax),\n    parallel=False)\nqei_vals,qei_data = qp.CubBayesNetG(qei_acq_vec_qmcpy,abs_tol=.025,rel_tol=0).integrate() # .0005\nprint(qei_data)\n\na = np.unravel_index(np.argmax(qei_vals,axis=None),qei_vals.shape)\nxnext = np.array([x0mesh[a[0],a[1]],x1mesh[a[0],a[1]]])\nfnext = f(xnext)\n</pre> import scipy from sklearn.gaussian_process import GaussianProcessRegressor,kernels  f = lambda x: np.cos(10*x)*np.exp(.2*x)+np.exp(-5*(x-.4)**2) xplt = np.linspace(0,1,100) yplt = f(xplt) x = np.array([.1, .2, .4, .7, .9]) y = f(x) ymax = y.max()  gp = GaussianProcessRegressor(kernel=kernels.RBF(length_scale=1.0,length_scale_bounds=(1e-2, 1e2)),     n_restarts_optimizer = 16).fit(x[:,None],y) yhatplt,stdhatplt = gp.predict(xplt[:,None],return_std=True)  tpax = 32 x0mesh,x1mesh = np.meshgrid(np.linspace(0,1,tpax),np.linspace(0,1,tpax)) post_mus = np.zeros((tpax,tpax,2),dtype=float) post_sqrtcovs = np.zeros((tpax,tpax,2,2),dtype=float) for j0 in range(tpax):     for j1 in range(tpax):         candidate = np.array([[x0mesh[j0,j1]],[x1mesh[j0,j1]]])         post_mus[j0,j1],post_cov = gp.predict(candidate,return_cov=True)         evals,evecs = scipy.linalg.eig(post_cov)         post_sqrtcovs[j0,j1] = np.sqrt(np.maximum(evals.real,0))*evecs  def qei_acq_vec(x,compute_flags):     xgauss = scipy.stats.norm.ppf(x)     n = len(x)     qei_vals = np.zeros((tpax,tpax,n),dtype=float)     for j0 in range(tpax):         for j1 in range(tpax):             if compute_flags[j0,j1]==False: continue             sqrt_cov = post_sqrtcovs[j0,j1]             mu_post = post_mus[j0,j1]             for i in range(len(x)):                 yij = sqrt_cov@xgauss[i]+mu_post                 qei_vals[j0,j1,i] = max((yij-ymax).max(),0)     return qei_vals  qei_acq_vec_qmcpy = qp.CustomFun(     true_measure = qp.Uniform(qp.DigitalNetB2(2,seed=7)),     g = qei_acq_vec,     dimension_indv = (tpax,tpax),     parallel=False) qei_vals,qei_data = qp.CubBayesNetG(qei_acq_vec_qmcpy,abs_tol=.025,rel_tol=0).integrate() # .0005 print(qei_data)  a = np.unravel_index(np.argmax(qei_vals,axis=None),qei_vals.shape) xnext = np.array([x0mesh[a[0],a[1]],x1mesh[a[0],a[1]]]) fnext = f(xnext) <pre>Data (Data)\n    solution        [[0.058 0.081 0.074 ... 0.062 0.062 0.068]\n                     [0.078 0.063 0.069 ... 0.066 0.066 0.072]\n                     [0.072 0.066 0.032 ... 0.033 0.033 0.039]\n                     ...\n                     [0.058 0.063 0.032 ... 0.    0.    0.006]\n                     [0.061 0.065 0.034 ... 0.003 0.    0.006]\n                     [0.067 0.072 0.041 ... 0.01  0.01  0.006]]\n    comb_bound_low  [[0.056 0.077 0.07  ... 0.058 0.058 0.064]\n                     [0.076 0.061 0.065 ... 0.063 0.063 0.069]\n                     [0.069 0.064 0.031 ... 0.032 0.032 0.037]\n                     ...\n                     [0.056 0.061 0.031 ... 0.    0.    0.005]\n                     [0.058 0.063 0.032 ... 0.001 0.    0.005]\n                     [0.064 0.069 0.038 ... 0.006 0.006 0.004]]\n    comb_bound_high [[0.061 0.086 0.078 ... 0.066 0.066 0.073]\n                     [0.081 0.065 0.072 ... 0.069 0.07  0.076]\n                     [0.074 0.068 0.033 ... 0.035 0.035 0.042]\n                     ...\n                     [0.061 0.065 0.032 ... 0.    0.    0.008]\n                     [0.064 0.067 0.036 ... 0.005 0.    0.008]\n                     [0.071 0.075 0.044 ... 0.014 0.014 0.007]]\n    comb_bound_diff [[0.004 0.008 0.009 ... 0.008 0.008 0.008]\n                     [0.005 0.003 0.007 ... 0.006 0.006 0.007]\n                     [0.005 0.004 0.002 ... 0.004 0.004 0.005]\n                     ...\n                     [0.004 0.003 0.002 ... 0.    0.    0.003]\n                     [0.006 0.004 0.003 ... 0.004 0.    0.003]\n                     [0.007 0.007 0.007 ... 0.008 0.008 0.003]]\n    comb_flags      [[ True  True  True ...  True  True  True]\n                     [ True  True  True ...  True  True  True]\n                     [ True  True  True ...  True  True  True]\n                     ...\n                     [ True  True  True ...  True  True  True]\n                     [ True  True  True ...  True  True  True]\n                     [ True  True  True ...  True  True  True]]\n    n_total         2^(8)\n    n               [[256 256 256 ... 256 256 256]\n                     [256 256 256 ... 256 256 256]\n                     [256 256 256 ... 256 256 256]\n                     ...\n                     [256 256 256 ... 256 256 256]\n                     [256 256 256 ... 256 256 256]\n                     [256 256 256 ... 256 256 256]]\n    time_integrate  5.308\nCubQMCBayesNetG (AbstractStoppingCriterion)\n    abs_tol         0.025\n    rel_tol         0\n    n_init          2^(8)\n    n_limit         2^(22)\n    order           1\nCustomFun (AbstractIntegrand)\nUniform (AbstractTrueMeasure)\n    lower_bound     0\n    upper_bound     1\nDigitalNetB2 (AbstractLDDiscreteDistribution)\n    d               2^(1)\n    replications    1\n    randomize       LMS_DS\n    gen_mats_source joe_kuo.6.21201.txt\n    order           NATURAL\n    t               63\n    alpha           1\n    n_limit         2^(32)\n    entropy         7\n</pre> In\u00a0[14]: Copied! <pre>from matplotlib import cm\nfig,ax = pyplot.subplots(nrows=1,ncols=2,figsize=(10,4))\nax[0].scatter(x,y,color='k',label='Query Points')\nax[0].plot(xplt,yplt,color='k',linestyle='--',label='True function',linewidth=1)\nax[0].plot(xplt,yhatplt,color='k',label='GP Mean',linewidth=1)\nax[0].fill_between(xplt,yhatplt-1.96*stdhatplt,yhatplt+1.96*stdhatplt,color='k',alpha=.25,label='95% CI')\nax[0].scatter(xnext,fnext,color='k',marker='*',s=200,zorder=10)\nax[0].set_xlim([0,1])\nax[0].set_xticks([0,1])\nax[0].set_xlabel(r'$x$')\nax[0].set_ylabel(r'$y$')\nfig.legend(labels=['data','true function','posterior mean',r'95\\% CI','next points by QEI'],loc='lower center',bbox_to_anchor=(.5,-.05),ncol=5)\ncontour = ax[1].contourf(x0mesh,x1mesh,qei_vals,cmap=cm.Greys_r)\nax[1].scatter([xnext[0]],[xnext[1]],color='k',marker='*',s=200)\nfig.colorbar(contour,ax=None,shrink=1,aspect=5)\nax[1].scatter(x0mesh.flatten(),x1mesh.flatten(),color='w',s=1)\nax[1].set_xlim([0,1])\nax[1].set_xticks([0,1])\nax[1].set_ylim([0,1])\nax[1].set_yticks([0,1])\nax[1].set_xlabel(r'$x_1$')\nax[1].set_ylabel(r'$x_2$')\nif root: fig.savefig(root+'gp.pdf',transparent=True)\n</pre> from matplotlib import cm fig,ax = pyplot.subplots(nrows=1,ncols=2,figsize=(10,4)) ax[0].scatter(x,y,color='k',label='Query Points') ax[0].plot(xplt,yplt,color='k',linestyle='--',label='True function',linewidth=1) ax[0].plot(xplt,yhatplt,color='k',label='GP Mean',linewidth=1) ax[0].fill_between(xplt,yhatplt-1.96*stdhatplt,yhatplt+1.96*stdhatplt,color='k',alpha=.25,label='95% CI') ax[0].scatter(xnext,fnext,color='k',marker='*',s=200,zorder=10) ax[0].set_xlim([0,1]) ax[0].set_xticks([0,1]) ax[0].set_xlabel(r'$x$') ax[0].set_ylabel(r'$y$') fig.legend(labels=['data','true function','posterior mean',r'95\\% CI','next points by QEI'],loc='lower center',bbox_to_anchor=(.5,-.05),ncol=5) contour = ax[1].contourf(x0mesh,x1mesh,qei_vals,cmap=cm.Greys_r) ax[1].scatter([xnext[0]],[xnext[1]],color='k',marker='*',s=200) fig.colorbar(contour,ax=None,shrink=1,aspect=5) ax[1].scatter(x0mesh.flatten(),x1mesh.flatten(),color='w',s=1) ax[1].set_xlim([0,1]) ax[1].set_xticks([0,1]) ax[1].set_ylim([0,1]) ax[1].set_yticks([0,1]) ax[1].set_xlabel(r'$x_1$') ax[1].set_ylabel(r'$x_2$') if root: fig.savefig(root+'gp.pdf',transparent=True) In\u00a0[15]: Copied! <pre>import pandas as pd\nfrom sklearn.model_selection import train_test_split\ndf = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/haberman/haberman.data',header=None)\ndf.columns = ['Age','1900 Year','Axillary Nodes','Survival Status']\ndf.loc[df['Survival Status']==2,'Survival Status'] = 0\nx,y = df[['Age','1900 Year','Axillary Nodes']],df['Survival Status']\nxt,xv,yt,yv = train_test_split(x,y,test_size=.33,random_state=7)\n</pre> import pandas as pd from sklearn.model_selection import train_test_split df = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/haberman/haberman.data',header=None) df.columns = ['Age','1900 Year','Axillary Nodes','Survival Status'] df.loc[df['Survival Status']==2,'Survival Status'] = 0 x,y = df[['Age','1900 Year','Axillary Nodes']],df['Survival Status'] xt,xv,yt,yv = train_test_split(x,y,test_size=.33,random_state=7) In\u00a0[16]: Copied! <pre>print(df.head(),'\\n')\nprint(df[['Age','1900 Year','Axillary Nodes']].describe(),'\\n')\nprint(df['Survival Status'].astype(str).describe())\nprint('\\ntrain samples: %d test samples: %d\\n'%(len(xt),len(xv)))\nprint('train positives %d   train negatives: %d'%(np.sum(yt==1),np.sum(yt==0)))\nprint(' test positives %d    test negatives: %d'%(np.sum(yv==1),np.sum(yv==0)))\nxt.head()\n</pre> print(df.head(),'\\n') print(df[['Age','1900 Year','Axillary Nodes']].describe(),'\\n') print(df['Survival Status'].astype(str).describe()) print('\\ntrain samples: %d test samples: %d\\n'%(len(xt),len(xv))) print('train positives %d   train negatives: %d'%(np.sum(yt==1),np.sum(yt==0))) print(' test positives %d    test negatives: %d'%(np.sum(yv==1),np.sum(yv==0))) xt.head() <pre>   Age  1900 Year  Axillary Nodes  Survival Status\n0   30         64               1                1\n1   30         62               3                1\n2   30         65               0                1\n3   31         59               2                1\n4   31         65               4                1 \n\n              Age   1900 Year  Axillary Nodes\ncount  306.000000  306.000000      306.000000\nmean    52.457516   62.852941        4.026144\nstd     10.803452    3.249405        7.189654\nmin     30.000000   58.000000        0.000000\n25%     44.000000   60.000000        0.000000\n50%     52.000000   63.000000        1.000000\n75%     60.750000   65.750000        4.000000\nmax     83.000000   69.000000       52.000000 \n\ncount     306\nunique      2\ntop         1\nfreq      225\nName: Survival Status, dtype: object\n\ntrain samples: 205 test samples: 101\n\ntrain positives 151   train negatives: 54\n test positives 74    test negatives: 27\n</pre> Out[16]: Age 1900 Year Axillary Nodes 46 41 58 0 199 57 64 1 115 49 64 10 128 50 61 0 249 63 63 0 In\u00a0[17]: Copied! <pre>blr = qp.BayesianLRCoeffs(\n    sampler = qp.DigitalNetB2(4,seed=1),\n    feature_array = xt, # np.ndarray of shape (n,d-1)\n    response_vector = yt, # np.ndarray of shape (n,)\n    prior_mean = 0, # normal prior mean = (0,0,...,0)\n    prior_covariance = 5)\nqmc_sc = qp.CubQMCNetG(blr,\n    abs_tol = .1,\n    rel_tol = .5,\n    error_fun = \"BOTH\",\n    n_limit=2**18)\nblr_coefs,blr_data = qmc_sc.integrate()\nprint(blr_data)\n\n# LDTransformData (AccumulateData Object)\n#     solution        [-0.004  0.13  -0.157  0.008]\n#     comb_bound_low  [-0.006  0.092 -0.205  0.007]\n#     comb_bound_high [-0.003  0.172 -0.109  0.012]\n#     comb_flags      [ True  True  True  True]\n#     n_total         2^(18)\n#     n               [[  1024.   1024. 262144.   2048.]\n#                     [  1024.   1024. 262144.   2048.]]\n#     time_integrate  2.229\n</pre> blr = qp.BayesianLRCoeffs(     sampler = qp.DigitalNetB2(4,seed=1),     feature_array = xt, # np.ndarray of shape (n,d-1)     response_vector = yt, # np.ndarray of shape (n,)     prior_mean = 0, # normal prior mean = (0,0,...,0)     prior_covariance = 5) qmc_sc = qp.CubQMCNetG(blr,     abs_tol = .1,     rel_tol = .5,     error_fun = \"BOTH\",     n_limit=2**18) blr_coefs,blr_data = qmc_sc.integrate() print(blr_data)  # LDTransformData (AccumulateData Object) #     solution        [-0.004  0.13  -0.157  0.008] #     comb_bound_low  [-0.006  0.092 -0.205  0.007] #     comb_bound_high [-0.003  0.172 -0.109  0.012] #     comb_flags      [ True  True  True  True] #     n_total         2^(18) #     n               [[  1024.   1024. 262144.   2048.] #                     [  1024.   1024. 262144.   2048.]] #     time_integrate  2.229 <pre>Data (Data)\n    solution        [ 0.262 -0.043 -0.226 -1.203]\n    comb_bound_low  [ 0.185 -0.067 -0.306 -1.656]\n    comb_bound_high [ 0.347 -0.035 -0.163 -0.75 ]\n    comb_bound_diff [0.162 0.031 0.143 0.906]\n    comb_flags      [ True  True  True False]\n    n_total         2^(18)\n    n               [[  1024   1024   1024 262144]\n                     [  1024   1024   1024 262144]]\n    time_integrate  3.175\nCubQMCNetG (AbstractStoppingCriterion)\n    abs_tol         0.100\n    rel_tol         2^(-1)\n    n_init          2^(10)\n    n_limit         2^(18)\nBayesianLRCoeffs (AbstractIntegrand)\nGaussian (AbstractTrueMeasure)\n    mean            0\n    covariance      5\n    decomp_type     PCA\nDigitalNetB2 (AbstractLDDiscreteDistribution)\n    d               2^(2)\n    replications    1\n    randomize       LMS_DS\n    gen_mats_source joe_kuo.6.21201.txt\n    order           NATURAL\n    t               63\n    alpha           1\n    n_limit         2^(32)\n    entropy         1\n</pre> <pre>/Users/alegresor/Desktop/QMCSoftware/qmcpy/stopping_criterion/abstract_cub_qmc_ld_g.py:215: MaxSamplesWarning: \n                Already generated 262144 samples.\n                Trying to generate 262144 new samples would exceeds n_limit = 262144.\n                No more samples will be generated.\n                Note that error tolerances may not be satisfied. \n  warnings.warn(warning_s, MaxSamplesWarning)\n</pre> In\u00a0[18]: Copied! <pre>from sklearn.linear_model import LogisticRegression\ndef metrics(y,yhat):\n    y,yhat = np.array(y),np.array(yhat)\n    tp = np.sum((y==1)*(yhat==1))\n    tn = np.sum((y==0)*(yhat==0))\n    fp = np.sum((y==0)*(yhat==1))\n    fn = np.sum((y==1)*(yhat==0))\n    accuracy = (tp+tn)/(len(y))\n    precision = tp/(tp+fp)\n    recall = tp/(tp+fn)\n    return [accuracy,precision,recall]\n\nresults = pd.DataFrame({name:[] for name in ['method','Age','1900 Year','Axillary Nodes','Intercept','Accuracy','Precision','Recall']})\nfor i,l1_ratio in enumerate([0,.5,1]):\n    lr = LogisticRegression(random_state=7,penalty=\"elasticnet\",solver='saga',l1_ratio=l1_ratio).fit(xt,yt)\n    results.loc[i] = [r'Elastic-Net \\lambda=%.1f'%l1_ratio]+lr.coef_.squeeze().tolist()+[lr.intercept_.item()]+metrics(yv,lr.predict(xv))\n\nblr_predict = lambda x: 1/(1+np.exp(-np.array(x)@blr_coefs[:-1]-blr_coefs[-1]))&gt;=.5\nblr_train_accuracy = np.mean(blr_predict(xt)==yt)\nblr_test_accuracy = np.mean(blr_predict(xv)==yv)\nresults.loc[len(results)] = ['Bayesian']+blr_coefs.squeeze().tolist()+metrics(yv,blr_predict(xv))\n\nimport warnings\nwarnings.simplefilter('ignore',FutureWarning)\nresults.set_index('method',inplace=True)\nprint(results.head())\n#root: results.to_latex(root+'lr_table.tex',formatters={'%s'%tt:lambda v:'%.1f'%(100*v) for tt in ['accuracy','precision','recall']},float_format=\"%.2e\")\n</pre> from sklearn.linear_model import LogisticRegression def metrics(y,yhat):     y,yhat = np.array(y),np.array(yhat)     tp = np.sum((y==1)*(yhat==1))     tn = np.sum((y==0)*(yhat==0))     fp = np.sum((y==0)*(yhat==1))     fn = np.sum((y==1)*(yhat==0))     accuracy = (tp+tn)/(len(y))     precision = tp/(tp+fp)     recall = tp/(tp+fn)     return [accuracy,precision,recall]  results = pd.DataFrame({name:[] for name in ['method','Age','1900 Year','Axillary Nodes','Intercept','Accuracy','Precision','Recall']}) for i,l1_ratio in enumerate([0,.5,1]):     lr = LogisticRegression(random_state=7,penalty=\"elasticnet\",solver='saga',l1_ratio=l1_ratio).fit(xt,yt)     results.loc[i] = [r'Elastic-Net \\lambda=%.1f'%l1_ratio]+lr.coef_.squeeze().tolist()+[lr.intercept_.item()]+metrics(yv,lr.predict(xv))  blr_predict = lambda x: 1/(1+np.exp(-np.array(x)@blr_coefs[:-1]-blr_coefs[-1]))&gt;=.5 blr_train_accuracy = np.mean(blr_predict(xt)==yt) blr_test_accuracy = np.mean(blr_predict(xv)==yv) results.loc[len(results)] = ['Bayesian']+blr_coefs.squeeze().tolist()+metrics(yv,blr_predict(xv))  import warnings warnings.simplefilter('ignore',FutureWarning) results.set_index('method',inplace=True) print(results.head()) #root: results.to_latex(root+'lr_table.tex',formatters={'%s'%tt:lambda v:'%.1f'%(100*v) for tt in ['accuracy','precision','recall']},float_format=\"%.2e\") <pre>                              Age  1900 Year  Axillary Nodes  Intercept  \\\nmethod                                                                    \nElastic-Net \\lambda=0.0 -0.012279   0.034401       -0.115153   0.001990   \nElastic-Net \\lambda=0.5 -0.012041   0.034170       -0.114770   0.002025   \nElastic-Net \\lambda=1.0 -0.011803   0.033940       -0.114387   0.002061   \nBayesian                 0.262086  -0.043253       -0.225631  -1.202777   \n\n                         Accuracy  Precision    Recall  \nmethod                                                  \nElastic-Net \\lambda=0.0  0.742574   0.766667  0.932432  \nElastic-Net \\lambda=0.5  0.742574   0.766667  0.932432  \nElastic-Net \\lambda=1.0  0.742574   0.766667  0.932432  \nBayesian                 0.732673   0.737374  0.986486  \n</pre> In\u00a0[19]: Copied! <pre>a,b = 7,0.1\ndnb2 = qp.DigitalNetB2(3,seed=7)\nishigami = qp.Ishigami(dnb2,a,b)\nidxs = np.array([\n    [True,False,False],\n    [False,True,False],\n    [False,False,True],\n    [True,True,False],\n    [True,False,True],\n    [False,True,True]],dtype=bool)\nishigami_si = qp.SensitivityIndices(ishigami,idxs)\nqmc_algo = qp.CubBayesNetG(ishigami_si,abs_tol=.05)\nsolution,data = qmc_algo.integrate()\nprint(data)\nsi_closed = solution[0].squeeze()\nsi_total = solution[1].squeeze()\nci_comb_low_closed = data.comb_bound_low[0].squeeze()\nci_comb_high_closed = data.comb_bound_high[0].squeeze()\nci_comb_low_total = data.comb_bound_low[1].squeeze()\nci_comb_high_total = data.comb_bound_high[1].squeeze()\nprint(\"\\nApprox took %.1f sec and n = 2^(%d)\"%\n    (data.time_integrate,np.log2(data.n_total)))\nprint('\\t si_closed:',si_closed)\nprint('\\t si_total:',si_total)\nprint('\\t ci_comb_low_closed:',ci_comb_low_closed)\nprint('\\t ci_comb_high_closed:',ci_comb_high_closed)\nprint('\\t ci_comb_low_total:',ci_comb_low_total)\nprint('\\t ci_comb_high_total:',ci_comb_high_total)\n\ntrue_indices = qp.Ishigami._exact_sensitivity_indices(idxs,a,b)\nsi_closed_true = true_indices[0]\nsi_total_true = true_indices[1]\n</pre> a,b = 7,0.1 dnb2 = qp.DigitalNetB2(3,seed=7) ishigami = qp.Ishigami(dnb2,a,b) idxs = np.array([     [True,False,False],     [False,True,False],     [False,False,True],     [True,True,False],     [True,False,True],     [False,True,True]],dtype=bool) ishigami_si = qp.SensitivityIndices(ishigami,idxs) qmc_algo = qp.CubBayesNetG(ishigami_si,abs_tol=.05) solution,data = qmc_algo.integrate() print(data) si_closed = solution[0].squeeze() si_total = solution[1].squeeze() ci_comb_low_closed = data.comb_bound_low[0].squeeze() ci_comb_high_closed = data.comb_bound_high[0].squeeze() ci_comb_low_total = data.comb_bound_low[1].squeeze() ci_comb_high_total = data.comb_bound_high[1].squeeze() print(\"\\nApprox took %.1f sec and n = 2^(%d)\"%     (data.time_integrate,np.log2(data.n_total))) print('\\t si_closed:',si_closed) print('\\t si_total:',si_total) print('\\t ci_comb_low_closed:',ci_comb_low_closed) print('\\t ci_comb_high_closed:',ci_comb_high_closed) print('\\t ci_comb_low_total:',ci_comb_low_total) print('\\t ci_comb_high_total:',ci_comb_high_total)  true_indices = qp.Ishigami._exact_sensitivity_indices(idxs,a,b) si_closed_true = true_indices[0] si_total_true = true_indices[1] <pre>Data (Data)\n    solution        [[0.272 0.451 0.03  0.756 0.561 0.481]\n                     [0.584 0.406 0.224 0.981 0.505 0.696]]\n    comb_bound_low  [[0.227 0.404 0.011 0.726 0.518 0.447]\n                     [0.544 0.369 0.204 0.961 0.461 0.658]]\n    comb_bound_high [[0.317 0.499 0.049 0.786 0.604 0.515]\n                     [0.624 0.442 0.244 1.    0.549 0.733]]\n    comb_bound_diff [[0.09  0.095 0.038 0.061 0.086 0.068]\n                     [0.08  0.074 0.04  0.039 0.088 0.076]]\n    comb_flags      [[ True  True  True  True  True  True]\n                     [ True  True  True  True  True  True]]\n    n_total         2^(10)\n    n               [[[ 256  256  256 1024  512  512]\n                      [ 256  256  256 1024  512  512]\n                      [ 256  256  256 1024  512  512]]\n                    \n                     [[ 512  256  256  512  256  512]\n                      [ 512  256  256  512  256  512]\n                      [ 512  256  256  512  256  512]]]\n    time_integrate  0.292\nCubQMCBayesNetG (AbstractStoppingCriterion)\n    abs_tol         0.050\n    rel_tol         0\n    n_init          2^(8)\n    n_limit         2^(22)\n    order           1\nSensitivityIndices (AbstractIntegrand)\n    indices         [[ True False False]\n                     [False  True False]\n                     [False False  True]\n                     [ True  True False]\n                     [ True False  True]\n                     [False  True  True]]\nUniform (AbstractTrueMeasure)\n    lower_bound     -3.142\n    upper_bound     3.142\nDigitalNetB2 (AbstractLDDiscreteDistribution)\n    d               3\n    replications    1\n    randomize       LMS_DS\n    gen_mats_source joe_kuo.6.21201.txt\n    order           NATURAL\n    t               63\n    alpha           1\n    n_limit         2^(32)\n    entropy         7\n\nApprox took 0.3 sec and n = 2^(10)\n\t si_closed: [0.27216675 0.45135598 0.03039673 0.7559953  0.56084862 0.48106082]\n\t si_total: [0.58380168 0.40555062 0.22413097 0.98050963 0.50473157 0.69561275]\n\t ci_comb_low_closed: [0.22703312 0.40380263 0.01145781 0.72574291 0.51804596 0.44706404]\n\t ci_comb_high_closed: [0.31730037 0.49890933 0.04933565 0.78624769 0.60365128 0.51505759]\n\t ci_comb_low_total: [0.54382805 0.36861946 0.20422796 0.96101926 0.46083634 0.65783325]\n\t ci_comb_high_total: [0.62377531 0.44248178 0.24403398 1.         0.5486268  0.73339225]\n</pre> In\u00a0[21]: Copied! <pre>fig,ax = pyplot.subplots(figsize=(8,4))\nax.grid(False)\nfor spine in ['top','left','right','bottom']: ax.spines[spine].set_visible(False)\nwidth = .75\nax.errorbar(fmt='none',color='k',\n    x = 1-si_total_true,\n    y = np.arange(len(si_closed)),\n    xerr = 0,\n    yerr = width/2,\n    alpha = 1)\nbar_closed = ax.barh(np.arange(len(si_closed)),np.flip(si_closed),width,label='Closed SI',color='w',edgecolor='k',alpha=.75,linestyle='--')\nax.errorbar(fmt='none',color='k',\n    x = si_closed,\n    y = np.flip(np.arange(len(si_closed)))+width/4,\n    xerr = np.vstack((si_closed-ci_comb_low_closed,ci_comb_high_closed-si_closed)),\n    yerr = 0,\n    #elinewidth = 5,\n    alpha = .75)\nbar_total = ax.barh(np.arange(len(si_closed)),si_total,width,label='Total SI',color='w',alpha=.25,edgecolor='k',left=1-si_total,zorder=10,linestyle='-.')\nax.errorbar(fmt='none',color='k',\n    x = 1-si_total,\n    y = np.arange(len(si_closed))-width/4,\n    xerr = np.vstack((si_total-ci_comb_low_total,ci_comb_high_total-si_total)),\n    yerr = 0,\n    #elinewidth = 5,\n    alpha = .25)\nclosed_labels = [r'$\\underline{s}_{\\{%s\\}} = %.2f$'%(','.join([str(i+1) for i in idx]),c) for idx,c in zip(idxs[::-1],np.flip(si_closed))]\nclosed_labels[3] = ''\ntotal_labels = [r'$\\overline{s}_{\\{%s\\}} = %.2f$'%(','.join([str(i+1) for i in idx]),t) for idx,t in zip(idxs,si_total)]\nax.bar_label(bar_closed,label_type='center',labels=closed_labels)\nax.bar_label(bar_total,label_type='center',labels=total_labels)\nax.set_xlim([-.001,1.001])\nax.axvline(x=0,ymin=0,ymax=len(si_closed),color='k',alpha=.25)\nax.axvline(x=1,ymin=0,ymax=len(si_closed),color='k',alpha=.25)\nax.set_yticklabels([])\nif root: fig.savefig(root+'ishigami.pdf',transparent=True)\n</pre> fig,ax = pyplot.subplots(figsize=(8,4)) ax.grid(False) for spine in ['top','left','right','bottom']: ax.spines[spine].set_visible(False) width = .75 ax.errorbar(fmt='none',color='k',     x = 1-si_total_true,     y = np.arange(len(si_closed)),     xerr = 0,     yerr = width/2,     alpha = 1) bar_closed = ax.barh(np.arange(len(si_closed)),np.flip(si_closed),width,label='Closed SI',color='w',edgecolor='k',alpha=.75,linestyle='--') ax.errorbar(fmt='none',color='k',     x = si_closed,     y = np.flip(np.arange(len(si_closed)))+width/4,     xerr = np.vstack((si_closed-ci_comb_low_closed,ci_comb_high_closed-si_closed)),     yerr = 0,     #elinewidth = 5,     alpha = .75) bar_total = ax.barh(np.arange(len(si_closed)),si_total,width,label='Total SI',color='w',alpha=.25,edgecolor='k',left=1-si_total,zorder=10,linestyle='-.') ax.errorbar(fmt='none',color='k',     x = 1-si_total,     y = np.arange(len(si_closed))-width/4,     xerr = np.vstack((si_total-ci_comb_low_total,ci_comb_high_total-si_total)),     yerr = 0,     #elinewidth = 5,     alpha = .25) closed_labels = [r'$\\underline{s}_{\\{%s\\}} = %.2f$'%(','.join([str(i+1) for i in idx]),c) for idx,c in zip(idxs[::-1],np.flip(si_closed))] closed_labels[3] = '' total_labels = [r'$\\overline{s}_{\\{%s\\}} = %.2f$'%(','.join([str(i+1) for i in idx]),t) for idx,t in zip(idxs,si_total)] ax.bar_label(bar_closed,label_type='center',labels=closed_labels) ax.bar_label(bar_total,label_type='center',labels=total_labels) ax.set_xlim([-.001,1.001]) ax.axvline(x=0,ymin=0,ymax=len(si_closed),color='k',alpha=.25) ax.axvline(x=1,ymin=0,ymax=len(si_closed),color='k',alpha=.25) ax.set_yticklabels([]) if root: fig.savefig(root+'ishigami.pdf',transparent=True) In\u00a0[22]: Copied! <pre>fig,ax = pyplot.subplots(nrows=1,ncols=3,figsize=(8,3))\nx_1d = np.linspace(0,1,num=128)\nx_1d_mat = np.tile(x_1d,(3,1)).T\ny_1d = qp.Ishigami._exact_fu_functions(x_1d_mat,indices=[[0],[1],[2]],a=a,b=b)\nfor i in range(2):\n    ax[i].plot(x_1d,y_1d[:,i],color='k')\n    ax[i].set_xlim([0,1])\n    ax[i].set_xticks([0,1])\n    ax[i].set_xlabel(r'$x_{%d}$'%(i+1))\n    ax[i].set_title(r'$f_{\\{%d\\}} \\in [%.1f,%.1f]$'%(i+1,y_1d[:,i].min(),y_1d[:,i].max()))\nx_mesh,y_mesh = np.meshgrid(x_1d,x_1d)\nxquery = np.zeros((x_mesh.size,3))\nfor i,idx in enumerate([[1,2]]): # [[0,1],[0,2],[1,2]]\n    xquery[:,idx[0]] = x_mesh.flatten()\n    xquery[:,idx[1]] = y_mesh.flatten()\n    zquery = qp.Ishigami._exact_fu_functions(xquery,indices=[idx],a=a,b=b)\n    z_mesh = zquery.reshape(x_mesh.shape)\n    ax[2+i].contourf(x_mesh,y_mesh,z_mesh,cmap=cm.Greys_r)\n    ax[2+i].set_xlabel(r'$x_{%d}$'%(idx[0]+1))\n    ax[2+i].set_ylabel(r'$x_{%d}$'%(idx[1]+1))\n    ax[2+i].set_title(r'$f_{\\{%d,%d\\}} \\in [%.1f,%.1f]$'%(tuple([i+1 for i in idx])+(z_mesh.min(),z_mesh.max())))\n    ax[2+i].set_xlim([0,1])\n    ax[2+i].set_ylim([0,1])\n    ax[2+i].set_xticks([0,1])\n    ax[2+i].set_yticks([0,1])\nif root: fig.savefig(root+'ishigami_fu.pdf')\n</pre> fig,ax = pyplot.subplots(nrows=1,ncols=3,figsize=(8,3)) x_1d = np.linspace(0,1,num=128) x_1d_mat = np.tile(x_1d,(3,1)).T y_1d = qp.Ishigami._exact_fu_functions(x_1d_mat,indices=[[0],[1],[2]],a=a,b=b) for i in range(2):     ax[i].plot(x_1d,y_1d[:,i],color='k')     ax[i].set_xlim([0,1])     ax[i].set_xticks([0,1])     ax[i].set_xlabel(r'$x_{%d}$'%(i+1))     ax[i].set_title(r'$f_{\\{%d\\}} \\in [%.1f,%.1f]$'%(i+1,y_1d[:,i].min(),y_1d[:,i].max())) x_mesh,y_mesh = np.meshgrid(x_1d,x_1d) xquery = np.zeros((x_mesh.size,3)) for i,idx in enumerate([[1,2]]): # [[0,1],[0,2],[1,2]]     xquery[:,idx[0]] = x_mesh.flatten()     xquery[:,idx[1]] = y_mesh.flatten()     zquery = qp.Ishigami._exact_fu_functions(xquery,indices=[idx],a=a,b=b)     z_mesh = zquery.reshape(x_mesh.shape)     ax[2+i].contourf(x_mesh,y_mesh,z_mesh,cmap=cm.Greys_r)     ax[2+i].set_xlabel(r'$x_{%d}$'%(idx[0]+1))     ax[2+i].set_ylabel(r'$x_{%d}$'%(idx[1]+1))     ax[2+i].set_title(r'$f_{\\{%d,%d\\}} \\in [%.1f,%.1f]$'%(tuple([i+1 for i in idx])+(z_mesh.min(),z_mesh.max())))     ax[2+i].set_xlim([0,1])     ax[2+i].set_ylim([0,1])     ax[2+i].set_xticks([0,1])     ax[2+i].set_yticks([0,1]) if root: fig.savefig(root+'ishigami_fu.pdf') In\u00a0[23]: Copied! <pre>from sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neural_network import MLPClassifier\ndata = load_iris()\nfeature_names = data[\"feature_names\"]\nfeature_names = [fn.replace('sepal ','S')\\\n    .replace('length ','L')\\\n    .replace('petal ','P')\\\n    .replace('width ','W')\\\n    .replace('(cm)','') for fn in feature_names]\ntarget_names = data[\"target_names\"]\nxt,xv,yt,yv = train_test_split(data[\"data\"],data[\"target\"],\n    test_size = 1/3,\n    random_state = 7)\n</pre> from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split from sklearn.neural_network import MLPClassifier data = load_iris() feature_names = data[\"feature_names\"] feature_names = [fn.replace('sepal ','S')\\     .replace('length ','L')\\     .replace('petal ','P')\\     .replace('width ','W')\\     .replace('(cm)','') for fn in feature_names] target_names = data[\"target_names\"] xt,xv,yt,yv = train_test_split(data[\"data\"],data[\"target\"],     test_size = 1/3,     random_state = 7) In\u00a0[25]: Copied! <pre>mlpc = MLPClassifier(random_state=7,max_iter=1024).fit(xt,yt)\nyhat = mlpc.predict(xv)\nprint(\"accuracy: %.1f%%\"%(100*(yv==yhat).mean()))\n# accuracy: 98.0%\nsampler = qp.DigitalNetB2(4,seed=7)\ntrue_measure =  qp.Uniform(sampler,\n    lower_bound = xt.min(0),\n    upper_bound = xt.max(0))\nfun = qp.CustomFun(\n    true_measure = true_measure,\n    g = lambda x: mlpc.predict_proba(x).T,\n    dimension_indv = 3)\nsi_fun = qp.SensitivityIndices(fun,indices=\"all\")\nqmc_algo = qp.CubBayesNetG(si_fun,abs_tol=.005)\nnn_sis,nn_sis_data = qmc_algo.integrate()\n</pre> mlpc = MLPClassifier(random_state=7,max_iter=1024).fit(xt,yt) yhat = mlpc.predict(xv) print(\"accuracy: %.1f%%\"%(100*(yv==yhat).mean())) # accuracy: 98.0% sampler = qp.DigitalNetB2(4,seed=7) true_measure =  qp.Uniform(sampler,     lower_bound = xt.min(0),     upper_bound = xt.max(0)) fun = qp.CustomFun(     true_measure = true_measure,     g = lambda x: mlpc.predict_proba(x).T,     dimension_indv = 3) si_fun = qp.SensitivityIndices(fun,indices=\"all\") qmc_algo = qp.CubBayesNetG(si_fun,abs_tol=.005) nn_sis,nn_sis_data = qmc_algo.integrate() <pre>accuracy: 98.0%\n</pre> In\u00a0[26]: Copied! <pre>#print(nn_sis_data.flags_indv.shape)\n#print(nn_sis_data.flags_comb.shape)\nprint('samples: 2^(%d)'%np.log2(nn_sis_data.n_total))\nprint('time: %.1e'%nn_sis_data.time_integrate)\nprint('indices:\\n%s'%nn_sis_data.integrand.indices)\n\nimport pandas as pd\n\ndf_closed = pd.DataFrame(nn_sis[0],columns=target_names,index=[str(np.where(idx)[0]) for idx in nn_sis_data.integrand.indices])\nprint('\\nClosed Indices')\nprint(df_closed)\ndf_total = pd.DataFrame(nn_sis[1],columns=target_names,index=[str(np.where(idx)[0]) for idx in nn_sis_data.integrand.indices])\nprint('\\nTotal Indices')\ndf_closed_singletons = df_closed.loc[['[%d]'%i for i in range(4)]].T\ndf_closed_singletons['sum singletons'] = df_closed_singletons[['[%d]'%i for i in range(4)]].sum(1)\ndf_closed_singletons.columns = data['feature_names']+['sum']\ndf_closed_singletons = df_closed_singletons*100\ndf_closed_singletons\n</pre> #print(nn_sis_data.flags_indv.shape) #print(nn_sis_data.flags_comb.shape) print('samples: 2^(%d)'%np.log2(nn_sis_data.n_total)) print('time: %.1e'%nn_sis_data.time_integrate) print('indices:\\n%s'%nn_sis_data.integrand.indices)  import pandas as pd  df_closed = pd.DataFrame(nn_sis[0],columns=target_names,index=[str(np.where(idx)[0]) for idx in nn_sis_data.integrand.indices]) print('\\nClosed Indices') print(df_closed) df_total = pd.DataFrame(nn_sis[1],columns=target_names,index=[str(np.where(idx)[0]) for idx in nn_sis_data.integrand.indices]) print('\\nTotal Indices') df_closed_singletons = df_closed.loc[['[%d]'%i for i in range(4)]].T df_closed_singletons['sum singletons'] = df_closed_singletons[['[%d]'%i for i in range(4)]].sum(1) df_closed_singletons.columns = data['feature_names']+['sum'] df_closed_singletons = df_closed_singletons*100 df_closed_singletons <pre>samples: 2^(14)\ntime: 4.5e+01\nindices:\n[[ True False False False]\n [False  True False False]\n [False False  True False]\n [False False False  True]\n [ True  True False False]\n [ True False  True False]\n [ True False False  True]\n [False  True  True False]\n [False  True False  True]\n [False False  True  True]\n [ True  True  True False]\n [ True  True False  True]\n [ True False  True  True]\n [False  True  True  True]]\n\nClosed Indices\n           setosa  versicolor  virginica\n[0]      0.007424    0.066607   0.099046\n[1]      0.054465    0.002571   0.002252\n[2]      0.712709    0.326649   0.498656\n[3]      0.047694    0.017899   0.116317\n[0 1]    0.054934    0.084423   0.097292\n[0 2]    0.714202    0.460625   0.641442\n[0 3]    0.047812    0.088780   0.205556\n[1 2]    0.840862    0.433312   0.514035\n[1 3]    0.102551    0.032345   0.128446\n[2 3]    0.822403    0.583336   0.705183\n[0 1 2]  0.843053    0.570898   0.660995\n[0 1 3]  0.108803    0.103398   0.216627\n[0 2 3]  0.824346    0.814563   0.946842\n[1 2 3]  0.995488    0.739790   0.726773\n\nTotal Indices\n</pre> Out[26]: sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) sum setosa 0.742441 5.446506 71.270924 4.769447 82.229319 versicolor 6.660730 0.257095 32.664932 1.789914 41.372671 virginica 9.904627 0.225186 49.865641 11.631687 71.627140 In\u00a0[27]: Copied! <pre>nindices = len(nn_sis_data.integrand.indices)\nfig,ax = pyplot.subplots(figsize=(9,5))\nticks = np.arange(nindices)\nwidth = .25\nfor i,(alpha,species) in enumerate(zip([.25,.5,.75],data['target_names'])):\n    cvals = df_closed[species].to_numpy()\n    tvals = df_total[species].to_numpy()\n    ticks_i = ticks+i*width\n    ax.bar(ticks_i,cvals,width=width,align='edge',color='k',alpha=alpha,label=species)\n    #ax.bar(ticks_i,np.flip(tvals),width=width,align='edge',bottom=1-np.flip(tvals),color=color,alpha=.1)\nax.set_xlim([0,13+3*width])\nax.set_xticks(ticks+1.5*width)\n\n# closed_labels = [r'$\\underline{s}_{\\{%s\\}}$'%(','.join([r'\\text{%s}'%feature_names[i] for i in idx])) for idx in nn_sis_data.integrand.indices]\nclosed_labels = ['\\n'.join([feature_names[i] for i in np.where(idx)[0]]) for idx in nn_sis_data.integrand.indices]\nax.set_xticklabels(closed_labels,rotation=0)\nax.set_ylim([0,1]); ax.set_yticks([0,1])\nax.grid(False)\nfor spine in ['top','right','bottom']: ax.spines[spine].set_visible(False)\nax.legend(frameon=False,loc='lower center',bbox_to_anchor=(.5,-.2),ncol=3);\nfig.tight_layout()\nif root: fig.savefig(root+'nn_si.pdf')\n</pre> nindices = len(nn_sis_data.integrand.indices) fig,ax = pyplot.subplots(figsize=(9,5)) ticks = np.arange(nindices) width = .25 for i,(alpha,species) in enumerate(zip([.25,.5,.75],data['target_names'])):     cvals = df_closed[species].to_numpy()     tvals = df_total[species].to_numpy()     ticks_i = ticks+i*width     ax.bar(ticks_i,cvals,width=width,align='edge',color='k',alpha=alpha,label=species)     #ax.bar(ticks_i,np.flip(tvals),width=width,align='edge',bottom=1-np.flip(tvals),color=color,alpha=.1) ax.set_xlim([0,13+3*width]) ax.set_xticks(ticks+1.5*width)  # closed_labels = [r'$\\underline{s}_{\\{%s\\}}$'%(','.join([r'\\text{%s}'%feature_names[i] for i in idx])) for idx in nn_sis_data.integrand.indices] closed_labels = ['\\n'.join([feature_names[i] for i in np.where(idx)[0]]) for idx in nn_sis_data.integrand.indices] ax.set_xticklabels(closed_labels,rotation=0) ax.set_ylim([0,1]); ax.set_yticks([0,1]) ax.grid(False) for spine in ['top','right','bottom']: ax.spines[spine].set_visible(False) ax.legend(frameon=False,loc='lower center',bbox_to_anchor=(.5,-.2),ncol=3); fig.tight_layout() if root: fig.savefig(root+'nn_si.pdf') In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"demos/vectorized_qmc_bayes/#vectorized-qmc-bayesian","title":"Vectorized QMC (Bayesian)\u00b6","text":""},{"location":"demos/vectorized_qmc_bayes/#ld-sequence","title":"LD Sequence\u00b6","text":""},{"location":"demos/vectorized_qmc_bayes/#simple-example","title":"Simple Example\u00b6","text":""},{"location":"demos/vectorized_qmc_bayes/#bo-qei","title":"BO QEI\u00b6","text":"<p>See the QEI Demo in QMCPy or the BoTorch Acquisition documentation for details on Bayesian Optimization using q-Expected Improvement.</p>"},{"location":"demos/vectorized_qmc_bayes/#bayesian-logistic-regression","title":"Bayesian Logistic Regression\u00b6","text":""},{"location":"demos/vectorized_qmc_bayes/#sensitivity-indices","title":"Sensitivity Indices\u00b6","text":""},{"location":"demos/vectorized_qmc_bayes/#ishigami-function","title":"Ishigami Function\u00b6","text":""},{"location":"demos/vectorized_qmc_bayes/#neural-network","title":"Neural Network\u00b6","text":""},{"location":"demos/DAKOTA_Genz/dakota_genz/","title":"DAKOTA Halton Points","text":"In\u00a0[1]: Copied! <pre>from numpy import *\nfrom qmcpy import *\nimport pandas as pd\nfrom matplotlib import pyplot\nimport tempfile\nimport os\nimport subprocess\nimport numpy as np\nimport matplotlib.pyplot as plt\nplt.rcParams.update({\n\"text.usetex\": True,\n\"font.family\": \"serif\",\n\"text.latex.preamble\": r\"\\usepackage{amsmath}\\usepackage{amssymb}\\newcommand{\\bx}{\\boldsymbol{x}}\"\n})\n%matplotlib inline\n</pre> from numpy import * from qmcpy import * import pandas as pd from matplotlib import pyplot import tempfile import os import subprocess import numpy as np import matplotlib.pyplot as plt plt.rcParams.update({ \"text.usetex\": True, \"font.family\": \"serif\", \"text.latex.preamble\": r\"\\usepackage{amsmath}\\usepackage{amssymb}\\newcommand{\\bx}{\\boldsymbol{x}}\" }) %matplotlib inline In\u00a0[2]: Copied! <pre>kinds_func = ['oscillatory','corner-peak']\nkinds_coeff = [1,2,3]\nds = 2**arange(8)\nns = 2**arange(7,19)\nds\n</pre> kinds_func = ['oscillatory','corner-peak'] kinds_coeff = [1,2,3] ds = 2**arange(8) ns = 2**arange(7,19) ds Out[2]: <pre>array([  1,   2,   4,   8,  16,  32,  64, 128])</pre> In\u00a0[\u00a0]: Copied! <pre># takes about 5.5 min to run for me\nref_sols = {}\nprint('logging: ',end='',flush=True)\nx_full = DigitalNetB2(ds.max(),seed=7).gen_samples(2**22)\nfor kind_func in kinds_func:\n    for kind_coeff in kinds_coeff:\n        tag = '%s.%d'%(kind_func,kind_coeff)\n        print('%s, '%tag,end='',flush=True)\n        mu_hats = zeros(len(ds),dtype=float)\n        for j,d in enumerate(ds):\n            genz = Genz(IIDStdUniform(d),kind_func=kind_func,kind_coeff=kind_coeff)\n            y = genz.f(x_full[:,:d])\n            mu_hats[j] = y.mean()\n        ref_sols[tag] = mu_hats\nprint()\nref_sols = pd.DataFrame(ref_sols)\nref_sols['d'] = ds\nref_sols.set_index('d',inplace=True)\nref_sols \n</pre> # takes about 5.5 min to run for me ref_sols = {} print('logging: ',end='',flush=True) x_full = DigitalNetB2(ds.max(),seed=7).gen_samples(2**22) for kind_func in kinds_func:     for kind_coeff in kinds_coeff:         tag = '%s.%d'%(kind_func,kind_coeff)         print('%s, '%tag,end='',flush=True)         mu_hats = zeros(len(ds),dtype=float)         for j,d in enumerate(ds):             genz = Genz(IIDStdUniform(d),kind_func=kind_func,kind_coeff=kind_coeff)             y = genz.f(x_full[:,:d])             mu_hats[j] = y.mean()         ref_sols[tag] = mu_hats print() ref_sols = pd.DataFrame(ref_sols) ref_sols['d'] = ds ref_sols.set_index('d',inplace=True) ref_sols  Out[\u00a0]: oscillatory.1 oscillatory.2 oscillatory.3 corner-peak.1 corner-peak.2 corner-peak.3 d 1 -0.217229 -0.217229 -0.217229 8.000000e-01 0.800000 0.800000 2 -0.350528 -0.379658 -0.217290 7.133127e-01 0.712088 0.719996 4 -0.472868 -0.472147 -0.223335 5.663464e-01 0.566334 0.589674 8 -0.545580 -0.526053 -0.276837 3.573127e-01 0.360014 0.402641 16 -0.585497 -0.558159 -0.390467 1.423258e-01 0.147353 0.185776 32 -0.606470 -0.577987 -0.492894 2.259076e-02 0.025678 0.038375 64 -0.617227 -0.590764 -0.556348 5.692663e-04 0.000879 0.001606 128 -0.622677 -0.599348 -0.591170 3.615787e-07 0.000001 0.000003 In\u00a0[\u00a0]: Copied! <pre>if os.path.isfile(\"x_full_dakota.txt\"):\n    x_full_dakota = np.loadtxt(\"x_full_dakota.txt\")\nelse:\n    print(\"please download Dakota's Halton points from https://drive.google.com/uc?id=1ljmpq3w5L4OjjdinAMSLhXBeWGW6EJ3U\")\n    # with tempfile.TemporaryDirectory() as tmp:\n    #     with open(os.path.join(tmp, \"dakota.in\"), \"w\") as io:\n    #         io.write(f\"environment\\\n    #             \\ttabular_data\\n\\\n    #             method\\\n    #             \\tfsu_quasi_mc halton\\\n    #             \\t\\tsamples = {ns.max()}\\\n    #             \\toutput silent\\n\\\n    #             variables\\\n    #             \\tcontinuous_design = {ds.max()}\\\n    #             \\tlower_bounds = {' '.join(['0.0' for _ in range(ds.max())])}\\\n    #             \\tupper_bounds = {' '.join(['1.0' for _ in range(ds.max())])}\\n\\\n    #             interface\\\n    #             \\tfork\\\n    #             \\t\\tanalysis_driver = 'dummy'\\\n    #             \\tbatch\\\n    #             \\twork_directory named 'work'\\n\\\n    #             responses\\\n    #             \\tobjective_functions = 1\\\n    #             \\tno_gradients\\\n    #             \\tno_hessians\"\n    #         )\n    #     subprocess.run([\"dakota\", \"dakota.in\"], cwd=tmp, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n    #     file = os.listdir(os.path.join(tmp, \"work\"))[0]\n    #     with open(os.path.join(tmp, \"work\", file), \"r\") as io:\n    #         lines = io.readlines()\n    #         x_full_dakota = []\n    #         for n, line in enumerate(lines):\n    #             if f\"{ds.max()} variables\" in line:\n    #                 x_full_dakota.append([float(lines[n + 1 + j].split()[0]) for j in range(ds.max())])\n    #         x_full_dakota = np.vstack(x_full_dakota)\n    #     np.savetxt(\"data/x_full_dakota.txt\",x_full_dakota)\nprint(x_full_dakota.shape)\n</pre> if os.path.isfile(\"x_full_dakota.txt\"):     x_full_dakota = np.loadtxt(\"x_full_dakota.txt\") else:     print(\"please download Dakota's Halton points from https://drive.google.com/uc?id=1ljmpq3w5L4OjjdinAMSLhXBeWGW6EJ3U\")     # with tempfile.TemporaryDirectory() as tmp:     #     with open(os.path.join(tmp, \"dakota.in\"), \"w\") as io:     #         io.write(f\"environment\\     #             \\ttabular_data\\n\\     #             method\\     #             \\tfsu_quasi_mc halton\\     #             \\t\\tsamples = {ns.max()}\\     #             \\toutput silent\\n\\     #             variables\\     #             \\tcontinuous_design = {ds.max()}\\     #             \\tlower_bounds = {' '.join(['0.0' for _ in range(ds.max())])}\\     #             \\tupper_bounds = {' '.join(['1.0' for _ in range(ds.max())])}\\n\\     #             interface\\     #             \\tfork\\     #             \\t\\tanalysis_driver = 'dummy'\\     #             \\tbatch\\     #             \\twork_directory named 'work'\\n\\     #             responses\\     #             \\tobjective_functions = 1\\     #             \\tno_gradients\\     #             \\tno_hessians\"     #         )     #     subprocess.run([\"dakota\", \"dakota.in\"], cwd=tmp, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)     #     file = os.listdir(os.path.join(tmp, \"work\"))[0]     #     with open(os.path.join(tmp, \"work\", file), \"r\") as io:     #         lines = io.readlines()     #         x_full_dakota = []     #         for n, line in enumerate(lines):     #             if f\"{ds.max()} variables\" in line:     #                 x_full_dakota.append([float(lines[n + 1 + j].split()[0]) for j in range(ds.max())])     #         x_full_dakota = np.vstack(x_full_dakota)     #     np.savetxt(\"data/x_full_dakota.txt\",x_full_dakota) print(x_full_dakota.shape) <pre>(262144, 128)\n</pre> In\u00a0[19]: Copied! <pre>n_max,d_max = ns.max(),ds.max()\npts = {\n    'IID Standard Uniform': IIDStdUniform(d_max).gen_samples(n_max),\n    'Lattice (random shift)': Lattice(d_max).gen_samples(n_max),\n    'Digital Net (random scramble + shift)': DigitalNetB2(d_max).gen_samples(n_max),\n    'Halton (QMCPy)': Halton(d_max).gen_samples(n_max,warn=False),\n    'Halton (Dakota)': x_full_dakota[:n_max,:d_max]\n}\n</pre> n_max,d_max = ns.max(),ds.max() pts = {     'IID Standard Uniform': IIDStdUniform(d_max).gen_samples(n_max),     'Lattice (random shift)': Lattice(d_max).gen_samples(n_max),     'Digital Net (random scramble + shift)': DigitalNetB2(d_max).gen_samples(n_max),     'Halton (QMCPy)': Halton(d_max).gen_samples(n_max,warn=False),     'Halton (Dakota)': x_full_dakota[:n_max,:d_max] } In\u00a0[20]: Copied! <pre>nrows = len(ds)\nncols = len(kinds_func)*len(kinds_coeff)\nprint('logging')\nfig,ax = pyplot.subplots(nrows=nrows,ncols=ncols,figsize=(ncols*5,nrows*5),sharey=True,sharex=True)\nax = ax.reshape(nrows,ncols)\ncolors = pyplot.rcParams['axes.prop_cycle'].by_key()['color'] + [\"indigo\"]\nfor v,(name,x_full) in enumerate(pts.items()):\n    print('%20s d: '%name,end='',flush=True)\n    for j,d in enumerate(ds):\n        print('%d, '%d,end='',flush=True)\n        for i1,kind_func in enumerate(kinds_func):\n            for i2,kind_coeff in enumerate(kinds_coeff):\n                i = len(kinds_coeff)*i1+i2\n                tag = '%s.%d'%(kind_func,kind_coeff)\n                genz = Genz(IIDStdUniform(d),kind_func=kind_func,kind_coeff=kind_coeff)\n                y_full = genz.f(x_full[:,:d])\n                mu_hats = array([y_full[:n].mean() for n in ns],dtype=float)\n                error = abs(mu_hats-ref_sols.loc[d,tag])\n                ax[j,i].plot(ns,error,label=name, color=colors[v])\n                if v==(len(pts)-1): ax[j,i].legend(loc='lower left')\n                if v&gt;0: continue\n                ax[j,i].set_xscale('log',base=2)\n                ax[j,i].set_yscale('log',base=10)\n                if i==0: ax[j,i].set_ylabel(r'$d=%d$\\\\$\\varepsilon = \\lvert \\mu - \\hat{\\mu} \\rvert$'%d)\n                if j==0: ax[j,i].set_title(tag)\n                if j==(len(ds)-1):\n                    ax[j,i].set_xlabel(r'$n$')\n                    ax[j,i].set_xticks(ns)\n                    ax[j,i].set_xlim([ns.min(),ns.max()])\n    print()\n</pre> nrows = len(ds) ncols = len(kinds_func)*len(kinds_coeff) print('logging') fig,ax = pyplot.subplots(nrows=nrows,ncols=ncols,figsize=(ncols*5,nrows*5),sharey=True,sharex=True) ax = ax.reshape(nrows,ncols) colors = pyplot.rcParams['axes.prop_cycle'].by_key()['color'] + [\"indigo\"] for v,(name,x_full) in enumerate(pts.items()):     print('%20s d: '%name,end='',flush=True)     for j,d in enumerate(ds):         print('%d, '%d,end='',flush=True)         for i1,kind_func in enumerate(kinds_func):             for i2,kind_coeff in enumerate(kinds_coeff):                 i = len(kinds_coeff)*i1+i2                 tag = '%s.%d'%(kind_func,kind_coeff)                 genz = Genz(IIDStdUniform(d),kind_func=kind_func,kind_coeff=kind_coeff)                 y_full = genz.f(x_full[:,:d])                 mu_hats = array([y_full[:n].mean() for n in ns],dtype=float)                 error = abs(mu_hats-ref_sols.loc[d,tag])                 ax[j,i].plot(ns,error,label=name, color=colors[v])                 if v==(len(pts)-1): ax[j,i].legend(loc='lower left')                 if v&gt;0: continue                 ax[j,i].set_xscale('log',base=2)                 ax[j,i].set_yscale('log',base=10)                 if i==0: ax[j,i].set_ylabel(r'$d=%d$\\\\$\\varepsilon = \\lvert \\mu - \\hat{\\mu} \\rvert$'%d)                 if j==0: ax[j,i].set_title(tag)                 if j==(len(ds)-1):                     ax[j,i].set_xlabel(r'$n$')                     ax[j,i].set_xticks(ns)                     ax[j,i].set_xlim([ns.min(),ns.max()])     print() <pre>logging\nIID Standard Uniform d: 1, 2, 4, 8, 16, 32, 64, 128, \nLattice (random shift) d: 1, 2, 4, 8, 16, 32, 64, 128, \nDigital Net (random scramble + shift) d: 1, 2, 4, 8, 16, 32, 64, 128, \n      Halton (QMCPy) d: 1, 2, 4, 8, 16, 32, 64, 128, \n     Halton (Dakota) d: 1, 2, 4, 8, 16, 32, 64, 128, \n</pre>"},{"location":"demos/DAKOTA_Genz/dakota_genz/#genz-function-in-dakota-and-qmcpy","title":"Genz Function in Dakota and QMCPy\u00b6","text":"<p>A QMCPy implementation and comparison of Dakota's Genz function</p>"},{"location":"demos/GBM/","title":"Geometric Brownian Motion (GBM) Demonstrations","text":""},{"location":"demos/GBM/#geometric-brownian-motion-gbm-demonstrations","title":"Geometric Brownian Motion (GBM) Demonstrations","text":"<p>This directory contains implementations and demonstrations of Geometric Brownian Motion using QMCPy and QuantLib libraries.</p>"},{"location":"demos/GBM/#directories","title":"Directories","text":"<ul> <li><code>images/</code> - Generated plots and figures saved as PNG files</li> <li><code>outputs/</code> - Output data files and results</li> <li><code>gbm_code/</code> - Additional code and utilities</li> </ul>"},{"location":"demos/GBM/#files","title":"Files","text":""},{"location":"demos/GBM/#jupyter-notebooks","title":"Jupyter Notebooks","text":"<ul> <li><code>gbm_demo.ipynb</code> - Comprehensive demonstration of GBM with interactive visualizations and library comparisons</li> <li><code>examples.ipynb</code> - Examples of MAE analysis across different replications and configurations</li> </ul>"},{"location":"demos/GBM/#python-scripts","title":"Python Scripts","text":"<ul> <li><code>__init__.py</code> - Package initialization file</li> <li><code>config.py</code> - Configuration parameters for experiments (debug settings, GBM parameters, sampler configurations)</li> <li><code>qmcpy_util.py</code> - QMCPy sampler creation and path generation utilities</li> <li><code>qmcpy_util_replications.py</code> - QMCPy path generation with multiple independent replications</li> <li><code>quantlib_util.py</code> - QuantLib path generation utilities for IID and Sobol samplers</li> <li><code>averaged_mae.py</code> - Compute and plot average MAE for both QMCPy and QuantLib samplers over multiple replications</li> <li><code>data_util.py</code> - Data processing functions for experimental results and benchmarking</li> <li><code>latex_util.py</code> - LaTeX table generation and formatting utilities for publication</li> <li><code>plot_util.py</code> - Visualization functions for GBM paths, distributions, theoretical statistics, error comparisons, performance benchmarks, and parameter sweeps</li> </ul>"},{"location":"demos/GBM/__init__/","title":"init","text":"In\u00a0[\u00a0]: Copied! <pre>\"\"\"\nGBM demonstration package.\n\nThis package provides utilities for Geometric Brownian Motion demonstrations.\n\"\"\"\n</pre> \"\"\" GBM demonstration package.  This package provides utilities for Geometric Brownian Motion demonstrations. \"\"\""},{"location":"demos/GBM/averaged_mae/","title":"Averaged mae","text":"In\u00a0[\u00a0]: Copied! <pre>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport quantlib_util as qlu\nimport qmcpy_util as qpu\nimport config as cf\nimport plot_util as pu\nfrom matplotlib.ticker import FixedLocator, FixedFormatter\n</pre> import numpy as np import pandas as pd import matplotlib.pyplot as plt import quantlib_util as qlu import qmcpy_util as qpu import config as cf import plot_util as pu from matplotlib.ticker import FixedLocator, FixedFormatter In\u00a0[\u00a0]: Copied! <pre>def _compute_mae(\n    method: str,\n    sampler: str,\n    sweep_type: str,\n    replications: int = 5,\n    qp_seed: int = 42,\n    ql_seed: int = 7,\n) -&gt; tuple:\n    \"\"\"\n    Compute averaged MAE for a given sweep type (paths or steps).\n\n    This is the unified internal function that computes Mean Absolute Error (MAE)\n    of the MEAN ESTIMATOR, defined as:\n        MAE = E[|mean(paths) - theoretical_mean|]\n    where the expectation is taken over replications.\n\n    Args:\n        method: Either \"QMCPy\" or \"QuantLib\"\n        sampler: Sampler type (e.g., 'IIDStdUniform', 'Sobol', 'Lattice', 'Halton')\n        sweep_type: Either \"paths\" (sweep n_paths) or \"steps\" (sweep n_steps)\n        replications: Number of independent replications to average over\n        qp_seed: Random seed for QMCPy samplers\n        ql_seed: Random seed for QuantLib samplers\n\n    Returns:\n        tuple: (sweep_range, mean_errors) - sweep values and corresponding MAEs\n    \"\"\"\n    # Experiment configuration\n    exp_cfg = cf.get_experiment_configurations()[\n        sweep_type if sweep_type == \"paths\" else \"time_steps\"\n    ]\n    sweep_range = exp_cfg[\"range\"]\n\n    # Determine fixed parameter based on sweep type\n    if sweep_type == \"paths\":\n        fixed_steps = exp_cfg[\"fixed_steps\"]\n    else:  # steps\n        fixed_paths = exp_cfg[\"fixed_paths\"]\n\n    # GBM parameters\n    gbm_params = cf.get_gbm_parameters()\n    initial_value = gbm_params[\"initial_value\"]\n    mu = gbm_params[\"mu\"]\n    sigma = gbm_params[\"sigma\"]\n    diffusion = sigma**2\n    maturity = gbm_params[\"maturity\"]\n\n    # Pre-compute theoretical mean (constant across all iterations)\n    theoretical_mean = initial_value * np.exp(mu * maturity)\n\n    mean_errors = []\n\n    for sweep_val in sweep_range:\n        # Set n_paths and n_steps based on sweep type\n        if sweep_type == \"paths\":\n            n_paths = sweep_val\n            n_steps = fixed_steps\n        else:\n            n_paths = fixed_paths\n            n_steps = sweep_val\n\n        if method == \"QMCPy\":\n            seed = qp_seed\n            paths, _ = qpu.generate_qmcpy_paths(\n                initial_value=initial_value,\n                mu=mu,\n                diffusion=diffusion,\n                maturity=maturity,\n                n_steps=n_steps,\n                n_paths=n_paths,\n                sampler_type=sampler,\n                replications=replications,\n                seed=seed,\n            )\n            # Extract values at the final time\n            final_vals = paths[:, :, -1]\n            # Compute MAE for each replication and take the average\n            errors = np.abs(final_vals.mean(axis=1) - theoretical_mean)\n\n        elif method == \"QuantLib\":\n            # Pre-allocate array for better performance\n            errors = np.empty(replications)\n            for r in range(replications):\n                seed = ql_seed + r\n                paths, _ = qlu.generate_quantlib_paths(\n                    initial_value=initial_value,\n                    mu=mu,\n                    sigma=sigma,\n                    maturity=maturity,\n                    n_steps=n_steps,\n                    n_paths=n_paths,\n                    sampler_type=sampler,\n                    seed=seed,\n                )\n                # Vectorized operations for final values and error\n                final_vals = paths[:, -1]\n                errors[r] = np.abs(final_vals.mean() - theoretical_mean)\n\n        mean_errors.append(errors.mean())\n\n    return sweep_range, mean_errors\n</pre> def _compute_mae(     method: str,     sampler: str,     sweep_type: str,     replications: int = 5,     qp_seed: int = 42,     ql_seed: int = 7, ) -&gt; tuple:     \"\"\"     Compute averaged MAE for a given sweep type (paths or steps).      This is the unified internal function that computes Mean Absolute Error (MAE)     of the MEAN ESTIMATOR, defined as:         MAE = E[|mean(paths) - theoretical_mean|]     where the expectation is taken over replications.      Args:         method: Either \"QMCPy\" or \"QuantLib\"         sampler: Sampler type (e.g., 'IIDStdUniform', 'Sobol', 'Lattice', 'Halton')         sweep_type: Either \"paths\" (sweep n_paths) or \"steps\" (sweep n_steps)         replications: Number of independent replications to average over         qp_seed: Random seed for QMCPy samplers         ql_seed: Random seed for QuantLib samplers      Returns:         tuple: (sweep_range, mean_errors) - sweep values and corresponding MAEs     \"\"\"     # Experiment configuration     exp_cfg = cf.get_experiment_configurations()[         sweep_type if sweep_type == \"paths\" else \"time_steps\"     ]     sweep_range = exp_cfg[\"range\"]      # Determine fixed parameter based on sweep type     if sweep_type == \"paths\":         fixed_steps = exp_cfg[\"fixed_steps\"]     else:  # steps         fixed_paths = exp_cfg[\"fixed_paths\"]      # GBM parameters     gbm_params = cf.get_gbm_parameters()     initial_value = gbm_params[\"initial_value\"]     mu = gbm_params[\"mu\"]     sigma = gbm_params[\"sigma\"]     diffusion = sigma**2     maturity = gbm_params[\"maturity\"]      # Pre-compute theoretical mean (constant across all iterations)     theoretical_mean = initial_value * np.exp(mu * maturity)      mean_errors = []      for sweep_val in sweep_range:         # Set n_paths and n_steps based on sweep type         if sweep_type == \"paths\":             n_paths = sweep_val             n_steps = fixed_steps         else:             n_paths = fixed_paths             n_steps = sweep_val          if method == \"QMCPy\":             seed = qp_seed             paths, _ = qpu.generate_qmcpy_paths(                 initial_value=initial_value,                 mu=mu,                 diffusion=diffusion,                 maturity=maturity,                 n_steps=n_steps,                 n_paths=n_paths,                 sampler_type=sampler,                 replications=replications,                 seed=seed,             )             # Extract values at the final time             final_vals = paths[:, :, -1]             # Compute MAE for each replication and take the average             errors = np.abs(final_vals.mean(axis=1) - theoretical_mean)          elif method == \"QuantLib\":             # Pre-allocate array for better performance             errors = np.empty(replications)             for r in range(replications):                 seed = ql_seed + r                 paths, _ = qlu.generate_quantlib_paths(                     initial_value=initial_value,                     mu=mu,                     sigma=sigma,                     maturity=maturity,                     n_steps=n_steps,                     n_paths=n_paths,                     sampler_type=sampler,                     seed=seed,                 )                 # Vectorized operations for final values and error                 final_vals = paths[:, -1]                 errors[r] = np.abs(final_vals.mean() - theoretical_mean)          mean_errors.append(errors.mean())      return sweep_range, mean_errors In\u00a0[\u00a0]: Copied! <pre>def compute_mae_vs_paths(\n    method: str,\n    sampler: str,\n    replications: int = 5,\n    qp_seed: int = 42,\n    ql_seed: int = 7,\n) -&gt; tuple:\n    \"\"\"\n    Compute averaged MAE vs number of paths for all samplers.\n\n    This computes the Mean Absolute Error (MAE) of the MEAN ESTIMATOR, defined as:\n        MAE = E[|mean(paths) - theoretical_mean|]\n    where the expectation is taken over replications. This measures how accurately\n    the (quasi) Monte Carlo mean estimator approximates the true theoretical mean.\n\n    Args:\n        method: Either \"QMCPy\" or \"QuantLib\"\n        sampler: Sampler type (e.g., 'IIDStdUniform', 'Sobol', 'Lattice', 'Halton')\n        replications: Number of independent replications to average over\n        qp_seed: Random seed for QMCPy samplers\n        ql_seed: Random seed for QuantLib samplers\n\n    Returns:\n        tuple: (n_paths_range, mean_errors) - path counts and corresponding MAEs\n    \"\"\"\n    return _compute_mae(method, sampler, \"paths\", replications, qp_seed, ql_seed)\n</pre> def compute_mae_vs_paths(     method: str,     sampler: str,     replications: int = 5,     qp_seed: int = 42,     ql_seed: int = 7, ) -&gt; tuple:     \"\"\"     Compute averaged MAE vs number of paths for all samplers.      This computes the Mean Absolute Error (MAE) of the MEAN ESTIMATOR, defined as:         MAE = E[|mean(paths) - theoretical_mean|]     where the expectation is taken over replications. This measures how accurately     the (quasi) Monte Carlo mean estimator approximates the true theoretical mean.      Args:         method: Either \"QMCPy\" or \"QuantLib\"         sampler: Sampler type (e.g., 'IIDStdUniform', 'Sobol', 'Lattice', 'Halton')         replications: Number of independent replications to average over         qp_seed: Random seed for QMCPy samplers         ql_seed: Random seed for QuantLib samplers      Returns:         tuple: (n_paths_range, mean_errors) - path counts and corresponding MAEs     \"\"\"     return _compute_mae(method, sampler, \"paths\", replications, qp_seed, ql_seed) In\u00a0[\u00a0]: Copied! <pre>def compute_mae_vs_steps(\n    method: str,\n    sampler: str,\n    replications: int = 5,\n    qp_seed: int = 42,\n    ql_seed: int = 7,\n) -&gt; tuple:\n    \"\"\"\n    Compute averaged MAE vs number of time steps for all samplers.\n\n    This computes the Mean Absolute Error (MAE) of the MEAN ESTIMATOR, defined as:\n        MAE = E[|mean(paths) - theoretical_mean|]\n    where the expectation is taken over replications. This measures how the\n    discretization error (number of time steps) affects the accuracy of the\n    Monte Carlo mean estimator.\n\n    Args:\n        method: Either \"QMCPy\" or \"QuantLib\"\n        sampler: Sampler type (e.g., 'IIDStdUniform', 'Sobol', 'Lattice', 'Halton')\n        replications: Number of independent replications to average over\n        qp_seed: Random seed for QMCPy samplers\n        ql_seed: Random seed for QuantLib samplers\n\n    Returns:\n        tuple: (n_steps_range, mean_errors) - step counts and corresponding MAEs\n    \"\"\"\n    return _compute_mae(method, sampler, \"steps\", replications, qp_seed, ql_seed)\n</pre> def compute_mae_vs_steps(     method: str,     sampler: str,     replications: int = 5,     qp_seed: int = 42,     ql_seed: int = 7, ) -&gt; tuple:     \"\"\"     Compute averaged MAE vs number of time steps for all samplers.      This computes the Mean Absolute Error (MAE) of the MEAN ESTIMATOR, defined as:         MAE = E[|mean(paths) - theoretical_mean|]     where the expectation is taken over replications. This measures how the     discretization error (number of time steps) affects the accuracy of the     Monte Carlo mean estimator.      Args:         method: Either \"QMCPy\" or \"QuantLib\"         sampler: Sampler type (e.g., 'IIDStdUniform', 'Sobol', 'Lattice', 'Halton')         replications: Number of independent replications to average over         qp_seed: Random seed for QMCPy samplers         ql_seed: Random seed for QuantLib samplers      Returns:         tuple: (n_steps_range, mean_errors) - step counts and corresponding MAEs     \"\"\"     return _compute_mae(method, sampler, \"steps\", replications, qp_seed, ql_seed) In\u00a0[\u00a0]: Copied! <pre>def _plot_mae(sweep_type: str, replications: int = 5) -&gt; None:\n    \"\"\"\n    Plot averaged MAE for a given sweep type (paths or steps).\n\n    This is the unified internal function that plots MAE.\n\n    Args:\n        sweep_type: Either \"paths\" or \"steps\"\n        replications: Number of independent replications\n    \"\"\"\n    styling = pu.get_plot_styling()\n    sampler_cfg = cf.get_sampler_configurations()\n    qmcpy_samplers = sampler_cfg[\"all_samplers\"]\n    ql_samplers = sampler_cfg[\"quantlib_samplers\"]\n\n    all_pairs = [(\"QuantLib\", s) for s in ql_samplers] + [\n        (\"QMCPy\", s) for s in qmcpy_samplers\n    ]\n\n    fig, ax = plt.subplots(figsize=(10, 6))\n\n    for method, sampler in all_pairs:\n        if sweep_type == \"paths\":\n            x_vals, mean_errors = compute_mae_vs_paths(method, sampler, replications)\n        else:  # steps\n            x_vals, mean_errors = compute_mae_vs_steps(method, sampler, replications)\n\n        colors = styling[\"colors\"][method]\n        markers = styling[\"markers\"][method]\n\n        ax.loglog(\n            x_vals,\n            mean_errors,\n            marker=markers.get(sampler),\n            color=colors.get(sampler),\n            linewidth=2,\n            markersize=6,\n            label=f\"{method} - {sampler}\",\n        )\n\n    ax.xaxis.set_major_locator(FixedLocator(x_vals))\n    ax.xaxis.set_major_formatter(FixedFormatter([str(int(x)) for x in x_vals]))\n    ax.xaxis.set_minor_locator(FixedLocator([]))\n\n    if sweep_type == \"paths\":\n        ax.set_xlabel(\"Number of Paths\", fontsize=12, fontweight=\"bold\")\n        ax.set_title(\n            f\"MAE vs Number of Paths across {replications} replications (n_steps = 252)\",\n            fontsize=14,\n            fontweight=\"bold\",\n        )\n    else:\n        ax.set_xlabel(\"Number of Steps\", fontsize=12, fontweight=\"bold\")\n        ax.set_title(\n            f\"MAE vs Number of Time Steps across {replications} replications (n_paths = 4,096)\",\n            fontsize=14,\n            fontweight=\"bold\",\n        )\n\n    ax.set_ylabel(\"Mean Absolute Error\", fontsize=12, fontweight=\"bold\")\n    ax.grid(True, alpha=0.3)\n    ax.legend(fontsize=10)\n\n    plt.show()\n</pre> def _plot_mae(sweep_type: str, replications: int = 5) -&gt; None:     \"\"\"     Plot averaged MAE for a given sweep type (paths or steps).      This is the unified internal function that plots MAE.      Args:         sweep_type: Either \"paths\" or \"steps\"         replications: Number of independent replications     \"\"\"     styling = pu.get_plot_styling()     sampler_cfg = cf.get_sampler_configurations()     qmcpy_samplers = sampler_cfg[\"all_samplers\"]     ql_samplers = sampler_cfg[\"quantlib_samplers\"]      all_pairs = [(\"QuantLib\", s) for s in ql_samplers] + [         (\"QMCPy\", s) for s in qmcpy_samplers     ]      fig, ax = plt.subplots(figsize=(10, 6))      for method, sampler in all_pairs:         if sweep_type == \"paths\":             x_vals, mean_errors = compute_mae_vs_paths(method, sampler, replications)         else:  # steps             x_vals, mean_errors = compute_mae_vs_steps(method, sampler, replications)          colors = styling[\"colors\"][method]         markers = styling[\"markers\"][method]          ax.loglog(             x_vals,             mean_errors,             marker=markers.get(sampler),             color=colors.get(sampler),             linewidth=2,             markersize=6,             label=f\"{method} - {sampler}\",         )      ax.xaxis.set_major_locator(FixedLocator(x_vals))     ax.xaxis.set_major_formatter(FixedFormatter([str(int(x)) for x in x_vals]))     ax.xaxis.set_minor_locator(FixedLocator([]))      if sweep_type == \"paths\":         ax.set_xlabel(\"Number of Paths\", fontsize=12, fontweight=\"bold\")         ax.set_title(             f\"MAE vs Number of Paths across {replications} replications (n_steps = 252)\",             fontsize=14,             fontweight=\"bold\",         )     else:         ax.set_xlabel(\"Number of Steps\", fontsize=12, fontweight=\"bold\")         ax.set_title(             f\"MAE vs Number of Time Steps across {replications} replications (n_paths = 4,096)\",             fontsize=14,             fontweight=\"bold\",         )      ax.set_ylabel(\"Mean Absolute Error\", fontsize=12, fontweight=\"bold\")     ax.grid(True, alpha=0.3)     ax.legend(fontsize=10)      plt.show() In\u00a0[\u00a0]: Copied! <pre>def plot_mae_vs_paths(replications: int = 5) -&gt; None:\n    \"\"\"\n    Plot averaged MAE vs number of paths for all samplers.\n\n    Visualizes how the Mean Absolute Error of the mean estimator decreases\n    as the number of paths increases, comparing different sampling methods\n    from both QMCPy and QuantLib.\n\n    Args:\n        replications: Number of independent replications to average over (default: 5)\n    \"\"\"\n    _plot_mae(\"paths\", replications)\n</pre> def plot_mae_vs_paths(replications: int = 5) -&gt; None:     \"\"\"     Plot averaged MAE vs number of paths for all samplers.      Visualizes how the Mean Absolute Error of the mean estimator decreases     as the number of paths increases, comparing different sampling methods     from both QMCPy and QuantLib.      Args:         replications: Number of independent replications to average over (default: 5)     \"\"\"     _plot_mae(\"paths\", replications) In\u00a0[\u00a0]: Copied! <pre>def plot_mae_vs_steps(replications: int = 5) -&gt; None:\n    \"\"\"\n    Plot averaged MAE vs number of time steps for all samplers.\n\n    Visualizes how the Mean Absolute Error of the mean estimator changes\n    with the number of discretization time steps, comparing different\n    sampling methods from both QMCPy and QuantLib.\n\n    Args:\n        replications: Number of independent replications to average over (default: 5)\n    \"\"\"\n    _plot_mae(\"steps\", replications)\n</pre> def plot_mae_vs_steps(replications: int = 5) -&gt; None:     \"\"\"     Plot averaged MAE vs number of time steps for all samplers.      Visualizes how the Mean Absolute Error of the mean estimator changes     with the number of discretization time steps, comparing different     sampling methods from both QMCPy and QuantLib.      Args:         replications: Number of independent replications to average over (default: 5)     \"\"\"     _plot_mae(\"steps\", replications) In\u00a0[\u00a0]: Copied! <pre>def update_sweep_df(df: pd.DataFrame, replications: int = 5) -&gt; pd.DataFrame:\n    \"\"\"\n    Update DataFrame with averaged MAE values across replications.\n\n    Replaces values in the 'Mean Absolute Error' column with more accurate MAE\n    estimates computed by averaging over multiple independent replications.\n\n    Args:\n        df: DataFrame containing sweep results with MAE values\n        replications: Number of independent replications to average over (default: 5)\n\n    Returns:\n        DataFrame with updated MAE values\n    \"\"\"\n    df = df.copy()\n\n    # Get sampler configurations\n    exp_cfg = cf.get_experiment_configurations()\n    ql_samplers = cf.get_sampler_configurations()[\"quantlib_samplers\"]\n    qp_samplers = cf.get_sampler_configurations()[\"all_samplers\"]\n\n    # Compute MAE for all (method, sampler) pairs\n    mae_vals = {}\n\n    # Compute MAE vs number of paths\n    for method in [\"QMCPy\", \"QuantLib\"]:\n        method_samplers = qp_samplers if method == \"QMCPy\" else ql_samplers\n        for sampler in method_samplers:\n            x, y = compute_mae_vs_paths(method, sampler, replications=replications)\n            mae_vals[(method, sampler, \"paths\")] = dict(zip(x, y))\n\n    # Compute MAE vs number of time steps\n    for method in [\"QMCPy\", \"QuantLib\"]:\n        method_samplers = qp_samplers if method == \"QMCPy\" else ql_samplers\n        for sampler in method_samplers:\n            x, y = compute_mae_vs_steps(method, sampler, replications=replications)\n            mae_vals[(method, sampler, \"steps\")] = dict(zip(x, y))\n\n    # Update the dataframe\n\n    # Keep theoretical values\n    for idx, row in df.iterrows():\n        if row[\"Method\"] == \"Theoretical\":\n            continue\n\n        method = row[\"Method\"]\n        sampler = row[\"Sampler\"]\n\n        # Update MAEs for changing number of paths\n        if row[\"Series\"] == \"Paths\":\n            lookup_key = (method, sampler, \"paths\")\n            if lookup_key in mae_vals:\n                df.loc[idx, \"Mean Absolute Error\"] = mae_vals[lookup_key][\n                    row[\"n_paths\"]\n                ]\n\n        # Update MAEs for changing number of time steps\n        elif row[\"Series\"] == \"Time Steps\":\n            lookup_key = (method, sampler, \"steps\")\n            if lookup_key in mae_vals:\n                df.loc[idx, \"Mean Absolute Error\"] = mae_vals[lookup_key][\n                    row[\"n_steps\"]\n                ]\n\n    return df\n</pre> def update_sweep_df(df: pd.DataFrame, replications: int = 5) -&gt; pd.DataFrame:     \"\"\"     Update DataFrame with averaged MAE values across replications.      Replaces values in the 'Mean Absolute Error' column with more accurate MAE     estimates computed by averaging over multiple independent replications.      Args:         df: DataFrame containing sweep results with MAE values         replications: Number of independent replications to average over (default: 5)      Returns:         DataFrame with updated MAE values     \"\"\"     df = df.copy()      # Get sampler configurations     exp_cfg = cf.get_experiment_configurations()     ql_samplers = cf.get_sampler_configurations()[\"quantlib_samplers\"]     qp_samplers = cf.get_sampler_configurations()[\"all_samplers\"]      # Compute MAE for all (method, sampler) pairs     mae_vals = {}      # Compute MAE vs number of paths     for method in [\"QMCPy\", \"QuantLib\"]:         method_samplers = qp_samplers if method == \"QMCPy\" else ql_samplers         for sampler in method_samplers:             x, y = compute_mae_vs_paths(method, sampler, replications=replications)             mae_vals[(method, sampler, \"paths\")] = dict(zip(x, y))      # Compute MAE vs number of time steps     for method in [\"QMCPy\", \"QuantLib\"]:         method_samplers = qp_samplers if method == \"QMCPy\" else ql_samplers         for sampler in method_samplers:             x, y = compute_mae_vs_steps(method, sampler, replications=replications)             mae_vals[(method, sampler, \"steps\")] = dict(zip(x, y))      # Update the dataframe      # Keep theoretical values     for idx, row in df.iterrows():         if row[\"Method\"] == \"Theoretical\":             continue          method = row[\"Method\"]         sampler = row[\"Sampler\"]          # Update MAEs for changing number of paths         if row[\"Series\"] == \"Paths\":             lookup_key = (method, sampler, \"paths\")             if lookup_key in mae_vals:                 df.loc[idx, \"Mean Absolute Error\"] = mae_vals[lookup_key][                     row[\"n_paths\"]                 ]          # Update MAEs for changing number of time steps         elif row[\"Series\"] == \"Time Steps\":             lookup_key = (method, sampler, \"steps\")             if lookup_key in mae_vals:                 df.loc[idx, \"Mean Absolute Error\"] = mae_vals[lookup_key][                     row[\"n_steps\"]                 ]      return df"},{"location":"demos/GBM/config/","title":"Config","text":"In\u00a0[\u00a0]: Copied! <pre>is_debug = False\n</pre> is_debug = False In\u00a0[\u00a0]: Copied! <pre>def get_experiment_configurations() -&gt; dict:\n    \"\"\"\n    Define experimental parameter ranges for GBM simulations.\n\n    Returns:\n        dict: Configuration dictionary with 'time_steps' and 'paths' experiments,\n              each containing 'fixed_paths'/'fixed_steps', 'range', and 'series_name'\n    \"\"\"\n    return {\n        \"time_steps\": {\n            \"fixed_paths\": 2**3 if is_debug else 2**12,\n            \"range\": (\n                [2**i for i in range(4, 7)]\n                if is_debug\n                else [2**i for i in range(4, 10)]\n            ),\n            \"series_name\": \"Time Steps\",\n        },\n        \"paths\": {\n            \"fixed_steps\": 2**5 if is_debug else 252,\n            \"range\": (\n                [2**i for i in range(6, 9)]\n                if is_debug\n                else [2**i for i in range(9, 15)]\n            ),\n            \"series_name\": \"Paths\",\n        },\n    }\n</pre> def get_experiment_configurations() -&gt; dict:     \"\"\"     Define experimental parameter ranges for GBM simulations.      Returns:         dict: Configuration dictionary with 'time_steps' and 'paths' experiments,               each containing 'fixed_paths'/'fixed_steps', 'range', and 'series_name'     \"\"\"     return {         \"time_steps\": {             \"fixed_paths\": 2**3 if is_debug else 2**12,             \"range\": (                 [2**i for i in range(4, 7)]                 if is_debug                 else [2**i for i in range(4, 10)]             ),             \"series_name\": \"Time Steps\",         },         \"paths\": {             \"fixed_steps\": 2**5 if is_debug else 252,             \"range\": (                 [2**i for i in range(6, 9)]                 if is_debug                 else [2**i for i in range(9, 15)]             ),             \"series_name\": \"Paths\",         },     } In\u00a0[\u00a0]: Copied! <pre>def get_sampler_configurations() -&gt; dict:\n    \"\"\"\n    Define sampler types available for testing.\n\n    Returns:\n        dict: Dictionary with 'all_samplers' (QMCPy samplers) and\n              'quantlib_samplers' (QuantLib-supported samplers)\n    \"\"\"\n    return {\n        \"all_samplers\": [\"IIDStdUniform\", \"Sobol\", \"Lattice\", \"Halton\"],\n        \"quantlib_samplers\": [\"IIDStdUniform\", \"Sobol\"],\n    }\n</pre> def get_sampler_configurations() -&gt; dict:     \"\"\"     Define sampler types available for testing.      Returns:         dict: Dictionary with 'all_samplers' (QMCPy samplers) and               'quantlib_samplers' (QuantLib-supported samplers)     \"\"\"     return {         \"all_samplers\": [\"IIDStdUniform\", \"Sobol\", \"Lattice\", \"Halton\"],         \"quantlib_samplers\": [\"IIDStdUniform\", \"Sobol\"],     } In\u00a0[\u00a0]: Copied! <pre>def get_gbm_parameters() -&gt; dict:\n    \"\"\"\n    Define base Geometric Brownian Motion parameters.\n\n    Returns:\n        dict: Parameters including 'initial_value' (S_0), 'mu' (drift),\n              'sigma' (volatility), and 'maturity' (time horizon T)\n    \"\"\"\n    return {\"initial_value\": 100, \"mu\": 0.05, \"sigma\": 0.2, \"maturity\": 1.0}\n</pre> def get_gbm_parameters() -&gt; dict:     \"\"\"     Define base Geometric Brownian Motion parameters.      Returns:         dict: Parameters including 'initial_value' (S_0), 'mu' (drift),               'sigma' (volatility), and 'maturity' (time horizon T)     \"\"\"     return {\"initial_value\": 100, \"mu\": 0.05, \"sigma\": 0.2, \"maturity\": 1.0}"},{"location":"demos/GBM/data_util/","title":"Data util","text":"In\u00a0[\u00a0]: Copied! <pre>import numpy as np\nimport numpy.typing as npt\nimport pandas as pd\nimport quantlib_util as qlu\nimport qmcpy_util as qpu\nimport config as cf\n</pre> import numpy as np import numpy.typing as npt import pandas as pd import quantlib_util as qlu import qmcpy_util as qpu import config as cf In\u00a0[\u00a0]: Copied! <pre>def add_theoretical_results(\n    results_data: list, theoretical_mean: float, theoretical_std: float\n) -&gt; None:\n    \"\"\"\n    Add theoretical benchmark values to results data.\n\n    Args:\n        results_data: List to append theoretical results to\n        theoretical_mean: Theoretical expected value $E[S_T]$\n        theoretical_std: Theoretical standard deviation of $S_T$\n    \"\"\"\n    results_data.append(\n        {\n            \"Method\": \"Theoretical\",\n            \"Sampler\": \"-\",\n            \"Mean\": theoretical_mean,\n            \"Std Dev\": theoretical_std,\n            \"Mean Absolute Error\": 0,\n            \"Std Dev Error\": 0,\n        }\n    )\n</pre> def add_theoretical_results(     results_data: list, theoretical_mean: float, theoretical_std: float ) -&gt; None:     \"\"\"     Add theoretical benchmark values to results data.      Args:         results_data: List to append theoretical results to         theoretical_mean: Theoretical expected value $E[S_T]$         theoretical_std: Theoretical standard deviation of $S_T$     \"\"\"     results_data.append(         {             \"Method\": \"Theoretical\",             \"Sampler\": \"-\",             \"Mean\": theoretical_mean,             \"Std Dev\": theoretical_std,             \"Mean Absolute Error\": 0,             \"Std Dev Error\": 0,         }     ) In\u00a0[\u00a0]: Copied! <pre>def add_quantlib_results(\n    results_data: list,\n    sampler_type: str,\n    quantlib_final: npt.NDArray[np.floating],  # per replication mean\n    theoretical_mean: float,\n    theoretical_std: float,\n) -&gt; None:\n    \"\"\"\n    Add summary statistics for QuantLib simulations based on per-replication means.\n\n    The `quantlib_final` array is expected to contain one value per replication,\n    where each value is the mean of all simulated terminal asset prices S_T in\n    that replication.\n\n    Args:\n        results_data: List to which the QuantLib summary row is appended.\n        sampler_type: Identifier for the sampler used in the QuantLib experiment.\n        quantlib_final: 1D array of per-replication sample means of $S_T$.\n        theoretical_mean: Theoretical expected value $E[S_T]$ used as a benchmark.\n        theoretical_std: Theoretical standard deviation of $S_T$ used as a benchmark.\n    \"\"\"\n    ql_emp_mean = np.mean(quantlib_final)\n    ql_emp_std = np.std(quantlib_final, ddof=1)\n    ql_mae = np.mean(np.abs(quantlib_final - theoretical_mean))\n\n    results_data.append(\n        {\n            \"Method\": \"QuantLib\",\n            \"Sampler\": sampler_type,\n            \"Mean\": ql_emp_mean,\n            \"Std Dev\": ql_emp_std,\n            \"Mean Absolute Error\": ql_mae,\n            \"Std Dev Error\": abs(ql_emp_std - theoretical_std),\n        }\n    )\n</pre> def add_quantlib_results(     results_data: list,     sampler_type: str,     quantlib_final: npt.NDArray[np.floating],  # per replication mean     theoretical_mean: float,     theoretical_std: float, ) -&gt; None:     \"\"\"     Add summary statistics for QuantLib simulations based on per-replication means.      The `quantlib_final` array is expected to contain one value per replication,     where each value is the mean of all simulated terminal asset prices S_T in     that replication.      Args:         results_data: List to which the QuantLib summary row is appended.         sampler_type: Identifier for the sampler used in the QuantLib experiment.         quantlib_final: 1D array of per-replication sample means of $S_T$.         theoretical_mean: Theoretical expected value $E[S_T]$ used as a benchmark.         theoretical_std: Theoretical standard deviation of $S_T$ used as a benchmark.     \"\"\"     ql_emp_mean = np.mean(quantlib_final)     ql_emp_std = np.std(quantlib_final, ddof=1)     ql_mae = np.mean(np.abs(quantlib_final - theoretical_mean))      results_data.append(         {             \"Method\": \"QuantLib\",             \"Sampler\": sampler_type,             \"Mean\": ql_emp_mean,             \"Std Dev\": ql_emp_std,             \"Mean Absolute Error\": ql_mae,             \"Std Dev Error\": abs(ql_emp_std - theoretical_std),         }     ) In\u00a0[\u00a0]: Copied! <pre>def add_qmcpy_results(\n    results_data: list,\n    sampler_type: str,\n    qmcpy_final: npt.NDArray[np.floating],  # per replication mean\n    qp_emp_mean: float,\n    theoretical_mean: float,\n    theoretical_std: float,\n) -&gt; None:\n    \"\"\"\n    Add empirical QMCPy results, computed from per-replication means, to results data.\n\n    The mean absolute error (MAE) is computed across replications as\n    the average of the absolute differences between each per-replication\n    mean in `qmcpy_final` and `theoretical_mean`.\n\n    Args:\n        results_data: List to which the QMCPy summary for this sampler is appended.\n        sampler_type: Name or type of the QMCPy sampler used.\n        qmcpy_final: 1D array where each entry is the mean payoff from a single\n            replication of the QMCPy experiment.\n        qp_emp_mean: Overall empirical mean across all replications.\n        theoretical_mean: Theoretical expected value used as a benchmark.\n        theoretical_std: Theoretical standard deviation used as a benchmark.\n    \"\"\"\n    qp_emp_std = np.std(qmcpy_final, ddof=1)\n    qp_mae = np.mean(np.abs(qmcpy_final - theoretical_mean))\n\n    results_data.append(\n        {\n            \"Method\": \"QMCPy\",\n            \"Sampler\": sampler_type,\n            \"Mean\": qp_emp_mean,\n            \"Std Dev\": qp_emp_std,\n            \"Mean Absolute Error\": qp_mae,\n            \"Std Dev Error\": abs(qp_emp_std - theoretical_std),\n        }\n    )\n</pre> def add_qmcpy_results(     results_data: list,     sampler_type: str,     qmcpy_final: npt.NDArray[np.floating],  # per replication mean     qp_emp_mean: float,     theoretical_mean: float,     theoretical_std: float, ) -&gt; None:     \"\"\"     Add empirical QMCPy results, computed from per-replication means, to results data.      The mean absolute error (MAE) is computed across replications as     the average of the absolute differences between each per-replication     mean in `qmcpy_final` and `theoretical_mean`.      Args:         results_data: List to which the QMCPy summary for this sampler is appended.         sampler_type: Name or type of the QMCPy sampler used.         qmcpy_final: 1D array where each entry is the mean payoff from a single             replication of the QMCPy experiment.         qp_emp_mean: Overall empirical mean across all replications.         theoretical_mean: Theoretical expected value used as a benchmark.         theoretical_std: Theoretical standard deviation used as a benchmark.     \"\"\"     qp_emp_std = np.std(qmcpy_final, ddof=1)     qp_mae = np.mean(np.abs(qmcpy_final - theoretical_mean))      results_data.append(         {             \"Method\": \"QMCPy\",             \"Sampler\": sampler_type,             \"Mean\": qp_emp_mean,             \"Std Dev\": qp_emp_std,             \"Mean Absolute Error\": qp_mae,             \"Std Dev Error\": abs(qp_emp_std - theoretical_std),         }     ) In\u00a0[\u00a0]: Copied! <pre>def process_sampler_data(\n    sampler_type: str,\n    results_data: list,\n    theoretical_mean: float,\n    theoretical_std: float,\n    params_ql: dict,\n    params_qp: dict,\n) -&gt; tuple:\n    \"\"\"\n    Process and compare data for a single sampler type across both libraries.\n\n    Args:\n        sampler_type: Type of sampler to test\n        results_data: List to append comparison results to\n        theoretical_mean: Theoretical expected value\n        theoretical_std: Theoretical standard deviation\n        params_ql: Dictionary of QuantLib parameters\n        params_qp: Dictionary of QMCPy parameters\n\n    Returns:\n        tuple: (quantlib_paths, qmcpy_paths, ql_gbm, qp_gbm, params_ql, params_qp)\n    \"\"\"\n\n    params_ql[\"sampler_type\"] = sampler_type\n    params_qp[\"sampler_type\"] = sampler_type\n\n    replications = params_qp[\"replications\"]\n\n    quantlib_paths, ql_gbm = None, None\n\n    if sampler_type in [\"IIDStdUniform\", \"Sobol\"]:\n        ql_means = np.empty(replications)\n        ql_seed = params_ql[\"seed\"]\n\n        for r in range(replications):\n            params_ql[\"seed\"] = ql_seed + r\n            quantlib_paths, ql_gbm = qlu.generate_quantlib_paths(**params_ql)\n            ql_means[r] = quantlib_paths[:, -1].mean()\n\n        params_ql[\"seed\"] = ql_seed\n    else:\n        ql_means = None\n\n    qmcpy_paths, qp_gbm = qpu.generate_qmcpy_paths(**params_qp)\n\n    if qmcpy_paths.ndim == 3:\n        qp_means = qmcpy_paths[:, :, -1].mean(axis=1)\n    else:\n        qp_means = np.array([qmcpy_paths[:, -1].mean()])\n\n    if ql_means is not None:\n        add_quantlib_results(\n            results_data,\n            sampler_type,\n            ql_means,\n            theoretical_mean,\n            theoretical_std,\n        )\n\n    add_qmcpy_results(\n        results_data,\n        sampler_type,\n        qp_means,\n        qp_means.mean(),\n        theoretical_mean,\n        theoretical_std,\n    )\n\n    return quantlib_paths, qmcpy_paths, ql_gbm, qp_gbm, params_ql, params_qp\n</pre> def process_sampler_data(     sampler_type: str,     results_data: list,     theoretical_mean: float,     theoretical_std: float,     params_ql: dict,     params_qp: dict, ) -&gt; tuple:     \"\"\"     Process and compare data for a single sampler type across both libraries.      Args:         sampler_type: Type of sampler to test         results_data: List to append comparison results to         theoretical_mean: Theoretical expected value         theoretical_std: Theoretical standard deviation         params_ql: Dictionary of QuantLib parameters         params_qp: Dictionary of QMCPy parameters      Returns:         tuple: (quantlib_paths, qmcpy_paths, ql_gbm, qp_gbm, params_ql, params_qp)     \"\"\"      params_ql[\"sampler_type\"] = sampler_type     params_qp[\"sampler_type\"] = sampler_type      replications = params_qp[\"replications\"]      quantlib_paths, ql_gbm = None, None      if sampler_type in [\"IIDStdUniform\", \"Sobol\"]:         ql_means = np.empty(replications)         ql_seed = params_ql[\"seed\"]          for r in range(replications):             params_ql[\"seed\"] = ql_seed + r             quantlib_paths, ql_gbm = qlu.generate_quantlib_paths(**params_ql)             ql_means[r] = quantlib_paths[:, -1].mean()          params_ql[\"seed\"] = ql_seed     else:         ql_means = None      qmcpy_paths, qp_gbm = qpu.generate_qmcpy_paths(**params_qp)      if qmcpy_paths.ndim == 3:         qp_means = qmcpy_paths[:, :, -1].mean(axis=1)     else:         qp_means = np.array([qmcpy_paths[:, -1].mean()])      if ql_means is not None:         add_quantlib_results(             results_data,             sampler_type,             ql_means,             theoretical_mean,             theoretical_std,         )      add_qmcpy_results(         results_data,         sampler_type,         qp_means,         qp_means.mean(),         theoretical_mean,         theoretical_std,     )      return quantlib_paths, qmcpy_paths, ql_gbm, qp_gbm, params_ql, params_qp In\u00a0[\u00a0]: Copied! <pre>def create_timing_dataframe(\n    quantlib_results: dict, qmcpy_results: dict, baseline_sampler: str\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Create comprehensive timing comparison table from benchmark results.\n\n    Args:\n        quantlib_results: Dictionary mapping sampler names to timing results\n        qmcpy_results: Dictionary mapping sampler names to timing results\n        baseline_sampler: Sampler to use as baseline for speedup calculation\n\n    Returns:\n        DataFrame with timing statistics and speedup comparisons\n    \"\"\"\n    timing_data = []\n\n    # Add QuantLib data\n    for sampler_type, result in quantlib_results.items():\n        timing_data.append(\n            {\n                \"Method\": \"QuantLib\",\n                \"Sampler\": sampler_type,\n                \"Mean Time (s)\": result[\"average\"],\n                \"Std Dev (s)\": result[\"stdev\"],\n                \"Speedup\": \"-\",\n            }\n        )\n\n    # Add QMCPy data with speedup calculation\n    baseline_time = quantlib_results[baseline_sampler][\"average\"]\n    for sampler_type, result in qmcpy_results.items():\n        speedup = baseline_time / result[\"average\"]\n        timing_data.append(\n            {\n                \"Method\": \"QMCPy\",\n                \"Sampler\": sampler_type,\n                \"Mean Time (s)\": result[\"average\"],\n                \"Std Dev (s)\": result[\"stdev\"],\n                \"Speedup\": speedup,\n            }\n        )\n\n    return pd.DataFrame(timing_data)\n</pre> def create_timing_dataframe(     quantlib_results: dict, qmcpy_results: dict, baseline_sampler: str ) -&gt; pd.DataFrame:     \"\"\"     Create comprehensive timing comparison table from benchmark results.      Args:         quantlib_results: Dictionary mapping sampler names to timing results         qmcpy_results: Dictionary mapping sampler names to timing results         baseline_sampler: Sampler to use as baseline for speedup calculation      Returns:         DataFrame with timing statistics and speedup comparisons     \"\"\"     timing_data = []      # Add QuantLib data     for sampler_type, result in quantlib_results.items():         timing_data.append(             {                 \"Method\": \"QuantLib\",                 \"Sampler\": sampler_type,                 \"Mean Time (s)\": result[\"average\"],                 \"Std Dev (s)\": result[\"stdev\"],                 \"Speedup\": \"-\",             }         )      # Add QMCPy data with speedup calculation     baseline_time = quantlib_results[baseline_sampler][\"average\"]     for sampler_type, result in qmcpy_results.items():         speedup = baseline_time / result[\"average\"]         timing_data.append(             {                 \"Method\": \"QMCPy\",                 \"Sampler\": sampler_type,                 \"Mean Time (s)\": result[\"average\"],                 \"Std Dev (s)\": result[\"stdev\"],                 \"Speedup\": speedup,             }         )      return pd.DataFrame(timing_data) In\u00a0[\u00a0]: Copied! <pre>def extract_comparison_data(results_df: pd.DataFrame) -&gt; tuple:\n    \"\"\"\n    Extract data for comparison plotting from results dataframe.\n\n    Args:\n        results_df: DataFrame containing results from both libraries\n\n    Returns:\n        tuple: (samplers, qmcpy_errors, qmcpy_times, quantlib_errors,\n                quantlib_times, theoretical_mean)\n    \"\"\"\n    qmcpy_data = results_df[results_df[\"Method\"] == \"QMCPy\"].copy()\n    quantlib_data = results_df[results_df[\"Method\"] == \"QuantLib\"].copy()\n    theoretical_data = results_df[results_df[\"Method\"] == \"Theoretical\"].copy()\n\n    samplers = qmcpy_data[\"Sampler\"].values\n    qmcpy_errors = qmcpy_data[\"Mean Absolute Error\"].values\n    qmcpy_times = (\n        qmcpy_data[\"Mean Time (s)\"].values\n        if \"Mean Time (s)\" in qmcpy_data.columns\n        else None\n    )\n\n    # Get QuantLib data (only available for some samplers\n    ql_error_dict = dict(\n        zip(quantlib_data[\"Sampler\"], quantlib_data[\"Mean Absolute Error\"])\n    )\n    quantlib_errors = [ql_error_dict.get(s) for s in samplers]\n\n    if \"Mean Time (s)\" in quantlib_data.columns:\n        ql_time_dict = dict(\n            zip(quantlib_data[\"Sampler\"], quantlib_data[\"Mean Time (s)\"])\n        )\n        quantlib_times = [ql_time_dict.get(s) for s in samplers]\n    else:\n        quantlib_times = [None] * len(samplers)\n\n    # Handle case where theoretical data might be missing\n    if not theoretical_data.empty:\n        theoretical_mean = theoretical_data[\"Mean\"].iloc[0]\n    else:\n        # Calculate theoretical mean from parameters if not in results_df\n        # Using the parameters from the comparison study\n        S0, mu, T = 100, 0.05, 1.0\n        theoretical_mean = S0 * np.exp(mu * T)\n\n    return (\n        samplers,\n        qmcpy_errors,\n        qmcpy_times,\n        quantlib_errors,\n        quantlib_times,\n        theoretical_mean,\n    )\n</pre> def extract_comparison_data(results_df: pd.DataFrame) -&gt; tuple:     \"\"\"     Extract data for comparison plotting from results dataframe.      Args:         results_df: DataFrame containing results from both libraries      Returns:         tuple: (samplers, qmcpy_errors, qmcpy_times, quantlib_errors,                 quantlib_times, theoretical_mean)     \"\"\"     qmcpy_data = results_df[results_df[\"Method\"] == \"QMCPy\"].copy()     quantlib_data = results_df[results_df[\"Method\"] == \"QuantLib\"].copy()     theoretical_data = results_df[results_df[\"Method\"] == \"Theoretical\"].copy()      samplers = qmcpy_data[\"Sampler\"].values     qmcpy_errors = qmcpy_data[\"Mean Absolute Error\"].values     qmcpy_times = (         qmcpy_data[\"Mean Time (s)\"].values         if \"Mean Time (s)\" in qmcpy_data.columns         else None     )      # Get QuantLib data (only available for some samplers     ql_error_dict = dict(         zip(quantlib_data[\"Sampler\"], quantlib_data[\"Mean Absolute Error\"])     )     quantlib_errors = [ql_error_dict.get(s) for s in samplers]      if \"Mean Time (s)\" in quantlib_data.columns:         ql_time_dict = dict(             zip(quantlib_data[\"Sampler\"], quantlib_data[\"Mean Time (s)\"])         )         quantlib_times = [ql_time_dict.get(s) for s in samplers]     else:         quantlib_times = [None] * len(samplers)      # Handle case where theoretical data might be missing     if not theoretical_data.empty:         theoretical_mean = theoretical_data[\"Mean\"].iloc[0]     else:         # Calculate theoretical mean from parameters if not in results_df         # Using the parameters from the comparison study         S0, mu, T = 100, 0.05, 1.0         theoretical_mean = S0 * np.exp(mu * T)      return (         samplers,         qmcpy_errors,         qmcpy_times,         quantlib_errors,         quantlib_times,         theoretical_mean,     ) In\u00a0[\u00a0]: Copied! <pre>def add_theoretical_row(\n    results: list,\n    series_name: str,\n    n_steps: int,\n    n_paths: int,\n    theoretical_mean: float,\n    theoretical_std: float,\n) -&gt; None:\n    \"\"\"Add theoretical benchmark row to results\"\"\"\n    results.append(\n        {\n            \"Series\": series_name,\n            \"n_steps\": n_steps,\n            \"n_paths\": n_paths,\n            \"Method\": \"Theoretical\",\n            \"Sampler\": \"-\",\n            \"Mean\": theoretical_mean,\n            \"Std Dev\": theoretical_std,\n            \"Mean Absolute Error\": 0,\n            \"Std Dev Error\": 0,\n            \"Runtime (s)\": 0,\n            \"Runtime Std (s)\": 0,\n        }\n    )\n</pre> def add_theoretical_row(     results: list,     series_name: str,     n_steps: int,     n_paths: int,     theoretical_mean: float,     theoretical_std: float, ) -&gt; None:     \"\"\"Add theoretical benchmark row to results\"\"\"     results.append(         {             \"Series\": series_name,             \"n_steps\": n_steps,             \"n_paths\": n_paths,             \"Method\": \"Theoretical\",             \"Sampler\": \"-\",             \"Mean\": theoretical_mean,             \"Std Dev\": theoretical_std,             \"Mean Absolute Error\": 0,             \"Std Dev Error\": 0,             \"Runtime (s)\": 0,             \"Runtime Std (s)\": 0,         }     ) In\u00a0[\u00a0]: Copied! <pre>def collect_library_results(\n    sampler: str,\n    series_name: str,\n    n_steps: int,\n    n_paths: int,\n    ql_timing: dict,\n    qp_timing: dict,\n    theoretical_mean: float,\n    theoretical_std: float,\n) -&gt; list:\n    \"\"\"Collect results for both QuantLib and QMCPy for a given sampler\"\"\"\n    results = []\n    gbm_params = cf.get_gbm_parameters()\n\n    # QuantLib parameters\n    ql_params = {**gbm_params, \"n_steps\": n_steps, \"n_paths\": n_paths}\n\n    # QMCPy parameters (note: diffusion = sigma^2)\n    qp_params = {\n        \"initial_value\": gbm_params[\"initial_value\"],\n        \"mu\": gbm_params[\"mu\"],\n        \"diffusion\": gbm_params[\"sigma\"] ** 2,  # Convert sigma to diffusion\n        \"maturity\": gbm_params[\"maturity\"],\n        \"n_steps\": n_steps,\n        \"n_paths\": n_paths,\n    }\n\n    # QuantLib results (if supported)\n    if sampler in cf.get_sampler_configurations()[\"quantlib_samplers\"]:\n        try:\n            ql_paths, _ = qlu.generate_quantlib_paths(sampler_type=sampler, **ql_params)\n            ql_final = ql_paths[:, -1]\n            ql_mean = np.mean(ql_final)\n            ql_std = np.std(ql_final, ddof=1)\n\n            results.append(\n                {\n                    \"Series\": series_name,\n                    \"n_steps\": n_steps,\n                    \"n_paths\": n_paths,\n                    \"Method\": \"QuantLib\",\n                    \"Sampler\": sampler,\n                    \"Mean\": ql_mean,\n                    \"Std Dev\": ql_std,\n                    \"Mean Absolute Error\": abs(ql_mean - theoretical_mean),\n                    \"Std Dev Error\": abs(ql_std - theoretical_std),\n                    \"Runtime (s)\": ql_timing[sampler][\"average\"],\n                    \"Runtime Std (s)\": ql_timing[sampler][\"stdev\"],\n                }\n            )\n        except Exception as e:\n            print(f\"      QuantLib {sampler} failed: {e}\")\n\n    # QMCPy results\n    try:\n        qp_paths, _ = qpu.generate_qmcpy_paths(sampler_type=sampler, **qp_params)\n        qp_final = qp_paths[:, -1]\n        qp_mean = np.mean(qp_final)\n        qp_std = np.std(qp_final, ddof=1)\n\n        results.append(\n            {\n                \"Series\": series_name,\n                \"n_steps\": n_steps,\n                \"n_paths\": n_paths,\n                \"Method\": \"QMCPy\",\n                \"Sampler\": sampler,\n                \"Mean\": qp_mean,\n                \"Std Dev\": qp_std,\n                \"Mean Absolute Error\": abs(qp_mean - theoretical_mean),\n                \"Std Dev Error\": abs(qp_std - theoretical_std),\n                \"Runtime (s)\": qp_timing[sampler][\"average\"],\n                \"Runtime Std (s)\": qp_timing[sampler][\"stdev\"],\n            }\n        )\n    except Exception as e:\n        print(f\"      QMCPy {sampler} failed: {e}\")\n\n    return results\n</pre> def collect_library_results(     sampler: str,     series_name: str,     n_steps: int,     n_paths: int,     ql_timing: dict,     qp_timing: dict,     theoretical_mean: float,     theoretical_std: float, ) -&gt; list:     \"\"\"Collect results for both QuantLib and QMCPy for a given sampler\"\"\"     results = []     gbm_params = cf.get_gbm_parameters()      # QuantLib parameters     ql_params = {**gbm_params, \"n_steps\": n_steps, \"n_paths\": n_paths}      # QMCPy parameters (note: diffusion = sigma^2)     qp_params = {         \"initial_value\": gbm_params[\"initial_value\"],         \"mu\": gbm_params[\"mu\"],         \"diffusion\": gbm_params[\"sigma\"] ** 2,  # Convert sigma to diffusion         \"maturity\": gbm_params[\"maturity\"],         \"n_steps\": n_steps,         \"n_paths\": n_paths,     }      # QuantLib results (if supported)     if sampler in cf.get_sampler_configurations()[\"quantlib_samplers\"]:         try:             ql_paths, _ = qlu.generate_quantlib_paths(sampler_type=sampler, **ql_params)             ql_final = ql_paths[:, -1]             ql_mean = np.mean(ql_final)             ql_std = np.std(ql_final, ddof=1)              results.append(                 {                     \"Series\": series_name,                     \"n_steps\": n_steps,                     \"n_paths\": n_paths,                     \"Method\": \"QuantLib\",                     \"Sampler\": sampler,                     \"Mean\": ql_mean,                     \"Std Dev\": ql_std,                     \"Mean Absolute Error\": abs(ql_mean - theoretical_mean),                     \"Std Dev Error\": abs(ql_std - theoretical_std),                     \"Runtime (s)\": ql_timing[sampler][\"average\"],                     \"Runtime Std (s)\": ql_timing[sampler][\"stdev\"],                 }             )         except Exception as e:             print(f\"      QuantLib {sampler} failed: {e}\")      # QMCPy results     try:         qp_paths, _ = qpu.generate_qmcpy_paths(sampler_type=sampler, **qp_params)         qp_final = qp_paths[:, -1]         qp_mean = np.mean(qp_final)         qp_std = np.std(qp_final, ddof=1)          results.append(             {                 \"Series\": series_name,                 \"n_steps\": n_steps,                 \"n_paths\": n_paths,                 \"Method\": \"QMCPy\",                 \"Sampler\": sampler,                 \"Mean\": qp_mean,                 \"Std Dev\": qp_std,                 \"Mean Absolute Error\": abs(qp_mean - theoretical_mean),                 \"Std Dev Error\": abs(qp_std - theoretical_std),                 \"Runtime (s)\": qp_timing[sampler][\"average\"],                 \"Runtime Std (s)\": qp_timing[sampler][\"stdev\"],             }         )     except Exception as e:         print(f\"      QMCPy {sampler} failed: {e}\")      return results"},{"location":"demos/GBM/gbm_demo/","title":"Geometric Brownian Motion","text":"<p>Larysa Matiukha, Aleksei Sorokin, and Sou-Cheng T. Choi</p> <p>Illinois Institute of Technology</p> <p>Modification date: 01/14/2026</p> <p>Creation date: 08/24/2024</p> <p>For reproducibility, this notebook was run with:</p> <ul> <li>Python 3.9.13, NumPy 1.23.4, SciPy 1.9.3, QMCPy 2.1, QuantLib 1.38, Matplotlib 3.5.2, ipywidgets 8.1.7</li> <li>OS: macOS 15.6.1</li> <li>Random seeds: 42 (QMCPy), 7 (QuantLib)</li> </ul> In\u00a0[1]: Copied! <pre>try:\n    import qmcpy as qp \nexcept ModuleNotFoundError:\n    !pip install -q qmcpy\n</pre> try:     import qmcpy as qp  except ModuleNotFoundError:     !pip install -q qmcpy In\u00a0[2]: Copied! <pre>import qmcpy as qp\nimport numpy as np\nimport scipy.stats as sc\nimport matplotlib.pyplot as plt\nimport ipywidgets as widgets\nimport pandas as pd\nimport os\n\nimport latex_util as lu\nimport plot_util as pu\nimport data_util as du\nimport quantlib_util as qlu\nimport qmcpy_util as qpu\nimport config as cf\nimport averaged_mae as am\n</pre> import qmcpy as qp import numpy as np import scipy.stats as sc import matplotlib.pyplot as plt import ipywidgets as widgets import pandas as pd import os  import latex_util as lu import plot_util as pu import data_util as du import quantlib_util as qlu import qmcpy_util as qpu import config as cf import averaged_mae as am In\u00a0[3]: Copied! <pre># Create output directory\nos.makedirs('outputs', exist_ok=True)\nos.makedirs('images', exist_ok=True)\n\n# Toogle debug mode\ncf.is_debug = False\n</pre> # Create output directory os.makedirs('outputs', exist_ok=True) os.makedirs('images', exist_ok=True)  # Toogle debug mode cf.is_debug = False <p>Geometric Brownian motion (GBM) is a continuous stochastic process in which the natural logarithm of its values follows a Brownian motion. $[1]$</p> <p>Mathematically, it can be defined as follows:</p> <p>$\\large{S_t = S_0 \\, e^{\\big(\\mu - \\frac{\\sigma^2}{2}\\big)  t + \\sigma W_t}}$,</p> <p>where</p> <ul> <li>$S_0$ is the initial value,</li> <li>$\\mu$ is a drift coefficient</li> <li>$\\sigma$ is the volatility</li> <li>$W_t$ is a (standard) Brownian motion.</li> </ul> <p>GBM is commonly used to model stock prices and options payoffs.</p> <p>Note that QMCPy uses the diffusion coefficient $\\sigma^2$ as a parameter.</p> <p>Geometric Brownian Motion in QMCPy inherits from BrownianMotion class $[2, 3]$.</p> <p>Let's explore the constructor and sample generation methods through the built-in help documentation:</p> In\u00a0[4]: Copied! <pre>help(qp.GeometricBrownianMotion.__init__)\n</pre> help(qp.GeometricBrownianMotion.__init__) <pre>Help on function __init__ in module qmcpy.true_measure.geometric_brownian_motion:\n\n__init__(self, sampler, t_final=1, initial_value=1, drift=0, diffusion=1, decomp_type='PCA', lazy_load=True, lazy_decomp=True)\n    Args:\n        sampler (DiscreteDistribution/TrueMeasure): A discrete distribution or true measure.\n        t_final (float): End time for the geometric Brownian motion, non-negative.\n        initial_value (float): Positive initial value of the process, $S_0$.\n        drift (float): Drift coefficient $\\gamma$.\n        diffusion (float): Positive diffusion coefficient $\\sigma^2$, where $\\sigma$ is volatility.\n        decomp_type (str): Method of decomposition, either \"PCA\" or \"Cholesky\".\n        lazy_load (bool): If True, defer GBM-specific computations until needed.\n        lazy_decomp (bool): If True, defer expensive matrix decomposition until needed.\n\n</pre> In\u00a0[5]: Copied! <pre>help(qp.GeometricBrownianMotion.gen_samples)\n</pre> help(qp.GeometricBrownianMotion.gen_samples) <pre>Help on function gen_samples in module qmcpy.true_measure.geometric_brownian_motion:\n\ngen_samples(self, n=None, n_min=None, n_max=None, return_weights=False, warn=True)\n    Generate GBM samples using the parent's transform pipeline.\n\n    Args:\n        n (int): number of samples to generate\n        n_min (int): minimum index of sequence\n        n_max (int): maximum index of sequence\n        return_weights (bool): whether to return Jacobian weights\n        warn (bool): whether to warn about sample generation\n\n    Returns:\n        ndarray or tuple: GBM samples, optionally with weights if return_weights=True\n\n</pre> <p>Now let's create a simple GBM instance and generate sample paths to see the class in action:</p> In\u00a0[6]: Copied! <pre>gbm = qp.GeometricBrownianMotion(qp.Lattice(2, seed=42))\ngbm\n</pre> gbm = qp.GeometricBrownianMotion(qp.Lattice(2, seed=42)) gbm Out[6]: <pre>GeometricBrownianMotion (AbstractTrueMeasure)\n    time_vec        [0.5 1. ]\n    drift           0\n    diffusion       1\n    mean_gbm        [1. 1.]\n    covariance_gbm  [[0.649 0.649]\n                     [0.649 1.718]]\n    decomp_type     PCA</pre> In\u00a0[7]: Copied! <pre>gbm.gen_samples(n=4)  # print four samples\n</pre> gbm.gen_samples(n=4)  # print four samples Out[7]: <pre>array([[0.72608046, 0.70071241],\n       [0.38739775, 0.07432173],\n       [0.81262942, 1.66867239],\n       [0.619371  , 0.31898397]])</pre> In\u00a0[8]: Copied! <pre># Generate GBM samples for theoretical validation\nS0, mu, sigma, T, n_samples = 100.0, 0.05, 0.20, 1.0, 2**12\ndiffusion = sigma**2\nsampler = qp.Lattice(5, seed=42)\nqp_gbm = qp.GeometricBrownianMotion(sampler, t_final=T, initial_value=S0, drift=mu, diffusion=diffusion)\npaths = qp_gbm.gen_samples(n_samples)\nS_T = paths[:, -1]  # Final values only\n\n# Calculate theoretical vs empirical sample moments\ntheo_mean = S0 * np.exp(mu * T)\ntheo_var = S0**2 * np.exp(2*mu*T) * (np.exp(diffusion * T) - 1)\nqp_emp_mean = np.mean(S_T)\nqp_emp_var = np.var(S_T, ddof=1) \nprint(f\"Mean: {qp_emp_mean:.3f} (theoretical: {theo_mean:.3f})\")\nprint(f\"Variance: {qp_emp_var:.3f} (theoretical: {theo_var:.3f})\")\nqp_gbm\n</pre> # Generate GBM samples for theoretical validation S0, mu, sigma, T, n_samples = 100.0, 0.05, 0.20, 1.0, 2**12 diffusion = sigma**2 sampler = qp.Lattice(5, seed=42) qp_gbm = qp.GeometricBrownianMotion(sampler, t_final=T, initial_value=S0, drift=mu, diffusion=diffusion) paths = qp_gbm.gen_samples(n_samples) S_T = paths[:, -1]  # Final values only  # Calculate theoretical vs empirical sample moments theo_mean = S0 * np.exp(mu * T) theo_var = S0**2 * np.exp(2*mu*T) * (np.exp(diffusion * T) - 1) qp_emp_mean = np.mean(S_T) qp_emp_var = np.var(S_T, ddof=1)  print(f\"Mean: {qp_emp_mean:.3f} (theoretical: {theo_mean:.3f})\") print(f\"Variance: {qp_emp_var:.3f} (theoretical: {theo_var:.3f})\") qp_gbm <pre>Mean: 105.127 (theoretical: 105.127)\nVariance: 449.776 (theoretical: 451.029)\n</pre> Out[8]: <pre>GeometricBrownianMotion (AbstractTrueMeasure)\n    time_vec        [0.2 0.4 0.6 0.8 1. ]\n    drift           0.050\n    diffusion       0.040\n    mean_gbm        [101.005 102.02  103.045 104.081 105.127]\n    covariance_gbm  [[ 81.943  82.767  83.599  84.439  85.288]\n                     [ 82.767 167.869 169.556 171.26  172.981]\n                     [ 83.599 169.556 257.923 260.516 263.134]\n                     [ 84.439 171.26  260.516 352.258 355.798]\n                     [ 85.288 172.981 263.134 355.798 451.029]]\n    decomp_type     PCA</pre> <p>Below we compare Brownian motion and geometric Brownian motion using the same parameters: <code>drift</code> = 0, <code>diffusion</code> = 1, <code>initial_value</code> = 1.</p> <p>First, let's define a utility function that will help us visualize GBM paths with different samplers and parameters:</p> In\u00a0[9]: Copied! <pre>def plot_paths(motion_type, sampler, t_final, initial_value, drift, diffusion, n, png_filename=None):\n    if motion_type.upper() == 'BM':\n        motion = qp.BrownianMotion(sampler, t_final, initial_value, drift, diffusion)\n        title = f'Realizations of Brownian Motion using {type(sampler).__name__} points'\n        ylabel = '$W(t)$'\n    elif motion_type.upper() == 'GBM':\n        motion = qp.GeometricBrownianMotion(sampler, t_final, initial_value, drift, diffusion)\n        title = f'Realizations of Geometric Brownian Motion using {type(sampler).__name__} points'\n        ylabel = '$S(t)$'\n    else:\n        raise ValueError(\"motion_type must be 'BM' or 'GBM'\")\n    \n    t = motion.gen_samples(n)\n    initial_values = np.full((n, 1), motion.initial_value)\n    t_w_init = np.hstack((initial_values, t))\n    tvec_w_0 = np.hstack(([0], motion.time_vec))\n\n    plt.figure(figsize=(7, 4));\n    _ = plt.plot(tvec_w_0, t_w_init.T); \n    _ = plt.title(title);\n    _ = plt.xlabel('$t$');\n    _ = plt.ylabel(ylabel);\n    _ = plt.xlim([tvec_w_0[0], tvec_w_0[-1]]);\n    if png_filename:   # save .png to images/\n        os.makedirs('images', exist_ok=True)\n        plt.savefig(f'images/{png_filename}.png', bbox_inches='tight');\n    plt.show();\n</pre> def plot_paths(motion_type, sampler, t_final, initial_value, drift, diffusion, n, png_filename=None):     if motion_type.upper() == 'BM':         motion = qp.BrownianMotion(sampler, t_final, initial_value, drift, diffusion)         title = f'Realizations of Brownian Motion using {type(sampler).__name__} points'         ylabel = '$W(t)$'     elif motion_type.upper() == 'GBM':         motion = qp.GeometricBrownianMotion(sampler, t_final, initial_value, drift, diffusion)         title = f'Realizations of Geometric Brownian Motion using {type(sampler).__name__} points'         ylabel = '$S(t)$'     else:         raise ValueError(\"motion_type must be 'BM' or 'GBM'\")          t = motion.gen_samples(n)     initial_values = np.full((n, 1), motion.initial_value)     t_w_init = np.hstack((initial_values, t))     tvec_w_0 = np.hstack(([0], motion.time_vec))      plt.figure(figsize=(7, 4));     _ = plt.plot(tvec_w_0, t_w_init.T);      _ = plt.title(title);     _ = plt.xlabel('$t$');     _ = plt.ylabel(ylabel);     _ = plt.xlim([tvec_w_0[0], tvec_w_0[-1]]);     if png_filename:   # save .png to images/         os.makedirs('images', exist_ok=True)         plt.savefig(f'images/{png_filename}.png', bbox_inches='tight');     plt.show(); In\u00a0[10]: Copied! <pre># Compare Brownian Motion and Geometric Brownian Motion using the unified plotting function\nn = 16\nsampler = qp.Lattice(2**7, seed=42)\nplot_paths('BM', sampler, t_final=1, initial_value=1, drift=0, diffusion=1, n=n, png_filename='figure_1')\nplot_paths('GBM', sampler, t_final=1, initial_value=1, drift=0, diffusion=1, n=n, png_filename='figure_2')\n</pre> # Compare Brownian Motion and Geometric Brownian Motion using the unified plotting function n = 16 sampler = qp.Lattice(2**7, seed=42) plot_paths('BM', sampler, t_final=1, initial_value=1, drift=0, diffusion=1, n=n, png_filename='figure_1') plot_paths('GBM', sampler, t_final=1, initial_value=1, drift=0, diffusion=1, n=n, png_filename='figure_2') <p>Now, using <code>plot_gbm_paths</code>, we generate 32 GBM paths to model stock price, $S(t)$, with initial value $S_0$ = 50, drift coeffient, $\\mu = 0.1$, diffusion coefficient $\\sigma = 0.2$ using IID points.</p> In\u00a0[11]: Copied! <pre>gbm_iid = plot_paths('GBM', qp.IIDStdUniform(2**8, seed=42), t_final=5, initial_value=50, drift=0.1, diffusion=0.2, n=32, png_filename='figure_3')\n</pre> gbm_iid = plot_paths('GBM', qp.IIDStdUniform(2**8, seed=42), t_final=5, initial_value=50, drift=0.1, diffusion=0.2, n=32, png_filename='figure_3') <p>Using the same parameter values as in example above, we generate 32 GBM paths to model stock price using low-discrepancy lattice points:</p> In\u00a0[12]: Copied! <pre>gbm_lattice = plot_paths('GBM', qp.Lattice(2**8, seed=42), t_final=5, initial_value=50, drift=0.1, diffusion=0.2, n=32, png_filename='figure_4')\n</pre> gbm_lattice = plot_paths('GBM', qp.Lattice(2**8, seed=42), t_final=5, initial_value=50, drift=0.1, diffusion=0.2, n=32, png_filename='figure_4') <p>Next, we define a more sophisticated visualization function that combines path plotting with statistical analysis by showing both the GBM trajectories and the distribution of final values:</p> In\u00a0[13]: Copied! <pre>def plot_gbm_paths_with_distribution(N, sampler, t_final, initial_value, drift, diffusion,n):\n    gbm = qp.GeometricBrownianMotion(sampler, t_final=t_final, initial_value=initial_value, drift=drift, diffusion=diffusion)\n    gbm_path = gbm.gen_samples(2**n)\n    \n    # Handle 3D array (replications, n_paths, n_steps) by taking first replication\n    if gbm_path.ndim == 3:\n        gbm_path = gbm_path[0]\n    \n    _, ax = plt.subplots(figsize=(14, 7))\n    T = max(gbm.time_vec)\n    \n    # Plot GBM paths\n    _ = ax.plot(gbm.time_vec, gbm_path.T, lw=0.75, alpha=0.7, color='skyblue')\n    \n    # Set up main plot\n    _ = ax.set_title(f'Geometric Brownian Motion Paths\\n{N} Simulations, T = {T}, $\\mu$ = {drift:.1f}, $\\sigma$ = {diffusion:.1f}, using {type(sampler).__name__} points')\n    _ = ax.set_xlabel(r'$t$')\n    _ = ax.set_ylabel(r'$S(t)$')\n    _ = ax.set_ylim(bottom=0)\n    _ = ax.set_xlim(0, T)\n    \n    # Add histogram\n    final_values = gbm_path[:, -1]\n    hist_ax = ax.inset_axes([1.05, 0., 0.5, 1])\n    _ = hist_ax.hist(final_values, bins=20, density=True, alpha=0.5, color='skyblue', orientation='horizontal')\n    \n    # Add theoretical lognormal PDF\n    shape, _, scale = sc.lognorm.fit(final_values, floc=0)\n    x = np.linspace(0, max(final_values), 1000)\n    pdf = sc.lognorm.pdf(x, shape, loc=0, scale=scale)\n    _ = hist_ax.plot(pdf, x, 'r-', lw=2, label='Lognormal PDF')\n    \n    # Finalize histogram\n    _ = hist_ax.set_title(f'E[$S_T$] = {np.mean(final_values):.4f}', pad=20)\n    _ = hist_ax.axhline(np.mean(final_values), color='blue', linestyle='--', lw=1.5, label=r'$E[S_T]$')\n    _ = hist_ax.set_yticks([])\n    _ = hist_ax.set_xlabel('Density')\n    _ = hist_ax.legend()\n    _ = hist_ax.set_ylim(bottom=0)\n    plt.tight_layout()  \n    plt.show()\n</pre> def plot_gbm_paths_with_distribution(N, sampler, t_final, initial_value, drift, diffusion,n):     gbm = qp.GeometricBrownianMotion(sampler, t_final=t_final, initial_value=initial_value, drift=drift, diffusion=diffusion)     gbm_path = gbm.gen_samples(2**n)          # Handle 3D array (replications, n_paths, n_steps) by taking first replication     if gbm_path.ndim == 3:         gbm_path = gbm_path[0]          _, ax = plt.subplots(figsize=(14, 7))     T = max(gbm.time_vec)          # Plot GBM paths     _ = ax.plot(gbm.time_vec, gbm_path.T, lw=0.75, alpha=0.7, color='skyblue')          # Set up main plot     _ = ax.set_title(f'Geometric Brownian Motion Paths\\n{N} Simulations, T = {T}, $\\mu$ = {drift:.1f}, $\\sigma$ = {diffusion:.1f}, using {type(sampler).__name__} points')     _ = ax.set_xlabel(r'$t$')     _ = ax.set_ylabel(r'$S(t)$')     _ = ax.set_ylim(bottom=0)     _ = ax.set_xlim(0, T)          # Add histogram     final_values = gbm_path[:, -1]     hist_ax = ax.inset_axes([1.05, 0., 0.5, 1])     _ = hist_ax.hist(final_values, bins=20, density=True, alpha=0.5, color='skyblue', orientation='horizontal')          # Add theoretical lognormal PDF     shape, _, scale = sc.lognorm.fit(final_values, floc=0)     x = np.linspace(0, max(final_values), 1000)     pdf = sc.lognorm.pdf(x, shape, loc=0, scale=scale)     _ = hist_ax.plot(pdf, x, 'r-', lw=2, label='Lognormal PDF')          # Finalize histogram     _ = hist_ax.set_title(f'E[$S_T$] = {np.mean(final_values):.4f}', pad=20)     _ = hist_ax.axhline(np.mean(final_values), color='blue', linestyle='--', lw=1.5, label=r'$E[S_T]$')     _ = hist_ax.set_yticks([])     _ = hist_ax.set_xlabel('Density')     _ = hist_ax.legend()     _ = hist_ax.set_ylim(bottom=0)     plt.tight_layout()       plt.show() In\u00a0[14]: Copied! <pre>eps = np.finfo(float).eps\nslider_style = {'handle_color': 'blue'}\n\n@widgets.interact\ndef f(n=widgets.IntSlider(min=0, max=8, step=1, value=7, style=slider_style),\n      t_final=widgets.FloatSlider(min=eps, max=10, step=0.1, value=5.0, style=slider_style),\n      initial_value=widgets.FloatSlider(min=eps, max=100, step=0.1, value=40, style=slider_style),\n      drift=widgets.FloatSlider(min=-2, max=2, step=0.1, value=0.1, style=slider_style),\n      diffusion=widgets.FloatSlider(min=eps, max=4, step=0.1, value=0.2, style=slider_style),\n      sampler=widgets.Dropdown(options=['IIDStdUniform', 'Lattice', 'Halton', 'Sobol'], value='IIDStdUniform', description='Sampler')\n):\n      sampler_instance = qpu.create_qmcpy_sampler(sampler, 2**n)\n      plot_gbm_paths_with_distribution(2**n, sampler_instance, t_final=t_final, initial_value=initial_value, drift=drift, diffusion=diffusion, n=n)\n</pre> eps = np.finfo(float).eps slider_style = {'handle_color': 'blue'}  @widgets.interact def f(n=widgets.IntSlider(min=0, max=8, step=1, value=7, style=slider_style),       t_final=widgets.FloatSlider(min=eps, max=10, step=0.1, value=5.0, style=slider_style),       initial_value=widgets.FloatSlider(min=eps, max=100, step=0.1, value=40, style=slider_style),       drift=widgets.FloatSlider(min=-2, max=2, step=0.1, value=0.1, style=slider_style),       diffusion=widgets.FloatSlider(min=eps, max=4, step=0.1, value=0.2, style=slider_style),       sampler=widgets.Dropdown(options=['IIDStdUniform', 'Lattice', 'Halton', 'Sobol'], value='IIDStdUniform', description='Sampler') ):       sampler_instance = qpu.create_qmcpy_sampler(sampler, 2**n)       plot_gbm_paths_with_distribution(2**n, sampler_instance, t_final=t_final, initial_value=initial_value, drift=drift, diffusion=diffusion, n=n) <pre>interactive(children=(IntSlider(value=7, description='n', max=8, style=SliderStyle(handle_color='blue')), Floa\u2026</pre> In\u00a0[15]: Copied! <pre>try:\n    import QuantLib as ql\nexcept ModuleNotFoundError:\n    !pip install -q QuantLib\n</pre> try:     import QuantLib as ql except ModuleNotFoundError:     !pip install -q QuantLib <p>First, we examine the accuracy of GBM simulations produced by QMCPy and QuantLib. For each sampler, we compare the theoretical mean with the empirical mean of the simulated paths at the final time, and report the corresponding empirical standard deviation.</p> In\u00a0[25]: Copied! <pre>def compute_theoretical_covariance(S0, mu, sigma, t1, t2):\n    \"\"\"Compute theoretical covariance matrix for GBM at two time points\"\"\"\n    return np.array([\n        [S0**2 * np.exp(2*mu*t1) * (np.exp(sigma**2 * t1) - 1), \n         S0**2 * np.exp(mu*(t1+t2)) * (np.exp(sigma**2 * t1) - 1)],\n        [S0**2 * np.exp(mu*(t1+t2)) * (np.exp(sigma**2 * t1) - 1), \n         S0**2 * np.exp(2*mu*t2) * (np.exp(sigma**2 * t2) - 1)]\n    ])\n\ndef calculate_theoretical_statistics(params):\n    \"\"\"Calculate theoretical mean and std for GBM\"\"\"\n    theoretical_mean = params['initial_value'] * np.exp(params['mu'] * params['maturity'])\n    theoretical_std = np.sqrt(params['initial_value']**2 * np.exp(2*params['mu']*params['maturity']) * (np.exp(params['sigma']**2 * params['maturity']) - 1))\n    return theoretical_mean, theoretical_std\n\ndef extract_covariance_samples(paths, n_steps, is_quantlib=True):\n    \"\"\"Extract samples at two time points and compute covariance matrix\"\"\"\n    # Handle 3D array (replications, n_paths, n_steps) by taking first replication\n    # if paths.ndim == 3:\n    #     paths = paths[0]\n    \n    if is_quantlib:\n        idx1, idx2 = int(0.5 * n_steps), n_steps\n        samples_t1, samples_t2 = paths[:, idx1], paths[:, idx2]\n    else:  # QMCPy\n        idx1, idx2 = int(0.5 * (n_steps - 1)), n_steps - 1\n        samples_t1, samples_t2 = paths[:, idx1], paths[:, idx2]\n    return np.cov(np.vstack((samples_t1, samples_t2)))\n\n#======= Parameters for GBM comparison\nresults_data = []\nparams_ql = {'initial_value': 100, 'mu': 0.05, 'sigma': 0.2, 'maturity': 1.0, 'n_steps': 252, 'n_paths': 2**14, 'seed': 7}\nparams_qp = {'initial_value': 100, 'mu': 0.05, 'diffusion': 0.2**2, 'maturity': 1.0, 'n_steps': 252, 'n_paths': 2**14, 'replications': 3}\ntheoretical_mean, theoretical_std = calculate_theoretical_statistics(params_ql)\n\n# Add theoretical values once\ndu.add_theoretical_results(results_data, theoretical_mean, theoretical_std)\ntheoretical_cov = compute_theoretical_covariance(params_ql['initial_value'], params_ql['mu'], params_ql['sigma'], 0.5, 1.0)\nprint(\"COMPARISON: QuantLib vs QMCPy GeometricBrownianMotion\")\nprint(\"=\"*55)\nprint(f\"\\nTheoretical covariance matrix:\\n{theoretical_cov}\\n\")\nprint(f\"{'Theoretical':&lt;12} Mean: {theoretical_mean:.2f}, Theoretical Std: {theoretical_std:.2f}\")\n# Process each sampler\nfor sampler_type in ['IIDStdUniform', 'Sobol', 'Lattice', 'Halton']:\n    print(f\"\\n\\n{sampler_type = }\")\n    print(\"~\"*30)\n    quantlib_paths, qmcpy_paths, ql_gbm, qp_gbm, params_ql, params_qp = du.process_sampler_data(sampler_type, results_data, theoretical_mean, theoretical_std, params_ql, params_qp)\n\n    # Compute covariance matrices\n    if sampler_type in ['IIDStdUniform', 'Sobol']:\n        quantlib_cov = extract_covariance_samples(quantlib_paths, params_ql['n_steps'], is_quantlib=True)\n        \n    qmcpy_cov = extract_covariance_samples(qmcpy_paths[0], params_qp['n_steps'], is_quantlib=False)\n\n    # Final value statistics\n    if quantlib_paths is not None:\n        quantlib_final = quantlib_paths[:, -1]\n\n    #qmcpy_final = qmcpy_paths[:, -1]\n    if qmcpy_paths.ndim == 3:\n        qmcpy_final = qmcpy_paths[:, :, -1]  # (R, n_paths)\n        qmcpy_means = qmcpy_final.mean(axis=1)\n    else:\n        qmcpy_means = np.array([qmcpy_paths[:, -1].mean()])\n\n\n    theoretical_mean = params_ql['initial_value'] * np.exp(params_ql['mu'] * params_ql['maturity'])\n    theoretical_std = np.sqrt(params_ql['initial_value']**2 * np.exp(2*params_ql['mu']*params_ql['maturity']) * \n                            (np.exp(params_ql['sigma']**2 * params_ql['maturity']) - 1))\n\n    if quantlib_paths is not None: print(f\"\\nQuantLib sample covariance matrix:\\n{quantlib_cov}\")\n    print(f\"\\nQMCPy sample covariance matrix:\\n{qmcpy_cov}\")\n\n    print(\"\\nFINAL VALUE STATISTICS (t=1 year)\")\n    print(\"-\"*35)\n   \n    for name, final_vals in [(\"QuantLib\", quantlib_final), (\"QMCPy\", qmcpy_final)]:\n        if name == \"QuantLib\" and quantlib_paths is None:\n            continue\n        print(f\"{name:&lt;12} Mean: {np.mean(final_vals):.2f}, Empirical Std:   {np.std(final_vals, ddof=1):.2f}\")\n\n# Create DataFrame\nresults_df = pd.DataFrame(results_data)\nresults_df.round(4)\n\n# Store variables for visualization cell (extract individual values from params)\npaths, qmcpy_paths = quantlib_paths, qmcpy_paths\ninitial_value = params_ql['initial_value']\nmu = params_ql['mu']\nsigma = params_ql['sigma']\nmaturity = params_ql['maturity']\nn_steps = params_ql['n_steps']\n</pre> def compute_theoretical_covariance(S0, mu, sigma, t1, t2):     \"\"\"Compute theoretical covariance matrix for GBM at two time points\"\"\"     return np.array([         [S0**2 * np.exp(2*mu*t1) * (np.exp(sigma**2 * t1) - 1),           S0**2 * np.exp(mu*(t1+t2)) * (np.exp(sigma**2 * t1) - 1)],         [S0**2 * np.exp(mu*(t1+t2)) * (np.exp(sigma**2 * t1) - 1),           S0**2 * np.exp(2*mu*t2) * (np.exp(sigma**2 * t2) - 1)]     ])  def calculate_theoretical_statistics(params):     \"\"\"Calculate theoretical mean and std for GBM\"\"\"     theoretical_mean = params['initial_value'] * np.exp(params['mu'] * params['maturity'])     theoretical_std = np.sqrt(params['initial_value']**2 * np.exp(2*params['mu']*params['maturity']) * (np.exp(params['sigma']**2 * params['maturity']) - 1))     return theoretical_mean, theoretical_std  def extract_covariance_samples(paths, n_steps, is_quantlib=True):     \"\"\"Extract samples at two time points and compute covariance matrix\"\"\"     # Handle 3D array (replications, n_paths, n_steps) by taking first replication     # if paths.ndim == 3:     #     paths = paths[0]          if is_quantlib:         idx1, idx2 = int(0.5 * n_steps), n_steps         samples_t1, samples_t2 = paths[:, idx1], paths[:, idx2]     else:  # QMCPy         idx1, idx2 = int(0.5 * (n_steps - 1)), n_steps - 1         samples_t1, samples_t2 = paths[:, idx1], paths[:, idx2]     return np.cov(np.vstack((samples_t1, samples_t2)))  #======= Parameters for GBM comparison results_data = [] params_ql = {'initial_value': 100, 'mu': 0.05, 'sigma': 0.2, 'maturity': 1.0, 'n_steps': 252, 'n_paths': 2**14, 'seed': 7} params_qp = {'initial_value': 100, 'mu': 0.05, 'diffusion': 0.2**2, 'maturity': 1.0, 'n_steps': 252, 'n_paths': 2**14, 'replications': 3} theoretical_mean, theoretical_std = calculate_theoretical_statistics(params_ql)  # Add theoretical values once du.add_theoretical_results(results_data, theoretical_mean, theoretical_std) theoretical_cov = compute_theoretical_covariance(params_ql['initial_value'], params_ql['mu'], params_ql['sigma'], 0.5, 1.0) print(\"COMPARISON: QuantLib vs QMCPy GeometricBrownianMotion\") print(\"=\"*55) print(f\"\\nTheoretical covariance matrix:\\n{theoretical_cov}\\n\") print(f\"{'Theoretical':&lt;12} Mean: {theoretical_mean:.2f}, Theoretical Std: {theoretical_std:.2f}\") # Process each sampler for sampler_type in ['IIDStdUniform', 'Sobol', 'Lattice', 'Halton']:     print(f\"\\n\\n{sampler_type = }\")     print(\"~\"*30)     quantlib_paths, qmcpy_paths, ql_gbm, qp_gbm, params_ql, params_qp = du.process_sampler_data(sampler_type, results_data, theoretical_mean, theoretical_std, params_ql, params_qp)      # Compute covariance matrices     if sampler_type in ['IIDStdUniform', 'Sobol']:         quantlib_cov = extract_covariance_samples(quantlib_paths, params_ql['n_steps'], is_quantlib=True)              qmcpy_cov = extract_covariance_samples(qmcpy_paths[0], params_qp['n_steps'], is_quantlib=False)      # Final value statistics     if quantlib_paths is not None:         quantlib_final = quantlib_paths[:, -1]      #qmcpy_final = qmcpy_paths[:, -1]     if qmcpy_paths.ndim == 3:         qmcpy_final = qmcpy_paths[:, :, -1]  # (R, n_paths)         qmcpy_means = qmcpy_final.mean(axis=1)     else:         qmcpy_means = np.array([qmcpy_paths[:, -1].mean()])       theoretical_mean = params_ql['initial_value'] * np.exp(params_ql['mu'] * params_ql['maturity'])     theoretical_std = np.sqrt(params_ql['initial_value']**2 * np.exp(2*params_ql['mu']*params_ql['maturity']) *                              (np.exp(params_ql['sigma']**2 * params_ql['maturity']) - 1))      if quantlib_paths is not None: print(f\"\\nQuantLib sample covariance matrix:\\n{quantlib_cov}\")     print(f\"\\nQMCPy sample covariance matrix:\\n{qmcpy_cov}\")      print(\"\\nFINAL VALUE STATISTICS (t=1 year)\")     print(\"-\"*35)         for name, final_vals in [(\"QuantLib\", quantlib_final), (\"QMCPy\", qmcpy_final)]:         if name == \"QuantLib\" and quantlib_paths is None:             continue         print(f\"{name:&lt;12} Mean: {np.mean(final_vals):.2f}, Empirical Std:   {np.std(final_vals, ddof=1):.2f}\")  # Create DataFrame results_df = pd.DataFrame(results_data) results_df.round(4)  # Store variables for visualization cell (extract individual values from params) paths, qmcpy_paths = quantlib_paths, qmcpy_paths initial_value = params_ql['initial_value'] mu = params_ql['mu'] sigma = params_ql['sigma'] maturity = params_ql['maturity'] n_steps = params_ql['n_steps'] <pre>COMPARISON: QuantLib vs QMCPy GeometricBrownianMotion\n=======================================================\n\nTheoretical covariance matrix:\n[[212.37084878 217.74704241]\n [217.74704241 451.02880782]]\n\nTheoretical  Mean: 105.13, Theoretical Std: 21.24\n\n\nsampler_type = 'IIDStdUniform'\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nQuantLib sample covariance matrix:\n[[213.75881386 221.36726548]\n [221.36726548 459.71391352]]\n\nQMCPy sample covariance matrix:\n[[214.20431097 220.84100284]\n [220.84100284 457.31146186]]\n\nFINAL VALUE STATISTICS (t=1 year)\n-----------------------------------\nQuantLib     Mean: 105.18, Empirical Std:   21.44\nQMCPy        Mean: 105.10, Empirical Std:   21.31\n\n\nsampler_type = 'Sobol'\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nQuantLib sample covariance matrix:\n[[207.99898718 212.19478243]\n [212.19478243 443.07564465]]\n\nQMCPy sample covariance matrix:\n[[212.2324965  217.66152872]\n [217.66152872 450.88713613]]\n\nFINAL VALUE STATISTICS (t=1 year)\n-----------------------------------\nQuantLib     Mean: 105.10, Empirical Std:   21.05\nQMCPy        Mean: 105.13, Empirical Std:   21.24\n\n\nsampler_type = 'Lattice'\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nQMCPy sample covariance matrix:\n[[212.47003554 217.97810024]\n [217.97810024 451.39048711]]\n\nFINAL VALUE STATISTICS (t=1 year)\n-----------------------------------\nQMCPy        Mean: 105.13, Empirical Std:   21.23\n\n\nsampler_type = 'Halton'\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nQMCPy sample covariance matrix:\n[[212.41984349 217.52845865]\n [217.52845865 450.66741059]]\n\nFINAL VALUE STATISTICS (t=1 year)\n-----------------------------------\nQMCPy        Mean: 105.13, Empirical Std:   21.24\n</pre> In\u00a0[26]: Copied! <pre># Visualization \nn_plot=50   # Number of paths to plot\ndef plot_paths_on_axis(ax, time_vec, paths_data, title, color, n_plot=50):\n    \"\"\"Helper function to plot GBM paths on a given axis\"\"\"\n    if paths_data is not None:\n        # Handle 3D array (replications, n_paths, n_steps) by squeezing first dimension\n        if paths_data.ndim == 3:\n            paths_data = paths_data[0]  # Take first replication\n        for i in range(min(n_plot, paths_data.shape[0])):\n            ax.plot(time_vec, paths_data[i, :], alpha=0.2 + 0.3 * (i / n_plot), color=color, linewidth=0.5)\n    ax.set_title(title, fontsize=12, fontweight='bold')\n    ax.set_xlabel('Time')\n    ax.set_ylabel('Stock Price')\n    ax.grid(True, alpha=0.3)\n\n# Create comparison visualization with 2x2 subplots\nfig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(11, 8))\nsampler_type = \"Sobol\"\nql_sampler = sampler_type  \nqp_sampler = sampler_type  \n\n# Generate specific data for visualization (ensure we have data for both libraries)\nparams_vis_ql = {'initial_value': 100, 'mu': 0.05, 'sigma': 0.2, 'maturity': 1.0, 'n_steps': 252, 'n_paths': 2**14, 'sampler_type': 'Sobol'}\nparams_vis_qp = {'initial_value': 100, 'mu': 0.05, 'diffusion': 0.2**2, 'maturity': 1.0, 'n_steps': 252, 'n_paths': 2**14, 'sampler_type': 'Sobol'}\n\n# Generate paths for visualization\nvis_quantlib_paths, _ = qlu.generate_quantlib_paths(**params_vis_ql)\nvis_qmcpy_paths, vis_qp_gbm = qpu.generate_qmcpy_paths(**params_vis_qp)\n\n# Handle 3D array for QMCPy paths (replications, n_paths, n_steps)\nif vis_qmcpy_paths.ndim == 3:\n    vis_qmcpy_paths = vis_qmcpy_paths[0]  # Take first replication\n\nvis_quantlib_final = vis_quantlib_paths[:, -1]\nvis_qmcpy_final = vis_qmcpy_paths[:, -1]\n\n# Get number of samples for titles\nn_samples = vis_quantlib_paths.shape[0] if vis_quantlib_paths is not None else vis_qmcpy_paths.shape[0]\n\n# Create time grids that match path dimensions:\n# QuantLib: includes t=0, so n_steps + 1 points\n# QMCPy: excludes t=0, so n_steps points (use vis_qp_gbm.time_vec directly)\nql_time_grid = np.linspace(0, maturity, n_steps + 1)\nqp_time_grid = vis_qp_gbm.time_vec  # This matches QMCPy path dimensions\n\nplot_data = [ (ax1, ql_time_grid, vis_quantlib_paths, f'QuantLib ({ql_sampler}) GBM Paths ({n_plot} from {n_samples})', 'blue'),\n              (ax2, qp_time_grid, vis_qmcpy_paths, f'QMCPy ({qp_sampler}) GBM Paths ({n_plot} from {n_samples})', 'red') ]\nfor ax, time_grid, data, title, color in plot_data:\n    plot_paths_on_axis(ax, time_grid, data, title, color)\n\n# Final value distributions\nfor final_vals, color, label, sampler in [(vis_quantlib_final, 'blue', 'QuantLib', ql_sampler), (vis_qmcpy_final, 'red', 'QMCPy', qp_sampler)]:\n    if final_vals is not None:\n        ax3.hist(final_vals, bins=50, alpha=0.6, color=color, label=f'{label} ({sampler})', density=True)\nax3.set_title(f'Final Value Distributions (t=1)', fontsize=11, fontweight='bold')\nax3.set_xlabel('Stock Price')\nax3.set_ylabel('Density')\nax3.legend()\nax3.grid(True, alpha=0.3)\n\n# QMCPy covariance matrix heatmap\nim = ax4.imshow(vis_qp_gbm.covariance_gbm, cmap='viridis', aspect='equal') # origin='lower'\nax4.set_title(f'QMCPy ({qp_sampler}) Covariance Matrix Heatmap', fontsize=11, fontweight='bold')\nax4.set_xlabel('Time')\nax4.set_ylabel('Time')\n\n# Set custom tick labels using time_vec\ntime_ticks = np.arange(0, len(vis_qp_gbm.time_vec), max(1, len(vis_qp_gbm.time_vec)//5))  # Show ~5 ticks\nax4.set_xticks(time_ticks)\nax4.set_yticks(time_ticks)\nax4.set_xticklabels([f'{vis_qp_gbm.time_vec[i]:.1f}' for i in time_ticks])\nax4.set_yticklabels([f'{vis_qp_gbm.time_vec[i]:.1f}' for i in time_ticks])\ncbar = plt.colorbar(im, ax=ax4, shrink=0.8)   # Add colorbar\ncbar.set_label('Covariance Value', rotation=270, labelpad=20)\n\nplt.tight_layout()\nplt.savefig('images/figure_5.png', bbox_inches='tight')\nplt.show();\n</pre> # Visualization  n_plot=50   # Number of paths to plot def plot_paths_on_axis(ax, time_vec, paths_data, title, color, n_plot=50):     \"\"\"Helper function to plot GBM paths on a given axis\"\"\"     if paths_data is not None:         # Handle 3D array (replications, n_paths, n_steps) by squeezing first dimension         if paths_data.ndim == 3:             paths_data = paths_data[0]  # Take first replication         for i in range(min(n_plot, paths_data.shape[0])):             ax.plot(time_vec, paths_data[i, :], alpha=0.2 + 0.3 * (i / n_plot), color=color, linewidth=0.5)     ax.set_title(title, fontsize=12, fontweight='bold')     ax.set_xlabel('Time')     ax.set_ylabel('Stock Price')     ax.grid(True, alpha=0.3)  # Create comparison visualization with 2x2 subplots fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(11, 8)) sampler_type = \"Sobol\" ql_sampler = sampler_type   qp_sampler = sampler_type    # Generate specific data for visualization (ensure we have data for both libraries) params_vis_ql = {'initial_value': 100, 'mu': 0.05, 'sigma': 0.2, 'maturity': 1.0, 'n_steps': 252, 'n_paths': 2**14, 'sampler_type': 'Sobol'} params_vis_qp = {'initial_value': 100, 'mu': 0.05, 'diffusion': 0.2**2, 'maturity': 1.0, 'n_steps': 252, 'n_paths': 2**14, 'sampler_type': 'Sobol'}  # Generate paths for visualization vis_quantlib_paths, _ = qlu.generate_quantlib_paths(**params_vis_ql) vis_qmcpy_paths, vis_qp_gbm = qpu.generate_qmcpy_paths(**params_vis_qp)  # Handle 3D array for QMCPy paths (replications, n_paths, n_steps) if vis_qmcpy_paths.ndim == 3:     vis_qmcpy_paths = vis_qmcpy_paths[0]  # Take first replication  vis_quantlib_final = vis_quantlib_paths[:, -1] vis_qmcpy_final = vis_qmcpy_paths[:, -1]  # Get number of samples for titles n_samples = vis_quantlib_paths.shape[0] if vis_quantlib_paths is not None else vis_qmcpy_paths.shape[0]  # Create time grids that match path dimensions: # QuantLib: includes t=0, so n_steps + 1 points # QMCPy: excludes t=0, so n_steps points (use vis_qp_gbm.time_vec directly) ql_time_grid = np.linspace(0, maturity, n_steps + 1) qp_time_grid = vis_qp_gbm.time_vec  # This matches QMCPy path dimensions  plot_data = [ (ax1, ql_time_grid, vis_quantlib_paths, f'QuantLib ({ql_sampler}) GBM Paths ({n_plot} from {n_samples})', 'blue'),               (ax2, qp_time_grid, vis_qmcpy_paths, f'QMCPy ({qp_sampler}) GBM Paths ({n_plot} from {n_samples})', 'red') ] for ax, time_grid, data, title, color in plot_data:     plot_paths_on_axis(ax, time_grid, data, title, color)  # Final value distributions for final_vals, color, label, sampler in [(vis_quantlib_final, 'blue', 'QuantLib', ql_sampler), (vis_qmcpy_final, 'red', 'QMCPy', qp_sampler)]:     if final_vals is not None:         ax3.hist(final_vals, bins=50, alpha=0.6, color=color, label=f'{label} ({sampler})', density=True) ax3.set_title(f'Final Value Distributions (t=1)', fontsize=11, fontweight='bold') ax3.set_xlabel('Stock Price') ax3.set_ylabel('Density') ax3.legend() ax3.grid(True, alpha=0.3)  # QMCPy covariance matrix heatmap im = ax4.imshow(vis_qp_gbm.covariance_gbm, cmap='viridis', aspect='equal') # origin='lower' ax4.set_title(f'QMCPy ({qp_sampler}) Covariance Matrix Heatmap', fontsize=11, fontweight='bold') ax4.set_xlabel('Time') ax4.set_ylabel('Time')  # Set custom tick labels using time_vec time_ticks = np.arange(0, len(vis_qp_gbm.time_vec), max(1, len(vis_qp_gbm.time_vec)//5))  # Show ~5 ticks ax4.set_xticks(time_ticks) ax4.set_yticks(time_ticks) ax4.set_xticklabels([f'{vis_qp_gbm.time_vec[i]:.1f}' for i in time_ticks]) ax4.set_yticklabels([f'{vis_qp_gbm.time_vec[i]:.1f}' for i in time_ticks]) cbar = plt.colorbar(im, ax=ax4, shrink=0.8)   # Add colorbar cbar.set_label('Covariance Value', rotation=270, labelpad=20)  plt.tight_layout() plt.savefig('images/figure_5.png', bbox_inches='tight') plt.show(); <p>Next, we focus on path generation runtime. For benchmarking, we can use IPython's built-in <code>%timeit</code> magic command which automatically handles warm-up, multiple runs, and statistical analysis.</p> In\u00a0[28]: Copied! <pre>def benchmark_quantlib_samplers(samplers_to_test, base_params):\n    \"\"\"Benchmark QuantLib with different samplers\"\"\"\n    timing_results = {}\n    for sampler_type in samplers_to_test:\n        print(f\"QuantLib ({sampler_type}) timing:\")\n        benchmark_func = lambda st=sampler_type: qlu.generate_quantlib_paths(**base_params, sampler_type=st)\n        timing_result = %timeit -n 10 -r 3 -o benchmark_func()\n        timing_results[sampler_type] = {\n            'average': timing_result.average,\n            'stdev': timing_result.stdev,\n            'loops': timing_result.loops,\n            'repeat': timing_result.repeat\n        }\n    return timing_results\n\ndef benchmark_qmcpy_samplers(samplers_to_test, base_params):\n    \"\"\"Benchmark QMCPy samplers with timing measurements\"\"\"\n    timing_results = {}\n    \n    # Convert QuantLib parameters to QMCPy parameters if needed\n    qp_params = base_params.copy()\n    if 'sigma' in qp_params:\n        qp_params['diffusion'] = qp_params.pop('sigma') ** 2  # Convert sigma to diffusion (sigma\u00b2)\n    \n    for sampler_type in samplers_to_test:\n        print(f\"QMCPy ({sampler_type}) timing:\")\n        benchmark_func = lambda st=sampler_type: qpu.generate_qmcpy_paths(**qp_params, sampler_type=st)\n        timing_result = %timeit -n 10 -r 3 -o benchmark_func()\n        timing_results[sampler_type] = {\n            'average': timing_result.average,\n            'stdev': timing_result.stdev,\n            'loops': timing_result.loops,\n            'repeat': timing_result.repeat\n        }\n    return timing_results\n</pre> def benchmark_quantlib_samplers(samplers_to_test, base_params):     \"\"\"Benchmark QuantLib with different samplers\"\"\"     timing_results = {}     for sampler_type in samplers_to_test:         print(f\"QuantLib ({sampler_type}) timing:\")         benchmark_func = lambda st=sampler_type: qlu.generate_quantlib_paths(**base_params, sampler_type=st)         timing_result = %timeit -n 10 -r 3 -o benchmark_func()         timing_results[sampler_type] = {             'average': timing_result.average,             'stdev': timing_result.stdev,             'loops': timing_result.loops,             'repeat': timing_result.repeat         }     return timing_results  def benchmark_qmcpy_samplers(samplers_to_test, base_params):     \"\"\"Benchmark QMCPy samplers with timing measurements\"\"\"     timing_results = {}          # Convert QuantLib parameters to QMCPy parameters if needed     qp_params = base_params.copy()     if 'sigma' in qp_params:         qp_params['diffusion'] = qp_params.pop('sigma') ** 2  # Convert sigma to diffusion (sigma\u00b2)          for sampler_type in samplers_to_test:         print(f\"QMCPy ({sampler_type}) timing:\")         benchmark_func = lambda st=sampler_type: qpu.generate_qmcpy_paths(**qp_params, sampler_type=st)         timing_result = %timeit -n 10 -r 3 -o benchmark_func()         timing_results[sampler_type] = {             'average': timing_result.average,             'stdev': timing_result.stdev,             'loops': timing_result.loops,             'repeat': timing_result.repeat         }     return timing_results In\u00a0[29]: Copied! <pre># Benchmark QuantLib and QMCPy with different samplers\nquantlib_samplers_to_benchmark = ['IIDStdUniform', 'Sobol']\nqmcpy_samplers_to_benchmark = ['IIDStdUniform', 'Sobol', 'Lattice', 'Halton']\n# Create base params without sampler_type to avoid conflicts  \nbase_ql_params = {k: v for k, v in params_ql.items() if k != 'sampler_type'}\n# Add the required parameters for QMCPy benchmarking\nbase_ql_params.update({'n_steps': 252, 'n_paths': 2**14})\n# Create QMCPy parameters (note: diffusion instead of sigma)\nbase_qp_params = {\n    'initial_value': 100, \n    'mu': 0.05, \n    'diffusion': 0.2**2,  # sigma^2 for QMCPy\n    'maturity': 1.0, \n    'n_steps': 252, \n    'n_paths': 2**14\n}\n# Run benchmarks\nquantlib_timing_results = benchmark_quantlib_samplers(quantlib_samplers_to_benchmark, base_ql_params)\nqmcpy_timing_results = benchmark_qmcpy_samplers(qmcpy_samplers_to_benchmark, base_qp_params)\n# Create comprehensive timing table\ntiming_df = du.create_timing_dataframe(quantlib_timing_results, qmcpy_timing_results, quantlib_samplers_to_benchmark[0])\ntiming_df.round(10)\n</pre> # Benchmark QuantLib and QMCPy with different samplers quantlib_samplers_to_benchmark = ['IIDStdUniform', 'Sobol'] qmcpy_samplers_to_benchmark = ['IIDStdUniform', 'Sobol', 'Lattice', 'Halton'] # Create base params without sampler_type to avoid conflicts   base_ql_params = {k: v for k, v in params_ql.items() if k != 'sampler_type'} # Add the required parameters for QMCPy benchmarking base_ql_params.update({'n_steps': 252, 'n_paths': 2**14}) # Create QMCPy parameters (note: diffusion instead of sigma) base_qp_params = {     'initial_value': 100,      'mu': 0.05,      'diffusion': 0.2**2,  # sigma^2 for QMCPy     'maturity': 1.0,      'n_steps': 252,      'n_paths': 2**14 } # Run benchmarks quantlib_timing_results = benchmark_quantlib_samplers(quantlib_samplers_to_benchmark, base_ql_params) qmcpy_timing_results = benchmark_qmcpy_samplers(qmcpy_samplers_to_benchmark, base_qp_params) # Create comprehensive timing table timing_df = du.create_timing_dataframe(quantlib_timing_results, qmcpy_timing_results, quantlib_samplers_to_benchmark[0]) timing_df.round(10) <pre>QuantLib (IIDStdUniform) timing:\n602 ms \u00b1 8.25 ms per loop (mean \u00b1 std. dev. of 3 runs, 10 loops each)\nQuantLib (Sobol) timing:\n704 ms \u00b1 8.49 ms per loop (mean \u00b1 std. dev. of 3 runs, 10 loops each)\nQMCPy (IIDStdUniform) timing:\n413 ms \u00b1 5.42 ms per loop (mean \u00b1 std. dev. of 3 runs, 10 loops each)\nQMCPy (Sobol) timing:\n410 ms \u00b1 5 ms per loop (mean \u00b1 std. dev. of 3 runs, 10 loops each)\nQMCPy (Lattice) timing:\n555 ms \u00b1 11.1 ms per loop (mean \u00b1 std. dev. of 3 runs, 10 loops each)\nQMCPy (Halton) timing:\n2.7 s \u00b1 26.2 ms per loop (mean \u00b1 std. dev. of 3 runs, 10 loops each)\n</pre> Out[29]: Method Sampler Mean Time (s) Std Dev (s) Speedup 0 QuantLib IIDStdUniform 0.602288 0.008245 - 1 QuantLib Sobol 0.703791 0.008493 - 2 QMCPy IIDStdUniform 0.412658 0.005421 1.459534 3 QMCPy Sobol 0.410019 0.005001 1.468928 4 QMCPy Lattice 0.554743 0.011150 1.085707 5 QMCPy Halton 2.699405 0.026199 0.223119 In\u00a0[30]: Copied! <pre># Output results to LaTeX\nresults_df = pd.merge(results_df, timing_df, on=['Method', 'Sampler'])\n# Format the results dataframe\nnumeric_cols = ['Mean', 'Std Dev', 'Mean Absolute Error', 'Std Dev Error', 'Mean Time (s)', 'Std Dev (s)', 'Speedup']\nresults_formatted = lu.format_results_dataframe(results_df, numeric_cols)\n# Generate and save LaTeX table\ntitle =  \"GBM Final Value Statistics and Performance Comparison\"\nlatex_table = lu.generate_latex_table(results_formatted, title, \"tab2\")\nwith open('outputs/gbm_comparison_table.tex', 'w') as f:\n    _ = f.write(latex_table)\n</pre> # Output results to LaTeX results_df = pd.merge(results_df, timing_df, on=['Method', 'Sampler']) # Format the results dataframe numeric_cols = ['Mean', 'Std Dev', 'Mean Absolute Error', 'Std Dev Error', 'Mean Time (s)', 'Std Dev (s)', 'Speedup'] results_formatted = lu.format_results_dataframe(results_df, numeric_cols) # Generate and save LaTeX table title =  \"GBM Final Value Statistics and Performance Comparison\" latex_table = lu.generate_latex_table(results_formatted, title, \"tab2\") with open('outputs/gbm_comparison_table.tex', 'w') as f:     _ = f.write(latex_table) <p>To compare how mean absolute error (MAE) differs between QMCPy and QuantLib samplers, we compute MAEs averaged across several independant replications. In QMCPy, samplers have a <code>replications</code> parameter, which specifies the number of independent randomizations of the underlying point set. This allows us to generate multiple independent sets of paths in a single call. QuantLib does not have a built-in <code>replications</code> parameter so we run path generation function multiple times with different seeds.</p> <p>For each replication, we compute the absolute error between the theoretical mean and the empirical mean of the simulated paths at the final time (maturity), and then average these errors to obtain the MAE values shown in the Mean Absolute Error Comparison subplot below.</p> In\u00a0[31]: Copied! <pre># Create comparison plot using sample data (2 subplots)\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n# Extract data for plotting\nsamplers, qmcpy_errors, qmcpy_times, quantlib_errors, quantlib_times, theoretical_mean = du.extract_comparison_data(results_df)\n# Create subplots \npu.plot_error_comparison(ax1, samplers, qmcpy_errors, quantlib_errors)\npu.plot_performance_comparison(ax2, samplers, qmcpy_times, quantlib_times)\nplt.tight_layout()\nplt.savefig('images/figure_6.png', bbox_inches='tight', dpi=150)\nplt.show();\n</pre> # Create comparison plot using sample data (2 subplots) fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6)) # Extract data for plotting samplers, qmcpy_errors, qmcpy_times, quantlib_errors, quantlib_times, theoretical_mean = du.extract_comparison_data(results_df) # Create subplots  pu.plot_error_comparison(ax1, samplers, qmcpy_errors, quantlib_errors) pu.plot_performance_comparison(ax2, samplers, qmcpy_times, quantlib_times) plt.tight_layout() plt.savefig('images/figure_6.png', bbox_inches='tight', dpi=150) plt.show(); <p>To explore how accuracy and runtime behave across different simulation settings, we perform a parameter sweep over a broader range of configurations.</p> <p>We consider four experiment configurations:</p> <ul> <li>MAE vs paths (with time steps fixed)</li> <li>MAE vs time steps (with number of paths fixed)</li> <li>Runtime vs paths (with time steps fixed)</li> <li>Runtime vs time steps (with number of paths fixed)</li> </ul> <p>For MAE comparisons here we use the same replication-based averaging described earlier. For runtime measurements we use <code>%timeit</code>, which performs multiple runs internally and reports averaged timing results.</p> In\u00a0[25]: Copied! <pre>%%capture\n# MAIN EXPERIMENT RUNNER\n\ndef run_single_configuration(series_name, n_steps, n_paths):\n    \"\"\"Run benchmarking for a single parameter configuration\"\"\"\n    print(f\"\\nTesting n_steps = {n_steps}, n_paths = {n_paths}\")\n    \n    # Calculate theoretical values\n    gbm_params = cf.get_gbm_parameters()\n    theoretical_mean, theoretical_std = calculate_theoretical_statistics(gbm_params)\n    \n    # Prepare benchmark parameters for QuantLib (with sigma)\n    ql_params = {**gbm_params, 'n_steps': n_steps, 'n_paths': n_paths}\n    \n    # Prepare benchmark parameters for QMCPy (with diffusion instead of sigma)\n    qp_params = {\n        'initial_value': gbm_params['initial_value'],\n        'mu': gbm_params['mu'],\n        'diffusion': gbm_params['sigma']**2,  # Convert sigma to diffusion\n        'maturity': gbm_params['maturity'],\n        'n_steps': n_steps,\n        'n_paths': n_paths\n    }\n    \n    # Run benchmarks\n    samplers = cf.get_sampler_configurations()\n    print(\"  Benchmarking QuantLib...\")\n    ql_timing = benchmark_quantlib_samplers(samplers['quantlib_samplers'], ql_params)\n    \n    print(\"  Benchmarking QMCPy...\")\n    qp_timing = benchmark_qmcpy_samplers(samplers['all_samplers'], qp_params)\n    \n    # Collect results for all samplers\n    results = []\n    du.add_theoretical_row(results, series_name, n_steps, n_paths, theoretical_mean, theoretical_std)\n    \n    for sampler in samplers['all_samplers']:\n        print(f\"    Processing {sampler}...\")\n        sampler_results = du.collect_library_results(\n            sampler, series_name, n_steps, n_paths,\n            ql_timing, qp_timing, theoretical_mean, theoretical_std\n        )\n        results.extend(sampler_results)\n    \n    return results\n\ndef run_time_steps_series():\n    \"\"\"Run experiments varying time steps with fixed paths\"\"\"\n    config = cf.get_experiment_configurations()['time_steps']\n    print(f\"\\nSERIES 1: Varying Time Steps (n_paths fixed at {config['fixed_paths']})\")\n    print(\"-\" * 60)\n    \n    all_results = []\n    for n_steps in config['range']:\n        results = run_single_configuration(\n            config['series_name'], n_steps, config['fixed_paths']\n        )\n        all_results.extend(results)\n    \n    return all_results\n\ndef run_paths_series():\n    \"\"\"Run experiments varying paths with fixed time steps\"\"\"\n    config = cf.get_experiment_configurations()['paths']\n    print(f\"\\nSERIES 2: Varying Number of Paths (n_steps fixed at {config['fixed_steps']})\")\n    print(\"-\" * 60)\n    \n    all_results = []\n    for n_paths in config['range']:\n        results = run_single_configuration(\n            config['series_name'], config['fixed_steps'], n_paths\n        )\n        all_results.extend(results)\n    \n    return all_results\n\ndef run_comprehensive_parameter_sweep():\n    \"\"\"\n    Run comprehensive parameter sweep experiments:\n    1. Vary time steps with fixed paths\n    2. Vary paths with fixed time steps\n    \"\"\"\n    all_results = []\n    all_results.extend(run_time_steps_series())\n    all_results.extend(run_paths_series())\n    \n    return pd.DataFrame(all_results)\n\n# MAIN EXECUTION\n\n# Run the comprehensive parameter sweep\nsweep_results_df = run_comprehensive_parameter_sweep()\n\n# Save results for later analysis\nsweep_results_df.to_csv('outputs/parameter_sweep_results.csv', index=False)\n\n# Display sample of results\nprint(\"\\nSample of results:\")\nsample_results = sweep_results_df[sweep_results_df['Method'] != 'Theoretical'].head(10)\nprint(sample_results[['Series', 'n_steps', 'n_paths', 'Method', 'Sampler', 'Runtime (s)', 'Mean Absolute Error']].round(10))\n</pre> %%capture # MAIN EXPERIMENT RUNNER  def run_single_configuration(series_name, n_steps, n_paths):     \"\"\"Run benchmarking for a single parameter configuration\"\"\"     print(f\"\\nTesting n_steps = {n_steps}, n_paths = {n_paths}\")          # Calculate theoretical values     gbm_params = cf.get_gbm_parameters()     theoretical_mean, theoretical_std = calculate_theoretical_statistics(gbm_params)          # Prepare benchmark parameters for QuantLib (with sigma)     ql_params = {**gbm_params, 'n_steps': n_steps, 'n_paths': n_paths}          # Prepare benchmark parameters for QMCPy (with diffusion instead of sigma)     qp_params = {         'initial_value': gbm_params['initial_value'],         'mu': gbm_params['mu'],         'diffusion': gbm_params['sigma']**2,  # Convert sigma to diffusion         'maturity': gbm_params['maturity'],         'n_steps': n_steps,         'n_paths': n_paths     }          # Run benchmarks     samplers = cf.get_sampler_configurations()     print(\"  Benchmarking QuantLib...\")     ql_timing = benchmark_quantlib_samplers(samplers['quantlib_samplers'], ql_params)          print(\"  Benchmarking QMCPy...\")     qp_timing = benchmark_qmcpy_samplers(samplers['all_samplers'], qp_params)          # Collect results for all samplers     results = []     du.add_theoretical_row(results, series_name, n_steps, n_paths, theoretical_mean, theoretical_std)          for sampler in samplers['all_samplers']:         print(f\"    Processing {sampler}...\")         sampler_results = du.collect_library_results(             sampler, series_name, n_steps, n_paths,             ql_timing, qp_timing, theoretical_mean, theoretical_std         )         results.extend(sampler_results)          return results  def run_time_steps_series():     \"\"\"Run experiments varying time steps with fixed paths\"\"\"     config = cf.get_experiment_configurations()['time_steps']     print(f\"\\nSERIES 1: Varying Time Steps (n_paths fixed at {config['fixed_paths']})\")     print(\"-\" * 60)          all_results = []     for n_steps in config['range']:         results = run_single_configuration(             config['series_name'], n_steps, config['fixed_paths']         )         all_results.extend(results)          return all_results  def run_paths_series():     \"\"\"Run experiments varying paths with fixed time steps\"\"\"     config = cf.get_experiment_configurations()['paths']     print(f\"\\nSERIES 2: Varying Number of Paths (n_steps fixed at {config['fixed_steps']})\")     print(\"-\" * 60)          all_results = []     for n_paths in config['range']:         results = run_single_configuration(             config['series_name'], config['fixed_steps'], n_paths         )         all_results.extend(results)          return all_results  def run_comprehensive_parameter_sweep():     \"\"\"     Run comprehensive parameter sweep experiments:     1. Vary time steps with fixed paths     2. Vary paths with fixed time steps     \"\"\"     all_results = []     all_results.extend(run_time_steps_series())     all_results.extend(run_paths_series())          return pd.DataFrame(all_results)  # MAIN EXECUTION  # Run the comprehensive parameter sweep sweep_results_df = run_comprehensive_parameter_sweep()  # Save results for later analysis sweep_results_df.to_csv('outputs/parameter_sweep_results.csv', index=False)  # Display sample of results print(\"\\nSample of results:\") sample_results = sweep_results_df[sweep_results_df['Method'] != 'Theoretical'].head(10) print(sample_results[['Series', 'n_steps', 'n_paths', 'Method', 'Sampler', 'Runtime (s)', 'Mean Absolute Error']].round(10))  In\u00a0[26]: Copied! <pre># Update values in the 'Mean Absolute Error' column with averaged MAE values across 3 replications\nreplications = 3\nnew_sweep_results_df = am.update_sweep_df(sweep_results_df, replications)\n\n# Save results \nnew_sweep_results_df.to_csv('outputs/new_parameter_sweep_results.csv', index=False)\n\n# Create visualizations\npu.create_parameter_sweep_plots(new_sweep_results_df, replications)\nplt.tight_layout()\nplt.savefig('images/figure_7.png', bbox_inches='tight', dpi=150)\nplt.show()\n</pre> # Update values in the 'Mean Absolute Error' column with averaged MAE values across 3 replications replications = 3 new_sweep_results_df = am.update_sweep_df(sweep_results_df, replications)  # Save results  new_sweep_results_df.to_csv('outputs/new_parameter_sweep_results.csv', index=False)  # Create visualizations pu.create_parameter_sweep_plots(new_sweep_results_df, replications) plt.tight_layout() plt.savefig('images/figure_7.png', bbox_inches='tight', dpi=150) plt.show() In\u00a0[27]: Copied! <pre>new_sweep_results_df\n</pre> new_sweep_results_df Out[27]: Series n_steps n_paths Method Sampler Mean Std Dev Mean Absolute Error Std Dev Error Runtime (s) Runtime Std (s) 0 Time Steps 16 4096 Theoretical - 105.127110 21.237439 0.000000 0.000000 0.000000 0.000000 1 Time Steps 16 4096 QuantLib IIDStdUniform 104.917012 20.944804 0.407318 0.292635 0.015165 0.000354 2 Time Steps 16 4096 QMCPy IIDStdUniform 116.220400 7.248263 0.239544 13.989176 0.001782 0.000089 3 Time Steps 16 4096 QuantLib Sobol 105.080437 21.122113 0.046672 0.115326 0.017942 0.000126 4 Time Steps 16 4096 QMCPy Sobol 116.940852 12.455526 0.001055 8.781913 0.002182 0.000010 ... ... ... ... ... ... ... ... ... ... ... ... 79 Paths 252 16384 QMCPy IIDStdUniform 105.968427 4.031370 0.113094 17.206069 0.286075 0.002492 80 Paths 252 16384 QuantLib Sobol 105.092907 21.051757 0.036463 0.185681 0.726662 0.005858 81 Paths 252 16384 QMCPy Sobol 110.041572 5.809188 0.000607 15.428251 0.292782 0.001719 82 Paths 252 16384 QMCPy Lattice 102.686316 6.154125 0.005287 15.083314 0.282264 0.001358 83 Paths 252 16384 QMCPy Halton 116.081983 21.559537 0.000925 0.322098 2.023128 0.000960 <p>84 rows \u00d7 11 columns</p> <p>QMCPy vs QuantLib: Both libraries produce statistically equivalent GBM simulations that match theoretical values. QMCPy typically runs 1.5 to 2 times faster due to vectorized operations, lazy loading, and optimized memory management. More importantly, it demonstrates superior numerical accuracy (lower mean absolute errors) with Sobol, lattice and Halton samplers, making it excellent for research and high-performance applications. QuantLib remains the industry standard for production systems requiring comprehensive derivatives support.</p> In\u00a0[28]: Copied! <pre># If you need different samplers, create efficiently as follows:\nbase_params = {'t_final': 1, 'initial_value': 100, 'drift': 0.05, 'diffusion': 0.2}\n\nsobol_gbm = qp.GeometricBrownianMotion(qp.Sobol(252), **base_params)\nlattice_gbm = qp.GeometricBrownianMotion(qp.Lattice(252), **base_params)\nhalton_gbm = qp.GeometricBrownianMotion(qp.Halton(252), **base_params)\n</pre> # If you need different samplers, create efficiently as follows: base_params = {'t_final': 1, 'initial_value': 100, 'drift': 0.05, 'diffusion': 0.2}  sobol_gbm = qp.GeometricBrownianMotion(qp.Sobol(252), **base_params) lattice_gbm = qp.GeometricBrownianMotion(qp.Lattice(252), **base_params) halton_gbm = qp.GeometricBrownianMotion(qp.Halton(252), **base_params)"},{"location":"demos/GBM/gbm_demo/#geometric-brownian-motion-demo","title":"Geometric Brownian Motion Demo\u00b6","text":""},{"location":"demos/GBM/gbm_demo/#gbm-objects-in-qmcpy","title":"GBM Objects in QMCPy\u00b6","text":""},{"location":"demos/GBM/gbm_demo/#log-normality-property","title":"Log-Normality Property\u00b6","text":"<p>At any time $t &gt; 0$, $S_t$ follows a log-normal distribution with expected value and variance as follows (see Section 3.2 in $[1]$):</p> <ul> <li>$E[S_t] = S_0 e^{\\mu t}$</li> <li>$\\text{Var}[S_t] = S_0^2 e^{2\\mu t}(e^{\\sigma^2 t} - 1)$</li> </ul> <p>The log-normal property is fundamental in financial modeling because it ensures asset prices remain strictly positive (as required in reality) while allowing for unlimited upside potential. This property makes GBM the cornerstone of the Black-Scholes model and many derivative pricing frameworks.</p> <p>Let's validate these theoretical properties by generating a large number of GBM samples and comparing the empirical moments with the theoretical values. Note that the theoretical values match the last values in <code>qp_gbm.mean_gbm</code> and <code>qp_gbm.covariance_gbm</code> for the final time point.</p>"},{"location":"demos/GBM/gbm_demo/#gmb-vs-brownian-motion-comparison","title":"GMB vs Brownian Motion Comparison\u00b6","text":""},{"location":"demos/GBM/gbm_demo/#gbm-vs-brownian-motion","title":"GBM vs Brownian Motion\u00b6","text":"<p>Paths of the driftless Brownian motion fluctuate symmetrically around the initial value (y = 1) and frequently cross zero into negative territory, while Geometric Brownian Motion paths remain strictly positive throughout their evolution.</p>"},{"location":"demos/GBM/gbm_demo/#gbm-using-low-discrepancy-lattice-sequence-distrubtion","title":"GBM Using Low-Discrepancy Lattice Sequence Distrubtion\u00b6","text":""},{"location":"demos/GBM/gbm_demo/#interactive-visualization","title":"Interactive Visualization\u00b6","text":"<p>The following code defines a set of sliders to control parameters for simulating paths of GBM. It sets the machine epsilon (eps) as the minimum value for <code>initial_value</code>,  <code>t_final</code>, and <code>diffusion</code>, ensuring they are always positive.  The function <code>plot_gbm_paths_with_distribution</code> then visualizes the GBM paths based on the specified parameters in the left subplot and fits a lognormal distribution to the histogram of the data values at the final time point in the right subplot.</p>"},{"location":"demos/GBM/gbm_demo/#quantlib-vs-qmcpy-comparison","title":"QuantLib vs QMCPy Comparison\u00b6","text":"<p>In this section, we compare QMCPy's GeometricBrownianMotion implementation with the industry-standard QuantLib library [6] to validate its accuracy and performance.</p>"},{"location":"demos/GBM/gbm_demo/#acknowledgments","title":"Acknowledgments\u00b6","text":"<p>The authors thank Joshua Jay Herman and Jiangrui Kang for their insightful feedback and help with the blog post and demo.</p>"},{"location":"demos/GBM/gbm_demo/#references","title":"References\u00b6","text":"<p>$[1]$ Glasserman, P. (2003). Monte Carlo Methods in Financial Engineering. Springer.</p> <p>$[2]$ Choi, S.-C. T., Hickernell, F. J., Jagadeeswaran, R., McCourt, M. J., and Sorokin, A. G. (2022). Quasi-Monte Carlo Software. In Alexander Keller, editor, Monte Carlo and Quasi-Monte Carlo Methods. Springer International Publishing.</p> <p>$[3]$ Choi, S.-C. T., Hickernell, F. J., Jagadeeswaran, R., McCourt, M., and Sorokin, A. (2020--2025). QMCPy: A quasi-Monte Carlo Python Library, Version 2.1. https://qmcpy.readthedocs.io/.</p> <p>$[4]$ Hull, J. C. (2017). Options, Futures, and Other Derivatives. Pearson, 10th edition.</p> <p>$[5]$ Ross, S. M. (2014). Introduction to Probability Models. Academic Press, 11th edition.</p> <p>$[6]$ The QuantLib contributors (2003--2025). QuantLib: A free/open-source library for quantitative finance, Version 1.38. https://www.quantlib.org.</p>"},{"location":"demos/GBM/gbm_demo/#appendix-covariance-matrix-derivation-for-geometric-brownian-motion","title":"Appendix: Covariance Matrix Derivation for Geometric Brownian Motion\u00b6","text":"<p>Here we derive the covariance matrix of $(S(t_1), \\ldots , S(t_n))$ for one-dimensional Geometric Brownian Motion defined as $S(t) = S_0 \\, e^{\\big(\\mu - \\frac{\\sigma^2}{2}\\big)  t + \\sigma W(t)}$, where $W(t)$ is a standard one-dimensional Brownian motion.</p> <ol> <li><p>Recall the definition of covariance: $$\\text{Cov}(S(t_i), S(t_j)) = E[S(t_i)S(t_j)] - E[S(t_i)]E[S(t_j)].$$</p> </li> <li><p>Calculate the product of expectations: The expected value of $S(t)$ is $E[S(t)] = S_0 e^{\\mu t}$. Therefore, the product of the expectations is: $$E[S(t_i)]E[S(t_j)] = (S_0 e^{\\mu t_i})(S_0 e^{\\mu t_j}) = S_0^2 e^{\\mu(t_i + t_j)}$$</p> </li> <li><p>Calculate the expectation of the product, $E[S(t_i)S(t_j)]$: $$\\begin{aligned} S(t_i)S(t_j) &amp;= S_0 e^{(\\mu - \\frac{\\sigma^2}{2})t_i + \\sigma W(t_i)} \\cdot S_0 e^{(\\mu - \\frac{\\sigma^2}{2})t_j + \\sigma W(t_j)} \\\\ &amp;= S_0^2 \\exp\\left( (\\mu - \\frac{\\sigma^2}{2})(t_i + t_j) + \\sigma \\left(W(t_i) + W(t_j) \\right) \\right) \\end{aligned}$$ The exponent is a normal random variable. Let's call it $Y$: $$Y = (\\mu - \\frac{\\sigma^2}{2})(t_i + t_j) + \\sigma\\left(W(t_i) + W(t_j)\\right)$$ To find the expectation of $e^Y$, we use the property that if $Y \\sim N(\\text{mean}, \\text{variance})$, then $E[e^Y] = e^{\\text{mean} + \\frac{1}{2}\\text{variance}}$.</p> <ul> <li><p>The mean of $Y$ is $E[Y] = E[(\\mu - \\frac{\\sigma^2}{2})(t_i + t_j) + \\sigma(W(t_i) + W(t_j))] = (\\mu - \\frac{\\sigma^2}{2})(t_i + t_j).$</p> </li> <li><p>The variance of $Y$ is $$\\begin{aligned}   \\text{Var}(Y) &amp;= \\text{Var}[(\\mu - \\frac{\\sigma^2}{2})(t_i + t_j) + \\sigma(W(t_i) + W(t_j))] \\\\   &amp;= \\text{Var}[\\sigma(W(t_i) + W(t_j))] \\\\   &amp;= \\sigma^2 \\text{Var}(W(t_i) + W(t_j)) \\\\   &amp;= \\sigma^2(\\text{Var}(W(t_i)) + \\text{Var}(W(t_j)) + 2\\text{Cov}(W(t_i), W(t_j))) \\\\   &amp;= \\sigma^2(t_i + t_j + 2\\min(t_i, t_j))    \\end{aligned}$$ Now we can compute $E[S(t_i)S(t_j)] = S_0^2 E[e^Y]$: $$\\begin{aligned} E[S(t_i)S(t_j)] &amp;= S_0^2 \\exp\\left( E[Y] + \\frac{1}{2}\\text{Var}(Y) \\right) \\\\ &amp;= S_0^2 \\exp\\left( (\\mu - \\frac{\\sigma^2}{2})(t_i + t_j) + \\frac{1}{2}\\sigma^2 \\left(t_i + t_j + 2\\min(t_i, t_j)\\right) \\right) \\end{aligned}$$ Simplifying the exponent: $$\\begin{aligned} &amp;\\mu(t_i + t_j) - \\frac{\\sigma^2}{2}(t_i+t_j) + \\frac{\\sigma^2}{2}(t_i+t_j) + \\sigma^2\\min(t_i,t_j) \\\\ &amp;= \\mu(t_i + t_j) + \\sigma^2\\min(t_i,t_j) \\end{aligned}$$ So, the final expression for the expectation of the product is: $$E[S(t_i)S(t_j)] = S_0^2 e^{\\mu(t_i + t_j) + \\sigma^2\\min(t_i, t_j)}$$</p> </li> </ul> </li> <li><p>Combine the terms to get the covariance: $$\\begin{aligned} \\text{Cov}(S(t_i), S(t_j)) &amp;= S_0^2 e^{\\mu(t_i + t_j) + \\sigma^2\\min(t_i, t_j)} - S_0^2 e^{\\mu(t_i + t_j)} \\\\ &amp;= S_0^2 e^{\\mu(t_i + t_j)} \\left(e^{\\sigma^2 \\min(t_i, t_j)} - 1\\right). \\end{aligned}$$</p> </li> </ol>"},{"location":"demos/GBM/gbm_examples/","title":"GBM Examples","text":"In\u00a0[1]: Copied! <pre>import sys\nfrom pathlib import Path\n\n# Add the current directory to Python path to import local modules\nnotebook_dir = Path().resolve()\nif str(notebook_dir) not in sys.path:\n    sys.path.insert(0, str(notebook_dir))\n</pre> import sys from pathlib import Path  # Add the current directory to Python path to import local modules notebook_dir = Path().resolve() if str(notebook_dir) not in sys.path:     sys.path.insert(0, str(notebook_dir)) In\u00a0[2]: Copied! <pre>import matplotlib.pyplot as plt\nfrom matplotlib.ticker import FixedLocator, FixedFormatter\n\nimport config as cf\nimport averaged_mae as am\nimport plot_util as pu\n</pre> import matplotlib.pyplot as plt from matplotlib.ticker import FixedLocator, FixedFormatter  import config as cf import averaged_mae as am import plot_util as pu In\u00a0[3]: Copied! <pre>cf.is_debug = False\n</pre> cf.is_debug = False <p>Standardize figure size across the notebook (width, height) in inches:</p> In\u00a0[4]: Copied! <pre>FIGSIZE = (6, 4)\nplt.rcParams['figure.figsize'] = FIGSIZE\n</pre> FIGSIZE = (6, 4) plt.rcParams['figure.figsize'] = FIGSIZE In\u00a0[5]: Copied! <pre>replications = 1\nam.plot_mae_vs_paths(replications)\n</pre> replications = 1 am.plot_mae_vs_paths(replications) In\u00a0[6]: Copied! <pre>replications = 1\nam.plot_mae_vs_paths(replications)\n</pre> replications = 1 am.plot_mae_vs_paths(replications) In\u00a0[7]: Copied! <pre>replications = 4\nam.plot_mae_vs_paths(replications)\n</pre> replications = 4 am.plot_mae_vs_paths(replications) In\u00a0[8]: Copied! <pre>replications = 1\nam.plot_mae_vs_steps(replications)\n</pre> replications = 1 am.plot_mae_vs_steps(replications) In\u00a0[9]: Copied! <pre>replications = 1\nam.plot_mae_vs_steps(replications)\n</pre> replications = 1 am.plot_mae_vs_steps(replications) In\u00a0[10]: Copied! <pre>replications = 4\nam.plot_mae_vs_steps(replications)\n</pre> replications = 4 am.plot_mae_vs_steps(replications)"},{"location":"demos/GBM/gbm_examples/#gbm-examples-notebook","title":"GBM Examples Notebook\u00b6","text":"<p>This notebook demonstrates MAE plots and comparisons for GBM samplers.</p>"},{"location":"demos/GBM/gbm_examples/#configuration","title":"Configuration\u00b6","text":"<p>Toggle debugging for the demo.</p>"},{"location":"demos/GBM/gbm_examples/#mae-vs-number-of-paths-in-qmcpy-and-quantlib","title":"MAE vs Number of Paths in QMCPy and QuantLib\u00b6","text":""},{"location":"demos/GBM/gbm_examples/#mae-vs-number-of-time-steps-in-qmcpy-and-quantlib","title":"MAE vs Number of Time Steps in QMCPy and QuantLib\u00b6","text":""},{"location":"demos/GBM/latex_util/","title":"Latex util","text":"In\u00a0[\u00a0]: Copied! <pre>import pandas as pd\n</pre> import pandas as pd In\u00a0[\u00a0]: Copied! <pre>def format_number(x: float) -&gt; str:\n    \"\"\"Custom formatting function to avoid unnecessary trailing zeros\"\"\"\n    if pd.isna(x) or x == \"-\":\n        return \"-\"\n    try:\n        num = float(x)\n        formatted = f\"{num:.8f}\"\n        if \".\" not in formatted and abs(num) &lt; 1 and num != 0:\n            formatted += \".0\"\n        return formatted\n    except (ValueError, TypeError):\n        return str(x)\n</pre> def format_number(x: float) -&gt; str:     \"\"\"Custom formatting function to avoid unnecessary trailing zeros\"\"\"     if pd.isna(x) or x == \"-\":         return \"-\"     try:         num = float(x)         formatted = f\"{num:.8f}\"         if \".\" not in formatted and abs(num) &lt; 1 and num != 0:             formatted += \".0\"         return formatted     except (ValueError, TypeError):         return str(x) In\u00a0[\u00a0]: Copied! <pre>def format_results_dataframe(df: pd.DataFrame, numeric_columns: list) -&gt; pd.DataFrame:\n    \"\"\"Apply custom formatting to numeric columns in results dataframe\"\"\"\n    results_formatted = df.copy()\n    for col in numeric_columns:\n        if col in results_formatted.columns:\n            results_formatted[col] = results_formatted[col].apply(format_number)\n    return results_formatted\n</pre> def format_results_dataframe(df: pd.DataFrame, numeric_columns: list) -&gt; pd.DataFrame:     \"\"\"Apply custom formatting to numeric columns in results dataframe\"\"\"     results_formatted = df.copy()     for col in numeric_columns:         if col in results_formatted.columns:             results_formatted[col] = results_formatted[col].apply(format_number)     return results_formatted In\u00a0[\u00a0]: Copied! <pre>def generate_latex_table(df: pd.DataFrame, caption: str, label: str) -&gt; str:\n    \"\"\"Generate LaTeX table with booktabs formatting\"\"\"\n    # Create custom header\n    header = (\n        \"Method &amp; Sampler &amp; Mean &amp; Std Dev &amp; Mean Absolute &amp; Std Dev  &amp; \"\n        \"Mean Time (s)  &amp; Std Dev (s) &amp; Speedup \\\\\\\\\\n &amp;  &amp;  &amp;   &amp;  \"\n        \"Error &amp;  Error &amp;   &amp; &amp;  \\\\\\\\\"\n    )\n\n    latex_table = df.style.hide(axis=\"index\").to_latex(\n        caption=caption,\n        label=label,\n        position=\"tbp\",\n        hrules=True,\n        column_format=(\n            \"ll@{\\hspace{0.4em}}r@{\\hspace{0.4em}}r@{\\hspace{0.4em}}\"\n            \"r@{\\hspace{0.4em}}r@{\\hspace{0.4em}}r@{\\hspace{0.4em}}\"\n            \"r@{\\hspace{0.4em}}r\"\n        ),\n    )\n    # Replace default LaTeX table environment with booktabs format\n    latex_table = latex_table.replace(\n        \"\\\\begin{table}[H]\", \"\\\\begin{table}[btp]\\\\centering\"\n    )\n    latex_table = latex_table.replace(\"\\\\hline\", \"\\\\toprule\", 1)\n    latex_table = latex_table.replace(\"\\\\hline\", \"\\\\midrule\", 1)\n    latex_table = latex_table.replace(\"\\\\hline\", \"\\\\bottomrule\")\n\n    # Replace the generated header with custom header\n    lines = latex_table.split(\"\\n\")\n    for i, line in enumerate(lines):\n        if \"\\\\toprule\" in line and i + 1 &lt; len(lines):\n            # Replace the line after \\toprule with custom header\n            lines[i + 1] = header\n            break\n\n    return \"\\n\".join(lines)\n</pre> def generate_latex_table(df: pd.DataFrame, caption: str, label: str) -&gt; str:     \"\"\"Generate LaTeX table with booktabs formatting\"\"\"     # Create custom header     header = (         \"Method &amp; Sampler &amp; Mean &amp; Std Dev &amp; Mean Absolute &amp; Std Dev  &amp; \"         \"Mean Time (s)  &amp; Std Dev (s) &amp; Speedup \\\\\\\\\\n &amp;  &amp;  &amp;   &amp;  \"         \"Error &amp;  Error &amp;   &amp; &amp;  \\\\\\\\\"     )      latex_table = df.style.hide(axis=\"index\").to_latex(         caption=caption,         label=label,         position=\"tbp\",         hrules=True,         column_format=(             \"ll@{\\hspace{0.4em}}r@{\\hspace{0.4em}}r@{\\hspace{0.4em}}\"             \"r@{\\hspace{0.4em}}r@{\\hspace{0.4em}}r@{\\hspace{0.4em}}\"             \"r@{\\hspace{0.4em}}r\"         ),     )     # Replace default LaTeX table environment with booktabs format     latex_table = latex_table.replace(         \"\\\\begin{table}[H]\", \"\\\\begin{table}[btp]\\\\centering\"     )     latex_table = latex_table.replace(\"\\\\hline\", \"\\\\toprule\", 1)     latex_table = latex_table.replace(\"\\\\hline\", \"\\\\midrule\", 1)     latex_table = latex_table.replace(\"\\\\hline\", \"\\\\bottomrule\")      # Replace the generated header with custom header     lines = latex_table.split(\"\\n\")     for i, line in enumerate(lines):         if \"\\\\toprule\" in line and i + 1 &lt; len(lines):             # Replace the line after \\toprule with custom header             lines[i + 1] = header             break      return \"\\n\".join(lines)"},{"location":"demos/GBM/plot_util/","title":"Plot util","text":"In\u00a0[\u00a0]: Copied! <pre>import numpy as np\nimport numpy.typing as npt\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom matplotlib.axes import Axes\nfrom matplotlib.ticker import FixedLocator, FixedFormatter\nfrom typing import Optional\nimport os\nimport scipy.stats as sc\nimport qmcpy as qp\n</pre> import numpy as np import numpy.typing as npt import pandas as pd import matplotlib.pyplot as plt from matplotlib.axes import Axes from matplotlib.ticker import FixedLocator, FixedFormatter from typing import Optional import os import scipy.stats as sc import qmcpy as qp In\u00a0[\u00a0]: Copied! <pre>def plot_error_comparison(\n    ax: Axes,\n    samplers: list,\n    qmcpy_errors: npt.NDArray[np.floating],\n    quantlib_errors: list,\n    replications: Optional[int] = None,\n) -&gt; None:\n    \"\"\"\n    Plot error comparison subplot.\n\n    Args:\n        ax: Matplotlib axis object\n        samplers: List of sampler names\n        qmcpy_errors: Array of QMCPy mean absolute errors\n        quantlib_errors: List of QuantLib mean absolute errors (may contain None)\n        replications: Number of replications used for averaging (optional, for title)\n    \"\"\"\n    x = np.arange(len(samplers))\n    width = 0.35\n    # Plot QuantLib data first (left side)\n    ql_x, ql_errors = [], []\n    for i, error in enumerate(quantlib_errors):\n        if error is not None:\n            ql_x.append(i)\n            ql_errors.append(error)\n    if ql_errors:\n        ax.bar(\n            [x - width / 2 for x in ql_x],\n            ql_errors,\n            width,\n            label=\"QuantLib\",\n            color=\"blue\",\n            alpha=0.8,\n        )\n    # Plot QMCPy data second (right side)\n    ax.bar(x + width / 2, qmcpy_errors, width, label=\"QMCPy\", color=\"red\", alpha=0.8)\n\n    ax.set_xlabel(\"Sampler Type\")\n    ax.set_ylabel(\"Mean Absolute Error (log scale)\")\n\n    # Add replications info to title if provided\n    if replications is not None:\n        ax.set_title(\n            f\"Mean Absolute Error Comparison\\n(averaged over {replications} replications)\",\n            fontsize=16,\n            fontweight=\"bold\",\n        )\n    else:\n        ax.set_title(\"Mean Absolute Error Comparison\", fontsize=16, fontweight=\"bold\")\n\n    ax.set_yscale(\"log\")\n    ax.set_xticks(x)\n    ax.set_xticklabels(samplers, rotation=45, ha=\"right\")\n    ax.legend()\n    ax.grid(True, alpha=0.3)\n</pre> def plot_error_comparison(     ax: Axes,     samplers: list,     qmcpy_errors: npt.NDArray[np.floating],     quantlib_errors: list,     replications: Optional[int] = None, ) -&gt; None:     \"\"\"     Plot error comparison subplot.      Args:         ax: Matplotlib axis object         samplers: List of sampler names         qmcpy_errors: Array of QMCPy mean absolute errors         quantlib_errors: List of QuantLib mean absolute errors (may contain None)         replications: Number of replications used for averaging (optional, for title)     \"\"\"     x = np.arange(len(samplers))     width = 0.35     # Plot QuantLib data first (left side)     ql_x, ql_errors = [], []     for i, error in enumerate(quantlib_errors):         if error is not None:             ql_x.append(i)             ql_errors.append(error)     if ql_errors:         ax.bar(             [x - width / 2 for x in ql_x],             ql_errors,             width,             label=\"QuantLib\",             color=\"blue\",             alpha=0.8,         )     # Plot QMCPy data second (right side)     ax.bar(x + width / 2, qmcpy_errors, width, label=\"QMCPy\", color=\"red\", alpha=0.8)      ax.set_xlabel(\"Sampler Type\")     ax.set_ylabel(\"Mean Absolute Error (log scale)\")      # Add replications info to title if provided     if replications is not None:         ax.set_title(             f\"Mean Absolute Error Comparison\\n(averaged over {replications} replications)\",             fontsize=16,             fontweight=\"bold\",         )     else:         ax.set_title(\"Mean Absolute Error Comparison\", fontsize=16, fontweight=\"bold\")      ax.set_yscale(\"log\")     ax.set_xticks(x)     ax.set_xticklabels(samplers, rotation=45, ha=\"right\")     ax.legend()     ax.grid(True, alpha=0.3) In\u00a0[\u00a0]: Copied! <pre>def plot_performance_comparison(\n    ax: Axes,\n    samplers: list,\n    qmcpy_times: Optional[npt.NDArray[np.floating]],\n    quantlib_times: list,\n) -&gt; None:\n    \"\"\"Plot performance comparison subplot\"\"\"\n    x = np.arange(len(samplers))\n    width = 0.35\n    if qmcpy_times is not None:\n        # Plot QuantLib timing data first (left side)\n        ql_x, ql_times = [], []\n        for i, time in enumerate(quantlib_times):\n            if time is not None:\n                ql_x.append(i)\n                ql_times.append(time)\n        if ql_times:\n            ax.bar(\n                [x - width / 2 for x in ql_x],\n                ql_times,\n                width,\n                label=\"QuantLib\",\n                color=\"blue\",\n                alpha=0.8,\n            )\n        # Plot QMCPy data second (right side)\n        ax.bar(x + width / 2, qmcpy_times, width, label=\"QMCPy\", color=\"red\", alpha=0.8)\n        # Add speedup annotations where QuantLib data is available, at center of QMCPy bars\n        if len(ql_times) &gt; 0:\n            for i, (qmc_time, ql_time) in enumerate(zip(qmcpy_times, quantlib_times)):\n                if ql_time is not None:\n                    speedup = ql_time / qmc_time\n                    annotation_height = qmc_time + max(qmcpy_times) * 0.3\n                    # Position arrow at center of QMCPy bar (i + width/2)\n                    ax.annotate(\n                        f\"{speedup:.1f}x faster\",\n                        xy=(i + width / 2, qmc_time),\n                        xytext=(i + width / 2, annotation_height),\n                        ha=\"center\",\n                        va=\"bottom\",\n                        fontsize=9,\n                        fontweight=\"bold\",\n                        arrowprops=dict(arrowstyle=\"-&gt;\", color=\"blue\", lw=1),\n                    )\n        ax.set_xlabel(\"Sampler Type\")\n        ax.set_ylabel(\"Execution Time (s)\")\n        ax.set_xticks(x)\n        ax.set_xticklabels(samplers, rotation=45, ha=\"right\")\n        ax.legend()\n        ax.grid(True, alpha=0.3)\n    else:\n        ax.text(\n            0.5,\n            0.5,\n            \"Timing data not available\\nRun previous cells to generate data\",\n            ha=\"center\",\n            va=\"center\",\n            transform=ax.transAxes,\n            fontsize=12,\n        )\n\n    ax.set_title(\"Performance Comparison\", fontsize=16, fontweight=\"bold\")\n</pre> def plot_performance_comparison(     ax: Axes,     samplers: list,     qmcpy_times: Optional[npt.NDArray[np.floating]],     quantlib_times: list, ) -&gt; None:     \"\"\"Plot performance comparison subplot\"\"\"     x = np.arange(len(samplers))     width = 0.35     if qmcpy_times is not None:         # Plot QuantLib timing data first (left side)         ql_x, ql_times = [], []         for i, time in enumerate(quantlib_times):             if time is not None:                 ql_x.append(i)                 ql_times.append(time)         if ql_times:             ax.bar(                 [x - width / 2 for x in ql_x],                 ql_times,                 width,                 label=\"QuantLib\",                 color=\"blue\",                 alpha=0.8,             )         # Plot QMCPy data second (right side)         ax.bar(x + width / 2, qmcpy_times, width, label=\"QMCPy\", color=\"red\", alpha=0.8)         # Add speedup annotations where QuantLib data is available, at center of QMCPy bars         if len(ql_times) &gt; 0:             for i, (qmc_time, ql_time) in enumerate(zip(qmcpy_times, quantlib_times)):                 if ql_time is not None:                     speedup = ql_time / qmc_time                     annotation_height = qmc_time + max(qmcpy_times) * 0.3                     # Position arrow at center of QMCPy bar (i + width/2)                     ax.annotate(                         f\"{speedup:.1f}x faster\",                         xy=(i + width / 2, qmc_time),                         xytext=(i + width / 2, annotation_height),                         ha=\"center\",                         va=\"bottom\",                         fontsize=9,                         fontweight=\"bold\",                         arrowprops=dict(arrowstyle=\"-&gt;\", color=\"blue\", lw=1),                     )         ax.set_xlabel(\"Sampler Type\")         ax.set_ylabel(\"Execution Time (s)\")         ax.set_xticks(x)         ax.set_xticklabels(samplers, rotation=45, ha=\"right\")         ax.legend()         ax.grid(True, alpha=0.3)     else:         ax.text(             0.5,             0.5,             \"Timing data not available\\nRun previous cells to generate data\",             ha=\"center\",             va=\"center\",             transform=ax.transAxes,             fontsize=12,         )      ax.set_title(\"Performance Comparison\", fontsize=16, fontweight=\"bold\") In\u00a0[\u00a0]: Copied! <pre>def get_plot_styling() -&gt; dict:\n    \"\"\"Define colors and markers for plotting\"\"\"\n    return {\n        \"colors\": {\n            \"QuantLib\": {\"IIDStdUniform\": \"#1f77b4\", \"Sobol\": \"#ff7f0e\"},\n            \"QMCPy\": {\n                \"IIDStdUniform\": \"#2ca02c\",\n                \"Sobol\": \"#d62728\",\n                \"Lattice\": \"#9467bd\",\n                \"Halton\": \"#8c564b\",\n            },\n        },\n        \"markers\": {\n            \"QuantLib\": {\"IIDStdUniform\": \"o\", \"Sobol\": \"s\"},\n            \"QMCPy\": {\n                \"IIDStdUniform\": \"^\",\n                \"Sobol\": \"v\",\n                \"Lattice\": \"D\",\n                \"Halton\": \"p\",\n            },\n        },\n    }\n</pre> def get_plot_styling() -&gt; dict:     \"\"\"Define colors and markers for plotting\"\"\"     return {         \"colors\": {             \"QuantLib\": {\"IIDStdUniform\": \"#1f77b4\", \"Sobol\": \"#ff7f0e\"},             \"QMCPy\": {                 \"IIDStdUniform\": \"#2ca02c\",                 \"Sobol\": \"#d62728\",                 \"Lattice\": \"#9467bd\",                 \"Halton\": \"#8c564b\",             },         },         \"markers\": {             \"QuantLib\": {\"IIDStdUniform\": \"o\", \"Sobol\": \"s\"},             \"QMCPy\": {                 \"IIDStdUniform\": \"^\",                 \"Sobol\": \"v\",                 \"Lattice\": \"D\",                 \"Halton\": \"p\",             },         },     } In\u00a0[\u00a0]: Copied! <pre>def plot_single_series(\n    ax: Axes,\n    plot_data: pd.DataFrame,\n    series_name: str,\n    x_col: str,\n    y_col: str,\n    title: str,\n    xlabel: str,\n    ylabel: str,\n    log_scale: bool = False,\n    is_legend: bool = False,\n) -&gt; None:\n    \"\"\"Plot a single series (runtime or error) for one experimental series\"\"\"\n    series_data = plot_data[plot_data[\"Series\"] == series_name]\n    styling = get_plot_styling()\n\n    # Collect all unique x values from the experiments\n    all_x_values = sorted(series_data[x_col].unique())\n\n    for method in [\"QuantLib\", \"QMCPy\"]:\n        method_data = series_data[series_data[\"Method\"] == method]\n        colors = styling[\"colors\"][method]\n        markers = styling[\"markers\"][method]\n\n        # Cache unique samplers to avoid recomputation\n        unique_samplers = method_data[\"Sampler\"].unique()\n\n        for sampler in unique_samplers:\n            sampler_data = method_data[method_data[\"Sampler\"] == sampler].sort_values(\n                x_col\n            )\n\n            if len(sampler_data) &gt; 0:\n                x_vals = sampler_data[x_col].values\n                y_vals = sampler_data[y_col].values\n\n                color = colors.get(sampler, \"#000000\")\n                marker = markers.get(sampler, \"o\")\n\n                # Plot with connecting lines for trend visualization\n                if log_scale:\n                    ax.loglog(\n                        x_vals,\n                        y_vals,\n                        marker=marker,\n                        color=color,\n                        linewidth=2,\n                        markersize=8,\n                        label=f\"{method} - {sampler}\",\n                    )\n                else:\n                    ax.semilogy(\n                        x_vals,\n                        y_vals,\n                        marker=marker,\n                        color=color,\n                        linewidth=2,\n                        markersize=8,\n                        label=f\"{method} - {sampler}\",\n                    )\n\n    # Set x-axis ticks to show only exact experimental values\n    # Pre-compute tick labels once to avoid redundant string conversions\n    tick_labels = [str(int(x)) for x in all_x_values]\n    ax.set_xticks(all_x_values)\n    ax.set_xticklabels(tick_labels)\n\n    # Disable minor ticks to prevent intermediate values from showing\n    ax.tick_params(axis=\"x\", which=\"minor\", bottom=False)\n\n    # For log plots, we need to explicitly control the x-axis formatter\n    if log_scale:\n        ax.xaxis.set_major_locator(FixedLocator(all_x_values))\n        ax.xaxis.set_major_formatter(FixedFormatter(tick_labels))\n        ax.xaxis.set_minor_locator(FixedLocator([]))  # Remove minor ticks\n\n    ax.set_xlabel(xlabel, fontsize=12, fontweight=\"bold\")\n    ax.set_ylabel(ylabel, fontsize=12, fontweight=\"bold\")\n    ax.set_title(title, fontsize=14, fontweight=\"bold\")\n    ax.grid(True, alpha=0.3)\n    if is_legend:\n        ax.legend(fontsize=10)\n</pre> def plot_single_series(     ax: Axes,     plot_data: pd.DataFrame,     series_name: str,     x_col: str,     y_col: str,     title: str,     xlabel: str,     ylabel: str,     log_scale: bool = False,     is_legend: bool = False, ) -&gt; None:     \"\"\"Plot a single series (runtime or error) for one experimental series\"\"\"     series_data = plot_data[plot_data[\"Series\"] == series_name]     styling = get_plot_styling()      # Collect all unique x values from the experiments     all_x_values = sorted(series_data[x_col].unique())      for method in [\"QuantLib\", \"QMCPy\"]:         method_data = series_data[series_data[\"Method\"] == method]         colors = styling[\"colors\"][method]         markers = styling[\"markers\"][method]          # Cache unique samplers to avoid recomputation         unique_samplers = method_data[\"Sampler\"].unique()          for sampler in unique_samplers:             sampler_data = method_data[method_data[\"Sampler\"] == sampler].sort_values(                 x_col             )              if len(sampler_data) &gt; 0:                 x_vals = sampler_data[x_col].values                 y_vals = sampler_data[y_col].values                  color = colors.get(sampler, \"#000000\")                 marker = markers.get(sampler, \"o\")                  # Plot with connecting lines for trend visualization                 if log_scale:                     ax.loglog(                         x_vals,                         y_vals,                         marker=marker,                         color=color,                         linewidth=2,                         markersize=8,                         label=f\"{method} - {sampler}\",                     )                 else:                     ax.semilogy(                         x_vals,                         y_vals,                         marker=marker,                         color=color,                         linewidth=2,                         markersize=8,                         label=f\"{method} - {sampler}\",                     )      # Set x-axis ticks to show only exact experimental values     # Pre-compute tick labels once to avoid redundant string conversions     tick_labels = [str(int(x)) for x in all_x_values]     ax.set_xticks(all_x_values)     ax.set_xticklabels(tick_labels)      # Disable minor ticks to prevent intermediate values from showing     ax.tick_params(axis=\"x\", which=\"minor\", bottom=False)      # For log plots, we need to explicitly control the x-axis formatter     if log_scale:         ax.xaxis.set_major_locator(FixedLocator(all_x_values))         ax.xaxis.set_major_formatter(FixedFormatter(tick_labels))         ax.xaxis.set_minor_locator(FixedLocator([]))  # Remove minor ticks      ax.set_xlabel(xlabel, fontsize=12, fontweight=\"bold\")     ax.set_ylabel(ylabel, fontsize=12, fontweight=\"bold\")     ax.set_title(title, fontsize=14, fontweight=\"bold\")     ax.grid(True, alpha=0.3)     if is_legend:         ax.legend(fontsize=10) In\u00a0[\u00a0]: Copied! <pre>def create_parameter_sweep_plots(df: pd.DataFrame, replications: int) -&gt; None:\n    \"\"\"Create 4-panel plots from parameter sweep data\"\"\"\n    # Filter out theoretical data\n    plot_data = df[df[\"Method\"] != \"Theoretical\"].copy()\n\n    # Create figure with 2x2 subplots\n    _, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))\n    # Panel 1: Mean Absolute Error vs n_steps (upper left)\n    plot_single_series(\n        ax1,\n        plot_data,\n        \"Time Steps\",\n        \"n_steps\",\n        \"Mean Absolute Error\",\n        f\"Mean Absolute Error vs Number of Time Steps across {replications} Replications\\n(n_paths = 4,096)\",\n        \"Number of Time Steps\",\n        \"Mean Absolute Error\",\n        log_scale=True,\n    )\n\n    # Panel 2: Runtime vs n_steps (upper right)\n    plot_single_series(\n        ax2,\n        plot_data,\n        \"Time Steps\",\n        \"n_steps\",\n        \"Runtime (s)\",\n        f\"Runtime vs Number of Time Steps\\n(n_paths = 4,096)\",\n        \"Number of Time Steps\",\n        \"Runtime (seconds)\",\n        log_scale=True,\n        is_legend=True,\n    )\n\n    # Panel 3: Mean Absolute Error vs n_paths (lower left)\n    plot_single_series(\n        ax3,\n        plot_data,\n        \"Paths\",\n        \"n_paths\",\n        \"Mean Absolute Error\",\n        f\"Mean Absolute Error vs Number of Paths across {replications} Replications\\n(n_steps = 252)\",\n        \"Number of Paths\",\n        \"Mean Absolute Error\",\n        log_scale=True,\n    )\n\n    # Panel 4: Runtime vs n_paths (lower right)\n    plot_single_series(\n        ax4,\n        plot_data,\n        \"Paths\",\n        \"n_paths\",\n        \"Runtime (s)\",\n        f\"Runtime vs Number of Paths\\n(n_steps = 252)\",\n        \"Number of Paths\",\n        \"Runtime (seconds)\",\n        log_scale=True,\n    )\n</pre> def create_parameter_sweep_plots(df: pd.DataFrame, replications: int) -&gt; None:     \"\"\"Create 4-panel plots from parameter sweep data\"\"\"     # Filter out theoretical data     plot_data = df[df[\"Method\"] != \"Theoretical\"].copy()      # Create figure with 2x2 subplots     _, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))     # Panel 1: Mean Absolute Error vs n_steps (upper left)     plot_single_series(         ax1,         plot_data,         \"Time Steps\",         \"n_steps\",         \"Mean Absolute Error\",         f\"Mean Absolute Error vs Number of Time Steps across {replications} Replications\\n(n_paths = 4,096)\",         \"Number of Time Steps\",         \"Mean Absolute Error\",         log_scale=True,     )      # Panel 2: Runtime vs n_steps (upper right)     plot_single_series(         ax2,         plot_data,         \"Time Steps\",         \"n_steps\",         \"Runtime (s)\",         f\"Runtime vs Number of Time Steps\\n(n_paths = 4,096)\",         \"Number of Time Steps\",         \"Runtime (seconds)\",         log_scale=True,         is_legend=True,     )      # Panel 3: Mean Absolute Error vs n_paths (lower left)     plot_single_series(         ax3,         plot_data,         \"Paths\",         \"n_paths\",         \"Mean Absolute Error\",         f\"Mean Absolute Error vs Number of Paths across {replications} Replications\\n(n_steps = 252)\",         \"Number of Paths\",         \"Mean Absolute Error\",         log_scale=True,     )      # Panel 4: Runtime vs n_paths (lower right)     plot_single_series(         ax4,         plot_data,         \"Paths\",         \"n_paths\",         \"Runtime (s)\",         f\"Runtime vs Number of Paths\\n(n_steps = 252)\",         \"Number of Paths\",         \"Runtime (seconds)\",         log_scale=True,     ) In\u00a0[\u00a0]: Copied! <pre>def plot_paths(\n    motion_type: str,\n    sampler,\n    t_final: float,\n    initial_value: float,\n    drift: float,\n    diffusion: float,\n    n: int,\n    png_filename: Optional[str] = None,\n):\n    \"\"\"\n    Plot realizations of Brownian Motion or Geometric Brownian Motion.\n\n    Args:\n        motion_type: 'BM' for Brownian Motion or 'GBM' for Geometric Brownian Motion\n        sampler: QMCPy sampler instance\n        t_final: Final time point\n        initial_value: Initial value S(0)\n        drift: Drift coefficient (mu)\n        diffusion: Diffusion coefficient (sigma^2 for GBM, sigma for BM)\n        n: Number of paths to generate\n        png_filename: Optional filename to save plot (saved to images/ directory)\n\n    Returns:\n        Motion object used for generation\n    \"\"\"\n    if motion_type.upper() == \"BM\":\n        motion = qp.BrownianMotion(sampler, t_final, initial_value, drift, diffusion)\n        title = (\n            f\"Realizations of Brownian Motion using \" f\"{type(sampler).__name__} points\"\n        )\n        ylabel = \"$W(t)$\"\n    elif motion_type.upper() == \"GBM\":\n        motion = qp.GeometricBrownianMotion(\n            sampler, t_final, initial_value, drift, diffusion\n        )\n        title = (\n            f\"Realizations of Geometric Brownian Motion using \"\n            f\"{type(sampler).__name__} points\"\n        )\n        ylabel = \"$S(t)$\"\n    else:\n        raise ValueError(\"motion_type must be 'BM' or 'GBM'\")\n\n    t = motion.gen_samples(n)\n    initial_values = np.full((n, 1), motion.initial_value)\n    t_w_init = np.hstack((initial_values, t))\n    tvec_w_0 = np.hstack(([0], motion.time_vec))\n\n    plt.figure(figsize=(7, 4))\n    plt.plot(tvec_w_0, t_w_init.T)\n    plt.title(title)\n    plt.xlabel(\"$t$\")\n    plt.ylabel(ylabel)\n    plt.xlim([tvec_w_0[0], tvec_w_0[-1]])\n    if png_filename:\n        os.makedirs(\"images\", exist_ok=True)\n        plt.savefig(f\"images/{png_filename}.png\", bbox_inches=\"tight\")\n    plt.show()\n\n    return motion\n</pre> def plot_paths(     motion_type: str,     sampler,     t_final: float,     initial_value: float,     drift: float,     diffusion: float,     n: int,     png_filename: Optional[str] = None, ):     \"\"\"     Plot realizations of Brownian Motion or Geometric Brownian Motion.      Args:         motion_type: 'BM' for Brownian Motion or 'GBM' for Geometric Brownian Motion         sampler: QMCPy sampler instance         t_final: Final time point         initial_value: Initial value S(0)         drift: Drift coefficient (mu)         diffusion: Diffusion coefficient (sigma^2 for GBM, sigma for BM)         n: Number of paths to generate         png_filename: Optional filename to save plot (saved to images/ directory)      Returns:         Motion object used for generation     \"\"\"     if motion_type.upper() == \"BM\":         motion = qp.BrownianMotion(sampler, t_final, initial_value, drift, diffusion)         title = (             f\"Realizations of Brownian Motion using \" f\"{type(sampler).__name__} points\"         )         ylabel = \"$W(t)$\"     elif motion_type.upper() == \"GBM\":         motion = qp.GeometricBrownianMotion(             sampler, t_final, initial_value, drift, diffusion         )         title = (             f\"Realizations of Geometric Brownian Motion using \"             f\"{type(sampler).__name__} points\"         )         ylabel = \"$S(t)$\"     else:         raise ValueError(\"motion_type must be 'BM' or 'GBM'\")      t = motion.gen_samples(n)     initial_values = np.full((n, 1), motion.initial_value)     t_w_init = np.hstack((initial_values, t))     tvec_w_0 = np.hstack(([0], motion.time_vec))      plt.figure(figsize=(7, 4))     plt.plot(tvec_w_0, t_w_init.T)     plt.title(title)     plt.xlabel(\"$t$\")     plt.ylabel(ylabel)     plt.xlim([tvec_w_0[0], tvec_w_0[-1]])     if png_filename:         os.makedirs(\"images\", exist_ok=True)         plt.savefig(f\"images/{png_filename}.png\", bbox_inches=\"tight\")     plt.show()      return motion In\u00a0[\u00a0]: Copied! <pre>def plot_gbm_paths_with_distribution(\n    N: int,\n    sampler,\n    t_final: float,\n    initial_value: float,\n    drift: float,\n    diffusion: float,\n    n: int,\n) -&gt; None:\n    \"\"\"\n    Plot GBM paths with distribution of final values.\n\n    Combines path visualization with histogram and fitted lognormal distribution.\n\n    Args:\n        N: Number of simulations (for display purposes)\n        sampler: QMCPy sampler instance\n        t_final: Final time point\n        initial_value: Initial value S(0)\n        drift: Drift coefficient (mu)\n        diffusion: Diffusion coefficient (sigma^2)\n        n: Power of 2 for number of paths (generates 2^n paths)\n    \"\"\"\n    gbm = qp.GeometricBrownianMotion(\n        sampler,\n        t_final=t_final,\n        initial_value=initial_value,\n        drift=drift,\n        diffusion=diffusion,\n    )\n    gbm_path = gbm.gen_samples(2**n)\n\n    _, ax = plt.subplots(figsize=(14, 7))\n    T = max(gbm.time_vec)\n\n    # Plot GBM paths\n    ax.plot(gbm.time_vec, gbm_path.T, lw=0.75, alpha=0.7, color=\"skyblue\")\n\n    # Set up main plot\n    ax.set_title(\n        f\"Geometric Brownian Motion Paths\\n\"\n        f\"{N} Simulations, T = {T}, $\\\\mu$ = {drift:.1f}, \"\n        f\"$\\\\sigma$ = {diffusion:.1f}, using {type(sampler).__name__} points\"\n    )\n    ax.set_xlabel(r\"$t$\")\n    ax.set_ylabel(r\"$S(t)$\")\n    ax.set_ylim(bottom=0)\n    ax.set_xlim(0, T)\n\n    # Add histogram\n    final_values = gbm_path[:, -1]\n    hist_ax = ax.inset_axes([1.05, 0.0, 0.5, 1])\n    hist_ax.hist(\n        final_values,\n        bins=20,\n        density=True,\n        alpha=0.5,\n        color=\"skyblue\",\n        orientation=\"horizontal\",\n    )\n\n    # Add theoretical lognormal PDF\n    shape, _, scale = sc.lognorm.fit(final_values, floc=0)\n    x = np.linspace(0, max(final_values), 1000)\n    pdf = sc.lognorm.pdf(x, shape, loc=0, scale=scale)\n    hist_ax.plot(pdf, x, \"r-\", lw=2, label=\"Lognormal PDF\")\n\n    # Finalize histogram\n    hist_ax.set_title(f\"E[$S_T$] = {np.mean(final_values):.4f}\", pad=20)\n    hist_ax.axhline(\n        np.mean(final_values), color=\"blue\", linestyle=\"--\", lw=1.5, label=r\"$E[S_T]$\"\n    )\n    hist_ax.set_yticks([])\n    hist_ax.set_xlabel(\"Density\")\n    hist_ax.legend()\n    hist_ax.set_ylim(bottom=0)\n    plt.tight_layout()\n    plt.show()\n</pre> def plot_gbm_paths_with_distribution(     N: int,     sampler,     t_final: float,     initial_value: float,     drift: float,     diffusion: float,     n: int, ) -&gt; None:     \"\"\"     Plot GBM paths with distribution of final values.      Combines path visualization with histogram and fitted lognormal distribution.      Args:         N: Number of simulations (for display purposes)         sampler: QMCPy sampler instance         t_final: Final time point         initial_value: Initial value S(0)         drift: Drift coefficient (mu)         diffusion: Diffusion coefficient (sigma^2)         n: Power of 2 for number of paths (generates 2^n paths)     \"\"\"     gbm = qp.GeometricBrownianMotion(         sampler,         t_final=t_final,         initial_value=initial_value,         drift=drift,         diffusion=diffusion,     )     gbm_path = gbm.gen_samples(2**n)      _, ax = plt.subplots(figsize=(14, 7))     T = max(gbm.time_vec)      # Plot GBM paths     ax.plot(gbm.time_vec, gbm_path.T, lw=0.75, alpha=0.7, color=\"skyblue\")      # Set up main plot     ax.set_title(         f\"Geometric Brownian Motion Paths\\n\"         f\"{N} Simulations, T = {T}, $\\\\mu$ = {drift:.1f}, \"         f\"$\\\\sigma$ = {diffusion:.1f}, using {type(sampler).__name__} points\"     )     ax.set_xlabel(r\"$t$\")     ax.set_ylabel(r\"$S(t)$\")     ax.set_ylim(bottom=0)     ax.set_xlim(0, T)      # Add histogram     final_values = gbm_path[:, -1]     hist_ax = ax.inset_axes([1.05, 0.0, 0.5, 1])     hist_ax.hist(         final_values,         bins=20,         density=True,         alpha=0.5,         color=\"skyblue\",         orientation=\"horizontal\",     )      # Add theoretical lognormal PDF     shape, _, scale = sc.lognorm.fit(final_values, floc=0)     x = np.linspace(0, max(final_values), 1000)     pdf = sc.lognorm.pdf(x, shape, loc=0, scale=scale)     hist_ax.plot(pdf, x, \"r-\", lw=2, label=\"Lognormal PDF\")      # Finalize histogram     hist_ax.set_title(f\"E[$S_T$] = {np.mean(final_values):.4f}\", pad=20)     hist_ax.axhline(         np.mean(final_values), color=\"blue\", linestyle=\"--\", lw=1.5, label=r\"$E[S_T]$\"     )     hist_ax.set_yticks([])     hist_ax.set_xlabel(\"Density\")     hist_ax.legend()     hist_ax.set_ylim(bottom=0)     plt.tight_layout()     plt.show() In\u00a0[\u00a0]: Copied! <pre>def compute_theoretical_covariance(\n    S0: float, mu: float, sigma: float, t1: float, t2: float\n) -&gt; npt.NDArray[np.floating]:\n    \"\"\"\n    Compute theoretical covariance matrix for GBM at two time points.\n\n    Args:\n        S0: Initial value\n        mu: Drift coefficient\n        sigma: Volatility\n        t1: First time point\n        t2: Second time point\n\n    Returns:\n        2x2 covariance matrix\n    \"\"\"\n    return np.array(\n        [\n            [\n                S0**2 * np.exp(2 * mu * t1) * (np.exp(sigma**2 * t1) - 1),\n                S0**2 * np.exp(mu * (t1 + t2)) * (np.exp(sigma**2 * t1) - 1),\n            ],\n            [\n                S0**2 * np.exp(mu * (t1 + t2)) * (np.exp(sigma**2 * t1) - 1),\n                S0**2 * np.exp(2 * mu * t2) * (np.exp(sigma**2 * t2) - 1),\n            ],\n        ]\n    )\n</pre> def compute_theoretical_covariance(     S0: float, mu: float, sigma: float, t1: float, t2: float ) -&gt; npt.NDArray[np.floating]:     \"\"\"     Compute theoretical covariance matrix for GBM at two time points.      Args:         S0: Initial value         mu: Drift coefficient         sigma: Volatility         t1: First time point         t2: Second time point      Returns:         2x2 covariance matrix     \"\"\"     return np.array(         [             [                 S0**2 * np.exp(2 * mu * t1) * (np.exp(sigma**2 * t1) - 1),                 S0**2 * np.exp(mu * (t1 + t2)) * (np.exp(sigma**2 * t1) - 1),             ],             [                 S0**2 * np.exp(mu * (t1 + t2)) * (np.exp(sigma**2 * t1) - 1),                 S0**2 * np.exp(2 * mu * t2) * (np.exp(sigma**2 * t2) - 1),             ],         ]     ) In\u00a0[\u00a0]: Copied! <pre>def calculate_theoretical_statistics(params: dict) -&gt; tuple[float, float]:\n    \"\"\"\n    Calculate theoretical mean and standard deviation for GBM.\n\n    Args:\n        params: Dictionary with keys 'initial_value', 'mu', 'sigma', 'maturity'\n\n    Returns:\n        tuple: (theoretical_mean, theoretical_std)\n    \"\"\"\n    theoretical_mean = params[\"initial_value\"] * np.exp(\n        params[\"mu\"] * params[\"maturity\"]\n    )\n    theoretical_std = np.sqrt(\n        params[\"initial_value\"] ** 2\n        * np.exp(2 * params[\"mu\"] * params[\"maturity\"])\n        * (np.exp(params[\"sigma\"] ** 2 * params[\"maturity\"]) - 1)\n    )\n    return theoretical_mean, theoretical_std\n</pre> def calculate_theoretical_statistics(params: dict) -&gt; tuple[float, float]:     \"\"\"     Calculate theoretical mean and standard deviation for GBM.      Args:         params: Dictionary with keys 'initial_value', 'mu', 'sigma', 'maturity'      Returns:         tuple: (theoretical_mean, theoretical_std)     \"\"\"     theoretical_mean = params[\"initial_value\"] * np.exp(         params[\"mu\"] * params[\"maturity\"]     )     theoretical_std = np.sqrt(         params[\"initial_value\"] ** 2         * np.exp(2 * params[\"mu\"] * params[\"maturity\"])         * (np.exp(params[\"sigma\"] ** 2 * params[\"maturity\"]) - 1)     )     return theoretical_mean, theoretical_std In\u00a0[\u00a0]: Copied! <pre>def extract_covariance_samples(\n    paths: npt.NDArray[np.floating], n_steps: int, is_quantlib: bool = True\n) -&gt; npt.NDArray[np.floating]:\n    \"\"\"\n    Extract samples at two time points and compute covariance matrix.\n\n    Args:\n        paths: Generated paths array\n        n_steps: Number of time steps\n        is_quantlib: True for QuantLib paths, False for QMCPy paths\n\n    Returns:\n        Covariance matrix for samples at two time points\n    \"\"\"\n    if is_quantlib:\n        idx1, idx2 = int(0.5 * n_steps), n_steps\n        samples_t1, samples_t2 = paths[:, idx1], paths[:, idx2]\n    else:  # QMCPy\n        idx1, idx2 = int(0.5 * (n_steps - 1)), n_steps - 1\n        samples_t1, samples_t2 = paths[:, idx1], paths[:, idx2]\n    return np.cov(np.vstack((samples_t1, samples_t2)))\n</pre> def extract_covariance_samples(     paths: npt.NDArray[np.floating], n_steps: int, is_quantlib: bool = True ) -&gt; npt.NDArray[np.floating]:     \"\"\"     Extract samples at two time points and compute covariance matrix.      Args:         paths: Generated paths array         n_steps: Number of time steps         is_quantlib: True for QuantLib paths, False for QMCPy paths      Returns:         Covariance matrix for samples at two time points     \"\"\"     if is_quantlib:         idx1, idx2 = int(0.5 * n_steps), n_steps         samples_t1, samples_t2 = paths[:, idx1], paths[:, idx2]     else:  # QMCPy         idx1, idx2 = int(0.5 * (n_steps - 1)), n_steps - 1         samples_t1, samples_t2 = paths[:, idx1], paths[:, idx2]     return np.cov(np.vstack((samples_t1, samples_t2)))"},{"location":"demos/GBM/qmcpy_util/","title":"Qmcpy util","text":"In\u00a0[\u00a0]: Copied! <pre>import qmcpy as qp\n</pre> import qmcpy as qp In\u00a0[\u00a0]: Copied! <pre>def create_qmcpy_sampler(\n    sampler_type: str, dimension: int, replications: int = 1, seed: int = 42\n):\n    \"\"\"\n    Create a sampler instance based on type and dimension.\n\n    Args:\n        sampler_type: Type of sampler ('IIDStdUniform', 'Sobol', 'Lattice', 'Halton')\n        dimension: Dimension of the sampler (typically n_steps)\n        replications: Number of independent replications\n        seed: Random seed for reproducibility\n\n    Returns:\n        QMCPy sampler instance\n    \"\"\"\n    if sampler_type == \"IIDStdUniform\":\n        return qp.IIDStdUniform(dimension, replications, seed=seed)\n    elif sampler_type == \"Sobol\":\n        return qp.Sobol(dimension, replications, seed=seed)\n    elif sampler_type == \"Lattice\":\n        return qp.Lattice(dimension, replications, seed=seed)\n    elif sampler_type == \"Halton\":\n        return qp.Halton(dimension, replications, seed=seed)\n    else:\n        raise ValueError(f\"Unsupported sampler type: {sampler_type}\")\n</pre> def create_qmcpy_sampler(     sampler_type: str, dimension: int, replications: int = 1, seed: int = 42 ):     \"\"\"     Create a sampler instance based on type and dimension.      Args:         sampler_type: Type of sampler ('IIDStdUniform', 'Sobol', 'Lattice', 'Halton')         dimension: Dimension of the sampler (typically n_steps)         replications: Number of independent replications         seed: Random seed for reproducibility      Returns:         QMCPy sampler instance     \"\"\"     if sampler_type == \"IIDStdUniform\":         return qp.IIDStdUniform(dimension, replications, seed=seed)     elif sampler_type == \"Sobol\":         return qp.Sobol(dimension, replications, seed=seed)     elif sampler_type == \"Lattice\":         return qp.Lattice(dimension, replications, seed=seed)     elif sampler_type == \"Halton\":         return qp.Halton(dimension, replications, seed=seed)     else:         raise ValueError(f\"Unsupported sampler type: {sampler_type}\") In\u00a0[\u00a0]: Copied! <pre>def generate_qmcpy_paths(\n    initial_value: float,\n    mu: float,\n    diffusion: float,\n    maturity: float,\n    n_steps: int,\n    n_paths: int,\n    sampler_type: str = \"IIDStdUniform\",\n    replications: int = 1,\n    seed: int = 42,\n):\n    \"\"\"\n    Generate Geometric Brownian Motion paths using QMCPy with multiple replications.\n\n    Args:\n        initial_value: Initial value of the GBM process (S_0)\n        mu: Drift parameter\n        diffusion: Diffusion coefficient (sigma^2)\n        maturity: Final time T\n        n_steps: Number of discretization time steps\n        n_paths: Number of paths to generate per replication\n        sampler_type: Type of sampler ('IIDStdUniform', 'Sobol', 'Lattice', 'Halton')\n        replications: Number of independent replications\n        seed: Random seed for reproducibility\n\n    Returns:\n        tuple: (paths, gbm) where paths has shape (n_paths, n_steps) if replications=1,\n               or (replications, n_paths, n_steps) if replications&gt;1,\n               and gbm is the GeometricBrownianMotion object\n    \"\"\"\n    sampler = create_qmcpy_sampler(sampler_type, n_steps, replications, seed)\n    gbm = qp.GeometricBrownianMotion(\n        sampler,\n        t_final=maturity,\n        initial_value=initial_value,\n        drift=mu,\n        diffusion=diffusion,\n    )\n    paths = gbm.gen_samples(n_paths)\n    return paths, gbm\n</pre> def generate_qmcpy_paths(     initial_value: float,     mu: float,     diffusion: float,     maturity: float,     n_steps: int,     n_paths: int,     sampler_type: str = \"IIDStdUniform\",     replications: int = 1,     seed: int = 42, ):     \"\"\"     Generate Geometric Brownian Motion paths using QMCPy with multiple replications.      Args:         initial_value: Initial value of the GBM process (S_0)         mu: Drift parameter         diffusion: Diffusion coefficient (sigma^2)         maturity: Final time T         n_steps: Number of discretization time steps         n_paths: Number of paths to generate per replication         sampler_type: Type of sampler ('IIDStdUniform', 'Sobol', 'Lattice', 'Halton')         replications: Number of independent replications         seed: Random seed for reproducibility      Returns:         tuple: (paths, gbm) where paths has shape (n_paths, n_steps) if replications=1,                or (replications, n_paths, n_steps) if replications&gt;1,                and gbm is the GeometricBrownianMotion object     \"\"\"     sampler = create_qmcpy_sampler(sampler_type, n_steps, replications, seed)     gbm = qp.GeometricBrownianMotion(         sampler,         t_final=maturity,         initial_value=initial_value,         drift=mu,         diffusion=diffusion,     )     paths = gbm.gen_samples(n_paths)     return paths, gbm"},{"location":"demos/GBM/quantlib_util/","title":"Quantlib util","text":"In\u00a0[\u00a0]: Copied! <pre>import QuantLib as ql\nimport numpy as np\n</pre> import QuantLib as ql import numpy as np In\u00a0[\u00a0]: Copied! <pre>def generate_quantlib_paths(\n    initial_value: float,\n    mu: float,\n    sigma: float,\n    maturity: float,\n    n_steps: int,\n    n_paths: int,\n    sampler_type: str = \"IIDStdUniform\",\n    seed: int = 7,\n) -&gt; tuple:\n    \"\"\"\n    Generate Geometric Brownian Motion paths using QuantLib.\n\n    Args:\n        initial_value: Initial value of the GBM process (S_0)\n        mu: Drift parameter\n        sigma: Volatility parameter (note: NOT diffusion coefficient)\n        maturity: Final time T\n        n_steps: Number of discretization time steps\n        n_paths: Number of paths to generate\n        sampler_type: Type of sampler ('IIDStdUniform' or 'Sobol')\n        seed: Random seed for reproducibility\n\n    Returns:\n        tuple: (paths, gbm) where paths has shape (n_paths, n_steps+1)\n               (includes initial value at t=0) and gbm is the\n               GeometricBrownianMotionProcess object\n\n    Raises:\n        ValueError: If sampler_type is not 'IIDStdUniform' or 'Sobol'\n    \"\"\"\n    gbm = ql.GeometricBrownianMotionProcess(initial_value, mu, sigma)\n    times = ql.TimeGrid(maturity, n_steps)\n    dimension = n_steps\n    if sampler_type == \"IIDStdUniform\":\n        uniform_rng = ql.UniformRandomGenerator(seed)\n        sequence_gen = ql.GaussianRandomSequenceGenerator(\n            ql.UniformRandomSequenceGenerator(n_steps, uniform_rng)\n        )\n        path_gen = ql.GaussianPathGenerator(gbm, maturity, n_steps, sequence_gen, False)\n        paths = np.zeros((n_paths, n_steps + 1))\n        for i in range(n_paths):\n            sample_path = path_gen.next().value()\n            paths[i, :] = np.array([sample_path[j] for j in range(n_steps + 1)])\n        return paths, gbm\n    elif sampler_type == \"Sobol\":\n        uniform_rsg = ql.UniformLowDiscrepancySequenceGenerator(dimension, seed)\n        sequence_gen = ql.GaussianLowDiscrepancySequenceGenerator(uniform_rsg)\n        path_gen = ql.GaussianSobolMultiPathGenerator(\n            gbm, list(times), sequence_gen, False\n        )\n        paths = np.zeros((n_paths, n_steps + 1))\n        paths[:, 0] = initial_value  # Set initial value\n        for i in range(n_paths):\n            sample_path = path_gen.next().value()\n            # For 1D process, get the first (and only) path\n            path_values = sample_path[0]\n            paths[i, :] = np.array([path_values[j] for j in range(n_steps + 1)])\n        return paths, gbm\n    else:\n        raise ValueError(\n            f\"Unsupported sampler type: {sampler_type}.  Use 'IIDStdUniform' or 'Sobol'\"\n        )\n</pre> def generate_quantlib_paths(     initial_value: float,     mu: float,     sigma: float,     maturity: float,     n_steps: int,     n_paths: int,     sampler_type: str = \"IIDStdUniform\",     seed: int = 7, ) -&gt; tuple:     \"\"\"     Generate Geometric Brownian Motion paths using QuantLib.      Args:         initial_value: Initial value of the GBM process (S_0)         mu: Drift parameter         sigma: Volatility parameter (note: NOT diffusion coefficient)         maturity: Final time T         n_steps: Number of discretization time steps         n_paths: Number of paths to generate         sampler_type: Type of sampler ('IIDStdUniform' or 'Sobol')         seed: Random seed for reproducibility      Returns:         tuple: (paths, gbm) where paths has shape (n_paths, n_steps+1)                (includes initial value at t=0) and gbm is the                GeometricBrownianMotionProcess object      Raises:         ValueError: If sampler_type is not 'IIDStdUniform' or 'Sobol'     \"\"\"     gbm = ql.GeometricBrownianMotionProcess(initial_value, mu, sigma)     times = ql.TimeGrid(maturity, n_steps)     dimension = n_steps     if sampler_type == \"IIDStdUniform\":         uniform_rng = ql.UniformRandomGenerator(seed)         sequence_gen = ql.GaussianRandomSequenceGenerator(             ql.UniformRandomSequenceGenerator(n_steps, uniform_rng)         )         path_gen = ql.GaussianPathGenerator(gbm, maturity, n_steps, sequence_gen, False)         paths = np.zeros((n_paths, n_steps + 1))         for i in range(n_paths):             sample_path = path_gen.next().value()             paths[i, :] = np.array([sample_path[j] for j in range(n_steps + 1)])         return paths, gbm     elif sampler_type == \"Sobol\":         uniform_rsg = ql.UniformLowDiscrepancySequenceGenerator(dimension, seed)         sequence_gen = ql.GaussianLowDiscrepancySequenceGenerator(uniform_rsg)         path_gen = ql.GaussianSobolMultiPathGenerator(             gbm, list(times), sequence_gen, False         )         paths = np.zeros((n_paths, n_steps + 1))         paths[:, 0] = initial_value  # Set initial value         for i in range(n_paths):             sample_path = path_gen.next().value()             # For 1D process, get the first (and only) path             path_values = sample_path[0]             paths[i, :] = np.array([path_values[j] for j in range(n_steps + 1)])         return paths, gbm     else:         raise ValueError(             f\"Unsupported sampler type: {sampler_type}.  Use 'IIDStdUniform' or 'Sobol'\"         )"},{"location":"demos/GBM/gbm_code/__init__/","title":"init","text":"<p>Code utilities for GBM demonstrations</p>"},{"location":"demos/GBM/gbm_code/bm_gbm_16/","title":"Bm gbm 16","text":"In\u00a0[\u00a0]: Copied! <pre>n = 16\nsampler = qp.Lattice(2**7, seed=42)\nplot_paths(\"BM\", sampler, t_final=1, initial_value=1, drift=0, diffusion=1, n=n)\nplot_paths(\"GBM\", sampler, t_final=1, initial_value=1, drift=0, diffusion=1, n=n)\n</pre> n = 16 sampler = qp.Lattice(2**7, seed=42) plot_paths(\"BM\", sampler, t_final=1, initial_value=1, drift=0, diffusion=1, n=n) plot_paths(\"GBM\", sampler, t_final=1, initial_value=1, drift=0, diffusion=1, n=n)"},{"location":"demos/GBM/gbm_code/config/","title":"Config","text":"In\u00a0[\u00a0]: Copied! <pre>is_debug = False\n</pre> is_debug = False In\u00a0[\u00a0]: Copied! <pre>def get_experiment_configurations() -&gt; dict:\n    \"\"\"\n    Define experimental parameter ranges for GBM simulations.\n\n    Returns:\n        dict: Configuration dictionary with 'time_steps' and 'paths' experiments,\n              each containing 'fixed_paths'/'fixed_steps', 'range', and 'series_name'\n    \"\"\"\n    return {\n        \"time_steps\": {\n            \"fixed_paths\": 2**3 if is_debug else 2**12,\n            \"range\": (\n                [2**i for i in range(4, 7)]\n                if is_debug\n                else [2**i for i in range(4, 10)]\n            ),\n            \"series_name\": \"Time Steps\",\n        },\n        \"paths\": {\n            \"fixed_steps\": 2**5 if is_debug else 252,\n            \"range\": (\n                [2**i for i in range(6, 9)]\n                if is_debug\n                else [2**i for i in range(9, 15)]\n            ),\n            \"series_name\": \"Paths\",\n        },\n    }\n</pre> def get_experiment_configurations() -&gt; dict:     \"\"\"     Define experimental parameter ranges for GBM simulations.      Returns:         dict: Configuration dictionary with 'time_steps' and 'paths' experiments,               each containing 'fixed_paths'/'fixed_steps', 'range', and 'series_name'     \"\"\"     return {         \"time_steps\": {             \"fixed_paths\": 2**3 if is_debug else 2**12,             \"range\": (                 [2**i for i in range(4, 7)]                 if is_debug                 else [2**i for i in range(4, 10)]             ),             \"series_name\": \"Time Steps\",         },         \"paths\": {             \"fixed_steps\": 2**5 if is_debug else 252,             \"range\": (                 [2**i for i in range(6, 9)]                 if is_debug                 else [2**i for i in range(9, 15)]             ),             \"series_name\": \"Paths\",         },     } In\u00a0[\u00a0]: Copied! <pre>def get_sampler_configurations() -&gt; dict:\n    \"\"\"\n    Define sampler types available for testing.\n\n    Returns:\n        dict: Dictionary with 'all_samplers' (QMCPy samplers) and\n              'quantlib_samplers' (QuantLib-supported samplers)\n    \"\"\"\n    return {\n        \"all_samplers\": [\"IIDStdUniform\", \"Sobol\", \"Lattice\", \"Halton\"],\n        \"quantlib_samplers\": [\"IIDStdUniform\", \"Sobol\"],\n    }\n</pre> def get_sampler_configurations() -&gt; dict:     \"\"\"     Define sampler types available for testing.      Returns:         dict: Dictionary with 'all_samplers' (QMCPy samplers) and               'quantlib_samplers' (QuantLib-supported samplers)     \"\"\"     return {         \"all_samplers\": [\"IIDStdUniform\", \"Sobol\", \"Lattice\", \"Halton\"],         \"quantlib_samplers\": [\"IIDStdUniform\", \"Sobol\"],     } In\u00a0[\u00a0]: Copied! <pre>def get_gbm_parameters() -&gt; dict:\n    \"\"\"\n    Define base Geometric Brownian Motion parameters.\n\n    Returns:\n        dict: Parameters including 'initial_value' (S_0), 'mu' (drift),\n              'sigma' (volatility), and 'maturity' (time horizon T)\n    \"\"\"\n    return {\"initial_value\": 100, \"mu\": 0.05, \"sigma\": 0.2, \"maturity\": 1.0}\n</pre> def get_gbm_parameters() -&gt; dict:     \"\"\"     Define base Geometric Brownian Motion parameters.      Returns:         dict: Parameters including 'initial_value' (S_0), 'mu' (drift),               'sigma' (volatility), and 'maturity' (time horizon T)     \"\"\"     return {\"initial_value\": 100, \"mu\": 0.05, \"sigma\": 0.2, \"maturity\": 1.0}"},{"location":"demos/GBM/gbm_code/data_util/","title":"Data util","text":"In\u00a0[\u00a0]: Copied! <pre>import numpy as np\nimport numpy.typing as npt\nimport pandas as pd\nimport quantlib_util as qlu\nimport qmcpy_util as qpu\nimport config as cf\n</pre> import numpy as np import numpy.typing as npt import pandas as pd import quantlib_util as qlu import qmcpy_util as qpu import config as cf In\u00a0[\u00a0]: Copied! <pre>def add_theoretical_results(\n    results_data: list, theoretical_mean: float, theoretical_std: float\n) -&gt; None:\n    \"\"\"\n    Add theoretical benchmark values to results data.\n\n    Args:\n        results_data: List to append theoretical results to\n        theoretical_mean: Theoretical expected value $E[S_T]$\n        theoretical_std: Theoretical standard deviation of $S_T$\n    \"\"\"\n    results_data.append(\n        {\n            \"Method\": \"Theoretical\",\n            \"Sampler\": \"-\",\n            \"Mean\": theoretical_mean,\n            \"Std Dev\": theoretical_std,\n            \"Mean Absolute Error\": 0,\n            \"Std Dev Error\": 0,\n        }\n    )\n</pre> def add_theoretical_results(     results_data: list, theoretical_mean: float, theoretical_std: float ) -&gt; None:     \"\"\"     Add theoretical benchmark values to results data.      Args:         results_data: List to append theoretical results to         theoretical_mean: Theoretical expected value $E[S_T]$         theoretical_std: Theoretical standard deviation of $S_T$     \"\"\"     results_data.append(         {             \"Method\": \"Theoretical\",             \"Sampler\": \"-\",             \"Mean\": theoretical_mean,             \"Std Dev\": theoretical_std,             \"Mean Absolute Error\": 0,             \"Std Dev Error\": 0,         }     ) In\u00a0[\u00a0]: Copied! <pre>def add_quantlib_results(\n    results_data: list,\n    sampler_type: str,\n    quantlib_final: npt.NDArray[np.floating],  # per replication mean\n    theoretical_mean: float,\n    theoretical_std: float,\n) -&gt; None:\n    \"\"\"\n    Add summary statistics for QuantLib simulations based on per-replication means.\n\n    The `quantlib_final` array is expected to contain one value per replication,\n    where each value is the mean of all simulated terminal asset prices S_T in\n    that replication.\n\n    Args:\n        results_data: List to which the QuantLib summary row is appended.\n        sampler_type: Identifier for the sampler used in the QuantLib experiment.\n        quantlib_final: 1D array of per-replication sample means of $S_T$.\n        theoretical_mean: Theoretical expected value $E[S_T]$ used as a benchmark.\n        theoretical_std: Theoretical standard deviation of $S_T$ used as a benchmark.\n    \"\"\"\n    ql_emp_mean = np.mean(quantlib_final)\n    ql_emp_std = np.std(quantlib_final, ddof=1)\n    ql_mae = np.mean(np.abs(quantlib_final - theoretical_mean))\n\n    results_data.append(\n        {\n            \"Method\": \"QuantLib\",\n            \"Sampler\": sampler_type,\n            \"Mean\": ql_emp_mean,\n            \"Std Dev\": ql_emp_std,\n            \"Mean Absolute Error\": ql_mae,\n            \"Std Dev Error\": abs(ql_emp_std - theoretical_std),\n        }\n    )\n</pre> def add_quantlib_results(     results_data: list,     sampler_type: str,     quantlib_final: npt.NDArray[np.floating],  # per replication mean     theoretical_mean: float,     theoretical_std: float, ) -&gt; None:     \"\"\"     Add summary statistics for QuantLib simulations based on per-replication means.      The `quantlib_final` array is expected to contain one value per replication,     where each value is the mean of all simulated terminal asset prices S_T in     that replication.      Args:         results_data: List to which the QuantLib summary row is appended.         sampler_type: Identifier for the sampler used in the QuantLib experiment.         quantlib_final: 1D array of per-replication sample means of $S_T$.         theoretical_mean: Theoretical expected value $E[S_T]$ used as a benchmark.         theoretical_std: Theoretical standard deviation of $S_T$ used as a benchmark.     \"\"\"     ql_emp_mean = np.mean(quantlib_final)     ql_emp_std = np.std(quantlib_final, ddof=1)     ql_mae = np.mean(np.abs(quantlib_final - theoretical_mean))      results_data.append(         {             \"Method\": \"QuantLib\",             \"Sampler\": sampler_type,             \"Mean\": ql_emp_mean,             \"Std Dev\": ql_emp_std,             \"Mean Absolute Error\": ql_mae,             \"Std Dev Error\": abs(ql_emp_std - theoretical_std),         }     ) In\u00a0[\u00a0]: Copied! <pre>def add_qmcpy_results(\n    results_data: list,\n    sampler_type: str,\n    qmcpy_final: npt.NDArray[np.floating],  # per replication mean\n    qp_emp_mean: float,\n    theoretical_mean: float,\n    theoretical_std: float,\n) -&gt; None:\n    \"\"\"\n    Add empirical QMCPy results, computed from per-replication means, to results data.\n\n    The mean absolute error (MAE) is computed across replications as\n    the average of the absolute differences between each per-replication\n    mean in `qmcpy_final` and `theoretical_mean`.\n\n    Args:\n        results_data: List to which the QMCPy summary for this sampler is appended.\n        sampler_type: Name or type of the QMCPy sampler used.\n        qmcpy_final: 1D array where each entry is the mean payoff from a single\n            replication of the QMCPy experiment.\n        qp_emp_mean: Overall empirical mean across all replications.\n        theoretical_mean: Theoretical expected value used as a benchmark.\n        theoretical_std: Theoretical standard deviation used as a benchmark.\n    \"\"\"\n    qp_emp_std = np.std(qmcpy_final, ddof=1)\n    qp_mae = np.mean(np.abs(qmcpy_final - theoretical_mean))\n\n    results_data.append(\n        {\n            \"Method\": \"QMCPy\",\n            \"Sampler\": sampler_type,\n            \"Mean\": qp_emp_mean,\n            \"Std Dev\": qp_emp_std,\n            \"Mean Absolute Error\": qp_mae,\n            \"Std Dev Error\": abs(qp_emp_std - theoretical_std),\n        }\n    )\n</pre> def add_qmcpy_results(     results_data: list,     sampler_type: str,     qmcpy_final: npt.NDArray[np.floating],  # per replication mean     qp_emp_mean: float,     theoretical_mean: float,     theoretical_std: float, ) -&gt; None:     \"\"\"     Add empirical QMCPy results, computed from per-replication means, to results data.      The mean absolute error (MAE) is computed across replications as     the average of the absolute differences between each per-replication     mean in `qmcpy_final` and `theoretical_mean`.      Args:         results_data: List to which the QMCPy summary for this sampler is appended.         sampler_type: Name or type of the QMCPy sampler used.         qmcpy_final: 1D array where each entry is the mean payoff from a single             replication of the QMCPy experiment.         qp_emp_mean: Overall empirical mean across all replications.         theoretical_mean: Theoretical expected value used as a benchmark.         theoretical_std: Theoretical standard deviation used as a benchmark.     \"\"\"     qp_emp_std = np.std(qmcpy_final, ddof=1)     qp_mae = np.mean(np.abs(qmcpy_final - theoretical_mean))      results_data.append(         {             \"Method\": \"QMCPy\",             \"Sampler\": sampler_type,             \"Mean\": qp_emp_mean,             \"Std Dev\": qp_emp_std,             \"Mean Absolute Error\": qp_mae,             \"Std Dev Error\": abs(qp_emp_std - theoretical_std),         }     ) In\u00a0[\u00a0]: Copied! <pre>def process_sampler_data(\n    sampler_type: str,\n    results_data: list,\n    theoretical_mean: float,\n    theoretical_std: float,\n    params_ql: dict,\n    params_qp: dict,\n) -&gt; tuple:\n    \"\"\"\n    Process and compare data for a single sampler type across both libraries.\n\n    Args:\n        sampler_type: Type of sampler to test\n        results_data: List to append comparison results to\n        theoretical_mean: Theoretical expected value\n        theoretical_std: Theoretical standard deviation\n        params_ql: Dictionary of QuantLib parameters\n        params_qp: Dictionary of QMCPy parameters\n\n    Returns:\n        tuple: (quantlib_paths, qmcpy_paths, ql_gbm, qp_gbm, params_ql, params_qp)\n    \"\"\"\n\n    params_ql[\"sampler_type\"] = sampler_type\n    params_qp[\"sampler_type\"] = sampler_type\n\n    replications = params_qp[\"replications\"]\n\n    quantlib_paths, ql_gbm = None, None\n\n    if sampler_type in [\"IIDStdUniform\", \"Sobol\"]:\n        ql_means = np.empty(replications)\n        ql_seed = params_ql[\"seed\"]\n\n        for r in range(replications):\n            params_ql[\"seed\"] = ql_seed + r\n            quantlib_paths, ql_gbm = qlu.generate_quantlib_paths(**params_ql)\n            ql_means[r] = quantlib_paths[:, -1].mean()\n\n        params_ql[\"seed\"] = ql_seed\n    else:\n        ql_means = None\n\n    qmcpy_paths, qp_gbm = qpu.generate_qmcpy_paths(**params_qp)\n\n    if qmcpy_paths.ndim == 3:\n        qp_means = qmcpy_paths[:, :, -1].mean(axis=1)\n    else:\n        qp_means = np.array([qmcpy_paths[:, -1].mean()])\n\n    if ql_means is not None:\n        add_quantlib_results(\n            results_data,\n            sampler_type,\n            ql_means,\n            theoretical_mean,\n            theoretical_std,\n        )\n\n    add_qmcpy_results(\n        results_data,\n        sampler_type,\n        qp_means,\n        qp_means.mean(),\n        theoretical_mean,\n        theoretical_std,\n    )\n\n    return quantlib_paths, qmcpy_paths, ql_gbm, qp_gbm, params_ql, params_qp\n</pre> def process_sampler_data(     sampler_type: str,     results_data: list,     theoretical_mean: float,     theoretical_std: float,     params_ql: dict,     params_qp: dict, ) -&gt; tuple:     \"\"\"     Process and compare data for a single sampler type across both libraries.      Args:         sampler_type: Type of sampler to test         results_data: List to append comparison results to         theoretical_mean: Theoretical expected value         theoretical_std: Theoretical standard deviation         params_ql: Dictionary of QuantLib parameters         params_qp: Dictionary of QMCPy parameters      Returns:         tuple: (quantlib_paths, qmcpy_paths, ql_gbm, qp_gbm, params_ql, params_qp)     \"\"\"      params_ql[\"sampler_type\"] = sampler_type     params_qp[\"sampler_type\"] = sampler_type      replications = params_qp[\"replications\"]      quantlib_paths, ql_gbm = None, None      if sampler_type in [\"IIDStdUniform\", \"Sobol\"]:         ql_means = np.empty(replications)         ql_seed = params_ql[\"seed\"]          for r in range(replications):             params_ql[\"seed\"] = ql_seed + r             quantlib_paths, ql_gbm = qlu.generate_quantlib_paths(**params_ql)             ql_means[r] = quantlib_paths[:, -1].mean()          params_ql[\"seed\"] = ql_seed     else:         ql_means = None      qmcpy_paths, qp_gbm = qpu.generate_qmcpy_paths(**params_qp)      if qmcpy_paths.ndim == 3:         qp_means = qmcpy_paths[:, :, -1].mean(axis=1)     else:         qp_means = np.array([qmcpy_paths[:, -1].mean()])      if ql_means is not None:         add_quantlib_results(             results_data,             sampler_type,             ql_means,             theoretical_mean,             theoretical_std,         )      add_qmcpy_results(         results_data,         sampler_type,         qp_means,         qp_means.mean(),         theoretical_mean,         theoretical_std,     )      return quantlib_paths, qmcpy_paths, ql_gbm, qp_gbm, params_ql, params_qp In\u00a0[\u00a0]: Copied! <pre>def create_timing_dataframe(\n    quantlib_results: dict, qmcpy_results: dict, baseline_sampler: str\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Create comprehensive timing comparison table from benchmark results.\n\n    Args:\n        quantlib_results: Dictionary mapping sampler names to timing results\n        qmcpy_results: Dictionary mapping sampler names to timing results\n        baseline_sampler: Sampler to use as baseline for speedup calculation\n\n    Returns:\n        DataFrame with timing statistics and speedup comparisons\n    \"\"\"\n    timing_data = []\n\n    # Add QuantLib data\n    for sampler_type, result in quantlib_results.items():\n        timing_data.append(\n            {\n                \"Method\": \"QuantLib\",\n                \"Sampler\": sampler_type,\n                \"Mean Time (s)\": result[\"average\"],\n                \"Std Dev (s)\": result[\"stdev\"],\n                \"Speedup\": \"-\",\n            }\n        )\n\n    # Add QMCPy data with speedup calculation\n    baseline_time = quantlib_results[baseline_sampler][\"average\"]\n    for sampler_type, result in qmcpy_results.items():\n        speedup = baseline_time / result[\"average\"]\n        timing_data.append(\n            {\n                \"Method\": \"QMCPy\",\n                \"Sampler\": sampler_type,\n                \"Mean Time (s)\": result[\"average\"],\n                \"Std Dev (s)\": result[\"stdev\"],\n                \"Speedup\": speedup,\n            }\n        )\n\n    return pd.DataFrame(timing_data)\n</pre> def create_timing_dataframe(     quantlib_results: dict, qmcpy_results: dict, baseline_sampler: str ) -&gt; pd.DataFrame:     \"\"\"     Create comprehensive timing comparison table from benchmark results.      Args:         quantlib_results: Dictionary mapping sampler names to timing results         qmcpy_results: Dictionary mapping sampler names to timing results         baseline_sampler: Sampler to use as baseline for speedup calculation      Returns:         DataFrame with timing statistics and speedup comparisons     \"\"\"     timing_data = []      # Add QuantLib data     for sampler_type, result in quantlib_results.items():         timing_data.append(             {                 \"Method\": \"QuantLib\",                 \"Sampler\": sampler_type,                 \"Mean Time (s)\": result[\"average\"],                 \"Std Dev (s)\": result[\"stdev\"],                 \"Speedup\": \"-\",             }         )      # Add QMCPy data with speedup calculation     baseline_time = quantlib_results[baseline_sampler][\"average\"]     for sampler_type, result in qmcpy_results.items():         speedup = baseline_time / result[\"average\"]         timing_data.append(             {                 \"Method\": \"QMCPy\",                 \"Sampler\": sampler_type,                 \"Mean Time (s)\": result[\"average\"],                 \"Std Dev (s)\": result[\"stdev\"],                 \"Speedup\": speedup,             }         )      return pd.DataFrame(timing_data) In\u00a0[\u00a0]: Copied! <pre>def extract_comparison_data(results_df: pd.DataFrame) -&gt; tuple:\n    \"\"\"\n    Extract data for comparison plotting from results dataframe.\n\n    Args:\n        results_df: DataFrame containing results from both libraries\n\n    Returns:\n        tuple: (samplers, qmcpy_errors, qmcpy_times, quantlib_errors,\n                quantlib_times, theoretical_mean)\n    \"\"\"\n    qmcpy_data = results_df[results_df[\"Method\"] == \"QMCPy\"].copy()\n    quantlib_data = results_df[results_df[\"Method\"] == \"QuantLib\"].copy()\n    theoretical_data = results_df[results_df[\"Method\"] == \"Theoretical\"].copy()\n\n    samplers = qmcpy_data[\"Sampler\"].values\n    qmcpy_errors = qmcpy_data[\"Mean Absolute Error\"].values\n    qmcpy_times = (\n        qmcpy_data[\"Mean Time (s)\"].values\n        if \"Mean Time (s)\" in qmcpy_data.columns\n        else None\n    )\n\n    # Get QuantLib data (only available for some samplers\n    ql_error_dict = dict(\n        zip(quantlib_data[\"Sampler\"], quantlib_data[\"Mean Absolute Error\"])\n    )\n    quantlib_errors = [ql_error_dict.get(s) for s in samplers]\n\n    if \"Mean Time (s)\" in quantlib_data.columns:\n        ql_time_dict = dict(\n            zip(quantlib_data[\"Sampler\"], quantlib_data[\"Mean Time (s)\"])\n        )\n        quantlib_times = [ql_time_dict.get(s) for s in samplers]\n    else:\n        quantlib_times = [None] * len(samplers)\n\n    # Handle case where theoretical data might be missing\n    if not theoretical_data.empty:\n        theoretical_mean = theoretical_data[\"Mean\"].iloc[0]\n    else:\n        # Calculate theoretical mean from parameters if not in results_df\n        # Using the parameters from the comparison study\n        S0, mu, T = 100, 0.05, 1.0\n        theoretical_mean = S0 * np.exp(mu * T)\n\n    return (\n        samplers,\n        qmcpy_errors,\n        qmcpy_times,\n        quantlib_errors,\n        quantlib_times,\n        theoretical_mean,\n    )\n</pre> def extract_comparison_data(results_df: pd.DataFrame) -&gt; tuple:     \"\"\"     Extract data for comparison plotting from results dataframe.      Args:         results_df: DataFrame containing results from both libraries      Returns:         tuple: (samplers, qmcpy_errors, qmcpy_times, quantlib_errors,                 quantlib_times, theoretical_mean)     \"\"\"     qmcpy_data = results_df[results_df[\"Method\"] == \"QMCPy\"].copy()     quantlib_data = results_df[results_df[\"Method\"] == \"QuantLib\"].copy()     theoretical_data = results_df[results_df[\"Method\"] == \"Theoretical\"].copy()      samplers = qmcpy_data[\"Sampler\"].values     qmcpy_errors = qmcpy_data[\"Mean Absolute Error\"].values     qmcpy_times = (         qmcpy_data[\"Mean Time (s)\"].values         if \"Mean Time (s)\" in qmcpy_data.columns         else None     )      # Get QuantLib data (only available for some samplers     ql_error_dict = dict(         zip(quantlib_data[\"Sampler\"], quantlib_data[\"Mean Absolute Error\"])     )     quantlib_errors = [ql_error_dict.get(s) for s in samplers]      if \"Mean Time (s)\" in quantlib_data.columns:         ql_time_dict = dict(             zip(quantlib_data[\"Sampler\"], quantlib_data[\"Mean Time (s)\"])         )         quantlib_times = [ql_time_dict.get(s) for s in samplers]     else:         quantlib_times = [None] * len(samplers)      # Handle case where theoretical data might be missing     if not theoretical_data.empty:         theoretical_mean = theoretical_data[\"Mean\"].iloc[0]     else:         # Calculate theoretical mean from parameters if not in results_df         # Using the parameters from the comparison study         S0, mu, T = 100, 0.05, 1.0         theoretical_mean = S0 * np.exp(mu * T)      return (         samplers,         qmcpy_errors,         qmcpy_times,         quantlib_errors,         quantlib_times,         theoretical_mean,     ) In\u00a0[\u00a0]: Copied! <pre>def add_theoretical_row(\n    results: list,\n    series_name: str,\n    n_steps: int,\n    n_paths: int,\n    theoretical_mean: float,\n    theoretical_std: float,\n) -&gt; None:\n    \"\"\"Add theoretical benchmark row to results\"\"\"\n    results.append(\n        {\n            \"Series\": series_name,\n            \"n_steps\": n_steps,\n            \"n_paths\": n_paths,\n            \"Method\": \"Theoretical\",\n            \"Sampler\": \"-\",\n            \"Mean\": theoretical_mean,\n            \"Std Dev\": theoretical_std,\n            \"Mean Absolute Error\": 0,\n            \"Std Dev Error\": 0,\n            \"Runtime (s)\": 0,\n            \"Runtime Std (s)\": 0,\n        }\n    )\n</pre> def add_theoretical_row(     results: list,     series_name: str,     n_steps: int,     n_paths: int,     theoretical_mean: float,     theoretical_std: float, ) -&gt; None:     \"\"\"Add theoretical benchmark row to results\"\"\"     results.append(         {             \"Series\": series_name,             \"n_steps\": n_steps,             \"n_paths\": n_paths,             \"Method\": \"Theoretical\",             \"Sampler\": \"-\",             \"Mean\": theoretical_mean,             \"Std Dev\": theoretical_std,             \"Mean Absolute Error\": 0,             \"Std Dev Error\": 0,             \"Runtime (s)\": 0,             \"Runtime Std (s)\": 0,         }     ) In\u00a0[\u00a0]: Copied! <pre>def collect_library_results(\n    sampler: str,\n    series_name: str,\n    n_steps: int,\n    n_paths: int,\n    ql_timing: dict,\n    qp_timing: dict,\n    theoretical_mean: float,\n    theoretical_std: float,\n) -&gt; list:\n    \"\"\"Collect results for both QuantLib and QMCPy for a given sampler\"\"\"\n    results = []\n    gbm_params = cf.get_gbm_parameters()\n\n    # QuantLib parameters\n    ql_params = {**gbm_params, \"n_steps\": n_steps, \"n_paths\": n_paths}\n\n    # QMCPy parameters (note: diffusion = sigma^2)\n    qp_params = {\n        \"initial_value\": gbm_params[\"initial_value\"],\n        \"mu\": gbm_params[\"mu\"],\n        \"diffusion\": gbm_params[\"sigma\"] ** 2,  # Convert sigma to diffusion\n        \"maturity\": gbm_params[\"maturity\"],\n        \"n_steps\": n_steps,\n        \"n_paths\": n_paths,\n    }\n\n    # QuantLib results (if supported)\n    if sampler in cf.get_sampler_configurations()[\"quantlib_samplers\"]:\n        try:\n            ql_paths, _ = qlu.generate_quantlib_paths(sampler_type=sampler, **ql_params)\n            ql_final = ql_paths[:, -1]\n            ql_mean = np.mean(ql_final)\n            ql_std = np.std(ql_final, ddof=1)\n\n            results.append(\n                {\n                    \"Series\": series_name,\n                    \"n_steps\": n_steps,\n                    \"n_paths\": n_paths,\n                    \"Method\": \"QuantLib\",\n                    \"Sampler\": sampler,\n                    \"Mean\": ql_mean,\n                    \"Std Dev\": ql_std,\n                    \"Mean Absolute Error\": abs(ql_mean - theoretical_mean),\n                    \"Std Dev Error\": abs(ql_std - theoretical_std),\n                    \"Runtime (s)\": ql_timing[sampler][\"average\"],\n                    \"Runtime Std (s)\": ql_timing[sampler][\"stdev\"],\n                }\n            )\n        except Exception as e:\n            print(f\"      QuantLib {sampler} failed: {e}\")\n\n    # QMCPy results\n    try:\n        qp_paths, _ = qpu.generate_qmcpy_paths(sampler_type=sampler, **qp_params)\n        qp_final = qp_paths[:, -1]\n        qp_mean = np.mean(qp_final)\n        qp_std = np.std(qp_final, ddof=1)\n\n        results.append(\n            {\n                \"Series\": series_name,\n                \"n_steps\": n_steps,\n                \"n_paths\": n_paths,\n                \"Method\": \"QMCPy\",\n                \"Sampler\": sampler,\n                \"Mean\": qp_mean,\n                \"Std Dev\": qp_std,\n                \"Mean Absolute Error\": abs(qp_mean - theoretical_mean),\n                \"Std Dev Error\": abs(qp_std - theoretical_std),\n                \"Runtime (s)\": qp_timing[sampler][\"average\"],\n                \"Runtime Std (s)\": qp_timing[sampler][\"stdev\"],\n            }\n        )\n    except Exception as e:\n        print(f\"      QMCPy {sampler} failed: {e}\")\n\n    return results\n</pre> def collect_library_results(     sampler: str,     series_name: str,     n_steps: int,     n_paths: int,     ql_timing: dict,     qp_timing: dict,     theoretical_mean: float,     theoretical_std: float, ) -&gt; list:     \"\"\"Collect results for both QuantLib and QMCPy for a given sampler\"\"\"     results = []     gbm_params = cf.get_gbm_parameters()      # QuantLib parameters     ql_params = {**gbm_params, \"n_steps\": n_steps, \"n_paths\": n_paths}      # QMCPy parameters (note: diffusion = sigma^2)     qp_params = {         \"initial_value\": gbm_params[\"initial_value\"],         \"mu\": gbm_params[\"mu\"],         \"diffusion\": gbm_params[\"sigma\"] ** 2,  # Convert sigma to diffusion         \"maturity\": gbm_params[\"maturity\"],         \"n_steps\": n_steps,         \"n_paths\": n_paths,     }      # QuantLib results (if supported)     if sampler in cf.get_sampler_configurations()[\"quantlib_samplers\"]:         try:             ql_paths, _ = qlu.generate_quantlib_paths(sampler_type=sampler, **ql_params)             ql_final = ql_paths[:, -1]             ql_mean = np.mean(ql_final)             ql_std = np.std(ql_final, ddof=1)              results.append(                 {                     \"Series\": series_name,                     \"n_steps\": n_steps,                     \"n_paths\": n_paths,                     \"Method\": \"QuantLib\",                     \"Sampler\": sampler,                     \"Mean\": ql_mean,                     \"Std Dev\": ql_std,                     \"Mean Absolute Error\": abs(ql_mean - theoretical_mean),                     \"Std Dev Error\": abs(ql_std - theoretical_std),                     \"Runtime (s)\": ql_timing[sampler][\"average\"],                     \"Runtime Std (s)\": ql_timing[sampler][\"stdev\"],                 }             )         except Exception as e:             print(f\"      QuantLib {sampler} failed: {e}\")      # QMCPy results     try:         qp_paths, _ = qpu.generate_qmcpy_paths(sampler_type=sampler, **qp_params)         qp_final = qp_paths[:, -1]         qp_mean = np.mean(qp_final)         qp_std = np.std(qp_final, ddof=1)          results.append(             {                 \"Series\": series_name,                 \"n_steps\": n_steps,                 \"n_paths\": n_paths,                 \"Method\": \"QMCPy\",                 \"Sampler\": sampler,                 \"Mean\": qp_mean,                 \"Std Dev\": qp_std,                 \"Mean Absolute Error\": abs(qp_mean - theoretical_mean),                 \"Std Dev Error\": abs(qp_std - theoretical_std),                 \"Runtime (s)\": qp_timing[sampler][\"average\"],                 \"Runtime Std (s)\": qp_timing[sampler][\"stdev\"],             }         )     except Exception as e:         print(f\"      QMCPy {sampler} failed: {e}\")      return results"},{"location":"demos/GBM/gbm_code/gbm_iid_32/","title":"Gbm iid 32","text":"In\u00a0[\u00a0]: Copied! <pre>gbm_iid = plot_paths(\n    \"GBM\",\n    qp.IIDStdUniform(2**8, seed=42),\n    t_final=5,\n    initial_value=50,\n    drift=0.1,\n    diffusion=0.2,\n    n=32,\n)\n</pre> gbm_iid = plot_paths(     \"GBM\",     qp.IIDStdUniform(2**8, seed=42),     t_final=5,     initial_value=50,     drift=0.1,     diffusion=0.2,     n=32, )"},{"location":"demos/GBM/gbm_code/gbm_lattice_32/","title":"Gbm lattice 32","text":"In\u00a0[\u00a0]: Copied! <pre>gbm_lattice = plot_paths(\n    \"GBM\",\n    qp.Lattice(2**8, seed=42),\n    t_final=5,\n    initial_value=50,\n    drift=0.1,\n    diffusion=0.2,\n    n=32,\n)\n</pre> gbm_lattice = plot_paths(     \"GBM\",     qp.Lattice(2**8, seed=42),     t_final=5,     initial_value=50,     drift=0.1,     diffusion=0.2,     n=32, )"},{"location":"demos/GBM/gbm_code/gbm_moments_validation/","title":"Gbm moments validation","text":"In\u00a0[\u00a0]: Copied! <pre># Generate GBM samples for theoretical validation\nimport qmcpy as qp\nimport numpy as np\n</pre> # Generate GBM samples for theoretical validation import qmcpy as qp import numpy as np In\u00a0[\u00a0]: Copied! <pre>S0, mu, sigma, T, n_samples = 100.0, 0.05, 0.20, 1.0, 2**12\ndiffusion = sigma**2\nsampler = qp.Lattice(5, seed=42)\nqp_gbm = qp.GeometricBrownianMotion(\n    sampler, t_final=T, initial_value=S0, drift=mu, diffusion=diffusion\n)\npaths = qp_gbm.gen_samples(n_samples)\nS_T = paths[:, -1]  # Final values only\n</pre> S0, mu, sigma, T, n_samples = 100.0, 0.05, 0.20, 1.0, 2**12 diffusion = sigma**2 sampler = qp.Lattice(5, seed=42) qp_gbm = qp.GeometricBrownianMotion(     sampler, t_final=T, initial_value=S0, drift=mu, diffusion=diffusion ) paths = qp_gbm.gen_samples(n_samples) S_T = paths[:, -1]  # Final values only In\u00a0[\u00a0]: Copied! <pre># Calculate theoretical vs empirical sample moments\ntheo_mean = S0 * np.exp(mu * T)\ntheo_var = S0**2 * np.exp(2 * mu * T) * (np.exp(diffusion * T) - 1)\nqp_emp_mean = np.mean(S_T)\nqp_emp_var = np.var(S_T, ddof=1)\nprint(f\"Mean: {qp_emp_mean:.3f} (theoretical: {theo_mean:.3f})\")\nprint(f\"Variance: {qp_emp_var:.3f} (theoretical: {theo_var:.3f})\")\nqp_gbm\n</pre> # Calculate theoretical vs empirical sample moments theo_mean = S0 * np.exp(mu * T) theo_var = S0**2 * np.exp(2 * mu * T) * (np.exp(diffusion * T) - 1) qp_emp_mean = np.mean(S_T) qp_emp_var = np.var(S_T, ddof=1) print(f\"Mean: {qp_emp_mean:.3f} (theoretical: {theo_mean:.3f})\") print(f\"Variance: {qp_emp_var:.3f} (theoretical: {theo_var:.3f})\") qp_gbm"},{"location":"demos/GBM/gbm_code/gbm_qmcpy/","title":"Gbm qmcpy","text":"In\u00a0[\u00a0]: Copied! <pre>import qmcpy as qp\n</pre> import qmcpy as qp In\u00a0[\u00a0]: Copied! <pre>qp_gbm = qp.GeometricBrownianMotion(qp.Lattice(2, seed=42))\nqp_gbm.gen_samples(n=4)\n</pre> qp_gbm = qp.GeometricBrownianMotion(qp.Lattice(2, seed=42)) qp_gbm.gen_samples(n=4)"},{"location":"demos/GBM/gbm_code/latex_util/","title":"Latex util","text":"In\u00a0[\u00a0]: Copied! <pre>import pandas as pd\n</pre> import pandas as pd In\u00a0[\u00a0]: Copied! <pre>def format_number(x: float) -&gt; str:\n    \"\"\"Custom formatting function to avoid unnecessary trailing zeros\"\"\"\n    if pd.isna(x) or x == \"-\":\n        return \"-\"\n    try:\n        num = float(x)\n        formatted = f\"{num:.8f}\"\n        if \".\" not in formatted and abs(num) &lt; 1 and num != 0:\n            formatted += \".0\"\n        return formatted\n    except (ValueError, TypeError):\n        return str(x)\n</pre> def format_number(x: float) -&gt; str:     \"\"\"Custom formatting function to avoid unnecessary trailing zeros\"\"\"     if pd.isna(x) or x == \"-\":         return \"-\"     try:         num = float(x)         formatted = f\"{num:.8f}\"         if \".\" not in formatted and abs(num) &lt; 1 and num != 0:             formatted += \".0\"         return formatted     except (ValueError, TypeError):         return str(x) In\u00a0[\u00a0]: Copied! <pre>def format_results_dataframe(df: pd.DataFrame, numeric_columns: list) -&gt; pd.DataFrame:\n    \"\"\"Apply custom formatting to numeric columns in results dataframe\"\"\"\n    results_formatted = df.copy()\n    for col in numeric_columns:\n        if col in results_formatted.columns:\n            results_formatted[col] = results_formatted[col].apply(format_number)\n    return results_formatted\n</pre> def format_results_dataframe(df: pd.DataFrame, numeric_columns: list) -&gt; pd.DataFrame:     \"\"\"Apply custom formatting to numeric columns in results dataframe\"\"\"     results_formatted = df.copy()     for col in numeric_columns:         if col in results_formatted.columns:             results_formatted[col] = results_formatted[col].apply(format_number)     return results_formatted In\u00a0[\u00a0]: Copied! <pre>def generate_latex_table(df: pd.DataFrame, caption: str, label: str) -&gt; str:\n    \"\"\"Generate LaTeX table with booktabs formatting\"\"\"\n    # Create custom header\n    header = (\n        \"Method &amp; Sampler &amp; Mean &amp; Std Dev &amp; Mean Absolute &amp; Std Dev  &amp; \"\n        \"Mean Time (s)  &amp; Std Dev (s) &amp; Speedup \\\\\\\\\\n &amp;  &amp;  &amp;   &amp;  \"\n        \"Error &amp;  Error &amp;   &amp; &amp;  \\\\\\\\\"\n    )\n\n    latex_table = df.style.hide(axis=\"index\").to_latex(\n        caption=caption,\n        label=label,\n        position=\"tbp\",\n        hrules=True,\n        column_format=(\n            \"ll@{\\hspace{0.4em}}r@{\\hspace{0.4em}}r@{\\hspace{0.4em}}\"\n            \"r@{\\hspace{0.4em}}r@{\\hspace{0.4em}}r@{\\hspace{0.4em}}\"\n            \"r@{\\hspace{0.4em}}r\"\n        ),\n    )\n    # Replace default LaTeX table environment with booktabs format\n    latex_table = latex_table.replace(\n        \"\\\\begin{table}[H]\", \"\\\\begin{table}[btp]\\\\centering\"\n    )\n    latex_table = latex_table.replace(\"\\\\hline\", \"\\\\toprule\", 1)\n    latex_table = latex_table.replace(\"\\\\hline\", \"\\\\midrule\", 1)\n    latex_table = latex_table.replace(\"\\\\hline\", \"\\\\bottomrule\")\n\n    # Replace the generated header with custom header\n    lines = latex_table.split(\"\\n\")\n    for i, line in enumerate(lines):\n        if \"\\\\toprule\" in line and i + 1 &lt; len(lines):\n            # Replace the line after \\toprule with custom header\n            lines[i + 1] = header\n            break\n\n    return \"\\n\".join(lines)\n</pre> def generate_latex_table(df: pd.DataFrame, caption: str, label: str) -&gt; str:     \"\"\"Generate LaTeX table with booktabs formatting\"\"\"     # Create custom header     header = (         \"Method &amp; Sampler &amp; Mean &amp; Std Dev &amp; Mean Absolute &amp; Std Dev  &amp; \"         \"Mean Time (s)  &amp; Std Dev (s) &amp; Speedup \\\\\\\\\\n &amp;  &amp;  &amp;   &amp;  \"         \"Error &amp;  Error &amp;   &amp; &amp;  \\\\\\\\\"     )      latex_table = df.style.hide(axis=\"index\").to_latex(         caption=caption,         label=label,         position=\"tbp\",         hrules=True,         column_format=(             \"ll@{\\hspace{0.4em}}r@{\\hspace{0.4em}}r@{\\hspace{0.4em}}\"             \"r@{\\hspace{0.4em}}r@{\\hspace{0.4em}}r@{\\hspace{0.4em}}\"             \"r@{\\hspace{0.4em}}r\"         ),     )     # Replace default LaTeX table environment with booktabs format     latex_table = latex_table.replace(         \"\\\\begin{table}[H]\", \"\\\\begin{table}[btp]\\\\centering\"     )     latex_table = latex_table.replace(\"\\\\hline\", \"\\\\toprule\", 1)     latex_table = latex_table.replace(\"\\\\hline\", \"\\\\midrule\", 1)     latex_table = latex_table.replace(\"\\\\hline\", \"\\\\bottomrule\")      # Replace the generated header with custom header     lines = latex_table.split(\"\\n\")     for i, line in enumerate(lines):         if \"\\\\toprule\" in line and i + 1 &lt; len(lines):             # Replace the line after \\toprule with custom header             lines[i + 1] = header             break      return \"\\n\".join(lines)"},{"location":"demos/GBM/gbm_code/plot_util/","title":"Plot util","text":"In\u00a0[\u00a0]: Copied! <pre>import numpy as np\nimport numpy.typing as npt\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom matplotlib.axes import Axes\nfrom matplotlib.ticker import FixedLocator, FixedFormatter\nfrom typing import Optional\nimport os\nimport scipy.stats as sc\nimport qmcpy as qp\n</pre> import numpy as np import numpy.typing as npt import pandas as pd import matplotlib.pyplot as plt from matplotlib.axes import Axes from matplotlib.ticker import FixedLocator, FixedFormatter from typing import Optional import os import scipy.stats as sc import qmcpy as qp In\u00a0[\u00a0]: Copied! <pre>def plot_error_comparison(\n    ax: Axes,\n    samplers: list,\n    qmcpy_errors: npt.NDArray[np.floating],\n    quantlib_errors: list,\n    replications: Optional[int] = None,\n) -&gt; None:\n    \"\"\"\n    Plot error comparison subplot.\n\n    Args:\n        ax: Matplotlib axis object\n        samplers: List of sampler names\n        qmcpy_errors: Array of QMCPy mean absolute errors\n        quantlib_errors: List of QuantLib mean absolute errors (may contain None)\n        replications: Number of replications used for averaging (optional, for title)\n    \"\"\"\n    x = np.arange(len(samplers))\n    width = 0.35\n    # Plot QuantLib data first (left side)\n    ql_x, ql_errors = [], []\n    for i, error in enumerate(quantlib_errors):\n        if error is not None:\n            ql_x.append(i)\n            ql_errors.append(error)\n    if ql_errors:\n        ax.bar(\n            [x - width / 2 for x in ql_x],\n            ql_errors,\n            width,\n            label=\"QuantLib\",\n            color=\"blue\",\n            alpha=0.8,\n        )\n    # Plot QMCPy data second (right side)\n    ax.bar(x + width / 2, qmcpy_errors, width, label=\"QMCPy\", color=\"red\", alpha=0.8)\n\n    ax.set_xlabel(\"Sampler Type\")\n    ax.set_ylabel(\"Mean Absolute Error (log scale)\")\n\n    # Add replications info to title if provided\n    if replications is not None:\n        ax.set_title(\n            f\"Mean Absolute Error Comparison\\n(averaged over {replications} replications)\",\n            fontsize=16,\n            fontweight=\"bold\",\n        )\n    else:\n        ax.set_title(\"Mean Absolute Error Comparison\", fontsize=16, fontweight=\"bold\")\n\n    ax.set_yscale(\"log\")\n    ax.set_xticks(x)\n    ax.set_xticklabels(samplers, rotation=45, ha=\"right\")\n    ax.legend()\n    ax.grid(True, alpha=0.3)\n</pre> def plot_error_comparison(     ax: Axes,     samplers: list,     qmcpy_errors: npt.NDArray[np.floating],     quantlib_errors: list,     replications: Optional[int] = None, ) -&gt; None:     \"\"\"     Plot error comparison subplot.      Args:         ax: Matplotlib axis object         samplers: List of sampler names         qmcpy_errors: Array of QMCPy mean absolute errors         quantlib_errors: List of QuantLib mean absolute errors (may contain None)         replications: Number of replications used for averaging (optional, for title)     \"\"\"     x = np.arange(len(samplers))     width = 0.35     # Plot QuantLib data first (left side)     ql_x, ql_errors = [], []     for i, error in enumerate(quantlib_errors):         if error is not None:             ql_x.append(i)             ql_errors.append(error)     if ql_errors:         ax.bar(             [x - width / 2 for x in ql_x],             ql_errors,             width,             label=\"QuantLib\",             color=\"blue\",             alpha=0.8,         )     # Plot QMCPy data second (right side)     ax.bar(x + width / 2, qmcpy_errors, width, label=\"QMCPy\", color=\"red\", alpha=0.8)      ax.set_xlabel(\"Sampler Type\")     ax.set_ylabel(\"Mean Absolute Error (log scale)\")      # Add replications info to title if provided     if replications is not None:         ax.set_title(             f\"Mean Absolute Error Comparison\\n(averaged over {replications} replications)\",             fontsize=16,             fontweight=\"bold\",         )     else:         ax.set_title(\"Mean Absolute Error Comparison\", fontsize=16, fontweight=\"bold\")      ax.set_yscale(\"log\")     ax.set_xticks(x)     ax.set_xticklabels(samplers, rotation=45, ha=\"right\")     ax.legend()     ax.grid(True, alpha=0.3) In\u00a0[\u00a0]: Copied! <pre>def plot_performance_comparison(\n    ax: Axes,\n    samplers: list,\n    qmcpy_times: Optional[npt.NDArray[np.floating]],\n    quantlib_times: list,\n) -&gt; None:\n    \"\"\"Plot performance comparison subplot\"\"\"\n    x = np.arange(len(samplers))\n    width = 0.35\n    if qmcpy_times is not None:\n        # Plot QuantLib timing data first (left side)\n        ql_x, ql_times = [], []\n        for i, time in enumerate(quantlib_times):\n            if time is not None:\n                ql_x.append(i)\n                ql_times.append(time)\n        if ql_times:\n            ax.bar(\n                [x - width / 2 for x in ql_x],\n                ql_times,\n                width,\n                label=\"QuantLib\",\n                color=\"blue\",\n                alpha=0.8,\n            )\n        # Plot QMCPy data second (right side)\n        ax.bar(x + width / 2, qmcpy_times, width, label=\"QMCPy\", color=\"red\", alpha=0.8)\n        # Add speedup annotations where QuantLib data is available, at center of QMCPy bars\n        if len(ql_times) &gt; 0:\n            for i, (qmc_time, ql_time) in enumerate(zip(qmcpy_times, quantlib_times)):\n                if ql_time is not None:\n                    speedup = ql_time / qmc_time\n                    annotation_height = qmc_time + max(qmcpy_times) * 0.3\n                    # Position arrow at center of QMCPy bar (i + width/2)\n                    ax.annotate(\n                        f\"{speedup:.1f}x faster\",\n                        xy=(i + width / 2, qmc_time),\n                        xytext=(i + width / 2, annotation_height),\n                        ha=\"center\",\n                        va=\"bottom\",\n                        fontsize=9,\n                        fontweight=\"bold\",\n                        arrowprops=dict(arrowstyle=\"-&gt;\", color=\"blue\", lw=1),\n                    )\n        ax.set_xlabel(\"Sampler Type\")\n        ax.set_ylabel(\"Execution Time (s)\")\n        ax.set_xticks(x)\n        ax.set_xticklabels(samplers, rotation=45, ha=\"right\")\n        ax.legend()\n        ax.grid(True, alpha=0.3)\n    else:\n        ax.text(\n            0.5,\n            0.5,\n            \"Timing data not available\\nRun previous cells to generate data\",\n            ha=\"center\",\n            va=\"center\",\n            transform=ax.transAxes,\n            fontsize=12,\n        )\n\n    ax.set_title(\"Performance Comparison\", fontsize=16, fontweight=\"bold\")\n</pre> def plot_performance_comparison(     ax: Axes,     samplers: list,     qmcpy_times: Optional[npt.NDArray[np.floating]],     quantlib_times: list, ) -&gt; None:     \"\"\"Plot performance comparison subplot\"\"\"     x = np.arange(len(samplers))     width = 0.35     if qmcpy_times is not None:         # Plot QuantLib timing data first (left side)         ql_x, ql_times = [], []         for i, time in enumerate(quantlib_times):             if time is not None:                 ql_x.append(i)                 ql_times.append(time)         if ql_times:             ax.bar(                 [x - width / 2 for x in ql_x],                 ql_times,                 width,                 label=\"QuantLib\",                 color=\"blue\",                 alpha=0.8,             )         # Plot QMCPy data second (right side)         ax.bar(x + width / 2, qmcpy_times, width, label=\"QMCPy\", color=\"red\", alpha=0.8)         # Add speedup annotations where QuantLib data is available, at center of QMCPy bars         if len(ql_times) &gt; 0:             for i, (qmc_time, ql_time) in enumerate(zip(qmcpy_times, quantlib_times)):                 if ql_time is not None:                     speedup = ql_time / qmc_time                     annotation_height = qmc_time + max(qmcpy_times) * 0.3                     # Position arrow at center of QMCPy bar (i + width/2)                     ax.annotate(                         f\"{speedup:.1f}x faster\",                         xy=(i + width / 2, qmc_time),                         xytext=(i + width / 2, annotation_height),                         ha=\"center\",                         va=\"bottom\",                         fontsize=9,                         fontweight=\"bold\",                         arrowprops=dict(arrowstyle=\"-&gt;\", color=\"blue\", lw=1),                     )         ax.set_xlabel(\"Sampler Type\")         ax.set_ylabel(\"Execution Time (s)\")         ax.set_xticks(x)         ax.set_xticklabels(samplers, rotation=45, ha=\"right\")         ax.legend()         ax.grid(True, alpha=0.3)     else:         ax.text(             0.5,             0.5,             \"Timing data not available\\nRun previous cells to generate data\",             ha=\"center\",             va=\"center\",             transform=ax.transAxes,             fontsize=12,         )      ax.set_title(\"Performance Comparison\", fontsize=16, fontweight=\"bold\") In\u00a0[\u00a0]: Copied! <pre>def get_plot_styling() -&gt; dict:\n    \"\"\"Define colors and markers for plotting\"\"\"\n    return {\n        \"colors\": {\n            \"QuantLib\": {\"IIDStdUniform\": \"#1f77b4\", \"Sobol\": \"#ff7f0e\"},\n            \"QMCPy\": {\n                \"IIDStdUniform\": \"#2ca02c\",\n                \"Sobol\": \"#d62728\",\n                \"Lattice\": \"#9467bd\",\n                \"Halton\": \"#8c564b\",\n            },\n        },\n        \"markers\": {\n            \"QuantLib\": {\"IIDStdUniform\": \"o\", \"Sobol\": \"s\"},\n            \"QMCPy\": {\n                \"IIDStdUniform\": \"^\",\n                \"Sobol\": \"v\",\n                \"Lattice\": \"D\",\n                \"Halton\": \"p\",\n            },\n        },\n    }\n</pre> def get_plot_styling() -&gt; dict:     \"\"\"Define colors and markers for plotting\"\"\"     return {         \"colors\": {             \"QuantLib\": {\"IIDStdUniform\": \"#1f77b4\", \"Sobol\": \"#ff7f0e\"},             \"QMCPy\": {                 \"IIDStdUniform\": \"#2ca02c\",                 \"Sobol\": \"#d62728\",                 \"Lattice\": \"#9467bd\",                 \"Halton\": \"#8c564b\",             },         },         \"markers\": {             \"QuantLib\": {\"IIDStdUniform\": \"o\", \"Sobol\": \"s\"},             \"QMCPy\": {                 \"IIDStdUniform\": \"^\",                 \"Sobol\": \"v\",                 \"Lattice\": \"D\",                 \"Halton\": \"p\",             },         },     } In\u00a0[\u00a0]: Copied! <pre>def plot_single_series(\n    ax: Axes,\n    plot_data: pd.DataFrame,\n    series_name: str,\n    x_col: str,\n    y_col: str,\n    title: str,\n    xlabel: str,\n    ylabel: str,\n    log_scale: bool = False,\n    is_legend: bool = False,\n) -&gt; None:\n    \"\"\"Plot a single series (runtime or error) for one experimental series\"\"\"\n    series_data = plot_data[plot_data[\"Series\"] == series_name]\n    styling = get_plot_styling()\n\n    # Collect all unique x values from the experiments\n    all_x_values = sorted(series_data[x_col].unique())\n\n    for method in [\"QuantLib\", \"QMCPy\"]:\n        method_data = series_data[series_data[\"Method\"] == method]\n        colors = styling[\"colors\"][method]\n        markers = styling[\"markers\"][method]\n\n        # Cache unique samplers to avoid recomputation\n        unique_samplers = method_data[\"Sampler\"].unique()\n\n        for sampler in unique_samplers:\n            sampler_data = method_data[method_data[\"Sampler\"] == sampler].sort_values(\n                x_col\n            )\n\n            if len(sampler_data) &gt; 0:\n                x_vals = sampler_data[x_col].values\n                y_vals = sampler_data[y_col].values\n\n                color = colors.get(sampler, \"#000000\")\n                marker = markers.get(sampler, \"o\")\n\n                # Plot with connecting lines for trend visualization\n                if log_scale:\n                    ax.loglog(\n                        x_vals,\n                        y_vals,\n                        marker=marker,\n                        color=color,\n                        linewidth=2,\n                        markersize=8,\n                        label=f\"{method} - {sampler}\",\n                    )\n                else:\n                    ax.semilogy(\n                        x_vals,\n                        y_vals,\n                        marker=marker,\n                        color=color,\n                        linewidth=2,\n                        markersize=8,\n                        label=f\"{method} - {sampler}\",\n                    )\n\n    # Set x-axis ticks to show only exact experimental values\n    # Pre-compute tick labels once to avoid redundant string conversions\n    tick_labels = [str(int(x)) for x in all_x_values]\n    ax.set_xticks(all_x_values)\n    ax.set_xticklabels(tick_labels)\n\n    # Disable minor ticks to prevent intermediate values from showing\n    ax.tick_params(axis=\"x\", which=\"minor\", bottom=False)\n\n    # For log plots, we need to explicitly control the x-axis formatter\n    if log_scale:\n        ax.xaxis.set_major_locator(FixedLocator(all_x_values))\n        ax.xaxis.set_major_formatter(FixedFormatter(tick_labels))\n        ax.xaxis.set_minor_locator(FixedLocator([]))  # Remove minor ticks\n\n    ax.set_xlabel(xlabel, fontsize=12, fontweight=\"bold\")\n    ax.set_ylabel(ylabel, fontsize=12, fontweight=\"bold\")\n    ax.set_title(title, fontsize=14, fontweight=\"bold\")\n    ax.grid(True, alpha=0.3)\n    if is_legend:\n        ax.legend(fontsize=10)\n</pre> def plot_single_series(     ax: Axes,     plot_data: pd.DataFrame,     series_name: str,     x_col: str,     y_col: str,     title: str,     xlabel: str,     ylabel: str,     log_scale: bool = False,     is_legend: bool = False, ) -&gt; None:     \"\"\"Plot a single series (runtime or error) for one experimental series\"\"\"     series_data = plot_data[plot_data[\"Series\"] == series_name]     styling = get_plot_styling()      # Collect all unique x values from the experiments     all_x_values = sorted(series_data[x_col].unique())      for method in [\"QuantLib\", \"QMCPy\"]:         method_data = series_data[series_data[\"Method\"] == method]         colors = styling[\"colors\"][method]         markers = styling[\"markers\"][method]          # Cache unique samplers to avoid recomputation         unique_samplers = method_data[\"Sampler\"].unique()          for sampler in unique_samplers:             sampler_data = method_data[method_data[\"Sampler\"] == sampler].sort_values(                 x_col             )              if len(sampler_data) &gt; 0:                 x_vals = sampler_data[x_col].values                 y_vals = sampler_data[y_col].values                  color = colors.get(sampler, \"#000000\")                 marker = markers.get(sampler, \"o\")                  # Plot with connecting lines for trend visualization                 if log_scale:                     ax.loglog(                         x_vals,                         y_vals,                         marker=marker,                         color=color,                         linewidth=2,                         markersize=8,                         label=f\"{method} - {sampler}\",                     )                 else:                     ax.semilogy(                         x_vals,                         y_vals,                         marker=marker,                         color=color,                         linewidth=2,                         markersize=8,                         label=f\"{method} - {sampler}\",                     )      # Set x-axis ticks to show only exact experimental values     # Pre-compute tick labels once to avoid redundant string conversions     tick_labels = [str(int(x)) for x in all_x_values]     ax.set_xticks(all_x_values)     ax.set_xticklabels(tick_labels)      # Disable minor ticks to prevent intermediate values from showing     ax.tick_params(axis=\"x\", which=\"minor\", bottom=False)      # For log plots, we need to explicitly control the x-axis formatter     if log_scale:         ax.xaxis.set_major_locator(FixedLocator(all_x_values))         ax.xaxis.set_major_formatter(FixedFormatter(tick_labels))         ax.xaxis.set_minor_locator(FixedLocator([]))  # Remove minor ticks      ax.set_xlabel(xlabel, fontsize=12, fontweight=\"bold\")     ax.set_ylabel(ylabel, fontsize=12, fontweight=\"bold\")     ax.set_title(title, fontsize=14, fontweight=\"bold\")     ax.grid(True, alpha=0.3)     if is_legend:         ax.legend(fontsize=10) In\u00a0[\u00a0]: Copied! <pre>def create_parameter_sweep_plots(df: pd.DataFrame, replications: int) -&gt; None:\n    \"\"\"Create 4-panel plots from parameter sweep data\"\"\"\n    # Filter out theoretical data\n    plot_data = df[df[\"Method\"] != \"Theoretical\"].copy()\n\n    # Create figure with 2x2 subplots\n    _, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))\n    # Panel 1: Mean Absolute Error vs n_steps (upper left)\n    plot_single_series(\n        ax1,\n        plot_data,\n        \"Time Steps\",\n        \"n_steps\",\n        \"Mean Absolute Error\",\n        f\"Mean Absolute Error vs Number of Time Steps across {replications} Replications\\n(n_paths = 4,096)\",\n        \"Number of Time Steps\",\n        \"Mean Absolute Error\",\n        log_scale=True,\n    )\n\n    # Panel 2: Runtime vs n_steps (upper right)\n    plot_single_series(\n        ax2,\n        plot_data,\n        \"Time Steps\",\n        \"n_steps\",\n        \"Runtime (s)\",\n        f\"Runtime vs Number of Time Steps\\n(n_paths = 4,096)\",\n        \"Number of Time Steps\",\n        \"Runtime (seconds)\",\n        log_scale=True,\n        is_legend=True,\n    )\n\n    # Panel 3: Mean Absolute Error vs n_paths (lower left)\n    plot_single_series(\n        ax3,\n        plot_data,\n        \"Paths\",\n        \"n_paths\",\n        \"Mean Absolute Error\",\n        f\"Mean Absolute Error vs Number of Paths across {replications} Replications\\n(n_steps = 252)\",\n        \"Number of Paths\",\n        \"Mean Absolute Error\",\n        log_scale=True,\n    )\n\n    # Panel 4: Runtime vs n_paths (lower right)\n    plot_single_series(\n        ax4,\n        plot_data,\n        \"Paths\",\n        \"n_paths\",\n        \"Runtime (s)\",\n        f\"Runtime vs Number of Paths\\n(n_steps = 252)\",\n        \"Number of Paths\",\n        \"Runtime (seconds)\",\n        log_scale=True,\n    )\n</pre> def create_parameter_sweep_plots(df: pd.DataFrame, replications: int) -&gt; None:     \"\"\"Create 4-panel plots from parameter sweep data\"\"\"     # Filter out theoretical data     plot_data = df[df[\"Method\"] != \"Theoretical\"].copy()      # Create figure with 2x2 subplots     _, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))     # Panel 1: Mean Absolute Error vs n_steps (upper left)     plot_single_series(         ax1,         plot_data,         \"Time Steps\",         \"n_steps\",         \"Mean Absolute Error\",         f\"Mean Absolute Error vs Number of Time Steps across {replications} Replications\\n(n_paths = 4,096)\",         \"Number of Time Steps\",         \"Mean Absolute Error\",         log_scale=True,     )      # Panel 2: Runtime vs n_steps (upper right)     plot_single_series(         ax2,         plot_data,         \"Time Steps\",         \"n_steps\",         \"Runtime (s)\",         f\"Runtime vs Number of Time Steps\\n(n_paths = 4,096)\",         \"Number of Time Steps\",         \"Runtime (seconds)\",         log_scale=True,         is_legend=True,     )      # Panel 3: Mean Absolute Error vs n_paths (lower left)     plot_single_series(         ax3,         plot_data,         \"Paths\",         \"n_paths\",         \"Mean Absolute Error\",         f\"Mean Absolute Error vs Number of Paths across {replications} Replications\\n(n_steps = 252)\",         \"Number of Paths\",         \"Mean Absolute Error\",         log_scale=True,     )      # Panel 4: Runtime vs n_paths (lower right)     plot_single_series(         ax4,         plot_data,         \"Paths\",         \"n_paths\",         \"Runtime (s)\",         f\"Runtime vs Number of Paths\\n(n_steps = 252)\",         \"Number of Paths\",         \"Runtime (seconds)\",         log_scale=True,     ) In\u00a0[\u00a0]: Copied! <pre>def plot_paths(\n    motion_type: str,\n    sampler,\n    t_final: float,\n    initial_value: float,\n    drift: float,\n    diffusion: float,\n    n: int,\n    png_filename: Optional[str] = None,\n):\n    \"\"\"\n    Plot realizations of Brownian Motion or Geometric Brownian Motion.\n\n    Args:\n        motion_type: 'BM' for Brownian Motion or 'GBM' for Geometric Brownian Motion\n        sampler: QMCPy sampler instance\n        t_final: Final time point\n        initial_value: Initial value S(0)\n        drift: Drift coefficient (mu)\n        diffusion: Diffusion coefficient (sigma^2 for GBM, sigma for BM)\n        n: Number of paths to generate\n        png_filename: Optional filename to save plot (saved to images/ directory)\n\n    Returns:\n        Motion object used for generation\n    \"\"\"\n    if motion_type.upper() == \"BM\":\n        motion = qp.BrownianMotion(sampler, t_final, initial_value, drift, diffusion)\n        title = (\n            f\"Realizations of Brownian Motion using \" f\"{type(sampler).__name__} points\"\n        )\n        ylabel = \"$W(t)$\"\n    elif motion_type.upper() == \"GBM\":\n        motion = qp.GeometricBrownianMotion(\n            sampler, t_final, initial_value, drift, diffusion\n        )\n        title = (\n            f\"Realizations of Geometric Brownian Motion using \"\n            f\"{type(sampler).__name__} points\"\n        )\n        ylabel = \"$S(t)$\"\n    else:\n        raise ValueError(\"motion_type must be 'BM' or 'GBM'\")\n\n    t = motion.gen_samples(n)\n    initial_values = np.full((n, 1), motion.initial_value)\n    t_w_init = np.hstack((initial_values, t))\n    tvec_w_0 = np.hstack(([0], motion.time_vec))\n\n    plt.figure(figsize=(7, 4))\n    plt.plot(tvec_w_0, t_w_init.T)\n    plt.title(title)\n    plt.xlabel(\"$t$\")\n    plt.ylabel(ylabel)\n    plt.xlim([tvec_w_0[0], tvec_w_0[-1]])\n    if png_filename:\n        os.makedirs(\"images\", exist_ok=True)\n        plt.savefig(f\"images/{png_filename}.png\", bbox_inches=\"tight\")\n    plt.show()\n\n    return motion\n</pre> def plot_paths(     motion_type: str,     sampler,     t_final: float,     initial_value: float,     drift: float,     diffusion: float,     n: int,     png_filename: Optional[str] = None, ):     \"\"\"     Plot realizations of Brownian Motion or Geometric Brownian Motion.      Args:         motion_type: 'BM' for Brownian Motion or 'GBM' for Geometric Brownian Motion         sampler: QMCPy sampler instance         t_final: Final time point         initial_value: Initial value S(0)         drift: Drift coefficient (mu)         diffusion: Diffusion coefficient (sigma^2 for GBM, sigma for BM)         n: Number of paths to generate         png_filename: Optional filename to save plot (saved to images/ directory)      Returns:         Motion object used for generation     \"\"\"     if motion_type.upper() == \"BM\":         motion = qp.BrownianMotion(sampler, t_final, initial_value, drift, diffusion)         title = (             f\"Realizations of Brownian Motion using \" f\"{type(sampler).__name__} points\"         )         ylabel = \"$W(t)$\"     elif motion_type.upper() == \"GBM\":         motion = qp.GeometricBrownianMotion(             sampler, t_final, initial_value, drift, diffusion         )         title = (             f\"Realizations of Geometric Brownian Motion using \"             f\"{type(sampler).__name__} points\"         )         ylabel = \"$S(t)$\"     else:         raise ValueError(\"motion_type must be 'BM' or 'GBM'\")      t = motion.gen_samples(n)     initial_values = np.full((n, 1), motion.initial_value)     t_w_init = np.hstack((initial_values, t))     tvec_w_0 = np.hstack(([0], motion.time_vec))      plt.figure(figsize=(7, 4))     plt.plot(tvec_w_0, t_w_init.T)     plt.title(title)     plt.xlabel(\"$t$\")     plt.ylabel(ylabel)     plt.xlim([tvec_w_0[0], tvec_w_0[-1]])     if png_filename:         os.makedirs(\"images\", exist_ok=True)         plt.savefig(f\"images/{png_filename}.png\", bbox_inches=\"tight\")     plt.show()      return motion In\u00a0[\u00a0]: Copied! <pre>def plot_gbm_paths_with_distribution(\n    N: int,\n    sampler,\n    t_final: float,\n    initial_value: float,\n    drift: float,\n    diffusion: float,\n    n: int,\n) -&gt; None:\n    \"\"\"\n    Plot GBM paths with distribution of final values.\n\n    Combines path visualization with histogram and fitted lognormal distribution.\n\n    Args:\n        N: Number of simulations (for display purposes)\n        sampler: QMCPy sampler instance\n        t_final: Final time point\n        initial_value: Initial value S(0)\n        drift: Drift coefficient (mu)\n        diffusion: Diffusion coefficient (sigma^2)\n        n: Power of 2 for number of paths (generates 2^n paths)\n    \"\"\"\n    gbm = qp.GeometricBrownianMotion(\n        sampler,\n        t_final=t_final,\n        initial_value=initial_value,\n        drift=drift,\n        diffusion=diffusion,\n    )\n    gbm_path = gbm.gen_samples(2**n)\n\n    _, ax = plt.subplots(figsize=(14, 7))\n    T = max(gbm.time_vec)\n\n    # Plot GBM paths\n    ax.plot(gbm.time_vec, gbm_path.T, lw=0.75, alpha=0.7, color=\"skyblue\")\n\n    # Set up main plot\n    ax.set_title(\n        f\"Geometric Brownian Motion Paths\\n\"\n        f\"{N} Simulations, T = {T}, $\\\\mu$ = {drift:.1f}, \"\n        f\"$\\\\sigma$ = {diffusion:.1f}, using {type(sampler).__name__} points\"\n    )\n    ax.set_xlabel(r\"$t$\")\n    ax.set_ylabel(r\"$S(t)$\")\n    ax.set_ylim(bottom=0)\n    ax.set_xlim(0, T)\n\n    # Add histogram\n    final_values = gbm_path[:, -1]\n    hist_ax = ax.inset_axes([1.05, 0.0, 0.5, 1])\n    hist_ax.hist(\n        final_values,\n        bins=20,\n        density=True,\n        alpha=0.5,\n        color=\"skyblue\",\n        orientation=\"horizontal\",\n    )\n\n    # Add theoretical lognormal PDF\n    shape, _, scale = sc.lognorm.fit(final_values, floc=0)\n    x = np.linspace(0, max(final_values), 1000)\n    pdf = sc.lognorm.pdf(x, shape, loc=0, scale=scale)\n    hist_ax.plot(pdf, x, \"r-\", lw=2, label=\"Lognormal PDF\")\n\n    # Finalize histogram\n    hist_ax.set_title(f\"E[$S_T$] = {np.mean(final_values):.4f}\", pad=20)\n    hist_ax.axhline(\n        np.mean(final_values), color=\"blue\", linestyle=\"--\", lw=1.5, label=r\"$E[S_T]$\"\n    )\n    hist_ax.set_yticks([])\n    hist_ax.set_xlabel(\"Density\")\n    hist_ax.legend()\n    hist_ax.set_ylim(bottom=0)\n    plt.tight_layout()\n    plt.show()\n</pre> def plot_gbm_paths_with_distribution(     N: int,     sampler,     t_final: float,     initial_value: float,     drift: float,     diffusion: float,     n: int, ) -&gt; None:     \"\"\"     Plot GBM paths with distribution of final values.      Combines path visualization with histogram and fitted lognormal distribution.      Args:         N: Number of simulations (for display purposes)         sampler: QMCPy sampler instance         t_final: Final time point         initial_value: Initial value S(0)         drift: Drift coefficient (mu)         diffusion: Diffusion coefficient (sigma^2)         n: Power of 2 for number of paths (generates 2^n paths)     \"\"\"     gbm = qp.GeometricBrownianMotion(         sampler,         t_final=t_final,         initial_value=initial_value,         drift=drift,         diffusion=diffusion,     )     gbm_path = gbm.gen_samples(2**n)      _, ax = plt.subplots(figsize=(14, 7))     T = max(gbm.time_vec)      # Plot GBM paths     ax.plot(gbm.time_vec, gbm_path.T, lw=0.75, alpha=0.7, color=\"skyblue\")      # Set up main plot     ax.set_title(         f\"Geometric Brownian Motion Paths\\n\"         f\"{N} Simulations, T = {T}, $\\\\mu$ = {drift:.1f}, \"         f\"$\\\\sigma$ = {diffusion:.1f}, using {type(sampler).__name__} points\"     )     ax.set_xlabel(r\"$t$\")     ax.set_ylabel(r\"$S(t)$\")     ax.set_ylim(bottom=0)     ax.set_xlim(0, T)      # Add histogram     final_values = gbm_path[:, -1]     hist_ax = ax.inset_axes([1.05, 0.0, 0.5, 1])     hist_ax.hist(         final_values,         bins=20,         density=True,         alpha=0.5,         color=\"skyblue\",         orientation=\"horizontal\",     )      # Add theoretical lognormal PDF     shape, _, scale = sc.lognorm.fit(final_values, floc=0)     x = np.linspace(0, max(final_values), 1000)     pdf = sc.lognorm.pdf(x, shape, loc=0, scale=scale)     hist_ax.plot(pdf, x, \"r-\", lw=2, label=\"Lognormal PDF\")      # Finalize histogram     hist_ax.set_title(f\"E[$S_T$] = {np.mean(final_values):.4f}\", pad=20)     hist_ax.axhline(         np.mean(final_values), color=\"blue\", linestyle=\"--\", lw=1.5, label=r\"$E[S_T]$\"     )     hist_ax.set_yticks([])     hist_ax.set_xlabel(\"Density\")     hist_ax.legend()     hist_ax.set_ylim(bottom=0)     plt.tight_layout()     plt.show() In\u00a0[\u00a0]: Copied! <pre>def compute_theoretical_covariance(\n    S0: float, mu: float, sigma: float, t1: float, t2: float\n) -&gt; npt.NDArray[np.floating]:\n    \"\"\"\n    Compute theoretical covariance matrix for GBM at two time points.\n\n    Args:\n        S0: Initial value\n        mu: Drift coefficient\n        sigma: Volatility\n        t1: First time point\n        t2: Second time point\n\n    Returns:\n        2x2 covariance matrix\n    \"\"\"\n    return np.array(\n        [\n            [\n                S0**2 * np.exp(2 * mu * t1) * (np.exp(sigma**2 * t1) - 1),\n                S0**2 * np.exp(mu * (t1 + t2)) * (np.exp(sigma**2 * t1) - 1),\n            ],\n            [\n                S0**2 * np.exp(mu * (t1 + t2)) * (np.exp(sigma**2 * t1) - 1),\n                S0**2 * np.exp(2 * mu * t2) * (np.exp(sigma**2 * t2) - 1),\n            ],\n        ]\n    )\n</pre> def compute_theoretical_covariance(     S0: float, mu: float, sigma: float, t1: float, t2: float ) -&gt; npt.NDArray[np.floating]:     \"\"\"     Compute theoretical covariance matrix for GBM at two time points.      Args:         S0: Initial value         mu: Drift coefficient         sigma: Volatility         t1: First time point         t2: Second time point      Returns:         2x2 covariance matrix     \"\"\"     return np.array(         [             [                 S0**2 * np.exp(2 * mu * t1) * (np.exp(sigma**2 * t1) - 1),                 S0**2 * np.exp(mu * (t1 + t2)) * (np.exp(sigma**2 * t1) - 1),             ],             [                 S0**2 * np.exp(mu * (t1 + t2)) * (np.exp(sigma**2 * t1) - 1),                 S0**2 * np.exp(2 * mu * t2) * (np.exp(sigma**2 * t2) - 1),             ],         ]     ) In\u00a0[\u00a0]: Copied! <pre>def calculate_theoretical_statistics(params: dict) -&gt; tuple[float, float]:\n    \"\"\"\n    Calculate theoretical mean and standard deviation for GBM.\n\n    Args:\n        params: Dictionary with keys 'initial_value', 'mu', 'sigma', 'maturity'\n\n    Returns:\n        tuple: (theoretical_mean, theoretical_std)\n    \"\"\"\n    theoretical_mean = params[\"initial_value\"] * np.exp(\n        params[\"mu\"] * params[\"maturity\"]\n    )\n    theoretical_std = np.sqrt(\n        params[\"initial_value\"] ** 2\n        * np.exp(2 * params[\"mu\"] * params[\"maturity\"])\n        * (np.exp(params[\"sigma\"] ** 2 * params[\"maturity\"]) - 1)\n    )\n    return theoretical_mean, theoretical_std\n</pre> def calculate_theoretical_statistics(params: dict) -&gt; tuple[float, float]:     \"\"\"     Calculate theoretical mean and standard deviation for GBM.      Args:         params: Dictionary with keys 'initial_value', 'mu', 'sigma', 'maturity'      Returns:         tuple: (theoretical_mean, theoretical_std)     \"\"\"     theoretical_mean = params[\"initial_value\"] * np.exp(         params[\"mu\"] * params[\"maturity\"]     )     theoretical_std = np.sqrt(         params[\"initial_value\"] ** 2         * np.exp(2 * params[\"mu\"] * params[\"maturity\"])         * (np.exp(params[\"sigma\"] ** 2 * params[\"maturity\"]) - 1)     )     return theoretical_mean, theoretical_std In\u00a0[\u00a0]: Copied! <pre>def extract_covariance_samples(\n    paths: npt.NDArray[np.floating], n_steps: int, is_quantlib: bool = True\n) -&gt; npt.NDArray[np.floating]:\n    \"\"\"\n    Extract samples at two time points and compute covariance matrix.\n\n    Args:\n        paths: Generated paths array\n        n_steps: Number of time steps\n        is_quantlib: True for QuantLib paths, False for QMCPy paths\n\n    Returns:\n        Covariance matrix for samples at two time points\n    \"\"\"\n    if is_quantlib:\n        idx1, idx2 = int(0.5 * n_steps), n_steps\n        samples_t1, samples_t2 = paths[:, idx1], paths[:, idx2]\n    else:  # QMCPy\n        idx1, idx2 = int(0.5 * (n_steps - 1)), n_steps - 1\n        samples_t1, samples_t2 = paths[:, idx1], paths[:, idx2]\n    return np.cov(np.vstack((samples_t1, samples_t2)))\n</pre> def extract_covariance_samples(     paths: npt.NDArray[np.floating], n_steps: int, is_quantlib: bool = True ) -&gt; npt.NDArray[np.floating]:     \"\"\"     Extract samples at two time points and compute covariance matrix.      Args:         paths: Generated paths array         n_steps: Number of time steps         is_quantlib: True for QuantLib paths, False for QMCPy paths      Returns:         Covariance matrix for samples at two time points     \"\"\"     if is_quantlib:         idx1, idx2 = int(0.5 * n_steps), n_steps         samples_t1, samples_t2 = paths[:, idx1], paths[:, idx2]     else:  # QMCPy         idx1, idx2 = int(0.5 * (n_steps - 1)), n_steps - 1         samples_t1, samples_t2 = paths[:, idx1], paths[:, idx2]     return np.cov(np.vstack((samples_t1, samples_t2)))"},{"location":"demos/GBM/gbm_code/qmcpy_util/","title":"Qmcpy util","text":"In\u00a0[\u00a0]: Copied! <pre>import qmcpy as qp\n</pre> import qmcpy as qp In\u00a0[\u00a0]: Copied! <pre>def create_qmcpy_sampler(\n    sampler_type: str, dimension: int, replications: int = 1, seed: int = 42\n):\n    \"\"\"\n    Create a sampler instance based on type and dimension.\n\n    Args:\n        sampler_type: Type of sampler ('IIDStdUniform', 'Sobol', 'Lattice', 'Halton')\n        dimension: Dimension of the sampler (typically n_steps)\n        replications: Number of independent replications\n        seed: Random seed for reproducibility\n\n    Returns:\n        QMCPy sampler instance\n    \"\"\"\n    if sampler_type == \"IIDStdUniform\":\n        return qp.IIDStdUniform(dimension, replications, seed=seed)\n    elif sampler_type == \"Sobol\":\n        return qp.Sobol(dimension, replications, seed=seed)\n    elif sampler_type == \"Lattice\":\n        return qp.Lattice(dimension, replications, seed=seed)\n    elif sampler_type == \"Halton\":\n        return qp.Halton(dimension, replications, seed=seed)\n    else:\n        raise ValueError(f\"Unsupported sampler type: {sampler_type}\")\n</pre> def create_qmcpy_sampler(     sampler_type: str, dimension: int, replications: int = 1, seed: int = 42 ):     \"\"\"     Create a sampler instance based on type and dimension.      Args:         sampler_type: Type of sampler ('IIDStdUniform', 'Sobol', 'Lattice', 'Halton')         dimension: Dimension of the sampler (typically n_steps)         replications: Number of independent replications         seed: Random seed for reproducibility      Returns:         QMCPy sampler instance     \"\"\"     if sampler_type == \"IIDStdUniform\":         return qp.IIDStdUniform(dimension, replications, seed=seed)     elif sampler_type == \"Sobol\":         return qp.Sobol(dimension, replications, seed=seed)     elif sampler_type == \"Lattice\":         return qp.Lattice(dimension, replications, seed=seed)     elif sampler_type == \"Halton\":         return qp.Halton(dimension, replications, seed=seed)     else:         raise ValueError(f\"Unsupported sampler type: {sampler_type}\") In\u00a0[\u00a0]: Copied! <pre>def generate_qmcpy_paths(\n    initial_value: float,\n    mu: float,\n    diffusion: float,\n    maturity: float,\n    n_steps: int,\n    n_paths: int,\n    sampler_type: str = \"IIDStdUniform\",\n    replications: int = 1,\n    seed: int = 42,\n):\n    \"\"\"\n    Generate Geometric Brownian Motion paths using QMCPy with multiple replications.\n\n    Args:\n        initial_value: Initial value of the GBM process (S_0)\n        mu: Drift parameter\n        diffusion: Diffusion coefficient (sigma^2)\n        maturity: Final time T\n        n_steps: Number of discretization time steps\n        n_paths: Number of paths to generate per replication\n        sampler_type: Type of sampler ('IIDStdUniform', 'Sobol', 'Lattice', 'Halton')\n        replications: Number of independent replications\n        seed: Random seed for reproducibility\n\n    Returns:\n        tuple: (paths, gbm) where paths has shape (n_paths, n_steps) if replications=1,\n               or (replications, n_paths, n_steps) if replications&gt;1,\n               and gbm is the GeometricBrownianMotion object\n    \"\"\"\n    sampler = create_qmcpy_sampler(sampler_type, n_steps, replications, seed)\n    gbm = qp.GeometricBrownianMotion(\n        sampler,\n        t_final=maturity,\n        initial_value=initial_value,\n        drift=mu,\n        diffusion=diffusion,\n    )\n    paths = gbm.gen_samples(n_paths)\n    return paths, gbm\n</pre> def generate_qmcpy_paths(     initial_value: float,     mu: float,     diffusion: float,     maturity: float,     n_steps: int,     n_paths: int,     sampler_type: str = \"IIDStdUniform\",     replications: int = 1,     seed: int = 42, ):     \"\"\"     Generate Geometric Brownian Motion paths using QMCPy with multiple replications.      Args:         initial_value: Initial value of the GBM process (S_0)         mu: Drift parameter         diffusion: Diffusion coefficient (sigma^2)         maturity: Final time T         n_steps: Number of discretization time steps         n_paths: Number of paths to generate per replication         sampler_type: Type of sampler ('IIDStdUniform', 'Sobol', 'Lattice', 'Halton')         replications: Number of independent replications         seed: Random seed for reproducibility      Returns:         tuple: (paths, gbm) where paths has shape (n_paths, n_steps) if replications=1,                or (replications, n_paths, n_steps) if replications&gt;1,                and gbm is the GeometricBrownianMotion object     \"\"\"     sampler = create_qmcpy_sampler(sampler_type, n_steps, replications, seed)     gbm = qp.GeometricBrownianMotion(         sampler,         t_final=maturity,         initial_value=initial_value,         drift=mu,         diffusion=diffusion,     )     paths = gbm.gen_samples(n_paths)     return paths, gbm"},{"location":"demos/GBM/gbm_code/quantlib_util/","title":"Quantlib util","text":"In\u00a0[\u00a0]: Copied! <pre>import QuantLib as ql\nimport numpy as np\n</pre> import QuantLib as ql import numpy as np In\u00a0[\u00a0]: Copied! <pre>def generate_quantlib_paths(\n    initial_value: float,\n    mu: float,\n    sigma: float,\n    maturity: float,\n    n_steps: int,\n    n_paths: int,\n    sampler_type: str = \"IIDStdUniform\",\n    seed: int = 7,\n) -&gt; tuple:\n    \"\"\"\n    Generate Geometric Brownian Motion paths using QuantLib.\n\n    Args:\n        initial_value: Initial value of the GBM process (S_0)\n        mu: Drift parameter\n        sigma: Volatility parameter (note: NOT diffusion coefficient)\n        maturity: Final time T\n        n_steps: Number of discretization time steps\n        n_paths: Number of paths to generate\n        sampler_type: Type of sampler ('IIDStdUniform' or 'Sobol')\n        seed: Random seed for reproducibility\n\n    Returns:\n        tuple: (paths, gbm) where paths has shape (n_paths, n_steps+1)\n               (includes initial value at t=0) and gbm is the\n               GeometricBrownianMotionProcess object\n\n    Raises:\n        ValueError: If sampler_type is not 'IIDStdUniform' or 'Sobol'\n    \"\"\"\n    gbm = ql.GeometricBrownianMotionProcess(initial_value, mu, sigma)\n    times = ql.TimeGrid(maturity, n_steps)\n    dimension = n_steps\n    if sampler_type == \"IIDStdUniform\":\n        uniform_rng = ql.UniformRandomGenerator(seed)\n        sequence_gen = ql.GaussianRandomSequenceGenerator(\n            ql.UniformRandomSequenceGenerator(n_steps, uniform_rng)\n        )\n        path_gen = ql.GaussianPathGenerator(gbm, maturity, n_steps, sequence_gen, False)\n        paths = np.zeros((n_paths, n_steps + 1))\n        for i in range(n_paths):\n            sample_path = path_gen.next().value()\n            paths[i, :] = np.array([sample_path[j] for j in range(n_steps + 1)])\n        return paths, gbm\n    elif sampler_type == \"Sobol\":\n        uniform_rsg = ql.UniformLowDiscrepancySequenceGenerator(dimension, seed)\n        sequence_gen = ql.GaussianLowDiscrepancySequenceGenerator(uniform_rsg)\n        path_gen = ql.GaussianSobolMultiPathGenerator(\n            gbm, list(times), sequence_gen, False\n        )\n        paths = np.zeros((n_paths, n_steps + 1))\n        paths[:, 0] = initial_value  # Set initial value\n        for i in range(n_paths):\n            sample_path = path_gen.next().value()\n            # For 1D process, get the first (and only) path\n            path_values = sample_path[0]\n            paths[i, :] = np.array([path_values[j] for j in range(n_steps + 1)])\n        return paths, gbm\n    else:\n        raise ValueError(\n            f\"Unsupported sampler type: {sampler_type}.  Use 'IIDStdUniform' or 'Sobol'\"\n        )\n</pre> def generate_quantlib_paths(     initial_value: float,     mu: float,     sigma: float,     maturity: float,     n_steps: int,     n_paths: int,     sampler_type: str = \"IIDStdUniform\",     seed: int = 7, ) -&gt; tuple:     \"\"\"     Generate Geometric Brownian Motion paths using QuantLib.      Args:         initial_value: Initial value of the GBM process (S_0)         mu: Drift parameter         sigma: Volatility parameter (note: NOT diffusion coefficient)         maturity: Final time T         n_steps: Number of discretization time steps         n_paths: Number of paths to generate         sampler_type: Type of sampler ('IIDStdUniform' or 'Sobol')         seed: Random seed for reproducibility      Returns:         tuple: (paths, gbm) where paths has shape (n_paths, n_steps+1)                (includes initial value at t=0) and gbm is the                GeometricBrownianMotionProcess object      Raises:         ValueError: If sampler_type is not 'IIDStdUniform' or 'Sobol'     \"\"\"     gbm = ql.GeometricBrownianMotionProcess(initial_value, mu, sigma)     times = ql.TimeGrid(maturity, n_steps)     dimension = n_steps     if sampler_type == \"IIDStdUniform\":         uniform_rng = ql.UniformRandomGenerator(seed)         sequence_gen = ql.GaussianRandomSequenceGenerator(             ql.UniformRandomSequenceGenerator(n_steps, uniform_rng)         )         path_gen = ql.GaussianPathGenerator(gbm, maturity, n_steps, sequence_gen, False)         paths = np.zeros((n_paths, n_steps + 1))         for i in range(n_paths):             sample_path = path_gen.next().value()             paths[i, :] = np.array([sample_path[j] for j in range(n_steps + 1)])         return paths, gbm     elif sampler_type == \"Sobol\":         uniform_rsg = ql.UniformLowDiscrepancySequenceGenerator(dimension, seed)         sequence_gen = ql.GaussianLowDiscrepancySequenceGenerator(uniform_rsg)         path_gen = ql.GaussianSobolMultiPathGenerator(             gbm, list(times), sequence_gen, False         )         paths = np.zeros((n_paths, n_steps + 1))         paths[:, 0] = initial_value  # Set initial value         for i in range(n_paths):             sample_path = path_gen.next().value()             # For 1D process, get the first (and only) path             path_values = sample_path[0]             paths[i, :] = np.array([path_values[j] for j in range(n_steps + 1)])         return paths, gbm     else:         raise ValueError(             f\"Unsupported sampler type: {sampler_type}.  Use 'IIDStdUniform' or 'Sobol'\"         )"},{"location":"demos/GBM/gbm_code/quickstart/","title":"Quickstart","text":"In\u00a0[\u00a0]: Copied! <pre>import numpy as np\nimport matplotlib.pyplot as plt\nimport qmcpy as qp\n</pre> import numpy as np import matplotlib.pyplot as plt import qmcpy as qp In\u00a0[\u00a0]: Copied! <pre>sampler = qp.Lattice(252, seed=42)  # daily steps for 1 year\ngbm = qp.GeometricBrownianMotion(\n    sampler, t_final=1, initial_value=1, drift=0.05, diffusion=0.2\n)\npaths = gbm.gen_samples(16)\nt = np.linspace(0, 1.0, paths.shape[1])\nplt.plot(t, paths[:5].T, alpha=0.8)\nplt.xlabel(\"$t$\")\nplt.ylabel(\"$S_t$\")\nplt.title(\"GBM paths\")\nplt.show()\n</pre> sampler = qp.Lattice(252, seed=42)  # daily steps for 1 year gbm = qp.GeometricBrownianMotion(     sampler, t_final=1, initial_value=1, drift=0.05, diffusion=0.2 ) paths = gbm.gen_samples(16) t = np.linspace(0, 1.0, paths.shape[1]) plt.plot(t, paths[:5].T, alpha=0.8) plt.xlabel(\"$t$\") plt.ylabel(\"$S_t$\") plt.title(\"GBM paths\") plt.show()"},{"location":"demos/gaussian_diagnostics/gaussian_diagnostics/","title":"Gaussian diagnostics","text":"In\u00a0[\u00a0]: Copied! <pre>import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.optimize import fmin as fminsearch\nfrom numpy import prod, sin, cos, pi\nfrom scipy.stats import norm as gaussnorm\nfrom matplotlib import cm\nimport os\n</pre> import numpy as np import matplotlib.pyplot as plt from scipy.optimize import fmin as fminsearch from numpy import prod, sin, cos, pi from scipy.stats import norm as gaussnorm from matplotlib import cm import os In\u00a0[\u00a0]: Copied! <pre>from qmcpy.integrand import Keister\nfrom qmcpy.discrete_distribution.lattice import Lattice\n</pre> from qmcpy.integrand import Keister from qmcpy.discrete_distribution.lattice import Lattice In\u00a0[\u00a0]: Copied! <pre># print(plt.style.available)\n# plt.style.use('./presentation.mplstyle')  # custom settings\n# plt.style.library['seaborn-darkgrid']   # Showing the style settings\nplt.style.use(\"seaborn-v0_8-notebook\")\nROOT = os.path.dirname(os.path.realpath(__file__))\nOUTDIR = ROOT + \"/outputs/\"\n</pre> # print(plt.style.available) # plt.style.use('./presentation.mplstyle')  # custom settings # plt.style.library['seaborn-darkgrid']   # Showing the style settings plt.style.use(\"seaborn-v0_8-notebook\") ROOT = os.path.dirname(os.path.realpath(__file__)) OUTDIR = ROOT + \"/outputs/\" In\u00a0[\u00a0]: Copied! <pre>def ObjectiveFunction(theta, order, xun, ftilde):\n    tol = 100 * np.finfo(float).eps\n    n = len(ftilde)\n    arbMean = True\n    Lambda = kernel2(theta, order, xun)\n\n    # compute RKHSnorm\n    # temp = abs(ftilde(Lambda  != 0).^ 2)./ (Lambda(Lambda != 0))\n    temp = abs(ftilde[Lambda &gt; tol] ** 2) / (Lambda[Lambda &gt; tol])\n\n    # compute loss: MLE\n    if arbMean == True:\n        RKHSnorm = sum(temp[1:]) / n\n        temp_1 = sum(temp[1:])\n    else:\n        RKHSnorm = sum(temp) / n\n        temp_1 = sum(temp)\n\n    # ignore all zero eigenvalues\n    loss1 = sum(np.log(Lambda[Lambda &gt; tol])) / n\n    loss2 = np.log(temp_1)\n    loss = loss1 + loss2\n    if np.imag(loss) != 0:\n        # keyboard\n        raise (\"error ! : loss value is complex\")\n\n    # print('L1 %1.3f L2 %1.3f L %1.3f r %1.3e theta %1.3e\\n'.format(loss1, loss2, loss, order, theta))\n    return loss, Lambda, RKHSnorm\n</pre> def ObjectiveFunction(theta, order, xun, ftilde):     tol = 100 * np.finfo(float).eps     n = len(ftilde)     arbMean = True     Lambda = kernel2(theta, order, xun)      # compute RKHSnorm     # temp = abs(ftilde(Lambda  != 0).^ 2)./ (Lambda(Lambda != 0))     temp = abs(ftilde[Lambda &gt; tol] ** 2) / (Lambda[Lambda &gt; tol])      # compute loss: MLE     if arbMean == True:         RKHSnorm = sum(temp[1:]) / n         temp_1 = sum(temp[1:])     else:         RKHSnorm = sum(temp) / n         temp_1 = sum(temp)      # ignore all zero eigenvalues     loss1 = sum(np.log(Lambda[Lambda &gt; tol])) / n     loss2 = np.log(temp_1)     loss = loss1 + loss2     if np.imag(loss) != 0:         # keyboard         raise (\"error ! : loss value is complex\")      # print('L1 %1.3f L2 %1.3f L %1.3f r %1.3e theta %1.3e\\n'.format(loss1, loss2, loss, order, theta))     return loss, Lambda, RKHSnorm In\u00a0[\u00a0]: Copied! <pre>def kernel2(theta, r, xun):\n    n = xun.shape[0]\n    m = np.arange(1, (n / 2))\n    tilde_g_h1 = m ** (-r)\n    tilde_g = np.hstack([0, tilde_g_h1, 0, tilde_g_h1[::-1]])\n    g = np.fft.fft(tilde_g)\n    temp_ = (theta / 2) * g[(xun * n).astype(int)]\n    C1 = prod(1 + temp_, 1)\n    # eigenvalues must be real : Symmetric pos definite Kernel\n    vlambda = np.real(np.fft.fft(C1))\n    return vlambda\n</pre> def kernel2(theta, r, xun):     n = xun.shape[0]     m = np.arange(1, (n / 2))     tilde_g_h1 = m ** (-r)     tilde_g = np.hstack([0, tilde_g_h1, 0, tilde_g_h1[::-1]])     g = np.fft.fft(tilde_g)     temp_ = (theta / 2) * g[(xun * n).astype(int)]     C1 = prod(1 + temp_, 1)     # eigenvalues must be real : Symmetric pos definite Kernel     vlambda = np.real(np.fft.fft(C1))     return vlambda In\u00a0[\u00a0]: Copied! <pre># gaussian random function\ndef f_rand(xpts, rfun, a, b, c, seed):\n    dim = xpts.shape[1]\n    np.random.seed(seed)  # initialize random number generator for reproducability\n    N1 = int(2 ** np.floor(16 / dim))\n    Nall = N1**dim\n    kvec = np.zeros([dim, Nall])  # initialize kvec\n    kvec[0, 0:N1] = range(0, N1)  # first dimension\n    Nd = N1\n    for d in range(1, dim):\n        Ndm1 = Nd\n        Nd = Nd * N1\n        kvec[0 : d + 1, 0:Nd] = np.vstack(\n            [\n                np.tile(kvec[0:d, 0:Ndm1], (1, N1)),\n                np.reshape(np.tile(np.arange(0, N1), (Ndm1, 1)), (1, Nd), order=\"F\"),\n            ]\n        )\n\n    kvec = kvec[:, 1:Nall]  # remove the zero wavenumber\n    whZero = np.sum(kvec == 0, axis=0)\n    abfac = a ** (dim - whZero) * b**whZero\n    kbar = np.prod(np.maximum(kvec, 1), axis=0)\n    totfac = abfac / (kbar**rfun)\n\n    f_c = a * np.random.randn(1, Nall - 1) * totfac\n    f_s = a * np.random.randn(1, Nall - 1) * totfac\n\n    f_0 = c + (b**dim) * np.random.randn()\n    argx = np.matmul((2 * np.pi * xpts), kvec)\n    f_c_ = f_c * np.cos(argx)\n    f_s_ = f_s * np.sin(argx)\n    fval = f_0 + np.sum(f_c_ + f_s_, axis=1)\n    return fval\n</pre> # gaussian random function def f_rand(xpts, rfun, a, b, c, seed):     dim = xpts.shape[1]     np.random.seed(seed)  # initialize random number generator for reproducability     N1 = int(2 ** np.floor(16 / dim))     Nall = N1**dim     kvec = np.zeros([dim, Nall])  # initialize kvec     kvec[0, 0:N1] = range(0, N1)  # first dimension     Nd = N1     for d in range(1, dim):         Ndm1 = Nd         Nd = Nd * N1         kvec[0 : d + 1, 0:Nd] = np.vstack(             [                 np.tile(kvec[0:d, 0:Ndm1], (1, N1)),                 np.reshape(np.tile(np.arange(0, N1), (Ndm1, 1)), (1, Nd), order=\"F\"),             ]         )      kvec = kvec[:, 1:Nall]  # remove the zero wavenumber     whZero = np.sum(kvec == 0, axis=0)     abfac = a ** (dim - whZero) * b**whZero     kbar = np.prod(np.maximum(kvec, 1), axis=0)     totfac = abfac / (kbar**rfun)      f_c = a * np.random.randn(1, Nall - 1) * totfac     f_s = a * np.random.randn(1, Nall - 1) * totfac      f_0 = c + (b**dim) * np.random.randn()     argx = np.matmul((2 * np.pi * xpts), kvec)     f_c_ = f_c * np.cos(argx)     f_s_ = f_s * np.sin(argx)     fval = f_0 + np.sum(f_c_ + f_s_, axis=1)     return fval In\u00a0[\u00a0]: Copied! <pre>def doPeriodTx(x, integrand, ptransform):\n    ptransform = ptransform.upper()\n    if ptransform == \"BAKER\":  # Baker's transform\n        xp = 1 - 2 * abs(x - 1 / 2)\n        w = 1\n    elif ptransform == \"C0\":  # C^0 transform\n        xp = 3 * x**2 - 2 * x**3\n        w = prod(6 * x * (1 - x), 1)\n    elif ptransform == \"C1\":  # C^1 transform\n        xp = x**3 * (10 - 15 * x + 6 * x**2)\n        w = prod(30 * x**2 * (1 - x) ** 2, 1)\n    elif ptransform == \"C1SIN\":  # Sidi C^1 transform\n        xp = x - sin(2 * pi * x) / (2 * pi)\n        w = prod(2 * sin(pi * x) ** 2, 1)\n    elif ptransform == \"C2SIN\":  # Sidi C^2 transform\n        xp = (8 - 9 * cos(pi * x) + cos(3 * pi * x)) / 16  # psi3\n        w = prod((9 * sin(pi * x) * pi - sin(3 * pi * x) * 3 * pi) / 16, 1)  # psi3_1\n    elif ptransform == \"C3SIN\":  # Sidi C^3 transform\n        xp = (12 * pi * x - 8 * sin(2 * pi * x) + sin(4 * pi * x)) / (12 * pi)  # psi4\n        w = prod(\n            (12 * pi - 8 * cos(2 * pi * x) * 2 * pi + sin(4 * pi * x) * 4 * pi)\n            / (12 * pi),\n            1,\n        )  # psi4_1\n    elif ptransform == \"NONE\":\n        xp = x\n        w = 1\n    else:\n        raise (f\"The {ptransform} periodization transform is not implemented\")\n    y = integrand(xp) * w\n    return y\n</pre> def doPeriodTx(x, integrand, ptransform):     ptransform = ptransform.upper()     if ptransform == \"BAKER\":  # Baker's transform         xp = 1 - 2 * abs(x - 1 / 2)         w = 1     elif ptransform == \"C0\":  # C^0 transform         xp = 3 * x**2 - 2 * x**3         w = prod(6 * x * (1 - x), 1)     elif ptransform == \"C1\":  # C^1 transform         xp = x**3 * (10 - 15 * x + 6 * x**2)         w = prod(30 * x**2 * (1 - x) ** 2, 1)     elif ptransform == \"C1SIN\":  # Sidi C^1 transform         xp = x - sin(2 * pi * x) / (2 * pi)         w = prod(2 * sin(pi * x) ** 2, 1)     elif ptransform == \"C2SIN\":  # Sidi C^2 transform         xp = (8 - 9 * cos(pi * x) + cos(3 * pi * x)) / 16  # psi3         w = prod((9 * sin(pi * x) * pi - sin(3 * pi * x) * 3 * pi) / 16, 1)  # psi3_1     elif ptransform == \"C3SIN\":  # Sidi C^3 transform         xp = (12 * pi * x - 8 * sin(2 * pi * x) + sin(4 * pi * x)) / (12 * pi)  # psi4         w = prod(             (12 * pi - 8 * cos(2 * pi * x) * 2 * pi + sin(4 * pi * x) * 4 * pi)             / (12 * pi),             1,         )  # psi4_1     elif ptransform == \"NONE\":         xp = x         w = 1     else:         raise (f\"The {ptransform} periodization transform is not implemented\")     y = integrand(xp) * w     return y In\u00a0[\u00a0]: Copied! <pre>def create_plots(type, vz_real, fName, dim, iii, r, rOpt, theta, thetaOpt):\n    hFigNormplot, axFigNormplot = plt.subplots()\n\n    # plt.rcParams.update({'font.size': 12})\n    # plt.rcParams.update({'lines.linewidth': 2})\n\n    # set(hFigNormplot,'defaultaxesfontsize',16,\n    #   'defaulttextfontsize',12,   # make font larger\n    #   'defaultLineLineWidth',2, 'defaultLineMarkerSize',6)\n    n = len(vz_real)\n    if type == \"normplot\":\n        axFigNormplot.normplot(vz_real)\n    else:\n        q = (np.arange(1, n + 1) - 1 / 2) / n\n        stNorm = gaussnorm.ppf(q)  # norminv: quantiles of standard normal\n        axFigNormplot.scatter(stNorm, sorted(vz_real), s=20)  # marker='.',\n        axFigNormplot.plot([-3, 3], [-3, 3], marker=\"_\", linewidth=4, color=\"red\")\n        axFigNormplot.set_xlabel(\"Standard Gaussian Quantiles\")\n        axFigNormplot.set_ylabel(\"Data Quantiles\")\n\n    if theta:\n        plt_title = f\"$d={dim}, n={n}, r={r:1.2f}, r_{{opt}}={rOpt:1.2f}, \\\\theta={theta:1.2f}, \\\\theta_{{opt}}={thetaOpt:1.2f}$\"\n        plt_filename = (\n            f\"{fName}-QQPlot_n-{n}_d-{dim}_r-{r * 100}_th-{100 * theta}_case-{iii}.jpg\"\n        )\n    else:\n        plt_title = (\n            f\"$d={dim}, n={n}, r_{{opt}}={rOpt:1.2f}, \\\\theta_{{opt}}={thetaOpt:1.2f}$\"\n        )\n        plt_filename = f\"{fName}-QQPlot_n-{n}_d-{dim}_case-{iii}.jpg\"\n    axFigNormplot.set_title(plt_title)\n    hFigNormplot.savefig(OUTDIR + plt_filename)\n</pre> def create_plots(type, vz_real, fName, dim, iii, r, rOpt, theta, thetaOpt):     hFigNormplot, axFigNormplot = plt.subplots()      # plt.rcParams.update({'font.size': 12})     # plt.rcParams.update({'lines.linewidth': 2})      # set(hFigNormplot,'defaultaxesfontsize',16,     #   'defaulttextfontsize',12,   # make font larger     #   'defaultLineLineWidth',2, 'defaultLineMarkerSize',6)     n = len(vz_real)     if type == \"normplot\":         axFigNormplot.normplot(vz_real)     else:         q = (np.arange(1, n + 1) - 1 / 2) / n         stNorm = gaussnorm.ppf(q)  # norminv: quantiles of standard normal         axFigNormplot.scatter(stNorm, sorted(vz_real), s=20)  # marker='.',         axFigNormplot.plot([-3, 3], [-3, 3], marker=\"_\", linewidth=4, color=\"red\")         axFigNormplot.set_xlabel(\"Standard Gaussian Quantiles\")         axFigNormplot.set_ylabel(\"Data Quantiles\")      if theta:         plt_title = f\"$d={dim}, n={n}, r={r:1.2f}, r_{{opt}}={rOpt:1.2f}, \\\\theta={theta:1.2f}, \\\\theta_{{opt}}={thetaOpt:1.2f}$\"         plt_filename = (             f\"{fName}-QQPlot_n-{n}_d-{dim}_r-{r * 100}_th-{100 * theta}_case-{iii}.jpg\"         )     else:         plt_title = (             f\"$d={dim}, n={n}, r_{{opt}}={rOpt:1.2f}, \\\\theta_{{opt}}={thetaOpt:1.2f}$\"         )         plt_filename = f\"{fName}-QQPlot_n-{n}_d-{dim}_case-{iii}.jpg\"     axFigNormplot.set_title(plt_title)     hFigNormplot.savefig(OUTDIR + plt_filename) In\u00a0[\u00a0]: Copied! <pre>def create_surf_plot(\n    fName, lnthth, lnordord, objfun, objobj, lnParamsOpt, r, theta, iii\n):\n    figH, axH = plt.subplots(subplot_kw={\"projection\": \"3d\"})\n    axH.view_init(40, 30)\n    shandle = axH.plot_surface(\n        lnthth,\n        lnordord,\n        objobj,\n        cmap=cm.coolwarm,\n        linewidth=0,\n        antialiased=False,\n        alpha=0.8,\n    )\n    xt = np.array([0.2, 0.4, 1, 3, 7])\n    axH.set_xticks(np.log(xt))\n    axH.set_xticklabels(xt.astype(str))\n    yt = np.array([1.4, 1.6, 2, 2.6, 3.7])\n    axH.set_yticks(np.log(yt - 1))\n    axH.set_yticklabels(yt.astype(str))\n    axH.set_xlabel(\"$\\\\theta$\")\n    axH.set_ylabel(\"$r$\")\n\n    axH.scatter(\n        lnParamsOpt[0],\n        lnParamsOpt[1],\n        objfun(lnParamsOpt) * 1.002,\n        s=200,\n        color=\"orange\",\n        marker=\"*\",\n        alpha=0.8,\n    )\n    if theta:\n        filename = f\"{fName}-ObjFun_n-{npts}_d-{dim}_r-{r * 100}_th-{100 * theta}_case-{iii}.jpg\"\n    else:\n        filename = f\"{fName}-ObjFun_n-{npts}_d-{dim}_case-{iii}.jpg\"\n    figH.savefig(OUTDIR + filename)\n</pre> def create_surf_plot(     fName, lnthth, lnordord, objfun, objobj, lnParamsOpt, r, theta, iii ):     figH, axH = plt.subplots(subplot_kw={\"projection\": \"3d\"})     axH.view_init(40, 30)     shandle = axH.plot_surface(         lnthth,         lnordord,         objobj,         cmap=cm.coolwarm,         linewidth=0,         antialiased=False,         alpha=0.8,     )     xt = np.array([0.2, 0.4, 1, 3, 7])     axH.set_xticks(np.log(xt))     axH.set_xticklabels(xt.astype(str))     yt = np.array([1.4, 1.6, 2, 2.6, 3.7])     axH.set_yticks(np.log(yt - 1))     axH.set_yticklabels(yt.astype(str))     axH.set_xlabel(\"$\\\\theta$\")     axH.set_ylabel(\"$r$\")      axH.scatter(         lnParamsOpt[0],         lnParamsOpt[1],         objfun(lnParamsOpt) * 1.002,         s=200,         color=\"orange\",         marker=\"*\",         alpha=0.8,     )     if theta:         filename = f\"{fName}-ObjFun_n-{npts}_d-{dim}_r-{r * 100}_th-{100 * theta}_case-{iii}.jpg\"     else:         filename = f\"{fName}-ObjFun_n-{npts}_d-{dim}_case-{iii}.jpg\"     figH.savefig(OUTDIR + filename) In\u00a0[\u00a0]: Copied! <pre>#\n# Minimum working example to demonstrate Gaussian diagnostics concept\n#\ndef MWE_gaussian_diagnostics_engine(whEx, dim, npts, r, fpar, nReps, nPlots):\n    whEx = whEx - 1\n    fNames = [\"ExpCos\", \"Keister\", \"rand\"]\n    ptransforms = [\"none\", \"C1sin\", \"none\"]\n    fName = fNames[whEx]\n    ptransform = ptransforms[whEx]\n\n    rOptAll = [0] * nRep\n    thOptAll = [0] * nRep\n\n    # parameters for random function\n    # seed = 202326\n    if whEx == 2:\n        rfun = r / 2\n        f_mean = fpar[2]\n        f_std_a = fpar[0]  # this is square root of the a in the talk\n        f_std_b = fpar[1]  # this is square root of the b in the talk\n        theta = (f_std_a / f_std_b) ** 2\n    else:\n        theta = None\n\n    for iii in range(nReps):\n        seed = np.random.randint(low=1, high=1e6)  # different each rep\n        shift = np.random.rand(1, dim)\n\n        distribution = Lattice(dimension=dim, order=\"linear\")\n        xpts = distribution.gen_samples(n_min=0, n_max=npts, warn=False)\n        xlat = (xpts - distribution.shift) % 1\n\n        if fName == \"ExpCos\":\n            integrand = lambda x: np.exp(np.sum(np.cos(2 * np.pi * x), axis=1))\n        elif fName == \"Keister\":\n            keister = Keister(Lattice(dimension=dim, order=\"linear\"))\n            integrand = lambda x: keister.f(x)\n        elif fName == \"rand\":\n            integrand = lambda x: f_rand(x, rfun, f_std_a, f_std_b, f_mean, seed)\n        else:\n            print(\"Invalid function name\")\n            return\n\n        y = doPeriodTx(xpts, integrand, ptransform)\n\n        ftilde = np.fft.fft(y)  # fourier coefficients\n        ftilde[0] = 0  # ftilde = \\mV**H(\\vf - m \\vone), subtract mean\n        if dim == 1:\n            hFigIntegrand = plt.figure()\n            plt.scatter(xpts, y, 10)\n            plt.title(f\"{fName}_n-{npts}_Tx-{ptransform}\")\n            hFigIntegrand.savefig(\n                OUTDIR + f\"{fName}_n-{npts}_Tx-{ptransform}_rFun-{rfun:1.2f}.jpg\"\n            )\n\n        def objfun(lnParams):\n            loss, Lambda, RKHSnorm = ObjectiveFunction(\n                np.exp(lnParams[0]), 1 + np.exp(lnParams[1]), xlat, ftilde\n            )\n            return loss\n\n        ## Plot the objective function\n        lnthetarange = np.arange(-2, 2.2, 0.2)  # range of log(theta) for plotting\n        lnorderrange = np.arange(-1, 1.1, 0.1)  # range of log(r) for plotting\n        [lnthth, lnordord] = np.meshgrid(lnthetarange, lnorderrange)\n        objobj = np.zeros(lnthth.shape)\n        for ii in range(lnthth.shape[0]):\n            for jj in range(lnthth.shape[1]):\n                objobj[ii, jj] = objfun([lnthth[ii, jj], lnordord[ii, jj]])\n\n        objMinAppx, which = objobj.min(), objobj.argmin()\n        # [whichrow, whichcol] = ind2sub(lnthth.shape, which)\n        [whichrow, whichcol] = np.unravel_index(which, lnthth.shape)\n        lnthOptAppx = lnthth[whichrow, whichcol]\n        thetaOptAppx = np.exp(lnthOptAppx)\n        lnordOptAppx = lnordord[whichrow, whichcol]\n        orderOptAppx = 1 + np.exp(lnordOptAppx)\n        # print(objMinAppx)  # minimum objective function by brute force search\n\n        ## Optimize the objective function\n        result = fminsearch(\n            objfun,\n            x0=[lnthOptAppx, lnordOptAppx],\n            xtol=1e-3,\n            full_output=True,\n            disp=False,\n        )\n        lnParamsOpt, objMin = result[0], result[1]\n        # print(objMin)  # minimum objective function by Nelder-Mead\n        thetaOpt = np.exp(lnParamsOpt[0])\n        rOpt = 1 + np.exp(lnParamsOpt[1])\n        rOptAll[iii] = rOpt\n        thOptAll[iii] = thetaOpt\n        print(\n            f\"thetaOptAppx={thetaOptAppx:7.5f}, rOptAppx={orderOptAppx:7.5f}, \"\n            f\"objMinAppx={objMinAppx:7.5f}, objMin={objMin:7.5f}\"\n        )\n\n        if iii &lt;= nPlots:\n            create_surf_plot(\n                fName, lnthth, lnordord, objfun, objobj, lnParamsOpt, r, theta, iii\n            )\n\n        vlambda = kernel2(thetaOpt, rOpt, xlat)\n        s2 = sum(abs(ftilde[2:] ** 2) / vlambda[2:]) / (npts**2)\n        vlambda = s2 * vlambda\n\n        # apply transform\n        # $\\vZ = \\frac 1n \\mV \\mLambda**{-\\frac 12} \\mV**H(\\vf - m \\vone)$\n        # np.fft also includes 1/n division\n        vz = np.fft.ifft(ftilde / np.sqrt(vlambda))\n        vz_real = np.real(vz)  # vz must be real as intended by the transformation\n\n        if iii &lt;= nPlots:\n            create_plots(\"qqplot\", vz_real, fName, dim, iii, r, rOpt, theta, thetaOpt)\n\n        r_str = f\"{r: 7.5f}\" if type(r) == float else str(r)\n        theta_str = f\"{theta: 7.5f}\" if type(theta) == float else str(theta)\n        print(\n            f\"\\t r = {r_str}, rOpt = {rOpt:7.5f}, theta = {theta_str}, thetaOpt = {thetaOpt:7.5f}\\n\"\n        )\n\n    return [theta, rOptAll, thOptAll, fName]\n</pre> # # Minimum working example to demonstrate Gaussian diagnostics concept # def MWE_gaussian_diagnostics_engine(whEx, dim, npts, r, fpar, nReps, nPlots):     whEx = whEx - 1     fNames = [\"ExpCos\", \"Keister\", \"rand\"]     ptransforms = [\"none\", \"C1sin\", \"none\"]     fName = fNames[whEx]     ptransform = ptransforms[whEx]      rOptAll = [0] * nRep     thOptAll = [0] * nRep      # parameters for random function     # seed = 202326     if whEx == 2:         rfun = r / 2         f_mean = fpar[2]         f_std_a = fpar[0]  # this is square root of the a in the talk         f_std_b = fpar[1]  # this is square root of the b in the talk         theta = (f_std_a / f_std_b) ** 2     else:         theta = None      for iii in range(nReps):         seed = np.random.randint(low=1, high=1e6)  # different each rep         shift = np.random.rand(1, dim)          distribution = Lattice(dimension=dim, order=\"linear\")         xpts = distribution.gen_samples(n_min=0, n_max=npts, warn=False)         xlat = (xpts - distribution.shift) % 1          if fName == \"ExpCos\":             integrand = lambda x: np.exp(np.sum(np.cos(2 * np.pi * x), axis=1))         elif fName == \"Keister\":             keister = Keister(Lattice(dimension=dim, order=\"linear\"))             integrand = lambda x: keister.f(x)         elif fName == \"rand\":             integrand = lambda x: f_rand(x, rfun, f_std_a, f_std_b, f_mean, seed)         else:             print(\"Invalid function name\")             return          y = doPeriodTx(xpts, integrand, ptransform)          ftilde = np.fft.fft(y)  # fourier coefficients         ftilde[0] = 0  # ftilde = \\mV**H(\\vf - m \\vone), subtract mean         if dim == 1:             hFigIntegrand = plt.figure()             plt.scatter(xpts, y, 10)             plt.title(f\"{fName}_n-{npts}_Tx-{ptransform}\")             hFigIntegrand.savefig(                 OUTDIR + f\"{fName}_n-{npts}_Tx-{ptransform}_rFun-{rfun:1.2f}.jpg\"             )          def objfun(lnParams):             loss, Lambda, RKHSnorm = ObjectiveFunction(                 np.exp(lnParams[0]), 1 + np.exp(lnParams[1]), xlat, ftilde             )             return loss          ## Plot the objective function         lnthetarange = np.arange(-2, 2.2, 0.2)  # range of log(theta) for plotting         lnorderrange = np.arange(-1, 1.1, 0.1)  # range of log(r) for plotting         [lnthth, lnordord] = np.meshgrid(lnthetarange, lnorderrange)         objobj = np.zeros(lnthth.shape)         for ii in range(lnthth.shape[0]):             for jj in range(lnthth.shape[1]):                 objobj[ii, jj] = objfun([lnthth[ii, jj], lnordord[ii, jj]])          objMinAppx, which = objobj.min(), objobj.argmin()         # [whichrow, whichcol] = ind2sub(lnthth.shape, which)         [whichrow, whichcol] = np.unravel_index(which, lnthth.shape)         lnthOptAppx = lnthth[whichrow, whichcol]         thetaOptAppx = np.exp(lnthOptAppx)         lnordOptAppx = lnordord[whichrow, whichcol]         orderOptAppx = 1 + np.exp(lnordOptAppx)         # print(objMinAppx)  # minimum objective function by brute force search          ## Optimize the objective function         result = fminsearch(             objfun,             x0=[lnthOptAppx, lnordOptAppx],             xtol=1e-3,             full_output=True,             disp=False,         )         lnParamsOpt, objMin = result[0], result[1]         # print(objMin)  # minimum objective function by Nelder-Mead         thetaOpt = np.exp(lnParamsOpt[0])         rOpt = 1 + np.exp(lnParamsOpt[1])         rOptAll[iii] = rOpt         thOptAll[iii] = thetaOpt         print(             f\"thetaOptAppx={thetaOptAppx:7.5f}, rOptAppx={orderOptAppx:7.5f}, \"             f\"objMinAppx={objMinAppx:7.5f}, objMin={objMin:7.5f}\"         )          if iii &lt;= nPlots:             create_surf_plot(                 fName, lnthth, lnordord, objfun, objobj, lnParamsOpt, r, theta, iii             )          vlambda = kernel2(thetaOpt, rOpt, xlat)         s2 = sum(abs(ftilde[2:] ** 2) / vlambda[2:]) / (npts**2)         vlambda = s2 * vlambda          # apply transform         # $\\vZ = \\frac 1n \\mV \\mLambda**{-\\frac 12} \\mV**H(\\vf - m \\vone)$         # np.fft also includes 1/n division         vz = np.fft.ifft(ftilde / np.sqrt(vlambda))         vz_real = np.real(vz)  # vz must be real as intended by the transformation          if iii &lt;= nPlots:             create_plots(\"qqplot\", vz_real, fName, dim, iii, r, rOpt, theta, thetaOpt)          r_str = f\"{r: 7.5f}\" if type(r) == float else str(r)         theta_str = f\"{theta: 7.5f}\" if type(theta) == float else str(theta)         print(             f\"\\t r = {r_str}, rOpt = {rOpt:7.5f}, theta = {theta_str}, thetaOpt = {thetaOpt:7.5f}\\n\"         )      return [theta, rOptAll, thOptAll, fName] In\u00a0[\u00a0]: Copied! <pre>if __name__ == \"__main__\":\n\n    ## Exponential Cosine example\n    fwh = 1\n    dim = 3\n    npts = 2**6\n    nRep = 20\n    nPlot = 2\n    [_, rOptAll, thOptAll, fName] = MWE_gaussian_diagnostics_engine(\n        fwh, dim, npts, None, None, nRep, nPlot\n    )\n\n    ## Plot Exponential Cosine example\n    figH = plt.figure()\n    plt.scatter(rOptAll, thOptAll, s=20, color=\"blue\")\n    # axis([4 6 0.1 10])\n    # set(gca,'yscale','log')\n    plt.title(f\"$d = {dim}, n = {npts}$\")\n    plt.xlabel(\"Inferred $r$\")\n    plt.ylabel(\"Inferred $\\\\theta$\")\n    # print('-depsc',[fName '-rthInfer-n-' int2str(npts) '-d-' \\\n    #   int2str(dim)])\n    figH.savefig(OUTDIR + f\"{fName}-rthInfer-n-{npts}-d-{dim}.jpg\")\n    plt.close(\"all\")\n\n    ## Tests with random function\n    rArray = [1.5, 2, 4]\n    nrArr = len(rArray)\n    fParArray = [[0.5, 1, 2], [1, 1, 1], [1, 1, 1]]\n    nfPArr = len(fParArray)\n    fwh = 3\n    dim = 2\n    npts = 2**6\n    nRep = 20\n    nPlot = 2\n    thetaAll = np.zeros((nrArr, nfPArr))\n    rOptAll = np.zeros((nrArr, nfPArr, nRep))\n    thOptAll = np.zeros((nrArr, nfPArr, nRep))\n    for jjj in range(nrArr):\n        for kkk in range(nfPArr):\n            thetaAll[jjj, kkk], rOptAll[jjj, kkk, :], thOptAll[jjj, kkk, :], fName = (\n                MWE_gaussian_diagnostics_engine(\n                    fwh, dim, npts, rArray[jjj], fParArray[kkk], nRep, nPlot\n                )\n            )\n    plt.close(\"all\")\n\n    ## Plot figures for random function\n    figH, axH = plt.subplots()\n    colorArray = [\"blue\", \"orange\", \"green\", \"cyan\", \"maroon\", \"purple\"]\n    nColArray = len(colorArray)\n    for jjj in range(nrArr):\n        for kkk in range(nfPArr):\n            clrInd = np.mod(nfPArr * (jjj) + kkk, nColArray)\n            clr = colorArray[clrInd]\n            axH.scatter(\n                rOptAll[jjj, kkk, :].reshape((nRep, 1)),\n                thOptAll[jjj, kkk, :].reshape((nRep, 1)),\n                marker=\".\",\n                s=50,\n                color=clr,\n            )\n            axH.scatter(rArray[jjj], thetaAll[jjj, kkk], s=50, c=clr, marker=\"D\")\n\n    axH.set(xlim=[1, 6], ylim=[0.01, 100])\n    axH.set_yscale(\"log\")\n    axH.set_title(f\"$d = {dim}, n = {npts}$\")\n    axH.set_xlabel(\"Inferred $r$\")\n    axH.set_ylabel(\"Inferred $\\\\theta$\")\n    figH.savefig(OUTDIR + f\"{fName}-rthInfer-n-{npts}-d-{dim}.jpg\")\n\n    ## Keister example\n    fwh = 2\n    dim = 3\n    npts = 2**6\n    nRep = 20\n    nPlot = 2\n    _, rOptAll, thOptAll, fName = MWE_gaussian_diagnostics_engine(\n        fwh, dim, npts, None, None, nRep, nPlot\n    )\n\n    ## Plot Keister example\n    figH = plt.figure()\n    plt.scatter(rOptAll, thOptAll, s=20, color=\"blue\")\n    # axis([4 6 0.5 1.5])\n    # set(gca,'yscale','log')\n    plt.xlabel(\"Inferred $r$\")\n    plt.ylabel(\"Inferred $\\\\theta$\")\n    plt.title(f\"$d = {dim}, n = {npts}$\")\n    figH.savefig(OUTDIR + f\"{fName}-rthInfer-n-{npts}-d-{dim}.jpg\")\n\n    ## Keister example\n    fwh = 2\n    dim = 3\n    npts = 2**10\n    nRep = 20\n    nPlot = 2\n    _, rOptAll, thOptAll, fName = MWE_gaussian_diagnostics_engine(\n        fwh, dim, npts, None, None, nRep, nPlot\n    )\n\n    ## Plot Keister example\n    figH = plt.figure()\n    plt.scatter(rOptAll, thOptAll, s=20, color=\"blue\")\n    # axis([4 6 0.5 1.5])\n    # set(gca,'yscale','log')\n    plt.xlabel(\"Inferred $r$\")\n    plt.ylabel(\"Inferred $\\\\theta$\")\n    plt.title(f\"$d = {dim}, n = {npts}$\")\n    figH.savefig(OUTDIR + f\"{fName}-rthInfer-n-{npts}-d-{dim}.jpg\")\n</pre> if __name__ == \"__main__\":      ## Exponential Cosine example     fwh = 1     dim = 3     npts = 2**6     nRep = 20     nPlot = 2     [_, rOptAll, thOptAll, fName] = MWE_gaussian_diagnostics_engine(         fwh, dim, npts, None, None, nRep, nPlot     )      ## Plot Exponential Cosine example     figH = plt.figure()     plt.scatter(rOptAll, thOptAll, s=20, color=\"blue\")     # axis([4 6 0.1 10])     # set(gca,'yscale','log')     plt.title(f\"$d = {dim}, n = {npts}$\")     plt.xlabel(\"Inferred $r$\")     plt.ylabel(\"Inferred $\\\\theta$\")     # print('-depsc',[fName '-rthInfer-n-' int2str(npts) '-d-' \\     #   int2str(dim)])     figH.savefig(OUTDIR + f\"{fName}-rthInfer-n-{npts}-d-{dim}.jpg\")     plt.close(\"all\")      ## Tests with random function     rArray = [1.5, 2, 4]     nrArr = len(rArray)     fParArray = [[0.5, 1, 2], [1, 1, 1], [1, 1, 1]]     nfPArr = len(fParArray)     fwh = 3     dim = 2     npts = 2**6     nRep = 20     nPlot = 2     thetaAll = np.zeros((nrArr, nfPArr))     rOptAll = np.zeros((nrArr, nfPArr, nRep))     thOptAll = np.zeros((nrArr, nfPArr, nRep))     for jjj in range(nrArr):         for kkk in range(nfPArr):             thetaAll[jjj, kkk], rOptAll[jjj, kkk, :], thOptAll[jjj, kkk, :], fName = (                 MWE_gaussian_diagnostics_engine(                     fwh, dim, npts, rArray[jjj], fParArray[kkk], nRep, nPlot                 )             )     plt.close(\"all\")      ## Plot figures for random function     figH, axH = plt.subplots()     colorArray = [\"blue\", \"orange\", \"green\", \"cyan\", \"maroon\", \"purple\"]     nColArray = len(colorArray)     for jjj in range(nrArr):         for kkk in range(nfPArr):             clrInd = np.mod(nfPArr * (jjj) + kkk, nColArray)             clr = colorArray[clrInd]             axH.scatter(                 rOptAll[jjj, kkk, :].reshape((nRep, 1)),                 thOptAll[jjj, kkk, :].reshape((nRep, 1)),                 marker=\".\",                 s=50,                 color=clr,             )             axH.scatter(rArray[jjj], thetaAll[jjj, kkk], s=50, c=clr, marker=\"D\")      axH.set(xlim=[1, 6], ylim=[0.01, 100])     axH.set_yscale(\"log\")     axH.set_title(f\"$d = {dim}, n = {npts}$\")     axH.set_xlabel(\"Inferred $r$\")     axH.set_ylabel(\"Inferred $\\\\theta$\")     figH.savefig(OUTDIR + f\"{fName}-rthInfer-n-{npts}-d-{dim}.jpg\")      ## Keister example     fwh = 2     dim = 3     npts = 2**6     nRep = 20     nPlot = 2     _, rOptAll, thOptAll, fName = MWE_gaussian_diagnostics_engine(         fwh, dim, npts, None, None, nRep, nPlot     )      ## Plot Keister example     figH = plt.figure()     plt.scatter(rOptAll, thOptAll, s=20, color=\"blue\")     # axis([4 6 0.5 1.5])     # set(gca,'yscale','log')     plt.xlabel(\"Inferred $r$\")     plt.ylabel(\"Inferred $\\\\theta$\")     plt.title(f\"$d = {dim}, n = {npts}$\")     figH.savefig(OUTDIR + f\"{fName}-rthInfer-n-{npts}-d-{dim}.jpg\")      ## Keister example     fwh = 2     dim = 3     npts = 2**10     nRep = 20     nPlot = 2     _, rOptAll, thOptAll, fName = MWE_gaussian_diagnostics_engine(         fwh, dim, npts, None, None, nRep, nPlot     )      ## Plot Keister example     figH = plt.figure()     plt.scatter(rOptAll, thOptAll, s=20, color=\"blue\")     # axis([4 6 0.5 1.5])     # set(gca,'yscale','log')     plt.xlabel(\"Inferred $r$\")     plt.ylabel(\"Inferred $\\\\theta$\")     plt.title(f\"$d = {dim}, n = {npts}$\")     figH.savefig(OUTDIR + f\"{fName}-rthInfer-n-{npts}-d-{dim}.jpg\")"},{"location":"demos/gaussian_diagnostics/gaussian_diagnostics_demo/","title":"2022 Bayesian Cubature Stopping Criterion","text":"In\u00a0[1]: Copied! <pre>import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.optimize import fmin as fminsearch\nfrom numpy import prod, sin, cos, pi\nfrom scipy.stats import norm as gaussnorm\nfrom matplotlib import cm\n</pre> import numpy as np import matplotlib.pyplot as plt from scipy.optimize import fmin as fminsearch from numpy import prod, sin, cos, pi from scipy.stats import norm as gaussnorm from matplotlib import cm In\u00a0[2]: Copied! <pre>from qmcpy.integrand import Keister\nfrom qmcpy.discrete_distribution.lattice import Lattice\n</pre> from qmcpy.integrand import Keister from qmcpy.discrete_distribution.lattice import Lattice In\u00a0[\u00a0]: Copied! <pre># print(plt.style.available)\n# plt.style.use('./presentation.mplstyle')  # use custom settings\n#plt.style.use('seaborn-v0_8-poster')\nOUTDIR = \"./outputs_nb/\"\n# plt.rcParams.update({'font.size': 12})\n# plt.rcParams.update({'lines.linewidth': 2})\n# plt.rcParams.update({'lines.markersize': 6})\n</pre> # print(plt.style.available) # plt.style.use('./presentation.mplstyle')  # use custom settings #plt.style.use('seaborn-v0_8-poster') OUTDIR = \"./outputs_nb/\" # plt.rcParams.update({'font.size': 12}) # plt.rcParams.update({'lines.linewidth': 2}) # plt.rcParams.update({'lines.markersize': 6}) <p>Let us define the objective function. (<code>cubBayesLattice</code>) finds optimal parameters by minimizing the objective function</p> In\u00a0[5]: Copied! <pre>def ObjectiveFunction(theta, order, xun, ftilde):\n    tol = 100 * np.finfo(float).eps\n    n = len(ftilde)\n    arbMean = True\n    Lambda = kernel2(theta, order, xun)\n\n    # compute RKHSnorm\n    # temp = abs(ftilde(Lambda  != 0).^ 2)./ (Lambda(Lambda != 0))\n    temp = abs(ftilde[Lambda &gt; tol] ** 2) / (Lambda[Lambda &gt; tol])\n\n    # compute loss: MLE\n    if arbMean == True:\n        RKHSnorm = sum(temp[1:]) / n\n        temp_1 = sum(temp[1:])\n    else:\n        RKHSnorm = sum(temp) / n\n        temp_1 = sum(temp)\n\n    # ignore all zero eigenvalues\n    loss1 = sum(np.log(Lambda[Lambda &gt; tol])) / n\n    loss2 = np.log(temp_1)\n    loss = (loss1 + loss2)\n    if np.imag(loss) != 0:\n        # keyboard\n        raise('error ! : loss value is complex')\n\n    # print('L1 %1.3f L2 %1.3f L %1.3f r %1.3e theta %1.3e\\n'.format(loss1, loss2, loss, order, theta))\n    return loss, Lambda, RKHSnorm\n</pre> def ObjectiveFunction(theta, order, xun, ftilde):     tol = 100 * np.finfo(float).eps     n = len(ftilde)     arbMean = True     Lambda = kernel2(theta, order, xun)      # compute RKHSnorm     # temp = abs(ftilde(Lambda  != 0).^ 2)./ (Lambda(Lambda != 0))     temp = abs(ftilde[Lambda &gt; tol] ** 2) / (Lambda[Lambda &gt; tol])      # compute loss: MLE     if arbMean == True:         RKHSnorm = sum(temp[1:]) / n         temp_1 = sum(temp[1:])     else:         RKHSnorm = sum(temp) / n         temp_1 = sum(temp)      # ignore all zero eigenvalues     loss1 = sum(np.log(Lambda[Lambda &gt; tol])) / n     loss2 = np.log(temp_1)     loss = (loss1 + loss2)     if np.imag(loss) != 0:         # keyboard         raise('error ! : loss value is complex')      # print('L1 %1.3f L2 %1.3f L %1.3f r %1.3e theta %1.3e\\n'.format(loss1, loss2, loss, order, theta))     return loss, Lambda, RKHSnorm <p>Series approximation of the shift invariant kernel</p> In\u00a0[6]: Copied! <pre>def kernel2(theta, r, xun):\n    n = xun.shape[0]\n    m = np.arange(1, (n / 2))\n    tilde_g_h1 = m ** (-r)\n    tilde_g = np.hstack([0, tilde_g_h1, 0, tilde_g_h1[::-1]])\n    g = np.fft.fft(tilde_g)\n    temp_ = (theta / 2) * g[(xun * n).astype(int)]\n    C1 = prod(1 + temp_, 1)\n    # eigenvalues must be real : Symmetric pos definite Kernel\n    vlambda = np.real(np.fft.fft(C1))\n    return vlambda\n</pre> def kernel2(theta, r, xun):     n = xun.shape[0]     m = np.arange(1, (n / 2))     tilde_g_h1 = m ** (-r)     tilde_g = np.hstack([0, tilde_g_h1, 0, tilde_g_h1[::-1]])     g = np.fft.fft(tilde_g)     temp_ = (theta / 2) * g[(xun * n).astype(int)]     C1 = prod(1 + temp_, 1)     # eigenvalues must be real : Symmetric pos definite Kernel     vlambda = np.real(np.fft.fft(C1))     return vlambda <p>Gaussian random function</p> In\u00a0[7]: Copied! <pre>def f_rand(xpts, rfun, a, b, c, seed):\n    dim = xpts.shape[1]\n    np.random.seed(seed)  # initialize random number generator for reproducability\n    N1 = int(2 ** np.floor(16 / dim))\n    Nall = N1 ** dim\n    kvec = np.zeros([dim, Nall])  # initialize kvec\n    kvec[0, 0:N1] = range(0, N1)  # first dimension\n    Nd = N1\n    for d in range(1, dim):\n        Ndm1 = Nd\n        Nd = Nd * N1\n        kvec[0:d+1, 0:Nd] = np.vstack([\n            np.tile(kvec[0:d, 0:Ndm1], (1, N1)),\n            np.reshape(np.tile(np.arange(0, N1), (Ndm1, 1)), (1, Nd), order=\"F\")\n        ])\n\n    kvec = kvec[:, 1: Nall]  # remove the zero wavenumber\n    whZero = np.sum(kvec == 0, axis=0)\n    abfac = a ** (dim - whZero) * b ** whZero\n    kbar = np.prod(np.maximum(kvec, 1), axis=0)\n    totfac = abfac / (kbar ** rfun)\n\n    f_c = a * np.random.randn(1, Nall - 1) * totfac\n    f_s = a * np.random.randn(1, Nall - 1) * totfac\n\n    f_0 = c + (b ** dim) * np.random.randn()\n    argx = np.matmul((2 * np.pi * xpts), kvec)\n    f_c_ = f_c * np.cos(argx)\n    f_s_ = f_s * np.sin(argx)\n    fval = f_0 + np.sum(f_c_ + f_s_, axis=1)\n    return fval\n</pre> def f_rand(xpts, rfun, a, b, c, seed):     dim = xpts.shape[1]     np.random.seed(seed)  # initialize random number generator for reproducability     N1 = int(2 ** np.floor(16 / dim))     Nall = N1 ** dim     kvec = np.zeros([dim, Nall])  # initialize kvec     kvec[0, 0:N1] = range(0, N1)  # first dimension     Nd = N1     for d in range(1, dim):         Ndm1 = Nd         Nd = Nd * N1         kvec[0:d+1, 0:Nd] = np.vstack([             np.tile(kvec[0:d, 0:Ndm1], (1, N1)),             np.reshape(np.tile(np.arange(0, N1), (Ndm1, 1)), (1, Nd), order=\"F\")         ])      kvec = kvec[:, 1: Nall]  # remove the zero wavenumber     whZero = np.sum(kvec == 0, axis=0)     abfac = a ** (dim - whZero) * b ** whZero     kbar = np.prod(np.maximum(kvec, 1), axis=0)     totfac = abfac / (kbar ** rfun)      f_c = a * np.random.randn(1, Nall - 1) * totfac     f_s = a * np.random.randn(1, Nall - 1) * totfac      f_0 = c + (b ** dim) * np.random.randn()     argx = np.matmul((2 * np.pi * xpts), kvec)     f_c_ = f_c * np.cos(argx)     f_s_ = f_s * np.sin(argx)     fval = f_0 + np.sum(f_c_ + f_s_, axis=1)     return fval <p>Periodization transforms</p> In\u00a0[8]: Copied! <pre>def doPeriodTx(x, integrand, ptransform):\n    ptransform = ptransform.upper()\n    if ptransform == 'BAKER':  # Baker's transform\n        xp = 1 - 2 * abs(x - 1 / 2)\n        w = 1\n    elif ptransform == 'C0':  # C^0 transform\n        xp = 3 * x ** 2 - 2 * x ** 3\n        w = prod(6 * x * (1 - x), 1)\n    elif ptransform == 'C1':  # C^1 transform\n        xp = x ** 3 * (10 - 15 * x + 6 * x ** 2)\n        w = prod(30 * x ** 2 * (1 - x) ** 2, 1)\n    elif ptransform == 'C1SIN':  # Sidi C^1 transform\n        xp = x - sin(2 * pi * x) / (2 * pi)\n        w = prod(2 * sin(pi * x) ** 2, 1)\n    elif ptransform == 'C2SIN':  # Sidi C^2 transform\n        xp = (8 - 9 * cos(pi * x) + cos(3 * pi * x)) / 16  # psi3\n        w = prod((9 * sin(pi * x) * pi - sin(3 * pi * x) * 3 * pi) / 16, 1)  # psi3_1\n    elif ptransform == 'C3SIN':  # Sidi C^3 transform\n        xp = (12 * pi * x - 8 * sin(2 * pi * x) + sin(4 * pi * x)) / (12 * pi)  # psi4\n        w = prod((12 * pi - 8 * cos(2 * pi * x) * 2 * pi + sin(4 * pi * x) * 4 * pi) / (12 * pi), 1)  # psi4_1\n    elif ptransform == 'NONE':\n        xp = x\n        w = 1\n    else:\n        raise (f\"The {ptransform} periodization transform is not implemented\")\n    y = integrand(xp) * w\n    return y\n</pre> def doPeriodTx(x, integrand, ptransform):     ptransform = ptransform.upper()     if ptransform == 'BAKER':  # Baker's transform         xp = 1 - 2 * abs(x - 1 / 2)         w = 1     elif ptransform == 'C0':  # C^0 transform         xp = 3 * x ** 2 - 2 * x ** 3         w = prod(6 * x * (1 - x), 1)     elif ptransform == 'C1':  # C^1 transform         xp = x ** 3 * (10 - 15 * x + 6 * x ** 2)         w = prod(30 * x ** 2 * (1 - x) ** 2, 1)     elif ptransform == 'C1SIN':  # Sidi C^1 transform         xp = x - sin(2 * pi * x) / (2 * pi)         w = prod(2 * sin(pi * x) ** 2, 1)     elif ptransform == 'C2SIN':  # Sidi C^2 transform         xp = (8 - 9 * cos(pi * x) + cos(3 * pi * x)) / 16  # psi3         w = prod((9 * sin(pi * x) * pi - sin(3 * pi * x) * 3 * pi) / 16, 1)  # psi3_1     elif ptransform == 'C3SIN':  # Sidi C^3 transform         xp = (12 * pi * x - 8 * sin(2 * pi * x) + sin(4 * pi * x)) / (12 * pi)  # psi4         w = prod((12 * pi - 8 * cos(2 * pi * x) * 2 * pi + sin(4 * pi * x) * 4 * pi) / (12 * pi), 1)  # psi4_1     elif ptransform == 'NONE':         xp = x         w = 1     else:         raise (f\"The {ptransform} periodization transform is not implemented\")     y = integrand(xp) * w     return y <p>Utility function to draw qqplot or normplot</p> In\u00a0[9]: Copied! <pre>def create_quant_plot(type, vz_real, fName, dim, iii, r, rOpt, theta, thetaOpt):\n    hFigNormplot, axFigNormplot = plt.subplots()\n\n    n = len(vz_real)\n    if type == 'normplot':\n        axFigNormplot.normplot(vz_real)\n    else:\n        q = (np.arange(1, n + 1) - 1 / 2) / n\n        stNorm = gaussnorm.ppf(q)  # norminv: quantiles of standard normal\n        axFigNormplot.scatter(stNorm, sorted(vz_real), s=20)  # marker='.',\n        axFigNormplot.plot([-3, 3], [-3, 3], marker='_', linewidth=4, color='red')\n        axFigNormplot.set_xlabel('Standard Gaussian Quantiles')\n        axFigNormplot.set_ylabel('Data Quantiles')\n\n    if theta:\n        plt_title = f'$d={dim}, n={n}, r={r:1.2f}, r_{{opt}}={rOpt:1.2f}, \\\\theta={theta:1.2f}, \\\\theta_{{opt}}={thetaOpt:1.2f}$'\n        plt_filename = f'{fName}-QQPlot_n-{n}_d-{dim}_r-{r * 100}_th-{100 * theta}_case-{iii}.jpg'\n    else:\n        plt_title = f'$d={dim}, n={n}, r_{{opt}}={rOpt:1.2f}, \\\\theta_{{opt}}={thetaOpt:1.2f}$'\n        plt_filename = f'{fName}-QQPlot_n-{n}_d-{dim}_case-{iii}.jpg'\n        \n    axFigNormplot.set_title(plt_title)\n    hFigNormplot.savefig(OUTDIR+plt_filename)\n</pre> def create_quant_plot(type, vz_real, fName, dim, iii, r, rOpt, theta, thetaOpt):     hFigNormplot, axFigNormplot = plt.subplots()      n = len(vz_real)     if type == 'normplot':         axFigNormplot.normplot(vz_real)     else:         q = (np.arange(1, n + 1) - 1 / 2) / n         stNorm = gaussnorm.ppf(q)  # norminv: quantiles of standard normal         axFigNormplot.scatter(stNorm, sorted(vz_real), s=20)  # marker='.',         axFigNormplot.plot([-3, 3], [-3, 3], marker='_', linewidth=4, color='red')         axFigNormplot.set_xlabel('Standard Gaussian Quantiles')         axFigNormplot.set_ylabel('Data Quantiles')      if theta:         plt_title = f'$d={dim}, n={n}, r={r:1.2f}, r_{{opt}}={rOpt:1.2f}, \\\\theta={theta:1.2f}, \\\\theta_{{opt}}={thetaOpt:1.2f}$'         plt_filename = f'{fName}-QQPlot_n-{n}_d-{dim}_r-{r * 100}_th-{100 * theta}_case-{iii}.jpg'     else:         plt_title = f'$d={dim}, n={n}, r_{{opt}}={rOpt:1.2f}, \\\\theta_{{opt}}={thetaOpt:1.2f}$'         plt_filename = f'{fName}-QQPlot_n-{n}_d-{dim}_case-{iii}.jpg'              axFigNormplot.set_title(plt_title)     hFigNormplot.savefig(OUTDIR+plt_filename) <p>Utility function to plot the objective function and minimum</p> In\u00a0[10]: Copied! <pre>def create_surf_plot(fName, lnthth, lnordord, objfun, objobj, lnParamsOpt, r, theta, iii):\n    figH, axH = plt.subplots(subplot_kw={\"projection\": \"3d\"})\n    axH.view_init(40, 30)\n    shandle = axH.plot_surface(lnthth, lnordord, objobj, cmap=cm.coolwarm,\n                               linewidth=0, antialiased=False, alpha=0.8)\n    xt = np.array([.2, 0.4, 1, 3, 7])\n    axH.set_xticks(np.log(xt))\n    axH.set_xticklabels(xt.astype(str))\n    yt = np.array([1.4, 1.6, 2, 2.6, 3.7])\n    axH.set_yticks(np.log(yt - 1))\n    axH.set_yticklabels(yt.astype(str))\n    axH.set_xlabel('$\\\\theta$')\n    axH.set_ylabel('$r$')    \n\n    axH.scatter(lnParamsOpt[0], lnParamsOpt[1], objfun(lnParamsOpt) * 1.002,\n                s=200, color='orange', marker='*', alpha=0.8)\n    if theta:\n        filename = f'{fName}-ObjFun_n-{npts}_d-{dim}_r-{r * 100}_th-{100 * theta}_case-{iii}.jpg'\n    else:\n        filename = f'{fName}-ObjFun_n-{npts}_d-{dim}_case-{iii}.jpg'                \n    figH.savefig(OUTDIR+filename)    \n</pre> def create_surf_plot(fName, lnthth, lnordord, objfun, objobj, lnParamsOpt, r, theta, iii):     figH, axH = plt.subplots(subplot_kw={\"projection\": \"3d\"})     axH.view_init(40, 30)     shandle = axH.plot_surface(lnthth, lnordord, objobj, cmap=cm.coolwarm,                                linewidth=0, antialiased=False, alpha=0.8)     xt = np.array([.2, 0.4, 1, 3, 7])     axH.set_xticks(np.log(xt))     axH.set_xticklabels(xt.astype(str))     yt = np.array([1.4, 1.6, 2, 2.6, 3.7])     axH.set_yticks(np.log(yt - 1))     axH.set_yticklabels(yt.astype(str))     axH.set_xlabel('$\\\\theta$')     axH.set_ylabel('$r$')          axH.scatter(lnParamsOpt[0], lnParamsOpt[1], objfun(lnParamsOpt) * 1.002,                 s=200, color='orange', marker='*', alpha=0.8)     if theta:         filename = f'{fName}-ObjFun_n-{npts}_d-{dim}_r-{r * 100}_th-{100 * theta}_case-{iii}.jpg'     else:         filename = f'{fName}-ObjFun_n-{npts}_d-{dim}_case-{iii}.jpg'                     figH.savefig(OUTDIR+filename)     <p>Minimum working example to demonstrate Gaussian diagnostics concept</p> In\u00a0[13]: Copied! <pre>def gaussian_diagnostics_engine(whEx, dim, npts, r, fpar, nReps, nPlots):\n    whEx = whEx - 1\n    fNames = ['ExpCos', 'Keister', 'rand']\n    ptransforms = ['none', 'C1sin', 'none']\n    fName = fNames[whEx]\n    ptransform = ptransforms[whEx]\n\n    rOptAll = [0]*nRep\n    thOptAll = [0]*nRep\n\n    # parameters for random function\n    # seed = 202326\n    if whEx == 2:\n        rfun = r / 2\n        f_mean = fpar[2]\n        f_std_a = fpar[0]  # this is square root of the a in the talk\n        f_std_b = fpar[1]  # this is square root of the b in the talk\n        theta = (f_std_a / f_std_b) ** 2\n    else:\n        theta = None\n\n    for iii in range(nReps):\n        seed = np.random.randint(low=1, high=1e6)  # different each rep\n        shift = np.random.rand(1, dim)\n\n        distribution = Lattice(dimension=dim, order='linear')\n        xpts  = distribution.gen_samples(n_min=0, n_max=npts, warn=False)\n        xlat = (xpts-distribution.shift)%1\n        if fName == 'ExpCos':\n            integrand = lambda x: np.exp(np.sum(np.cos(2 * np.pi * x), axis=1))\n        elif fName == 'Keister':\n            keister = Keister(Lattice(dimension=dim, order='linear'))\n            integrand = lambda x: keister.f(x)\n        elif fName == 'rand':\n            integrand = lambda x: f_rand(x, rfun, f_std_a, f_std_b, f_mean, seed)\n        else:\n            print('Invalid function name')\n            return\n\n        y = doPeriodTx(xpts, integrand, ptransform)\n\n        ftilde = np.fft.fft(y)  # fourier coefficients\n        ftilde[0] = 0  # ftilde = \\mV**H(\\vf - m \\vone), subtract mean\n        if dim == 1:\n            hFigIntegrand = plt.figure()\n            plt.scatter(xpts, y, 10)\n            plt.title(f'{fName}_n-{npts}_Tx-{ptransform}')\n            hFigIntegrand.savefig(OUTDIR+f'{fName}_n-{npts}_Tx-{ptransform}_rFun-{rfun:1.2f}.png')\n\n        def objfun(lnParams):\n            loss, Lambda, RKHSnorm = ObjectiveFunction(np.exp(lnParams[0]), 1 + np.exp(lnParams[1]), xlat, ftilde)\n            return loss\n\n        ## Plot the objective function\n        lnthetarange = np.arange(-2, 2.2, 0.2)  # range of log(theta) for plotting\n        lnorderrange = np.arange(-1, 1.1, 0.1)  # range of log(r) for plotting\n        [lnthth, lnordord] = np.meshgrid(lnthetarange, lnorderrange)\n        objobj = np.zeros(lnthth.shape)\n        for ii in range(lnthth.shape[0]):\n            for jj in range(lnthth.shape[1]):\n                objobj[ii, jj] = objfun([lnthth[ii, jj], lnordord[ii, jj]])\n\n        objMinAppx, which = objobj.min(), objobj.argmin()\n        # [whichrow, whichcol] = ind2sub(lnthth.shape, which)\n        [whichrow, whichcol] = np.unravel_index(which, lnthth.shape)\n        lnthOptAppx = lnthth[whichrow, whichcol]\n        thetaOptAppx = np.exp(lnthOptAppx)\n        lnordOptAppx = lnordord[whichrow, whichcol]\n        orderOptAppx = 1 + np.exp(lnordOptAppx)\n        # print(objMinAppx)  # minimum objective function by brute force search\n\n        ## Optimize the objective function\n        result = fminsearch(objfun, x0=[lnthOptAppx, lnordOptAppx], xtol=1e-3, full_output=True, disp=False)\n        lnParamsOpt, objMin = result[0], result[1]\n        # print(objMin)  # minimum objective function by Nelder-Mead\n        thetaOpt = np.exp(lnParamsOpt[0])\n        rOpt = 1 + np.exp(lnParamsOpt[1])\n        rOptAll[iii] = rOpt\n        thOptAll[iii] = thetaOpt\n        print(f'{iii}: thetaOptAppx={thetaOptAppx:7.5f}, rOptAppx={orderOptAppx:7.5f}, '\n              f'objMinAppx={objMinAppx:7.5f}, objMin={objMin:7.5f}')\n\n        if iii &lt;= nPlots:\n            create_surf_plot(fName, lnthth, lnordord, objfun, objobj, lnParamsOpt, r, theta, iii)\n\n        vlambda = kernel2(thetaOpt, rOpt, xlat)\n        s2 = sum(abs(ftilde[2:] ** 2) / vlambda[2:]) / (npts ** 2)\n        vlambda = s2 * vlambda\n\n        # apply transform\n        # $\\vZ = \\frac 1n \\mV \\mLambda**{-\\frac 12} \\mV**H(\\vf - m \\vone)$\n        # np.fft also includes 1/n division\n        vz = np.fft.ifft(ftilde / np.sqrt(vlambda))\n        vz_real = np.real(vz)  # vz must be real as intended by the transformation\n\n        if iii &lt;= nPlots:\n            create_quant_plot('qqplot', vz_real, fName, dim, iii, r, rOpt, theta, thetaOpt)\n\n        r_str = f\"{r: 7.5f}\" if type(r) == float else str(r)\n        theta_str = f\"{theta: 7.5f}\" if type(theta) == float else str(theta)\n        print(f'\\t r = {r_str}, rOpt = {rOpt:7.5f}, theta = {theta_str}, thetaOpt = {thetaOpt:7.5f}\\n')\n\n    return [theta, rOptAll, thOptAll, fName]\n</pre> def gaussian_diagnostics_engine(whEx, dim, npts, r, fpar, nReps, nPlots):     whEx = whEx - 1     fNames = ['ExpCos', 'Keister', 'rand']     ptransforms = ['none', 'C1sin', 'none']     fName = fNames[whEx]     ptransform = ptransforms[whEx]      rOptAll = [0]*nRep     thOptAll = [0]*nRep      # parameters for random function     # seed = 202326     if whEx == 2:         rfun = r / 2         f_mean = fpar[2]         f_std_a = fpar[0]  # this is square root of the a in the talk         f_std_b = fpar[1]  # this is square root of the b in the talk         theta = (f_std_a / f_std_b) ** 2     else:         theta = None      for iii in range(nReps):         seed = np.random.randint(low=1, high=1e6)  # different each rep         shift = np.random.rand(1, dim)          distribution = Lattice(dimension=dim, order='linear')         xpts  = distribution.gen_samples(n_min=0, n_max=npts, warn=False)         xlat = (xpts-distribution.shift)%1         if fName == 'ExpCos':             integrand = lambda x: np.exp(np.sum(np.cos(2 * np.pi * x), axis=1))         elif fName == 'Keister':             keister = Keister(Lattice(dimension=dim, order='linear'))             integrand = lambda x: keister.f(x)         elif fName == 'rand':             integrand = lambda x: f_rand(x, rfun, f_std_a, f_std_b, f_mean, seed)         else:             print('Invalid function name')             return          y = doPeriodTx(xpts, integrand, ptransform)          ftilde = np.fft.fft(y)  # fourier coefficients         ftilde[0] = 0  # ftilde = \\mV**H(\\vf - m \\vone), subtract mean         if dim == 1:             hFigIntegrand = plt.figure()             plt.scatter(xpts, y, 10)             plt.title(f'{fName}_n-{npts}_Tx-{ptransform}')             hFigIntegrand.savefig(OUTDIR+f'{fName}_n-{npts}_Tx-{ptransform}_rFun-{rfun:1.2f}.png')          def objfun(lnParams):             loss, Lambda, RKHSnorm = ObjectiveFunction(np.exp(lnParams[0]), 1 + np.exp(lnParams[1]), xlat, ftilde)             return loss          ## Plot the objective function         lnthetarange = np.arange(-2, 2.2, 0.2)  # range of log(theta) for plotting         lnorderrange = np.arange(-1, 1.1, 0.1)  # range of log(r) for plotting         [lnthth, lnordord] = np.meshgrid(lnthetarange, lnorderrange)         objobj = np.zeros(lnthth.shape)         for ii in range(lnthth.shape[0]):             for jj in range(lnthth.shape[1]):                 objobj[ii, jj] = objfun([lnthth[ii, jj], lnordord[ii, jj]])          objMinAppx, which = objobj.min(), objobj.argmin()         # [whichrow, whichcol] = ind2sub(lnthth.shape, which)         [whichrow, whichcol] = np.unravel_index(which, lnthth.shape)         lnthOptAppx = lnthth[whichrow, whichcol]         thetaOptAppx = np.exp(lnthOptAppx)         lnordOptAppx = lnordord[whichrow, whichcol]         orderOptAppx = 1 + np.exp(lnordOptAppx)         # print(objMinAppx)  # minimum objective function by brute force search          ## Optimize the objective function         result = fminsearch(objfun, x0=[lnthOptAppx, lnordOptAppx], xtol=1e-3, full_output=True, disp=False)         lnParamsOpt, objMin = result[0], result[1]         # print(objMin)  # minimum objective function by Nelder-Mead         thetaOpt = np.exp(lnParamsOpt[0])         rOpt = 1 + np.exp(lnParamsOpt[1])         rOptAll[iii] = rOpt         thOptAll[iii] = thetaOpt         print(f'{iii}: thetaOptAppx={thetaOptAppx:7.5f}, rOptAppx={orderOptAppx:7.5f}, '               f'objMinAppx={objMinAppx:7.5f}, objMin={objMin:7.5f}')          if iii &lt;= nPlots:             create_surf_plot(fName, lnthth, lnordord, objfun, objobj, lnParamsOpt, r, theta, iii)          vlambda = kernel2(thetaOpt, rOpt, xlat)         s2 = sum(abs(ftilde[2:] ** 2) / vlambda[2:]) / (npts ** 2)         vlambda = s2 * vlambda          # apply transform         # $\\vZ = \\frac 1n \\mV \\mLambda**{-\\frac 12} \\mV**H(\\vf - m \\vone)$         # np.fft also includes 1/n division         vz = np.fft.ifft(ftilde / np.sqrt(vlambda))         vz_real = np.real(vz)  # vz must be real as intended by the transformation          if iii &lt;= nPlots:             create_quant_plot('qqplot', vz_real, fName, dim, iii, r, rOpt, theta, thetaOpt)          r_str = f\"{r: 7.5f}\" if type(r) == float else str(r)         theta_str = f\"{theta: 7.5f}\" if type(theta) == float else str(theta)         print(f'\\t r = {r_str}, rOpt = {rOpt:7.5f}, theta = {theta_str}, thetaOpt = {thetaOpt:7.5f}\\n')      return [theta, rOptAll, thOptAll, fName] In\u00a0[14]: Copied! <pre>fwh = 1\ndim = 3\nnpts = 2 ** 6\nnRep = 20\nnPlot = 2\n[_, rOptAll, thOptAll, fName] = \\\n    gaussian_diagnostics_engine(fwh, dim, npts, None, None, nRep, nPlot)\n\n## Plot Exponential Cosine example\nfigH = plt.figure()\nplt.scatter(rOptAll, thOptAll, s=20, color='blue')\n# axis([4 6 0.1 10])\n# set(gca,'yscale','log')\nplt.title(f'$d = {dim}, n = {npts}$')\nplt.xlabel('Inferred $r$')\nplt.ylabel('Inferred $\\\\theta$')\n# print(f'{fName}-rthInfer-n-{npts}-d-{dim}')\nfigH.savefig(OUTDIR+f'{fName}-rthInfer-n-{npts}-d-{dim}.jpg')\n</pre> fwh = 1 dim = 3 npts = 2 ** 6 nRep = 20 nPlot = 2 [_, rOptAll, thOptAll, fName] = \\     gaussian_diagnostics_engine(fwh, dim, npts, None, None, nRep, nPlot)  ## Plot Exponential Cosine example figH = plt.figure() plt.scatter(rOptAll, thOptAll, s=20, color='blue') # axis([4 6 0.1 10]) # set(gca,'yscale','log') plt.title(f'$d = {dim}, n = {npts}$') plt.xlabel('Inferred $r$') plt.ylabel('Inferred $\\\\theta$') # print(f'{fName}-rthInfer-n-{npts}-d-{dim}') figH.savefig(OUTDIR+f'{fName}-rthInfer-n-{npts}-d-{dim}.jpg') <pre>0: thetaOptAppx=0.36788, rOptAppx=3.71828, objMinAppx=8.99406, objMin=8.93753\n\t r = None, rOpt = 4.48471, theta = None, thetaOpt = 0.41416\n\n1: thetaOptAppx=0.44933, rOptAppx=3.71828, objMinAppx=9.07259, objMin=9.02278\n\t r = None, rOpt = 4.46910, theta = None, thetaOpt = 0.40931\n\n2: thetaOptAppx=0.44933, rOptAppx=3.71828, objMinAppx=8.82430, objMin=8.66711\n\t r = None, rOpt = 5.06778, theta = None, thetaOpt = 0.37222\n\n3: thetaOptAppx=0.36788, rOptAppx=3.71828, objMinAppx=8.70725, objMin=8.54519\n\t r = None, rOpt = 5.06449, theta = None, thetaOpt = 0.35315\n\n4: thetaOptAppx=0.54881, rOptAppx=3.71828, objMinAppx=9.44731, objMin=9.43752\n\t r = None, rOpt = 4.04484, theta = None, thetaOpt = 0.50257\n\n5: thetaOptAppx=0.44933, rOptAppx=3.71828, objMinAppx=9.05749, objMin=8.99965\n\t r = None, rOpt = 4.52695, theta = None, thetaOpt = 0.47314\n\n6: thetaOptAppx=0.36788, rOptAppx=3.71828, objMinAppx=8.80768, objMin=8.65659\n\t r = None, rOpt = 5.07095, theta = None, thetaOpt = 0.34964\n\n7: thetaOptAppx=0.36788, rOptAppx=3.71828, objMinAppx=8.66418, objMin=8.43411\n\t r = None, rOpt = 5.39659, theta = None, thetaOpt = 0.31540\n\n8: thetaOptAppx=0.36788, rOptAppx=3.71828, objMinAppx=8.66215, objMin=8.45730\n\t r = None, rOpt = 4.90284, theta = None, thetaOpt = 0.40416\n\n9: thetaOptAppx=0.36788, rOptAppx=3.71828, objMinAppx=8.72574, objMin=8.56897\n\t r = None, rOpt = 5.03484, theta = None, thetaOpt = 0.31357\n\n10: thetaOptAppx=0.44933, rOptAppx=3.71828, objMinAppx=9.34300, objMin=9.31896\n\t r = None, rOpt = 4.30372, theta = None, thetaOpt = 0.50097\n\n11: thetaOptAppx=0.36788, rOptAppx=3.71828, objMinAppx=8.75005, objMin=8.62916\n\t r = None, rOpt = 4.83743, theta = None, thetaOpt = 0.35090\n\n12: thetaOptAppx=0.36788, rOptAppx=3.71828, objMinAppx=8.87981, objMin=8.74778\n\t r = None, rOpt = 5.00868, theta = None, thetaOpt = 0.36687\n\n13: thetaOptAppx=0.36788, rOptAppx=3.71828, objMinAppx=8.93996, objMin=8.87062\n\t r = None, rOpt = 4.62564, theta = None, thetaOpt = 0.37758\n\n14: thetaOptAppx=0.36788, rOptAppx=3.71828, objMinAppx=8.79702, objMin=8.66612\n\t r = None, rOpt = 4.93187, theta = None, thetaOpt = 0.32310\n\n15: thetaOptAppx=0.36788, rOptAppx=3.71828, objMinAppx=8.83119, objMin=8.65661\n\t r = None, rOpt = 5.30991, theta = None, thetaOpt = 0.36354\n\n16: thetaOptAppx=0.36788, rOptAppx=3.71828, objMinAppx=8.69747, objMin=8.53796\n\t r = None, rOpt = 4.89472, theta = None, thetaOpt = 0.36814\n\n17: thetaOptAppx=0.44933, rOptAppx=3.71828, objMinAppx=9.24219, objMin=9.20810\n\t r = None, rOpt = 4.35318, theta = None, thetaOpt = 0.47006\n\n18: thetaOptAppx=0.36788, rOptAppx=3.71828, objMinAppx=8.71916, objMin=8.52738\n\t r = None, rOpt = 5.23099, theta = None, thetaOpt = 0.36578\n\n19: thetaOptAppx=0.36788, rOptAppx=3.71828, objMinAppx=8.84619, objMin=8.63422\n\t r = None, rOpt = 5.26904, theta = None, thetaOpt = 0.41594\n\n</pre> In\u00a0[15]: Copied! <pre># close all the previous plots to freeup memory\nplt.close('all')\n</pre> # close all the previous plots to freeup memory plt.close('all') In\u00a0[16]: Copied! <pre>## Tests with random function\nrArray = [1.5, 2, 4]\nnrArr = len(rArray)\nfParArray = [[0.5, 1, 2], [1, 1, 1], [1, 1, 1]]\nnfPArr = len(fParArray)\nfwh = 3\ndim = 2\nnpts = 2 ** 6\nnRep = 5  # reduced from 20 to reduce the plots\nnPlot = 2\nthetaAll = np.zeros((nrArr, nfPArr))\nrOptAll = np.zeros((nrArr, nfPArr, nRep))\nthOptAll = np.zeros((nrArr, nfPArr, nRep))\nfor jjj in range(nrArr):\n    for kkk in range(nfPArr):\n        thetaAll[jjj, kkk], rOptAll[jjj, kkk, :], thOptAll[jjj, kkk, :], fName = \\\n            gaussian_diagnostics_engine(fwh, dim, npts, rArray[jjj], fParArray[kkk], nRep, nPlot)\n</pre> ## Tests with random function rArray = [1.5, 2, 4] nrArr = len(rArray) fParArray = [[0.5, 1, 2], [1, 1, 1], [1, 1, 1]] nfPArr = len(fParArray) fwh = 3 dim = 2 npts = 2 ** 6 nRep = 5  # reduced from 20 to reduce the plots nPlot = 2 thetaAll = np.zeros((nrArr, nfPArr)) rOptAll = np.zeros((nrArr, nfPArr, nRep)) thOptAll = np.zeros((nrArr, nfPArr, nRep)) for jjj in range(nrArr):     for kkk in range(nfPArr):         thetaAll[jjj, kkk], rOptAll[jjj, kkk, :], thOptAll[jjj, kkk, :], fName = \\             gaussian_diagnostics_engine(fwh, dim, npts, rArray[jjj], fParArray[kkk], nRep, nPlot)  <pre>0: thetaOptAppx=0.24660, rOptAppx=1.40657, objMinAppx=7.14637, objMin=7.14629\n\t r =  1.50000, rOpt = 1.40165, theta =  0.25000, thetaOpt = 0.25968\n\n1: thetaOptAppx=0.54881, rOptAppx=1.36788, objMinAppx=6.86055, objMin=6.82118\n\t r =  1.50000, rOpt = 1.00000, theta =  0.25000, thetaOpt = 0.27367\n\n2: thetaOptAppx=0.81873, rOptAppx=1.49659, objMinAppx=7.29922, objMin=7.29904\n\t r =  1.50000, rOpt = 1.51253, theta =  0.25000, thetaOpt = 0.89951\n\n3: thetaOptAppx=0.20190, rOptAppx=1.36788, objMinAppx=6.85804, objMin=6.82400\n\t r =  1.50000, rOpt = 1.00334, theta =  0.25000, thetaOpt = 0.11906\n\n4: thetaOptAppx=0.36788, rOptAppx=1.36788, objMinAppx=7.08249, objMin=7.08235\n\t r =  1.50000, rOpt = 1.38827, theta =  0.25000, thetaOpt = 0.38423\n\n0: thetaOptAppx=3.32012, rOptAppx=1.36788, objMinAppx=10.63981, objMin=10.61913\n\t r =  1.50000, rOpt = 1.00000, theta =  1.00000, thetaOpt = 2.13670\n\n1: thetaOptAppx=3.32012, rOptAppx=1.81873, objMinAppx=10.63974, objMin=10.63958\n\t r =  1.50000, rOpt = 1.82252, theta =  1.00000, thetaOpt = 3.13414\n\n2: thetaOptAppx=1.00000, rOptAppx=1.60653, objMinAppx=10.31446, objMin=10.31433\n\t r =  1.50000, rOpt = 1.58466, theta =  1.00000, thetaOpt = 1.00000\n\n3: thetaOptAppx=7.38906, rOptAppx=1.36788, objMinAppx=10.77169, objMin=10.73339\n\t r =  1.50000, rOpt = 1.00000, theta =  1.00000, thetaOpt = 8.50588\n\n4: thetaOptAppx=3.32012, rOptAppx=1.36788, objMinAppx=10.90059, objMin=10.89579\n\t r =  1.50000, rOpt = 1.19371, theta =  1.00000, thetaOpt = 2.19191\n\n0: thetaOptAppx=3.32012, rOptAppx=1.36788, objMinAppx=10.57812, objMin=10.54021\n\t r =  1.50000, rOpt = 1.00000, theta =  1.00000, thetaOpt = 1.88466\n\n1: thetaOptAppx=2.22554, rOptAppx=1.36788, objMinAppx=10.38092, objMin=10.35134\n\t r =  1.50000, rOpt = 1.00000, theta =  1.00000, thetaOpt = 1.41946\n\n2: thetaOptAppx=1.00000, rOptAppx=1.60653, objMinAppx=10.49902, objMin=10.49902\n\t r =  1.50000, rOpt = 1.60511, theta =  1.00000, thetaOpt = 1.00000\n\n3: thetaOptAppx=7.38906, rOptAppx=1.36788, objMinAppx=10.61599, objMin=10.56274\n\t r =  1.50000, rOpt = 1.00000, theta =  1.00000, thetaOpt = 5933827228175301.00000\n\n4: thetaOptAppx=3.32012, rOptAppx=1.44933, objMinAppx=10.99286, objMin=10.99283\n\t r =  1.50000, rOpt = 1.44032, theta =  1.00000, thetaOpt = 3.41295\n\n0: thetaOptAppx=0.67032, rOptAppx=2.22140, objMinAppx=6.41351, objMin=6.41233\n\t r = 2, rOpt = 2.28962, theta =  0.25000, thetaOpt = 0.69375\n\n1: thetaOptAppx=0.24660, rOptAppx=1.81873, objMinAppx=5.83228, objMin=5.83186\n\t r = 2, rOpt = 1.83344, theta =  0.25000, thetaOpt = 0.22655\n\n</pre> <pre>/var/folders/rz/_ktvltjs49v_z33w0h6njx5w0000gn/T/ipykernel_70657/2271441997.py:2: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n  figH, axH = plt.subplots(subplot_kw={\"projection\": \"3d\"})\n</pre> <pre>2: thetaOptAppx=0.20190, rOptAppx=1.81873, objMinAppx=5.99832, objMin=5.99775\n\t r = 2, rOpt = 1.78303, theta =  0.25000, thetaOpt = 0.18575\n\n3: thetaOptAppx=0.30119, rOptAppx=1.74082, objMinAppx=6.18621, objMin=6.18560\n\t r = 2, rOpt = 1.77182, theta =  0.25000, thetaOpt = 0.33864\n\n4: thetaOptAppx=0.30119, rOptAppx=1.74082, objMinAppx=5.98469, objMin=5.98449\n\t r = 2, rOpt = 1.73013, theta =  0.25000, thetaOpt = 0.28315\n\n0: thetaOptAppx=0.44933, rOptAppx=1.54881, objMinAppx=9.30998, objMin=9.30986\n\t r = 2, rOpt = 1.56218, theta =  1.00000, thetaOpt = 0.44163\n\n1: thetaOptAppx=1.22140, rOptAppx=2.34986, objMinAppx=9.45655, objMin=9.45601\n\t r = 2, rOpt = 2.30027, theta =  1.00000, thetaOpt = 1.11415\n\n2: thetaOptAppx=1.49182, rOptAppx=1.90484, objMinAppx=9.80753, objMin=9.80727\n\t r = 2, rOpt = 1.88085, theta =  1.00000, thetaOpt = 1.52755\n\n3: thetaOptAppx=1.49182, rOptAppx=1.90484, objMinAppx=9.83320, objMin=9.83283\n\t r = 2, rOpt = 1.91255, theta =  1.00000, thetaOpt = 1.62186\n\n4: thetaOptAppx=2.22554, rOptAppx=2.10517, objMinAppx=9.68118, objMin=9.68072\n\t r = 2, rOpt = 2.12564, theta =  1.00000, thetaOpt = 2.09482\n\n0: thetaOptAppx=7.38906, rOptAppx=3.71828, objMinAppx=9.61269, objMin=9.45113\n</pre> <pre>/var/folders/rz/_ktvltjs49v_z33w0h6njx5w0000gn/T/ipykernel_70657/4237111989.py:93: RuntimeWarning: invalid value encountered in sqrt\n  vz = np.fft.ifft(ftilde / np.sqrt(vlambda))\n</pre> <pre>\t r = 2, rOpt = 3.53413, theta =  1.00000, thetaOpt = 14.37293\n\n1: thetaOptAppx=2.22554, rOptAppx=1.74082, objMinAppx=9.35057, objMin=9.35008\n\t r = 2, rOpt = 1.78488, theta =  1.00000, thetaOpt = 2.37197\n\n2: thetaOptAppx=2.71828, rOptAppx=1.67032, objMinAppx=10.10692, objMin=10.10650\n\t r = 2, rOpt = 1.69607, theta =  1.00000, thetaOpt = 2.58742\n\n3: thetaOptAppx=2.22554, rOptAppx=1.90484, objMinAppx=9.55155, objMin=9.55141\n\t r = 2, rOpt = 1.90988, theta =  1.00000, thetaOpt = 2.36814\n\n4: thetaOptAppx=1.49182, rOptAppx=1.44933, objMinAppx=9.91009, objMin=9.91004\n\t r = 2, rOpt = 1.45254, theta =  1.00000, thetaOpt = 1.45063\n\n0: thetaOptAppx=0.16530, rOptAppx=3.01375, objMinAppx=3.22085, objMin=3.21902\n\t r = 4, rOpt = 2.93602, theta =  0.25000, thetaOpt = 0.13681\n\n1: thetaOptAppx=0.24660, rOptAppx=3.01375, objMinAppx=3.99176, objMin=3.94911\n\t r = 4, rOpt = 3.08101, theta =  0.25000, thetaOpt = 0.30249\n\n2: thetaOptAppx=0.67032, rOptAppx=3.45960, objMinAppx=3.70233, objMin=3.70125\n\t r = 4, rOpt = 3.46039, theta =  0.25000, thetaOpt = 0.71094\n\n3: thetaOptAppx=0.67032, rOptAppx=3.71828, objMinAppx=3.76987, objMin=3.76755\n\t r = 4, rOpt = 3.62029, theta =  0.25000, thetaOpt = 0.64446\n\n4: thetaOptAppx=0.44933, rOptAppx=3.71828, objMinAppx=3.89213, objMin=3.89083\n\t r = 4, rOpt = 3.75463, theta =  0.25000, thetaOpt = 0.43057\n\n0: thetaOptAppx=1.00000, rOptAppx=3.71828, objMinAppx=7.04925, objMin=7.04883\n\t r = 4, rOpt = 3.68451, theta =  1.00000, thetaOpt = 1.00000\n\n1: thetaOptAppx=1.49182, rOptAppx=3.71828, objMinAppx=6.97971, objMin=6.95183\n\t r = 4, rOpt = 4.13146, theta =  1.00000, thetaOpt = 1.72858\n\n2: thetaOptAppx=1.22140, rOptAppx=3.71828, objMinAppx=7.23932, objMin=7.18983\n\t r = 4, rOpt = 4.27292, theta =  1.00000, thetaOpt = 1.87128\n\n3: thetaOptAppx=2.71828, rOptAppx=3.71828, objMinAppx=7.40308, objMin=7.36397\n\t r = 4, rOpt = 3.58459, theta =  1.00000, thetaOpt = 4.17137\n\n</pre> <pre>/var/folders/rz/_ktvltjs49v_z33w0h6njx5w0000gn/T/ipykernel_70657/4237111989.py:93: RuntimeWarning: invalid value encountered in sqrt\n  vz = np.fft.ifft(ftilde / np.sqrt(vlambda))\n</pre> <pre>4: thetaOptAppx=1.22140, rOptAppx=3.45960, objMinAppx=7.10667, objMin=7.10617\n\t r = 4, rOpt = 3.50364, theta =  1.00000, thetaOpt = 1.21819\n\n0: thetaOptAppx=1.22140, rOptAppx=3.71828, objMinAppx=7.16822, objMin=7.15972\n\t r = 4, rOpt = 3.97812, theta =  1.00000, thetaOpt = 1.55273\n\n1: thetaOptAppx=1.82212, rOptAppx=3.71828, objMinAppx=7.36156, objMin=7.34469\n\t r = 4, rOpt = 3.57702, theta =  1.00000, thetaOpt = 1.85305\n\n2: thetaOptAppx=1.82212, rOptAppx=3.71828, objMinAppx=7.05146, objMin=7.04365\n\t r = 4, rOpt = 3.94260, theta =  1.00000, thetaOpt = 1.72259\n\n3: thetaOptAppx=1.22140, rOptAppx=3.22554, objMinAppx=6.98588, objMin=6.97968\n\t r = 4, rOpt = 3.20799, theta =  1.00000, thetaOpt = 1.06432\n\n4: thetaOptAppx=1.22140, rOptAppx=3.71828, objMinAppx=6.77223, objMin=6.75074\n\t r = 4, rOpt = 4.11981, theta =  1.00000, thetaOpt = 1.31325\n\n</pre> In\u00a0[17]: Copied! <pre># close all the previous plots to freeup memory\nplt.close('all')\n</pre> # close all the previous plots to freeup memory plt.close('all') In\u00a0[18]: Copied! <pre>figH, axH = plt.subplots()\ncolorArray = ['blue', 'orange', 'green', 'cyan', 'maroon', 'purple']\nnColArray = len(colorArray)\nfor jjj in range(nrArr):\n    for kkk in range(nfPArr):\n        clrInd = np.mod(nfPArr * (jjj) + kkk, nColArray)\n        clr = colorArray[clrInd]\n        axH.scatter(rOptAll[jjj, kkk, :].reshape((nRep, 1)), thOptAll[jjj, kkk, :].reshape((nRep, 1)),\n                 s=50, c=clr, marker='.')\n        axH.scatter(rArray[jjj], thetaAll[jjj, kkk], s=50, c=clr, marker='D')\n\naxH.set(xlim=[1, 6], ylim=[0.01, 100])\naxH.set_yscale('log')\naxH.set_title(f'$d = {dim}, n = {npts}$')\naxH.set_xlabel('Inferred $r$')\naxH.set_ylabel('Inferred $\\\\theta$')\nfigH.savefig(OUTDIR+f'{fName}-rthInfer-n-{npts}-d-{dim}.jpg')\n</pre> figH, axH = plt.subplots() colorArray = ['blue', 'orange', 'green', 'cyan', 'maroon', 'purple'] nColArray = len(colorArray) for jjj in range(nrArr):     for kkk in range(nfPArr):         clrInd = np.mod(nfPArr * (jjj) + kkk, nColArray)         clr = colorArray[clrInd]         axH.scatter(rOptAll[jjj, kkk, :].reshape((nRep, 1)), thOptAll[jjj, kkk, :].reshape((nRep, 1)),                  s=50, c=clr, marker='.')         axH.scatter(rArray[jjj], thetaAll[jjj, kkk], s=50, c=clr, marker='D')  axH.set(xlim=[1, 6], ylim=[0.01, 100]) axH.set_yscale('log') axH.set_title(f'$d = {dim}, n = {npts}$') axH.set_xlabel('Inferred $r$') axH.set_ylabel('Inferred $\\\\theta$') figH.savefig(OUTDIR+f'{fName}-rthInfer-n-{npts}-d-{dim}.jpg') In\u00a0[19]: Copied! <pre># close all the previous plots to freeup memory\nplt.close('all')\n</pre> # close all the previous plots to freeup memory plt.close('all') In\u00a0[20]: Copied! <pre>## Keister example\nfwh = 2\ndim = 3\nnpts = 2 ** 6\nnRep = 20\nnPlot = 2\n_, rOptAll, thOptAll, fName = gaussian_diagnostics_engine(fwh, dim, npts, None, None, nRep, nPlot)\n\n## Plot Keister example\nfigH = plt.figure()\nplt.scatter(rOptAll, thOptAll, s=20, color='blue')\n# axis([4 6 0.5 1.5])\n# set(gca,'yscale','log')\nplt.xlabel('Inferred $r$')\nplt.ylabel('Inferred $\\\\theta$')\nplt.title(f'$d = {dim}, n = {npts}$')\nfigH.savefig(OUTDIR+f'{fName}-rthInfer-n-{npts}-d-{dim}.jpg')\n</pre> ## Keister example fwh = 2 dim = 3 npts = 2 ** 6 nRep = 20 nPlot = 2 _, rOptAll, thOptAll, fName = gaussian_diagnostics_engine(fwh, dim, npts, None, None, nRep, nPlot)  ## Plot Keister example figH = plt.figure() plt.scatter(rOptAll, thOptAll, s=20, color='blue') # axis([4 6 0.5 1.5]) # set(gca,'yscale','log') plt.xlabel('Inferred $r$') plt.ylabel('Inferred $\\\\theta$') plt.title(f'$d = {dim}, n = {npts}$') figH.savefig(OUTDIR+f'{fName}-rthInfer-n-{npts}-d-{dim}.jpg')  <pre>0: thetaOptAppx=0.67032, rOptAppx=3.71828, objMinAppx=10.95471, objMin=10.85287\n\t r = None, rOpt = 4.89011, theta = None, thetaOpt = 0.67160\n\n1: thetaOptAppx=0.81873, rOptAppx=3.71828, objMinAppx=10.97614, objMin=10.79973\n\t r = None, rOpt = 5.23417, theta = None, thetaOpt = 0.74064\n\n2: thetaOptAppx=0.81873, rOptAppx=3.71828, objMinAppx=11.01901, objMin=10.86699\n\t r = None, rOpt = 5.12195, theta = None, thetaOpt = 0.82888\n\n3: thetaOptAppx=0.67032, rOptAppx=3.71828, objMinAppx=11.03911, objMin=10.96269\n\t r = None, rOpt = 4.64960, theta = None, thetaOpt = 0.70302\n\n4: thetaOptAppx=0.81873, rOptAppx=3.71828, objMinAppx=11.07413, objMin=10.90736\n\t r = None, rOpt = 5.33026, theta = None, thetaOpt = 0.74945\n\n5: thetaOptAppx=0.81873, rOptAppx=3.71828, objMinAppx=11.16827, objMin=11.06970\n\t r = None, rOpt = 4.91810, theta = None, thetaOpt = 0.84299\n\n6: thetaOptAppx=0.81873, rOptAppx=3.71828, objMinAppx=11.11317, objMin=10.97738\n\t r = None, rOpt = 5.07012, theta = None, thetaOpt = 0.80554\n\n7: thetaOptAppx=0.81873, rOptAppx=3.71828, objMinAppx=11.04175, objMin=10.87923\n\t r = None, rOpt = 5.16208, theta = None, thetaOpt = 0.91717\n\n8: thetaOptAppx=0.81873, rOptAppx=3.71828, objMinAppx=11.24635, objMin=11.17003\n\t r = None, rOpt = 4.71247, theta = None, thetaOpt = 0.88444\n\n9: thetaOptAppx=0.67032, rOptAppx=3.71828, objMinAppx=10.98831, objMin=10.88986\n\t r = None, rOpt = 4.82958, theta = None, thetaOpt = 0.70734\n\n10: thetaOptAppx=0.81873, rOptAppx=3.71828, objMinAppx=11.05779, objMin=10.95596\n\t r = None, rOpt = 4.87874, theta = None, thetaOpt = 0.75186\n\n11: thetaOptAppx=0.81873, rOptAppx=3.71828, objMinAppx=11.08281, objMin=10.96752\n\t r = None, rOpt = 4.98171, theta = None, thetaOpt = 0.80117\n\n12: thetaOptAppx=0.81873, rOptAppx=3.71828, objMinAppx=11.17064, objMin=11.04885\n\t r = None, rOpt = 5.18872, theta = None, thetaOpt = 0.68626\n\n13: thetaOptAppx=0.81873, rOptAppx=3.71828, objMinAppx=10.97924, objMin=10.81975\n\t r = None, rOpt = 5.08933, theta = None, thetaOpt = 0.89380\n\n14: thetaOptAppx=0.81873, rOptAppx=3.71828, objMinAppx=11.19044, objMin=11.09012\n\t r = None, rOpt = 4.88557, theta = None, thetaOpt = 0.79560\n\n15: thetaOptAppx=0.67032, rOptAppx=3.71828, objMinAppx=10.93972, objMin=10.84403\n\t r = None, rOpt = 4.78263, theta = None, thetaOpt = 0.70404\n\n16: thetaOptAppx=0.81873, rOptAppx=3.71828, objMinAppx=11.07454, objMin=10.96320\n\t r = None, rOpt = 4.92201, theta = None, thetaOpt = 0.72613\n\n17: thetaOptAppx=0.67032, rOptAppx=3.71828, objMinAppx=10.98796, objMin=10.90210\n\t r = None, rOpt = 4.85879, theta = None, thetaOpt = 0.59679\n\n18: thetaOptAppx=0.81873, rOptAppx=3.71828, objMinAppx=11.11301, objMin=10.99340\n\t r = None, rOpt = 5.06588, theta = None, thetaOpt = 0.73588\n\n19: thetaOptAppx=0.81873, rOptAppx=3.71828, objMinAppx=11.15762, objMin=11.04832\n\t r = None, rOpt = 4.99360, theta = None, thetaOpt = 0.74271\n\n</pre> In\u00a0[21]: Copied! <pre>## Keister example\nfwh = 2\ndim = 3\nnpts = 2 ** 10\nnRep = 20\nnPlot = 2\n_, rOptAll, thOptAll, fName = gaussian_diagnostics_engine(fwh, dim, npts, None, None, nRep, nPlot)\n\n## Plot Keister example\nfigH = plt.figure()\nplt.scatter(rOptAll, thOptAll, s=20, color='blue')\n# axis([4 6 0.5 1.5])\n# set(gca,'yscale','log')\nplt.xlabel('Inferred $r$')\nplt.ylabel('Inferred $\\\\theta$')\nplt.title(f'$d = {dim}, n = {npts}$')\nfigH.savefig(OUTDIR+f'{fName}-rthInfer-n-{npts}-d-{dim}.jpg')\n</pre> ## Keister example fwh = 2 dim = 3 npts = 2 ** 10 nRep = 20 nPlot = 2 _, rOptAll, thOptAll, fName = gaussian_diagnostics_engine(fwh, dim, npts, None, None, nRep, nPlot)  ## Plot Keister example figH = plt.figure() plt.scatter(rOptAll, thOptAll, s=20, color='blue') # axis([4 6 0.5 1.5]) # set(gca,'yscale','log') plt.xlabel('Inferred $r$') plt.ylabel('Inferred $\\\\theta$') plt.title(f'$d = {dim}, n = {npts}$') figH.savefig(OUTDIR+f'{fName}-rthInfer-n-{npts}-d-{dim}.jpg') <pre>0: thetaOptAppx=1.00000, rOptAppx=3.71828, objMinAppx=10.85760, objMin=10.69482\n\t r = None, rOpt = 3.89437, theta = None, thetaOpt = 1.00000\n\n</pre> <pre>/var/folders/rz/_ktvltjs49v_z33w0h6njx5w0000gn/T/ipykernel_70657/4237111989.py:93: RuntimeWarning: invalid value encountered in sqrt\n  vz = np.fft.ifft(ftilde / np.sqrt(vlambda))\n</pre> <pre>1: thetaOptAppx=0.67032, rOptAppx=3.71828, objMinAppx=10.45233, objMin=6.74852\n\t r = None, rOpt = 7.22861, theta = None, thetaOpt = 0.67986\n\n2: thetaOptAppx=0.67032, rOptAppx=3.71828, objMinAppx=10.44838, objMin=6.69462\n\t r = None, rOpt = 7.27867, theta = None, thetaOpt = 0.76628\n\n3: thetaOptAppx=0.67032, rOptAppx=3.71828, objMinAppx=10.44846, objMin=6.67804\n\t r = None, rOpt = 7.36369, theta = None, thetaOpt = 0.72433\n\n4: thetaOptAppx=0.67032, rOptAppx=3.71828, objMinAppx=10.44802, objMin=6.67556\n\t r = None, rOpt = 7.33379, theta = None, thetaOpt = 0.75321\n\n5: thetaOptAppx=1.00000, rOptAppx=3.71828, objMinAppx=11.13038, objMin=11.11848\n\t r = None, rOpt = 3.72240, theta = None, thetaOpt = 1.00000\n\n</pre> <pre>/var/folders/rz/_ktvltjs49v_z33w0h6njx5w0000gn/T/ipykernel_70657/4237111989.py:93: RuntimeWarning: invalid value encountered in sqrt\n  vz = np.fft.ifft(ftilde / np.sqrt(vlambda))\n</pre> <pre>6: thetaOptAppx=0.67032, rOptAppx=3.71828, objMinAppx=10.45040, objMin=6.71403\n\t r = None, rOpt = 7.30300, theta = None, thetaOpt = 0.69226\n\n7: thetaOptAppx=0.67032, rOptAppx=3.71828, objMinAppx=10.45274, objMin=6.74595\n\t r = None, rOpt = 7.27447, theta = None, thetaOpt = 0.72335\n\n8: thetaOptAppx=0.67032, rOptAppx=3.71828, objMinAppx=10.45001, objMin=6.74406\n\t r = None, rOpt = 7.26377, theta = None, thetaOpt = 0.68239\n\n9: thetaOptAppx=0.81873, rOptAppx=3.71828, objMinAppx=10.52787, objMin=10.14371\n\t r = None, rOpt = 4.10478, theta = None, thetaOpt = 0.81984\n\n10: thetaOptAppx=1.00000, rOptAppx=3.71828, objMinAppx=10.59215, objMin=10.55441\n\t r = None, rOpt = 3.72814, theta = None, thetaOpt = 1.00000\n\n11: thetaOptAppx=1.22140, rOptAppx=3.45960, objMinAppx=11.24312, objMin=11.16721\n\t r = None, rOpt = 3.53017, theta = None, thetaOpt = 1.22183\n\n12: thetaOptAppx=1.00000, rOptAppx=3.71828, objMinAppx=11.06674, objMin=11.04394\n\t r = None, rOpt = 3.78815, theta = None, thetaOpt = 1.00000\n\n13: thetaOptAppx=0.13534, rOptAppx=1.36788, objMinAppx=    nan, objMin=    nan\n\t r = None, rOpt = 1.36788, theta = None, thetaOpt = 0.13534\n\n14: thetaOptAppx=0.67032, rOptAppx=3.71828, objMinAppx=10.44974, objMin=6.68183\n\t r = None, rOpt = 7.31864, theta = None, thetaOpt = 0.77082\n\n15: thetaOptAppx=0.67032, rOptAppx=3.71828, objMinAppx=10.53690, objMin=10.04346\n\t r = None, rOpt = 4.14307, theta = None, thetaOpt = 0.69386\n\n16: thetaOptAppx=1.00000, rOptAppx=3.71828, objMinAppx=10.87803, objMin=10.83007\n\t r = None, rOpt = 3.73673, theta = None, thetaOpt = 1.00000\n\n17: thetaOptAppx=0.67032, rOptAppx=3.71828, objMinAppx=10.44751, objMin=6.70502\n\t r = None, rOpt = 7.30048, theta = None, thetaOpt = 0.74603\n\n18: thetaOptAppx=0.67032, rOptAppx=3.71828, objMinAppx=10.57719, objMin=9.35705\n\t r = None, rOpt = 4.85788, theta = None, thetaOpt = 0.75327\n\n19: thetaOptAppx=0.67032, rOptAppx=3.71828, objMinAppx=10.44925, objMin=6.63290\n\t r = None, rOpt = 7.40375, theta = None, thetaOpt = 0.76246\n\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"demos/gaussian_diagnostics/gaussian_diagnostics_demo/#gaussian-diagnostics","title":"Gaussian Diagnostics\u00b6","text":"<p>Experiments to demonstrate Gaussian assumption used in <code>cubBayesLattice</code></p>"},{"location":"demos/gaussian_diagnostics/gaussian_diagnostics_demo/#example-1-exponential-of-cosine","title":"Example 1: Exponential of Cosine\u00b6","text":""},{"location":"demos/gaussian_diagnostics/gaussian_diagnostics_demo/#example-2-random-function","title":"Example 2: Random function\u00b6","text":""},{"location":"demos/gaussian_diagnostics/gaussian_diagnostics_demo/#plot-additional-figures-for-random-function","title":"Plot additional figures for random function\u00b6","text":""},{"location":"demos/gaussian_diagnostics/gaussian_diagnostics_demo/#example-3a-keister-integrand-npts-64","title":"Example 3a: Keister integrand: npts = 64\u00b6","text":""},{"location":"demos/gaussian_diagnostics/gaussian_diagnostics_demo/#example-3b-keister-integrand-npts-1024","title":"Example 3b: Keister integrand: npts = 1024\u00b6","text":""},{"location":"demos/scipywrapper_dependence_custom/","title":"SciPyWrapper dependence and custom distributions demo","text":""},{"location":"demos/scipywrapper_dependence_custom/#scipywrapper-dependence-and-custom-distributions-demo","title":"SciPyWrapper dependence and custom distributions demo","text":"<p>This folder contains the demo notebook and helper files for my project on extending <code>qmcpy.true_measure.SciPyWrapper</code> to handle</p> <ol> <li>Joint dependent distributions, and  </li> <li>User defined distributions that follow a SciPy like interface, with basic sanity checks.</li> </ol> <p>The goal is to show that we can move beyond independent marginals from <code>scipy.stats</code> and still plug everything into the usual QMCPy workflow.</p>"},{"location":"demos/scipywrapper_dependence_custom/#folder-structure","title":"Folder structure","text":"<p>From the top level of the QMCSoftware repo, the pieces for this project are:</p> <ul> <li><code>qmcpy/true_measure/scipy_wrapper.py</code>   Updated <code>SciPyWrapper</code> with:</li> <li>support for a joint distribution object with a <code>transform</code> and optional <code>logpdf</code> or <code>pdf</code>,</li> <li>support for user defined univariate distributions that look like frozen SciPy objects,</li> <li> <p>light weight validation that warns when a custom distribution looks suspicious.</p> </li> <li> <p><code>demos/scipywrapper_dependence_custom/scipywrapper_demo.ipynb</code>   Jupyter notebook that produces all figures and printed statistics used in the blog:</p> </li> <li>Example 1: independent vs dependent normals,</li> <li>Example 2: zero inflated exponential plus uniform,</li> <li>Example 3: acceptance rejection target with iid MC and with QMC,</li> <li>Example 4: custom triangular marginal,</li> <li>Example 5: intentionally broken custom distribution to show the warnings,</li> <li> <p>Example 6: multivariate Student t (joint, dependent) using SciPy\u2019s multivariate t.</p> </li> <li> <p><code>demos/scipywrapper_dependence_custom/README.md</code>   This file.</p> </li> <li> <p><code>demos/scipywrapper_dependence_custom/blog.md</code>   Draft of the blog describing the project at a higher level.</p> </li> <li> <p><code>test/test_scipy_wrapper_custom.py</code>   Unit tests that:</p> </li> <li>check joint sampling and correlations for the multivariate normal example,</li> <li>verify the zero inflated exponential marginal mass at zero,</li> <li>exercise the validation logic for user defined distributions.</li> </ul>"},{"location":"demos/scipywrapper_dependence_custom/#how-to-run-the-demo","title":"How to run the demo","text":"<p>These commands are all intended to be run from the root of the QMCSoftware repo, inside a Python environment where you plan to develop.</p> <ol> <li>Install QMCPy from source in editable mode</li> </ol> <p></p><pre><code>pip install -e .\n</code></pre> This makes sure Python will use your updated <code>SciPyWrapper</code> instead of any pip installed version.<p></p> <ol> <li> <p>Run the custom tests</p> <p></p><pre><code>pytest test/test_scipy_wrapper_custom.py\n</code></pre> You should see all tests pass. A warning about the zero inflated joint distribution not having logpdf is expected. That example is only used for sampling and visualisation so weights are intentionally treated as 1.<p></p> </li> <li> <p>Open the notebook</p> <p></p><pre><code>cd demos/scipywrapper_dependence_custom\njupyter notebook scipywrapper_demo.ipynb\n</code></pre> Run all cells from top to bottom. You will see: - a side by side scatter plot for independent normals and a correlated multivariate normal, - histograms and scatter plots for the zero inflated exponential plus uniform joint, - acceptance rejection clouds for iid MC and QMC on the same triangular target, - a histogram of the custom triangular marginal overlaid with its analytic pdf, - console output with warnings for the intentionally broken custom distribution, - a marginal histogram and joint scatter plot for a dependent multivariate Student t, along with printed checks for correlation and the theoretical covariance relationship.<p></p> </li> </ol> <p>These are the same figures that the blog refers to.</p>"},{"location":"demos/scipywrapper_dependence_custom/#how-to-use-the-new-features-in-your-own-code","title":"How to use the new features in your own code","text":"<p>Here is the minimal idea for each feature.</p>"},{"location":"demos/scipywrapper_dependence_custom/#1-joint-dependent-distributions","title":"1. Joint dependent distributions","text":"<p>Wrap any joint distribution that can map <code>[0, 1]^d</code> to your target space:</p> <p></p><pre><code>joint = MyJointDistribution(dim=2, some_param=...)\nsampler = DigitalNetB2(2, seed=17)\ntm = SciPyWrapper(sampler, joint)\nx = tm(1000)       # shape (1000, 2)\n</code></pre> <code>MyJointDistribution</code> needs: - a <code>dim</code> attribute, and - a <code>transform(u)</code> method that accepts an array in <code>[0, 1]^dim</code> and returns samples.<p></p> <p>If you also define <code>logpdf(x)</code> or <code>pdf(x)</code>, SciPyWrapper will use that for weights when required.</p>"},{"location":"demos/scipywrapper_dependence_custom/#2-user-defined-univariate-distributions","title":"2. User defined univariate distributions","text":"<p>Any univariate distribution with a SciPy-like interface can be used as a marginal:</p> <pre><code>tri = TriangularUserDistribution(c=0.3, loc=-1.0, scale=2.0)\nsampler = DigitalNetB2(1, seed=31)\ntm = SciPyWrapper(sampler, tri)\nx = tm(4096)        # samples from the triangular distribution\n</code></pre> <p>The wrapper will run a quick sanity check on your <code>ppf</code> and <code>pdf</code>. If it sees that the CDF is not increasing or the density does not look normalised, it will raise a <code>UserWarning</code> instead of failing silently.</p>"},{"location":"demos/scipywrapper_dependence_custom/#notes","title":"Notes","text":"<ul> <li>The behavior for the original use case (lists of SciPy univariate frozen distributions) is unchanged.</li> <li>Dependence is introduced only when the user passes a joint distribution object.</li> <li>Validation is kept intentionally light so that advanced users can still experiment, but beginners are warned when their custom distributions are clearly broken.</li> </ul>"},{"location":"demos/scipywrapper_dependence_custom/blog/","title":"SciPyWrapper","text":""},{"location":"demos/scipywrapper_dependence_custom/blog/#extending-qmcpys-scipywrapper-for-dependent-and-custom-distributions","title":"Extending QMCPy's <code>SciPyWrapper</code> for Dependent and Custom Distributions","text":"<p>When we first bump into <code>SciPyWrapper</code>, the examples live in very nice, controlled worlds. The true measures are clean, the distributions come straight from <code>scipy.stats</code>, and most variables behave independently of each other.</p> <p>My project started from the moments when that story breaks.</p> <ul> <li>What if the variables you care about are strongly correlated?</li> <li>What if your distribution is not in <code>scipy.stats</code> at all, but you still want to drive it with a low discrepancy point set?</li> <li>And what if a user hands you a custom distribution that is quietly wrong?</li> </ul> <p>This blog walks through the changes I made to <code>qmcpy.true_measure.SciPyWrapper</code> to handle those cases, and it uses five visual examples plus one \u201cfailure\u201d example to show that the new features really work.</p> <p>All of the code lives in:</p> <ul> <li><code>qmcpy/true_measure/scipy_wrapper.py</code></li> <li><code>demos/scipywrapper_dependence_custom/scipywrapper_demo.ipynb</code></li> <li><code>test/test_scipy_wrapper_custom.py</code></li> <li><code>demos/scipywrapper_dependence_custom/blog.md</code></li> </ul>"},{"location":"demos/scipywrapper_dependence_custom/blog/#1-what-scipywrapper-did-before","title":"1. What <code>SciPyWrapper</code> did before","text":"<p><code>SciPyWrapper</code> was originally designed for a very specific setting:</p> <p>Take a discrete sampler that produces points in <code>[0, 1]^d</code> and transform each coordinate independently using a list of frozen <code>scipy.stats</code> univariate distributions.</p> <p>So if you gave it a 3 dimensional <code>DigitalNetB2</code> and three frozen continuous distributions, it would apply each inverse CDF (<code>ppf</code>) coordinatewise. This works nicely for independent marginals, but it has some limitations:</p> <ol> <li>It ignores any dependence between coordinates.  </li> <li>It assumes everything comes from <code>scipy.stats</code> continuous distributions.  </li> <li>There is no structured way to plug in a completely user defined distribution while still catching obvious mistakes.</li> </ol> <p>The goal of this project is to keep the original behavior for simple cases, but extend it in two directions:</p> <ul> <li>support joint distributions that encode dependence, including non trivial constructions like zero inflated models and acceptance rejection transforms;</li> <li>support user defined marginals that follow a SciPy like interface, with sanity checks and clear warnings when something looks wrong.</li> </ul>"},{"location":"demos/scipywrapper_dependence_custom/blog/#2-new-interface","title":"2. New interface","text":"<p>At a high level, the extended <code>SciPyWrapper</code> can now work in two modes.</p>"},{"location":"demos/scipywrapper_dependence_custom/blog/#21-independent-marginal-mode","title":"2.1 Independent marginal mode","text":"<p>This is the original behavior.</p> <ul> <li>Input: list of frozen univariate distributions or a single frozen distribution from <code>scipy.stats</code>, or user defined univariate distributions that look similar.</li> <li>Transform: apply each <code>ppf</code> coordinatewise.</li> <li>Weight: multiply the marginal densities or log densities.</li> </ul> <p>This is still used in the examples for one dimensional custom marginals.</p>"},{"location":"demos/scipywrapper_dependence_custom/blog/#22-joint-distribution-mode","title":"2.2 Joint distribution mode","text":"<p>This is the new piece.</p> <p>Instead of giving a list of marginals, you can now pass a joint object that knows how to turn a point <code>u</code> from <code>[0, 1]^d</code> into a sample <code>x</code> in the physical space.</p> <p>The joint object is expected to provide:</p> <ul> <li><code>dim</code>: the dimension <code>d</code>;</li> <li><code>transform(u)</code>: vectorised map from <code>u</code> in <code>[0, 1]^d</code> to <code>x</code> in <code>R^d</code>;</li> <li>optional <code>logpdf(x)</code> or <code>pdf(x)</code>: to supply weights if needed.</li> </ul> <p><code>SciPyWrapper</code> detects this type and routes all calls through the joint transform. This gives us the flexibility to encode dependence directly in <code>transform</code>.</p> <p>In the rest of the blog, all the \u201cdependent\u201d examples use this joint mode.</p>"},{"location":"demos/scipywrapper_dependence_custom/blog/#3-example-1-independent-vs-dependent-normals","title":"3. Example 1: independent vs dependent normals","text":"<p>This is the example that checks that the joint transform behaves as expected for a familiar multivariate normal.</p> <p>We compare:</p> <ol> <li><code>SciPyWrapper</code> with two independent standard normal marginals.</li> <li><code>SciPyWrapper</code> with a 2-dimensional multivariate normal that has correlation <code>\u03c1 = 0.8</code>, provided through a joint adapter.</li> </ol> <p>For each case, we generate a few thousand samples from a <code>DigitalNetB2</code> sampler and estimate:</p> <ul> <li>the sample correlation between <code>X1</code> and <code>X2</code>;</li> <li>the expectation <code>E[X1 X2]</code>.</li> </ul>"},{"location":"demos/scipywrapper_dependence_custom/blog/#what-we-see","title":"What we see","text":"<p>Figure 1: Independent vs dependent normals</p> <p></p> <ul> <li>The left panel shows the independent case. The scatter plot is almost a round cloud, with no visible tilt. The caption in the notebook prints <code>\u03c1 \u2248 0.00</code>, which matches what our eyes see.</li> <li>The right panel shows the joint multivariate normal case with <code>\u03c1 \u2248 0.80</code>. Now the points are stretched along a diagonal line. The cloud is clearly elongated, and the sample correlation matches the target within a few thousandths. The estimated <code>E[X1 X2]</code> is around <code>0.799</code>, very close to the analytic value <code>0.8</code>.</li> </ul> <p>So this first figure confirms that:</p> <ul> <li>the joint transform preserves the covariance structure we bake into the multivariate normal;</li> <li>the independent mode still behaves the way it did originally.</li> </ul>"},{"location":"demos/scipywrapper_dependence_custom/blog/#4-example-2-zero-inflated-exponential-plus-uniform","title":"4. Example 2: zero inflated exponential plus uniform","text":"<p>The second example moves away from textbook distributions and into something more realistic: a zero inflated exponential marginal combined with a dependent uniform.</p> <p>The construction is:</p> <ul> <li>With probability <code>p_zero</code>, set <code>X = 0</code> and draw <code>Y</code> uniformly from <code>[0, 0.5]</code>.</li> <li>With probability <code>1 - p_zero</code>, draw <code>X</code> from an exponential distribution with rate <code>\u03bb</code> and draw <code>Y</code> uniformly from <code>[0.5, 1]</code>.</li> </ul> <p>This is implemented as a joint object <code>ZeroInflatedExpUniformJoint</code> that exposes <code>dim = 2</code> and a <code>transform</code> that applies this mixture using the input <code>u</code> from <code>[0, 1]^2</code>.</p>"},{"location":"demos/scipywrapper_dependence_custom/blog/#what-we-see_1","title":"What we see","text":"<p>Figure 2: Zero inflated exponential marginal and joint cloud</p> <p></p> <ul> <li>The left panel shows a histogram of the <code>X</code> marginal. There is a very tall spike at <code>X = 0</code>, which counts the zero inflated mass, and then a decaying bar plot for positive <code>X</code> that follows the exponential tail. This matches the qualitative shape we expect from the mixture.</li> <li>The right panel shows the joint scatter of <code>(X, Y)</code>. There is a dense vertical strip at <code>X = 0</code> with <code>Y</code> between <code>0</code> and <code>0.5</code>. For positive <code>X</code>, the points live in the horizontal band where <code>Y</code> is between <code>0.5</code> and <code>1</code>. The band is thicker near small <code>X</code> and thins out for large <code>X</code>, simply because the exponential tail decays.</li> </ul> <p>Numerically, the notebook prints:</p> <ul> <li>Target <code>P(X = 0)</code> (which is the parameter <code>p_zero</code>) and the empirical estimate. They agree up to three decimal places.</li> <li>Empirical <code>corr(X, Y)</code>, which is clearly positive, reflecting the fact that larger <code>Y</code> is associated with nonzero <code>X</code>.</li> </ul> <p>This example uses the new joint interface in a non Gaussian setting and shows that we can create quite structured dependence without changing any of the QMCPy internals that sit above <code>SciPyWrapper</code>.</p>"},{"location":"demos/scipywrapper_dependence_custom/blog/#5-example-3-acceptance-rejection-with-mc-and-qmc","title":"5. Example 3: acceptance rejection with MC and QMC","text":"<p>The third example uses acceptance rejection sampling to create a dependent target inside the unit square and then compares what the final samples look like when the proposals come from iid Monte Carlo vs a quasi Monte Carlo net.</p> <p>The target region is the triangle</p> \\[ \\{(x, y) : 0 &lt; y \\le x &lt; 1\\}. \\] <p>The algorithm is:</p> <ol> <li>Propose points uniformly on <code>[0, 1]^2</code> either from an iid sampler or from <code>DigitalNetB2</code>.</li> <li>Accept only those points with <code>y \u2264 x</code>.</li> </ol> <p>This automatically introduces dependence between <code>X</code> and <code>Y</code> because the support ignores the upper left half of the square.</p> <p>We then compute:</p> <ul> <li>the sample correlation between <code>X</code> and <code>Y</code> for both methods;</li> <li>the sample estimate of <code>E[X Y]</code> and compare it with the analytic value <code>0.25</code>.</li> </ul>"},{"location":"demos/scipywrapper_dependence_custom/blog/#what-we-see_2","title":"What we see","text":"<p>Figure 3: Acceptance rejection with iid versus QMC</p> <p></p> <ul> <li>The left panel shows the accepted points when the proposals come from iid uniform draws. The triangle is filled, but there are small gaps and some regions look a bit more crowded than others. This is normal Monte Carlo behavior.</li> <li>The right panel shows the same acceptance region when the proposals come from a digital net. The overall shape is exactly the same, but the fill looks more even. There are fewer \u201cclumps\u201d and voids. The points spread across the triangle in a more uniform way, which is what low discrepancy sequences are designed to do.</li> </ul> <p>The printed statistics highlight that:</p> <ul> <li><code>corr(X, Y)</code> is around <code>0.49</code> for MC and about <code>0.50</code> for QMC.</li> <li>The estimates of <code>E[X Y]</code> are <code>0.2485</code> for MC and <code>0.2505</code> for QMC, versus the true value <code>0.25</code>.</li> </ul> <p>So both samplers hit the correct dependent target, and in this particular run the QMC estimate lands almost exactly on the analytic value. More importantly, this example shows that the combination of:</p> <ul> <li>a custom acceptance rejection mapping,</li> <li>a joint transform interface, and</li> <li>a QMC sampler</li> </ul> <p>fits cleanly into the extended SciPyWrapper without any extra plumbing.</p>"},{"location":"demos/scipywrapper_dependence_custom/blog/#6-example-4-user-defined-triangular-marginal","title":"6. Example 4: user defined triangular marginal","text":"<p>The previous examples focused on dependence. The next one turns to user defined marginals that look and feel like SciPy distributions.</p> <p>Here we define a symmetric triangular distribution on <code>[-1, 1]</code> with its peak shifted by a parameter <code>c</code>:</p> <pre><code>TriangularUserDistribution(c=0.3, loc=-1.0, scale=2.0)\n</code></pre> <p>The class exposes:</p> <ul> <li><code>ppf(u)</code>: inverse CDF;</li> <li><code>pdf(x)</code>: density;</li> <li>attributes <code>loc</code>, <code>scale</code>, and <code>c</code>.</li> </ul> <p>This is passed to <code>SciPyWrapper</code> in the exact same way you would pass <code>scipy.stats.triang(c, loc, scale).rvs</code>.</p>"},{"location":"demos/scipywrapper_dependence_custom/blog/#what-we-see_3","title":"What we see","text":"<p>Figure 4: Custom triangular distribution</p> <p></p> <p>The figure has only one panel. It overlays:</p> <ul> <li>a histogram of the samples generated through <code>SciPyWrapper</code> using <code>DigitalNetB2</code>, shown as blue bars;</li> <li>the analytic pdf of the triangular distribution, shown as an orange line.</li> </ul> <p>The important part is that the bars sit right under the line across the whole interval. There are no obvious shifts or asymmetries. The notebook also prints:</p> <ul> <li>the sample mean, which is consistent with the analytic mean for the chosen parameters;</li> <li>the minimum and maximum of the samples, which are close to <code>-1</code> and <code>1</code>, matching the support.</li> </ul> <p>This example demonstrates that a user defined marginal can be plugged into the wrapper as long as it respects a SciPy like interface. No changes are required in the rest of QMCPy.</p>"},{"location":"demos/scipywrapper_dependence_custom/blog/#7-example-5-catching-a-broken-custom-distribution","title":"7. Example 5: catching a broken custom distribution","text":"<p>This example is intentionally not pretty. It builds a \u201cbad\u201d custom distribution whose <code>ppf</code> and <code>pdf</code> have obvious problems, then passes it into <code>SciPyWrapper</code>.</p> <p>Instead of failing silently, the extended wrapper runs a few quick sanity checks during construction:</p> <ul> <li>Does <code>ppf</code> look increasing in probability?</li> <li>Is the numerical integral of <code>pdf</code> over the support close to 1?</li> <li>Are there negative values or infinities?</li> </ul> <p>If something looks suspicious, <code>SciPyWrapper</code> raises a <code>UserWarning</code> that explains what went wrong.</p> <p>There is no figure for this example because the main output lives in the console. In your run you see warnings such as:</p> <pre><code>Custom distribution ppf appears non increasing\nCustom distribution pdf looks poorly normalised\n</code></pre> <p>This is exactly the behavior we want. It does not block experimentation, but it gives the user a clear signal that their distribution is probably not what they think it is.</p>"},{"location":"demos/scipywrapper_dependence_custom/blog/#8-example-6-multivariate-student-t-joint-dependent","title":"8. Example 6: multivariate Student t (joint, dependent)","text":"<p>Normals are a great starting point, but they can hide what happens when the data has heavy tails. So I added a joint multivariate Student\u2019s t example using SciPy\u2019s multivariate t distribution.</p> <p>This is not the same as using two independent <code>scipy.stats.t</code> marginals. Here the dependence is real: the joint distribution has a target correlation built into its shape matrix, and the samples reflect that directly.</p> <p>In the notebook I print:</p> <ul> <li>the target correlation (from the shape matrix),</li> <li>the empirical correlation from the samples, and</li> <li>a quick moment check using the fact that when <code>df &gt; 2</code> the covariance is</li> </ul> \\[ \\mathrm{Cov}(X) = \\frac{\\text{df}}{\\text{df}-2}\\,\\text{shape}. \\] <p>So the theoretical cross covariance should match <code>df/(df-2) * shape[0,1]</code> (and the empirical <code>E[X1 X2]</code> should land close to that since the mean is zero here).</p>"},{"location":"demos/scipywrapper_dependence_custom/blog/#what-we-see_4","title":"What we see","text":"<p>Figure 5: Multivariate Student t marginal and joint cloud</p> <p></p> <ul> <li>The marginal looks heavier tailed than the normal examples, which is exactly the point of using Student t.</li> <li>The joint scatter is clearly tilted, and the empirical correlation comes out very close to the target.</li> <li>The moment check (<code>E[X1 X2]</code> vs theoretical covariance) lines up well, which is a nice sanity check that the joint construction is doing what we expect.</li> </ul>"},{"location":"demos/scipywrapper_dependence_custom/blog/#9-how-to-run-the-notebook-and-tests","title":"9. How to run the notebook and tests","text":"<p>From the top level of the QMCPy repo:</p> <pre><code># Run the custom tests\npytest test/test_scipy_wrapper_custom.py\n\n# Open the demo notebook inside the conda or venv that has QMCPy installed\ncd demos/scipywrapper_dependence_custom\njupyter notebook scipywrapper_demo.ipynb\n</code></pre> <p>Running all cells in the notebook will regenerate the five figures from this blog along with the numeric summaries shown in the screenshots.</p>"},{"location":"demos/scipywrapper_dependence_custom/blog/#10-conclusion-and-future-work","title":"10. Conclusion and future work","text":"<p>The original <code>SciPyWrapper</code> already made it easy to plug standard <code>scipy.stats</code> marginals into QMCPy. For many applications that was enough. Once we step into more realistic models though, independence and \u201ceverything is already in SciPy\u201d become strong assumptions.</p> <p>Here, I tried to relax those assumptions while keeping the original interface familiar:</p> <ul> <li>Dependent variables are supported through a joint transform interface that plays nicely with QMCPy\u2019s existing samplers and stopping criteria.</li> <li>Custom user distributions can be passed in using a SciPy style frozen object pattern.</li> <li>Simple sanity checks help catch badly defined distributions so they do not quietly pollute simulation results.</li> <li>The examples tie the new features back to concrete use cases: correlated normals, zero inflated models, acceptance rejection, triangular marginals, and a dependent multivariate Student t.</li> </ul> <p>There are still several natural extensions.</p>"},{"location":"demos/scipywrapper_dependence_custom/blog/#future-directions","title":"Future directions","text":"<p>A few ideas that came up while I was working on this:</p> <ol> <li> <p>Richer logging for validators    Right now the sanity checks only emit warnings. It might be useful to let users opt into more detailed diagnostic plots that show where a <code>ppf</code> or <code>pdf</code> is failing.</p> </li> <li> <p>Higher dimensional custom joints    The current demos focus on dimension two. Building higher dimensional joint adapters for copulas or multivariate skew distributions would be a nice stress test for the interface.</p> </li> <li> <p>Automatic bridge to <code>scipy.stats</code> multivariate classes    For multivariate normal I wrote a small adapter by hand. In the future, we could provide a utility that wraps any SciPy multivariate distribution with CDF or inverse CDF methods in a standard way.</p> </li> <li> <p>Tighter coupling with integration error estimates    When log densities are available for joint distributions, the weights can be used inside some of the adaptive algorithms. Connecting the new joint weights to those estimators would open up more advanced applications.</p> </li> </ol> <p>For now, the main takeaway is that <code>SciPyWrapper</code> is no longer limited to independent, library provided marginals. It can live in a world where dependence is the rule, where distributions are sometimes hand built, and where users still get clear guidance when something looks off.</p>"},{"location":"demos/scipywrapper_dependence_custom/scipywrapper_demo/","title":"SciPyWrapper dependence and Custom distributions","text":"In\u00a0[1]: Copied! <pre>import numpy as np\nimport matplotlib.pyplot as plt\nimport scipy.stats as stats\nfrom pathlib import Path\n\nfrom qmcpy.discrete_distribution import DigitalNetB2\nfrom qmcpy.true_measure import SciPyWrapper, ZeroInflatedExpUniform, UniformTriangle, Triangular, StudentT\nfrom qmcpy.util import ParameterError, DimensionError\nfrom qmcpy.true_measure.triangular import TriangularDistribution\n\n\n# I like having a consistent look across all plots.\nplt.rcParams[\"figure.figsize\"] = (6, 4)\nplt.rcParams[\"axes.grid\"] = True\n\n# Folder where all figures for the blog will be saved.\nFIG_DIR = Path(\"figures\")\nFIG_DIR.mkdir(parents=True, exist_ok=True)\n\n\n# ======================================================================\n# 1. Acceptance rejection helpers for MC vs QMC\n# ======================================================================\n\ndef sample_triangle_ar_mc(n_target, batch_size=2048, rng=None):\n    \"\"\"\n    Acceptance rejection using iid uniform proposals.\n\n    Target is the uniform distribution on the triangle:\n        T = {(x, y) in (0,1)^2 : 0 &lt; y &lt;= x &lt; 1}.\n    \"\"\"\n    if rng is None:\n        rng = np.random.default_rng()\n\n    accepted = []\n    while sum(a.shape[0] for a in accepted) &lt; n_target:\n        u = rng.random((batch_size, 2))\n        mask = u[:, 1] &lt;= u[:, 0]\n        accepted.append(u[mask])\n\n    samples = np.concatenate(accepted, axis=0)[:n_target]\n    return samples\n\n\ndef sample_triangle_ar_qmc(n_target, batch_size=1024, seed_start=7):\n    \"\"\"\n    Acceptance rejection using QMC proposals from SciPyWrapper.\n\n    Here we wrap uniform(0,1) inside SciPyWrapper so this path uses\n    the same interface as all other examples.\n    \"\"\"\n    accepted = []\n    seed = seed_start\n\n    while sum(a.shape[0] for a in accepted) &lt; n_target:\n        sampler = DigitalNetB2(2, seed=seed)\n        tm = SciPyWrapper(sampler, scipy_distribs=stats.uniform())\n        u = tm(batch_size)\n        mask = u[:, 1] &lt;= u[:, 0]\n        accepted.append(u[mask])\n        seed += 1\n\n    samples = np.concatenate(accepted, axis=0)[:n_target]\n    return samples\n\n\n# ======================================================================\n# 2. Example 1: Dependent vs independent normals\n# ======================================================================\n\ndef example_dependent_vs_independent_normals():\n    print(\"\\n=== Example 1: dependent vs independent normals ===\")\n\n    sampler = DigitalNetB2(2, seed=13)\n\n    # Independent marginals. This is essentially the original SciPyWrapper.\n    indep_marginals = [stats.norm(0.0, 1.0), stats.norm(0.0, 1.0)]\n    tm_indep = SciPyWrapper(sampler, indep_marginals)\n\n    # Joint multivariate normal with a target correlation.\n    rho_target = 0.8\n    cov = [[1.0, rho_target], [rho_target, 1.0]]\n    mvn = stats.multivariate_normal(mean=[0.0, 0.0], cov=cov)\n    tm_dep = SciPyWrapper(sampler, mvn)\n\n    n = 4096\n    x_indep = tm_indep(n)\n    x_dep = tm_dep(n)\n\n    rho_indep = np.corrcoef(x_indep.T)[0, 1]\n    rho_dep = np.corrcoef(x_dep.T)[0, 1]\n\n    # Functional that is very sensitive to dependence.\n    est_indep = np.mean(x_indep[:, 0] * x_indep[:, 1])\n    est_dep = np.mean(x_dep[:, 0] * x_dep[:, 1])\n\n    true_indep = 0.0           # E[X1 X2] for independent zero mean normals.\n    true_dep = rho_target      # For MVN, E[X1 X2] = Cov(X1, X2) = rho.\n\n    print(f\"Sample corr (indep) : {rho_indep:.3f}\")\n    print(f\"Sample corr (joint) : {rho_dep:.3f}\")\n    print(f\"E[X1 X2] indep  est={est_indep:.3f}, true={true_indep:.3f}\")\n    print(f\"E[X1 X2] joint est={est_dep:.3f}, true={true_dep:.3f}\")\n\n    fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n\n    axes[0].scatter(x_indep[:, 0], x_indep[:, 1], s=6, alpha=0.4)\n    axes[0].set_title(f\"Independent normals, rho\u2248{rho_indep:.2f}\")\n    axes[0].set_xlabel(\"X1\")\n    axes[0].set_ylabel(\"X2\")\n\n    axes[1].scatter(x_dep[:, 0], x_dep[:, 1], s=6, alpha=0.4)\n    axes[1].set_title(f\"Dependent MVN, rho\u2248{rho_dep:.2f}\")\n    axes[1].set_xlabel(\"X1\")\n    axes[1].set_ylabel(\"X2\")\n\n    plt.tight_layout()\n    fig_path = FIG_DIR / \"fig01_mvn_indep_vs_dep.png\"\n    fig.savefig(fig_path, dpi=300, bbox_inches=\"tight\")\n    plt.show()\n    print(f\"Saved figure: {fig_path}\")\n\n\n# ======================================================================\n# 3. Example 2: Zero inflated exponential + uniform\n# ======================================================================\n\ndef example_zero_inflated_joint():\n    print(\"\\n=== Example 2: zero inflated exponential + uniform ===\")\n\n    p_zero = 0.4\n    sampler_zi = DigitalNetB2(2, seed=21)\n    tm_zi = ZeroInflatedExpUniform(sampler=sampler_zi, p_zero=p_zero, lam=1.5, y_split=0.5)\n\n\n    n = 4096\n    xy = tm_zi(n)\n    x = xy[:, 0]\n    y = xy[:, 1]\n\n    zero_rate = np.mean(x == 0.0)\n    corr_xy = np.corrcoef(x, y)[0, 1]\n\n    print(f\"Target P(X=0)              : {p_zero:.3f}\")\n    print(f\"Empirical P(X=0)           : {zero_rate:.3f}\")\n    print(f\"Empirical corr(X, Y)       : {corr_xy:.3f}\")\n\n    fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n\n    axes[0].hist(x, bins=60)\n    axes[0].set_title(\"Zero inflated exponential marginal X\")\n    axes[0].set_xlabel(\"X\")\n    axes[0].set_ylabel(\"count\")\n\n    axes[1].scatter(x, y, s=6, alpha=0.4)\n    axes[1].set_title(\"Joint (X, Y) for zero inflated example\")\n    axes[1].set_xlabel(\"X\")\n    axes[1].set_ylabel(\"Y\")\n\n    plt.tight_layout()\n    fig_path = FIG_DIR / \"fig02_zero_inflated_joint.png\"\n    fig.savefig(fig_path, dpi=300, bbox_inches=\"tight\")\n    plt.show()\n    print(f\"Saved figure: {fig_path}\")\n\n\n# ======================================================================\n# 4. Example 3: Acceptance rejection, MC vs QMC\n# ======================================================================\n\ndef example_accept_reject_mc_vs_qmc():\n    print(\"\\n=== Example 3: acceptance rejection, MC vs QMC ===\")\n\n    n_target = 4096\n\n    tri_mc = sample_triangle_ar_mc(n_target)\n    tri_qmc = sample_triangle_ar_qmc(n_target)\n\n    rho_mc = np.corrcoef(tri_mc.T)[0, 1]\n    rho_qmc = np.corrcoef(tri_qmc.T)[0, 1]\n\n    print(f\"corr(X, Y) MC  : {rho_mc:.3f}\")\n    print(f\"corr(X, Y) QMC : {rho_qmc:.3f}\")\n\n    # For uniform on the triangle T, the exact E[X Y] is 1/4.\n    def f(z):\n        return z[:, 0] * z[:, 1]\n\n    est_mc = f(tri_mc).mean()\n    est_qmc = f(tri_qmc).mean()\n    true_E = 0.25\n\n    print(f\"E[X Y] MC  : est={est_mc:.4f}, true={true_E:.4f}\")\n    print(f\"E[X Y] QMC : est={est_qmc:.4f}, true={true_E:.4f}\")\n\n    fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n\n    axes[0].scatter(tri_mc[:, 0], tri_mc[:, 1], s=6, alpha=0.4)\n    axes[0].set_title(\"Acceptance rejection with iid MC\")\n    axes[0].set_xlabel(\"X\")\n    axes[0].set_ylabel(\"Y\")\n\n    axes[1].scatter(tri_qmc[:, 0], tri_qmc[:, 1], s=6, alpha=0.4)\n    axes[1].set_title(\"Acceptance rejection with QMC\")\n    axes[1].set_xlabel(\"X\")\n    axes[1].set_ylabel(\"Y\")\n\n    plt.tight_layout()\n    fig_path = FIG_DIR / \"fig03_accept_reject_mc_vs_qmc.png\"\n    fig.savefig(fig_path, dpi=300, bbox_inches=\"tight\")\n    plt.show()\n    print(f\"Saved figure: {fig_path}\")\n\n\n# ======================================================================\n# 5. Example 4: Custom triangular user marginal\n# ======================================================================\n\ndef example_custom_triangular_marginal():\n    print(\"\\n=== Example 4: custom triangular user marginal ===\")\n\n    tri_custom = TriangularDistribution(c=0.3, loc=-1.0, scale=2.0)  # only used for pdf overlay\n    sampler_tri = DigitalNetB2(1, seed=31)\n    tm_tri = Triangular(sampler=sampler_tri, c=0.3, loc=-1.0, scale=2.0)\n\n    n = 4096\n    x_tri = tm_tri(n).ravel()\n\n    print(f\"Sample mean of custom triangular: {x_tri.mean():.3f}\")\n    print(f\"Sample min, max                 : {x_tri.min():.3f}, {x_tri.max():.3f}\")\n\n    fig, ax = plt.subplots()\n    ax.hist(x_tri, bins=50, density=True, alpha=0.6, label=\"QMC samples\")\n\n    xs = np.linspace(x_tri.min(), x_tri.max(), 200)\n    ax.plot(xs, tri_custom.pdf(xs), label=\"User pdf\", linewidth=2.0)\n\n    ax.set_title(\"Custom triangular user distribution\")\n    ax.set_xlabel(\"x\")\n    ax.set_ylabel(\"density\")\n    ax.legend()\n\n    plt.tight_layout()\n    fig_path = FIG_DIR / \"fig04_custom_triangular_marginal.png\"\n    fig.savefig(fig_path, dpi=300, bbox_inches=\"tight\")\n    plt.show()\n    print(f\"Saved figure: {fig_path}\")\n\n\n# ======================================================================\n# 6. Example 5: Intentionally broken distribution to trigger warnings\n# ======================================================================\n\ndef example_bad_custom_distribution():\n    print(\"\\n=== Example 5: intentionally broken custom distribution ===\")\n\n    class BadTriangular:\n        \"\"\"\n        This is a purposely broken \"distribution\".\n\n        - ppf is not monotone.\n        - pdf is constant and does not integrate to 1 on its support.\n\n        The goal is to see SciPyWrapper's sanity checks complain.\n        \"\"\"\n\n        def ppf(self, u):\n            u = np.asarray(u, dtype=float)\n            # Non monotone on purpose.\n            return np.sin(np.pi * u)\n\n        def pdf(self, x):\n            x = np.asarray(x, dtype=float)\n            # Constant density that will not normalise correctly.\n            return np.ones_like(x)\n\n    bad = BadTriangular()\n    sampler = DigitalNetB2(1, seed=99)\n\n    print(\"Constructing SciPyWrapper with a bad custom distribution...\")\n    tm_bad = SciPyWrapper(sampler, bad)  # warnings are expected here\n\n    # We can still draw samples, but they are not meaningful.\n    x = tm_bad(4)\n    print(\"Example samples from bad distribution:\", x.ravel())\n\n\n# ======================================================================\n# 7. Example 6: Multivariate Student t (joint, dependent)\n# ======================================================================\n\ndef example_multivariate_student_t_joint():\n    print(\"\\n=== Example 6: multivariate Student t (joint, dependent) ===\")\n\n    # Parameters\n    df = 5.0\n    rho_target = 0.8\n\n    loc = np.array([0.0, 0.0])\n    shape = np.array([\n        [1.0, rho_target],\n        [rho_target, 1.0],\n    ])\n\n    # Build joint distribution + SciPyWrapper\n    sampler = DigitalNetB2(2, seed=41)\n    tm = StudentT(sampler=sampler, loc=loc, shape=shape, df=df)\n\n    n = 4096\n    x = tm(n)\n    x1 = x[:, 0]\n    x2 = x[:, 1]\n\n    emp_corr = np.corrcoef(x.T)[0, 1]\n    emp_ex12 = np.mean(x1 * x2)\n\n    # Theory (exists only if df &gt; 2): Cov = df/(df-2) * shape\n    # Correlation is the same as in shape because the df/(df-2) factor cancels.\n    if df &gt; 2:\n        true_cov12 = df / (df - 2.0) * shape[0, 1]\n    else:\n        true_cov12 = np.nan\n\n    true_corr = rho_target\n\n    print(f\"df                          : {df:.1f}\")\n    print(f\"Target corr (from shape)    : {true_corr:.3f}\")\n    print(f\"Empirical corr(X1, X2)      : {emp_corr:.3f}\")\n    print(f\"E[X1 X2] empirical          : {emp_ex12:.3f}\")\n    print(f\"Cov(X1, X2) theoretical     : {true_cov12:.3f}\")\n\n    fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n\n    axes[0].hist(x1, bins=60)\n    axes[0].set_title(\"Multivariate t marginal X1\")\n    axes[0].set_xlabel(\"X1\")\n    axes[0].set_ylabel(\"count\")\n\n    axes[1].scatter(x1, x2, s=6, alpha=0.4)\n    axes[1].set_title(f\"Joint multivariate t, corr\u2248{emp_corr:.2f}\")\n    axes[1].set_xlabel(\"X1\")\n    axes[1].set_ylabel(\"X2\")\n\n    plt.tight_layout()\n    fig_path = FIG_DIR / \"fig05_multivariate_student_t_joint.png\"\n    fig.savefig(fig_path, dpi=300, bbox_inches=\"tight\")\n    plt.show()\n    print(f\"Saved figure: {fig_path}\")\n\n\n# ======================================================================\n# 8. Convenience entry point so this file runs as a script too\n# ======================================================================\n\nif __name__ == \"__main__\":\n    example_dependent_vs_independent_normals()\n    example_zero_inflated_joint()\n    example_accept_reject_mc_vs_qmc()\n    example_custom_triangular_marginal()\n    example_bad_custom_distribution()\n    example_multivariate_student_t_joint()\n</pre> import numpy as np import matplotlib.pyplot as plt import scipy.stats as stats from pathlib import Path  from qmcpy.discrete_distribution import DigitalNetB2 from qmcpy.true_measure import SciPyWrapper, ZeroInflatedExpUniform, UniformTriangle, Triangular, StudentT from qmcpy.util import ParameterError, DimensionError from qmcpy.true_measure.triangular import TriangularDistribution   # I like having a consistent look across all plots. plt.rcParams[\"figure.figsize\"] = (6, 4) plt.rcParams[\"axes.grid\"] = True  # Folder where all figures for the blog will be saved. FIG_DIR = Path(\"figures\") FIG_DIR.mkdir(parents=True, exist_ok=True)   # ====================================================================== # 1. Acceptance rejection helpers for MC vs QMC # ======================================================================  def sample_triangle_ar_mc(n_target, batch_size=2048, rng=None):     \"\"\"     Acceptance rejection using iid uniform proposals.      Target is the uniform distribution on the triangle:         T = {(x, y) in (0,1)^2 : 0 &lt; y &lt;= x &lt; 1}.     \"\"\"     if rng is None:         rng = np.random.default_rng()      accepted = []     while sum(a.shape[0] for a in accepted) &lt; n_target:         u = rng.random((batch_size, 2))         mask = u[:, 1] &lt;= u[:, 0]         accepted.append(u[mask])      samples = np.concatenate(accepted, axis=0)[:n_target]     return samples   def sample_triangle_ar_qmc(n_target, batch_size=1024, seed_start=7):     \"\"\"     Acceptance rejection using QMC proposals from SciPyWrapper.      Here we wrap uniform(0,1) inside SciPyWrapper so this path uses     the same interface as all other examples.     \"\"\"     accepted = []     seed = seed_start      while sum(a.shape[0] for a in accepted) &lt; n_target:         sampler = DigitalNetB2(2, seed=seed)         tm = SciPyWrapper(sampler, scipy_distribs=stats.uniform())         u = tm(batch_size)         mask = u[:, 1] &lt;= u[:, 0]         accepted.append(u[mask])         seed += 1      samples = np.concatenate(accepted, axis=0)[:n_target]     return samples   # ====================================================================== # 2. Example 1: Dependent vs independent normals # ======================================================================  def example_dependent_vs_independent_normals():     print(\"\\n=== Example 1: dependent vs independent normals ===\")      sampler = DigitalNetB2(2, seed=13)      # Independent marginals. This is essentially the original SciPyWrapper.     indep_marginals = [stats.norm(0.0, 1.0), stats.norm(0.0, 1.0)]     tm_indep = SciPyWrapper(sampler, indep_marginals)      # Joint multivariate normal with a target correlation.     rho_target = 0.8     cov = [[1.0, rho_target], [rho_target, 1.0]]     mvn = stats.multivariate_normal(mean=[0.0, 0.0], cov=cov)     tm_dep = SciPyWrapper(sampler, mvn)      n = 4096     x_indep = tm_indep(n)     x_dep = tm_dep(n)      rho_indep = np.corrcoef(x_indep.T)[0, 1]     rho_dep = np.corrcoef(x_dep.T)[0, 1]      # Functional that is very sensitive to dependence.     est_indep = np.mean(x_indep[:, 0] * x_indep[:, 1])     est_dep = np.mean(x_dep[:, 0] * x_dep[:, 1])      true_indep = 0.0           # E[X1 X2] for independent zero mean normals.     true_dep = rho_target      # For MVN, E[X1 X2] = Cov(X1, X2) = rho.      print(f\"Sample corr (indep) : {rho_indep:.3f}\")     print(f\"Sample corr (joint) : {rho_dep:.3f}\")     print(f\"E[X1 X2] indep  est={est_indep:.3f}, true={true_indep:.3f}\")     print(f\"E[X1 X2] joint est={est_dep:.3f}, true={true_dep:.3f}\")      fig, axes = plt.subplots(1, 2, figsize=(10, 4))      axes[0].scatter(x_indep[:, 0], x_indep[:, 1], s=6, alpha=0.4)     axes[0].set_title(f\"Independent normals, rho\u2248{rho_indep:.2f}\")     axes[0].set_xlabel(\"X1\")     axes[0].set_ylabel(\"X2\")      axes[1].scatter(x_dep[:, 0], x_dep[:, 1], s=6, alpha=0.4)     axes[1].set_title(f\"Dependent MVN, rho\u2248{rho_dep:.2f}\")     axes[1].set_xlabel(\"X1\")     axes[1].set_ylabel(\"X2\")      plt.tight_layout()     fig_path = FIG_DIR / \"fig01_mvn_indep_vs_dep.png\"     fig.savefig(fig_path, dpi=300, bbox_inches=\"tight\")     plt.show()     print(f\"Saved figure: {fig_path}\")   # ====================================================================== # 3. Example 2: Zero inflated exponential + uniform # ======================================================================  def example_zero_inflated_joint():     print(\"\\n=== Example 2: zero inflated exponential + uniform ===\")      p_zero = 0.4     sampler_zi = DigitalNetB2(2, seed=21)     tm_zi = ZeroInflatedExpUniform(sampler=sampler_zi, p_zero=p_zero, lam=1.5, y_split=0.5)       n = 4096     xy = tm_zi(n)     x = xy[:, 0]     y = xy[:, 1]      zero_rate = np.mean(x == 0.0)     corr_xy = np.corrcoef(x, y)[0, 1]      print(f\"Target P(X=0)              : {p_zero:.3f}\")     print(f\"Empirical P(X=0)           : {zero_rate:.3f}\")     print(f\"Empirical corr(X, Y)       : {corr_xy:.3f}\")      fig, axes = plt.subplots(1, 2, figsize=(10, 4))      axes[0].hist(x, bins=60)     axes[0].set_title(\"Zero inflated exponential marginal X\")     axes[0].set_xlabel(\"X\")     axes[0].set_ylabel(\"count\")      axes[1].scatter(x, y, s=6, alpha=0.4)     axes[1].set_title(\"Joint (X, Y) for zero inflated example\")     axes[1].set_xlabel(\"X\")     axes[1].set_ylabel(\"Y\")      plt.tight_layout()     fig_path = FIG_DIR / \"fig02_zero_inflated_joint.png\"     fig.savefig(fig_path, dpi=300, bbox_inches=\"tight\")     plt.show()     print(f\"Saved figure: {fig_path}\")   # ====================================================================== # 4. Example 3: Acceptance rejection, MC vs QMC # ======================================================================  def example_accept_reject_mc_vs_qmc():     print(\"\\n=== Example 3: acceptance rejection, MC vs QMC ===\")      n_target = 4096      tri_mc = sample_triangle_ar_mc(n_target)     tri_qmc = sample_triangle_ar_qmc(n_target)      rho_mc = np.corrcoef(tri_mc.T)[0, 1]     rho_qmc = np.corrcoef(tri_qmc.T)[0, 1]      print(f\"corr(X, Y) MC  : {rho_mc:.3f}\")     print(f\"corr(X, Y) QMC : {rho_qmc:.3f}\")      # For uniform on the triangle T, the exact E[X Y] is 1/4.     def f(z):         return z[:, 0] * z[:, 1]      est_mc = f(tri_mc).mean()     est_qmc = f(tri_qmc).mean()     true_E = 0.25      print(f\"E[X Y] MC  : est={est_mc:.4f}, true={true_E:.4f}\")     print(f\"E[X Y] QMC : est={est_qmc:.4f}, true={true_E:.4f}\")      fig, axes = plt.subplots(1, 2, figsize=(10, 4))      axes[0].scatter(tri_mc[:, 0], tri_mc[:, 1], s=6, alpha=0.4)     axes[0].set_title(\"Acceptance rejection with iid MC\")     axes[0].set_xlabel(\"X\")     axes[0].set_ylabel(\"Y\")      axes[1].scatter(tri_qmc[:, 0], tri_qmc[:, 1], s=6, alpha=0.4)     axes[1].set_title(\"Acceptance rejection with QMC\")     axes[1].set_xlabel(\"X\")     axes[1].set_ylabel(\"Y\")      plt.tight_layout()     fig_path = FIG_DIR / \"fig03_accept_reject_mc_vs_qmc.png\"     fig.savefig(fig_path, dpi=300, bbox_inches=\"tight\")     plt.show()     print(f\"Saved figure: {fig_path}\")   # ====================================================================== # 5. Example 4: Custom triangular user marginal # ======================================================================  def example_custom_triangular_marginal():     print(\"\\n=== Example 4: custom triangular user marginal ===\")      tri_custom = TriangularDistribution(c=0.3, loc=-1.0, scale=2.0)  # only used for pdf overlay     sampler_tri = DigitalNetB2(1, seed=31)     tm_tri = Triangular(sampler=sampler_tri, c=0.3, loc=-1.0, scale=2.0)      n = 4096     x_tri = tm_tri(n).ravel()      print(f\"Sample mean of custom triangular: {x_tri.mean():.3f}\")     print(f\"Sample min, max                 : {x_tri.min():.3f}, {x_tri.max():.3f}\")      fig, ax = plt.subplots()     ax.hist(x_tri, bins=50, density=True, alpha=0.6, label=\"QMC samples\")      xs = np.linspace(x_tri.min(), x_tri.max(), 200)     ax.plot(xs, tri_custom.pdf(xs), label=\"User pdf\", linewidth=2.0)      ax.set_title(\"Custom triangular user distribution\")     ax.set_xlabel(\"x\")     ax.set_ylabel(\"density\")     ax.legend()      plt.tight_layout()     fig_path = FIG_DIR / \"fig04_custom_triangular_marginal.png\"     fig.savefig(fig_path, dpi=300, bbox_inches=\"tight\")     plt.show()     print(f\"Saved figure: {fig_path}\")   # ====================================================================== # 6. Example 5: Intentionally broken distribution to trigger warnings # ======================================================================  def example_bad_custom_distribution():     print(\"\\n=== Example 5: intentionally broken custom distribution ===\")      class BadTriangular:         \"\"\"         This is a purposely broken \"distribution\".          - ppf is not monotone.         - pdf is constant and does not integrate to 1 on its support.          The goal is to see SciPyWrapper's sanity checks complain.         \"\"\"          def ppf(self, u):             u = np.asarray(u, dtype=float)             # Non monotone on purpose.             return np.sin(np.pi * u)          def pdf(self, x):             x = np.asarray(x, dtype=float)             # Constant density that will not normalise correctly.             return np.ones_like(x)      bad = BadTriangular()     sampler = DigitalNetB2(1, seed=99)      print(\"Constructing SciPyWrapper with a bad custom distribution...\")     tm_bad = SciPyWrapper(sampler, bad)  # warnings are expected here      # We can still draw samples, but they are not meaningful.     x = tm_bad(4)     print(\"Example samples from bad distribution:\", x.ravel())   # ====================================================================== # 7. Example 6: Multivariate Student t (joint, dependent) # ======================================================================  def example_multivariate_student_t_joint():     print(\"\\n=== Example 6: multivariate Student t (joint, dependent) ===\")      # Parameters     df = 5.0     rho_target = 0.8      loc = np.array([0.0, 0.0])     shape = np.array([         [1.0, rho_target],         [rho_target, 1.0],     ])      # Build joint distribution + SciPyWrapper     sampler = DigitalNetB2(2, seed=41)     tm = StudentT(sampler=sampler, loc=loc, shape=shape, df=df)      n = 4096     x = tm(n)     x1 = x[:, 0]     x2 = x[:, 1]      emp_corr = np.corrcoef(x.T)[0, 1]     emp_ex12 = np.mean(x1 * x2)      # Theory (exists only if df &gt; 2): Cov = df/(df-2) * shape     # Correlation is the same as in shape because the df/(df-2) factor cancels.     if df &gt; 2:         true_cov12 = df / (df - 2.0) * shape[0, 1]     else:         true_cov12 = np.nan      true_corr = rho_target      print(f\"df                          : {df:.1f}\")     print(f\"Target corr (from shape)    : {true_corr:.3f}\")     print(f\"Empirical corr(X1, X2)      : {emp_corr:.3f}\")     print(f\"E[X1 X2] empirical          : {emp_ex12:.3f}\")     print(f\"Cov(X1, X2) theoretical     : {true_cov12:.3f}\")      fig, axes = plt.subplots(1, 2, figsize=(10, 4))      axes[0].hist(x1, bins=60)     axes[0].set_title(\"Multivariate t marginal X1\")     axes[0].set_xlabel(\"X1\")     axes[0].set_ylabel(\"count\")      axes[1].scatter(x1, x2, s=6, alpha=0.4)     axes[1].set_title(f\"Joint multivariate t, corr\u2248{emp_corr:.2f}\")     axes[1].set_xlabel(\"X1\")     axes[1].set_ylabel(\"X2\")      plt.tight_layout()     fig_path = FIG_DIR / \"fig05_multivariate_student_t_joint.png\"     fig.savefig(fig_path, dpi=300, bbox_inches=\"tight\")     plt.show()     print(f\"Saved figure: {fig_path}\")   # ====================================================================== # 8. Convenience entry point so this file runs as a script too # ======================================================================  if __name__ == \"__main__\":     example_dependent_vs_independent_normals()     example_zero_inflated_joint()     example_accept_reject_mc_vs_qmc()     example_custom_triangular_marginal()     example_bad_custom_distribution()     example_multivariate_student_t_joint()  <pre>=== Example 1: dependent vs independent normals ===\nSample corr (indep) : -0.000\nSample corr (joint) : 0.800\nE[X1 X2] indep  est=-0.000, true=0.000\nE[X1 X2] joint est=0.799, true=0.800\n</pre> <pre>Saved figure: figures/fig01_mvn_indep_vs_dep.png\n\n=== Example 2: zero inflated exponential + uniform ===\nTarget P(X=0)              : 0.400\nEmpirical P(X=0)           : 0.400\nEmpirical corr(X, Y)       : 0.461\n</pre> <pre>/Users/alegresor/Desktop/QMCSoftware/qmcpy/true_measure/scipy_wrapper.py:301: UserWarning: SciPyWrapper joint distribution has no 'logpdf'. Weights will be treated as 1.\n  warnings.warn(\n</pre> <pre>Saved figure: figures/fig02_zero_inflated_joint.png\n\n=== Example 3: acceptance rejection, MC vs QMC ===\ncorr(X, Y) MC  : 0.506\ncorr(X, Y) QMC : 0.501\nE[X Y] MC  : est=0.2500, true=0.2500\nE[X Y] QMC : est=0.2505, true=0.2500\n</pre> <pre>Saved figure: figures/fig03_accept_reject_mc_vs_qmc.png\n\n=== Example 4: custom triangular user marginal ===\nSample mean of custom triangular: -0.133\nSample min, max                 : -0.993, 0.976\n</pre> <pre>Saved figure: figures/fig04_custom_triangular_marginal.png\n\n=== Example 5: intentionally broken custom distribution ===\nConstructing SciPyWrapper with a bad custom distribution...\nExample samples from bad distribution: [0.99779416 0.30984091 0.43378429 0.77064796]\n\n=== Example 6: multivariate Student t (joint, dependent) ===\ndf                          : 5.0\nTarget corr (from shape)    : 0.800\nEmpirical corr(X1, X2)      : 0.808\nE[X1 X2] empirical          : 1.389\nCov(X1, X2) theoretical     : 1.333\n</pre> <pre>/Users/alegresor/Desktop/QMCSoftware/qmcpy/true_measure/scipy_wrapper.py:228: UserWarning: SciPyWrapper received a custom univariate distribution (not a scipy.stats frozen distribution) and it failed sanity checks:\n  - ppf() is not nondecreasing (looks non-monotone)\n  self._setup_marginals(scipy_distribs)\n</pre> <pre>Saved figure: figures/fig05_multivariate_student_t_joint.png\n</pre>"},{"location":"demos/scipywrapper_dependence_custom/triangular_true_measure/","title":"Triangular true measure","text":"In\u00a0[\u00a0]: Copied! <pre>import numpy as np\n</pre> import numpy as np In\u00a0[\u00a0]: Copied! <pre>from qmcpy.util import ParameterError\nfrom qmcpy.true_measure import SciPyWrapper\n</pre> from qmcpy.util import ParameterError from qmcpy.true_measure import SciPyWrapper In\u00a0[\u00a0]: Copied! <pre>class TriangularUserDistribution:\n    \"\"\"\n    Triangular distribution that mirrors `scipy.stats.triang`.\n\n    Support: [loc, loc + scale]\n    Mode: loc + c * scale, where 0 &lt; c &lt; 1.\n    Implements ppf and pdf so SciPyWrapper can use it.\n    \"\"\"\n\n    def __init__(self, c=0.5, loc=0.0, scale=1.0):\n        c = float(c)\n        loc = float(loc)\n        scale = float(scale)\n\n        if not (0.0 &lt; c &lt; 1.0):\n            raise ParameterError(\"c must lie strictly between 0 and 1.\")\n        if scale &lt;= 0.0:\n            raise ParameterError(\"scale must be positive.\")\n\n        self.c = c\n        self.loc = loc\n        self.scale = scale\n\n        self._a = loc\n        self._b = loc + scale\n        self._m = loc + c * scale\n\n    def pdf(self, x):\n        x = np.asarray(x, dtype=float)\n        a, m, b = self._a, self._m, self._b\n        out = np.zeros_like(x, dtype=float)\n\n        left = (x &gt;= a) &amp; (x &lt; m)\n        right = (x &gt;= m) &amp; (x &lt;= b)\n\n        out[left] = 2.0 * (x[left] - a) / ((b - a) * (m - a))\n        out[right] = 2.0 * (b - x[right]) / ((b - a) * (b - m))\n        return out\n\n    def ppf(self, u):\n        u = np.asarray(u, dtype=float)\n        a, m, b = self._a, self._m, self._b\n        Fm = (m - a) / (b - a)\n\n        x = np.empty_like(u, dtype=float)\n        left = u &lt;= Fm\n        right = ~left\n\n        x[left] = a + np.sqrt(u[left] * (b - a) * (m - a))\n        x[right] = b - np.sqrt((1.0 - u[right]) * (b - a) * (b - m))\n        return x\n</pre> class TriangularUserDistribution:     \"\"\"     Triangular distribution that mirrors `scipy.stats.triang`.      Support: [loc, loc + scale]     Mode: loc + c * scale, where 0 &lt; c &lt; 1.     Implements ppf and pdf so SciPyWrapper can use it.     \"\"\"      def __init__(self, c=0.5, loc=0.0, scale=1.0):         c = float(c)         loc = float(loc)         scale = float(scale)          if not (0.0 &lt; c &lt; 1.0):             raise ParameterError(\"c must lie strictly between 0 and 1.\")         if scale &lt;= 0.0:             raise ParameterError(\"scale must be positive.\")          self.c = c         self.loc = loc         self.scale = scale          self._a = loc         self._b = loc + scale         self._m = loc + c * scale      def pdf(self, x):         x = np.asarray(x, dtype=float)         a, m, b = self._a, self._m, self._b         out = np.zeros_like(x, dtype=float)          left = (x &gt;= a) &amp; (x &lt; m)         right = (x &gt;= m) &amp; (x &lt;= b)          out[left] = 2.0 * (x[left] - a) / ((b - a) * (m - a))         out[right] = 2.0 * (b - x[right]) / ((b - a) * (b - m))         return out      def ppf(self, u):         u = np.asarray(u, dtype=float)         a, m, b = self._a, self._m, self._b         Fm = (m - a) / (b - a)          x = np.empty_like(u, dtype=float)         left = u &lt;= Fm         right = ~left          x[left] = a + np.sqrt(u[left] * (b - a) * (m - a))         x[right] = b - np.sqrt((1.0 - u[right]) * (b - a) * (b - m))         return x In\u00a0[\u00a0]: Copied! <pre>class BadTriangularDistribution:\n    \"\"\"\n    Intentionally broken distribution to trigger SciPyWrapper warnings.\n    Only for demo use. Do not export from qmcpy.true_measure.\n    \"\"\"\n\n    def ppf(self, u):\n        u = np.asarray(u, dtype=float)\n        return np.sin(np.pi * u)  # non-monotone on purpose\n\n    def pdf(self, x):\n        x = np.asarray(x, dtype=float)\n        return np.ones_like(x)  # not normalized on purpose\n</pre> class BadTriangularDistribution:     \"\"\"     Intentionally broken distribution to trigger SciPyWrapper warnings.     Only for demo use. Do not export from qmcpy.true_measure.     \"\"\"      def ppf(self, u):         u = np.asarray(u, dtype=float)         return np.sin(np.pi * u)  # non-monotone on purpose      def pdf(self, x):         x = np.asarray(x, dtype=float)         return np.ones_like(x)  # not normalized on purpose In\u00a0[\u00a0]: Copied! <pre>class TriangularTrueMeasure(SciPyWrapper):\n    \"\"\"\n    Convenience true measure for the custom triangular marginal example.\n\n    Example:\n    &gt;&gt;&gt; tm = TriangularTrueMeasure(\n    ...     sampler=DigitalNetB2(1, seed=7),\n    ...     c=0.3,\n    ...     loc=-1.0,\n    ...     scale=2.0,\n    ... )\n    &gt;&gt;&gt; tm(4).shape\n    (4, 1)\n    \"\"\"\n\n    def __init__(self, sampler, c=0.5, loc=0.0, scale=1.0):\n        dist = TriangularUserDistribution(c=c, loc=loc, scale=scale)\n        super().__init__(sampler=sampler, scipy_distribs=dist)\n</pre> class TriangularTrueMeasure(SciPyWrapper):     \"\"\"     Convenience true measure for the custom triangular marginal example.      Example:     &gt;&gt;&gt; tm = TriangularTrueMeasure(     ...     sampler=DigitalNetB2(1, seed=7),     ...     c=0.3,     ...     loc=-1.0,     ...     scale=2.0,     ... )     &gt;&gt;&gt; tm(4).shape     (4, 1)     \"\"\"      def __init__(self, sampler, c=0.5, loc=0.0, scale=1.0):         dist = TriangularUserDistribution(c=c, loc=loc, scale=scale)         super().__init__(sampler=sampler, scipy_distribs=dist)"},{"location":"demos/talk_paper_demos/pydata_chi_2023/","title":"2023 PyData Chicago Talk","text":"In\u00a0[1]: Copied! <pre>import qmcpy as qp\nimport numpy as np\nimport scipy.stats\nimport pandas as pd\nimport time\nfrom matplotlib import pyplot\ncolors = pyplot.rcParams['axes.prop_cycle'].by_key()['color']\n%matplotlib inline\n</pre> import qmcpy as qp import numpy as np import scipy.stats import pandas as pd import time from matplotlib import pyplot colors = pyplot.rcParams['axes.prop_cycle'].by_key()['color'] %matplotlib inline In\u00a0[2]: Copied! <pre>iid = qp.IIDStdUniform(dimension=3)\niid.gen_samples(n=4)\n</pre> iid = qp.IIDStdUniform(dimension=3) iid.gen_samples(n=4) Out[2]: <pre>array([[0.61584953, 0.08238197, 0.79597353],\n       [0.47972561, 0.36991422, 0.4520753 ],\n       [0.9705357 , 0.20129176, 0.05264096],\n       [0.12821756, 0.27695067, 0.4200091 ]])</pre> In\u00a0[3]: Copied! <pre>iid.gen_samples(4)\n</pre> iid.gen_samples(4) Out[3]: <pre>array([[0.75988525, 0.06535736, 0.88857573],\n       [0.93750365, 0.82111122, 0.62840155],\n       [0.73705715, 0.22116385, 0.86364701],\n       [0.76327139, 0.67695714, 0.73070693]])</pre> In\u00a0[4]: Copied! <pre>iid\n</pre> iid Out[4]: <pre>IIDStdUniform (AbstractIIDDiscreteDistribution)\n    d               3\n    replications    1\n    entropy         255445230289856938445909338179220309576</pre> In\u00a0[5]: Copied! <pre>ld_lattice = qp.Lattice(3)\nld_lattice.gen_samples(4)\n</pre> ld_lattice = qp.Lattice(3) ld_lattice.gen_samples(4) Out[5]: <pre>array([[0.7323192 , 0.47466547, 0.80517287],\n       [0.2323192 , 0.97466547, 0.30517287],\n       [0.9823192 , 0.22466547, 0.55517287],\n       [0.4823192 , 0.72466547, 0.05517287]])</pre> In\u00a0[6]: Copied! <pre>ld_lattice.gen_samples(4)\n</pre> ld_lattice.gen_samples(4) Out[6]: <pre>array([[0.7323192 , 0.47466547, 0.80517287],\n       [0.2323192 , 0.97466547, 0.30517287],\n       [0.9823192 , 0.22466547, 0.55517287],\n       [0.4823192 , 0.72466547, 0.05517287]])</pre> In\u00a0[7]: Copied! <pre>ld_lattice.gen_samples(n_min=2,n_max=4)\n</pre> ld_lattice.gen_samples(n_min=2,n_max=4) Out[7]: <pre>array([[0.9823192 , 0.22466547, 0.55517287],\n       [0.4823192 , 0.72466547, 0.05517287]])</pre> In\u00a0[8]: Copied! <pre>ld_lattice\n</pre> ld_lattice Out[8]: <pre>Lattice (AbstractLDDiscreteDistribution)\n    d               3\n    replications    1\n    randomize       SHIFT\n    gen_vec_source  kuo.lattice-33002-1024-1048576.9125.txt\n    order           RADICAL INVERSE\n    n_limit         2^(20)\n    entropy         218929209270440955509681448220425620652</pre> In\u00a0[9]: Copied! <pre>n = 2**7 # Lattice and Digital Net prefer powers of 2 sample sizes\ndiscrete_distribs = {\n    'IID': qp.IIDStdUniform(2),\n    'LD Lattice': qp.Lattice(2),\n    'LD Digital Net': qp.DigitalNetB2(2),\n    'LD Halton': qp.Halton(2)}\nfig,ax = pyplot.subplots(nrows=1,ncols=len(discrete_distribs),figsize=(3*len(discrete_distribs),3))\nax = np.atleast_1d(ax)\nfor i,(name,discrete_distrib) in enumerate(discrete_distribs.items()):\n    x = discrete_distrib.gen_samples(n)\n    ax[i].scatter(x[:,0],x[:,1],s=5,color=colors[i])\n    ax[i].set_title(name)\n    ax[i].set_aspect(1)\n    ax[i].set_xlabel(r'$X_{i0}$'); ax[i].set_ylabel(r'$X_{i1}$')\n    ax[i].set_xlim([0,1]); ax[i].set_ylim([0,1])\n    ax[i].set_xticks([0,1]); ax[i].set_yticks([0,1])\n</pre> n = 2**7 # Lattice and Digital Net prefer powers of 2 sample sizes discrete_distribs = {     'IID': qp.IIDStdUniform(2),     'LD Lattice': qp.Lattice(2),     'LD Digital Net': qp.DigitalNetB2(2),     'LD Halton': qp.Halton(2)} fig,ax = pyplot.subplots(nrows=1,ncols=len(discrete_distribs),figsize=(3*len(discrete_distribs),3)) ax = np.atleast_1d(ax) for i,(name,discrete_distrib) in enumerate(discrete_distribs.items()):     x = discrete_distrib.gen_samples(n)     ax[i].scatter(x[:,0],x[:,1],s=5,color=colors[i])     ax[i].set_title(name)     ax[i].set_aspect(1)     ax[i].set_xlabel(r'$X_{i0}$'); ax[i].set_ylabel(r'$X_{i1}$')     ax[i].set_xlim([0,1]); ax[i].set_ylim([0,1])     ax[i].set_xticks([0,1]); ax[i].set_yticks([0,1]) In\u00a0[10]: Copied! <pre>m_min,m_max = 6,8\nfig,ax = pyplot.subplots(nrows=1,ncols=len(discrete_distribs),figsize=(3*len(discrete_distribs),3))\nax = np.atleast_1d(ax)\nfor i,(name,discrete_distrib) in enumerate(discrete_distribs.items()):\n    x = discrete_distrib.gen_samples(2**m_max)\n    n_min = 0\n    for m in range(m_min,m_max+1):\n        n_max = 2**m\n        ax[i].scatter(x[n_min:n_max,0],x[n_min:n_max,1],s=5,color=colors[m-m_min],label='n_min = %d, n_max = %d'%(n_min,n_max))\n        n_min = 2**m\n    ax[i].set_title(name)\n    ax[i].set_aspect(1)\n    ax[i].set_xlabel(r'$X_{i0}$'); ax[i].set_ylabel(r'$X_{i1}$')\n    ax[i].set_xlim([0,1]); ax[i].set_ylim([0,1])\n    ax[i].set_xticks([0,1]); ax[i].set_yticks([0,1])\n</pre> m_min,m_max = 6,8 fig,ax = pyplot.subplots(nrows=1,ncols=len(discrete_distribs),figsize=(3*len(discrete_distribs),3)) ax = np.atleast_1d(ax) for i,(name,discrete_distrib) in enumerate(discrete_distribs.items()):     x = discrete_distrib.gen_samples(2**m_max)     n_min = 0     for m in range(m_min,m_max+1):         n_max = 2**m         ax[i].scatter(x[n_min:n_max,0],x[n_min:n_max,1],s=5,color=colors[m-m_min],label='n_min = %d, n_max = %d'%(n_min,n_max))         n_min = 2**m     ax[i].set_title(name)     ax[i].set_aspect(1)     ax[i].set_xlabel(r'$X_{i0}$'); ax[i].set_ylabel(r'$X_{i1}$')     ax[i].set_xlim([0,1]); ax[i].set_ylim([0,1])     ax[i].set_xticks([0,1]); ax[i].set_yticks([0,1]) In\u00a0[11]: Copied! <pre>discrete_distrib = qp.DigitalNetB2(4)\nx = discrete_distrib(2**7)\nd = discrete_distrib.d\nassert d&gt;=2\nfig,ax = pyplot.subplots(nrows=d,ncols=d,figsize=(3*d,3*d))\nfor i in range(d):\n    fig.delaxes(ax[i,i])\n    for j in range(i):\n        ax[i,j].scatter(x[:,i],x[:,j],s=5)\n        fig.delaxes(ax[j,i])\n        ax[i,j].set_aspect(1)\n        ax[i,j].set_xlabel(r'$X_{i%d}$'%i); ax[i,j].set_ylabel(r'$X_{i%d}$'%j)\n        ax[i,j].set_xlim([0,1]); ax[i,j].set_ylim([0,1])\n        ax[i,j].set_xticks([0,1]); ax[i,j].set_yticks([0,1])\n</pre> discrete_distrib = qp.DigitalNetB2(4) x = discrete_distrib(2**7) d = discrete_distrib.d assert d&gt;=2 fig,ax = pyplot.subplots(nrows=d,ncols=d,figsize=(3*d,3*d)) for i in range(d):     fig.delaxes(ax[i,i])     for j in range(i):         ax[i,j].scatter(x[:,i],x[:,j],s=5)         fig.delaxes(ax[j,i])         ax[i,j].set_aspect(1)         ax[i,j].set_xlabel(r'$X_{i%d}$'%i); ax[i,j].set_ylabel(r'$X_{i%d}$'%j)         ax[i,j].set_xlim([0,1]); ax[i,j].set_ylim([0,1])         ax[i,j].set_xticks([0,1]); ax[i,j].set_yticks([0,1]) In\u00a0[12]: Copied! <pre>discrete_distrib = qp.Halton(3)\ntrue_measure = qp.Gaussian(discrete_distrib,mean=[1,2,3],covariance=[4,5,6])\ntrue_measure.gen_samples(4)\n</pre> discrete_distrib = qp.Halton(3) true_measure = qp.Gaussian(discrete_distrib,mean=[1,2,3],covariance=[4,5,6]) true_measure.gen_samples(4) Out[12]: <pre>array([[ 2.18348289,  4.27924141,  1.15343988],\n       [-0.37229914,  2.15929196,  4.43156846],\n       [ 0.92311253, -0.56539553,  1.3713884 ],\n       [ 4.69593279,  3.41246141,  4.68304706]])</pre> In\u00a0[13]: Copied! <pre>true_measure.gen_samples(n_min=2,n_max=4)\n</pre> true_measure.gen_samples(n_min=2,n_max=4) Out[13]: <pre>array([[ 0.92311253, -0.56539553,  1.3713884 ],\n       [ 4.69593279,  3.41246141,  4.68304706]])</pre> In\u00a0[14]: Copied! <pre>true_measure\n</pre> true_measure Out[14]: <pre>Gaussian (AbstractTrueMeasure)\n    mean            [1 2 3]\n    covariance      [4 5 6]\n    decomp_type     PCA</pre> In\u00a0[15]: Copied! <pre>n = 2**7\ndiscrete_distrib = qp.DigitalNetB2(2)\ntrue_measures = {\n    'Non-Standard Uniform': qp.Uniform(discrete_distrib,lower_bound=[-3,-2],upper_bound=[3,2]),\n    'Standard Gaussian': qp.Gaussian(discrete_distrib),\n    'Non-Standard Gaussian': qp.Gaussian(discrete_distrib,mean=[1,2],covariance=[[5,4],[4,9]]),\n    'SciPy Based\\nIndependent Beta-Gamma': qp.SciPyWrapper(discrete_distrib,[scipy.stats.beta(a=1,b=5),scipy.stats.gamma(a=1)])}\nfig,ax = pyplot.subplots(nrows=1,ncols=len(true_measures),figsize=(3*len(true_measures),3))\nax = np.atleast_1d(ax)\nfor i,(name,true_measure) in enumerate(true_measures.items()):\n    t = true_measure.gen_samples(n)\n    ax[i].scatter(t[:,0],t[:,1],s=5,color=colors[i])\n    ax[i].set_title(name)    \n</pre> n = 2**7 discrete_distrib = qp.DigitalNetB2(2) true_measures = {     'Non-Standard Uniform': qp.Uniform(discrete_distrib,lower_bound=[-3,-2],upper_bound=[3,2]),     'Standard Gaussian': qp.Gaussian(discrete_distrib),     'Non-Standard Gaussian': qp.Gaussian(discrete_distrib,mean=[1,2],covariance=[[5,4],[4,9]]),     'SciPy Based\\nIndependent Beta-Gamma': qp.SciPyWrapper(discrete_distrib,[scipy.stats.beta(a=1,b=5),scipy.stats.gamma(a=1)])} fig,ax = pyplot.subplots(nrows=1,ncols=len(true_measures),figsize=(3*len(true_measures),3)) ax = np.atleast_1d(ax) for i,(name,true_measure) in enumerate(true_measures.items()):     t = true_measure.gen_samples(n)     ax[i].scatter(t[:,0],t[:,1],s=5,color=colors[i])     ax[i].set_title(name)     In\u00a0[16]: Copied! <pre>n = 32\ndiscrete_distrib = qp.Lattice(365)\nbrownian_motions = {\n    'Standard Brownian Motion': qp.BrownianMotion(discrete_distrib),\n    'Drifted Brownian Motion': qp.BrownianMotion(discrete_distrib,t_final=5,initial_value=5,drift=-1,diffusion=2)}\nfig,ax = pyplot.subplots(nrows=len(brownian_motions),ncols=1,figsize=(6,3*len(brownian_motions)))\nax = np.atleast_1d(ax)\nfor i,(name,brownian_motion) in enumerate(brownian_motions.items()):\n    t = brownian_motion.gen_samples(n)\n    t_w_init = np.hstack([brownian_motion.initial_value*np.ones((n,1)),t])\n    tvec_w_0 = np.hstack([0,brownian_motion.time_vec])\n    ax[i].plot(tvec_w_0,t_w_init.T)\n    ax[i].set_xlim([tvec_w_0[0],tvec_w_0[-1]])\n    ax[i].set_title(name)\n</pre> n = 32 discrete_distrib = qp.Lattice(365) brownian_motions = {     'Standard Brownian Motion': qp.BrownianMotion(discrete_distrib),     'Drifted Brownian Motion': qp.BrownianMotion(discrete_distrib,t_final=5,initial_value=5,drift=-1,diffusion=2)} fig,ax = pyplot.subplots(nrows=len(brownian_motions),ncols=1,figsize=(6,3*len(brownian_motions))) ax = np.atleast_1d(ax) for i,(name,brownian_motion) in enumerate(brownian_motions.items()):     t = brownian_motion.gen_samples(n)     t_w_init = np.hstack([brownian_motion.initial_value*np.ones((n,1)),t])     tvec_w_0 = np.hstack([0,brownian_motion.time_vec])     ax[i].plot(tvec_w_0,t_w_init.T)     ax[i].set_xlim([tvec_w_0[0],tvec_w_0[-1]])     ax[i].set_title(name) In\u00a0[17]: Copied! <pre>def myfun(t): # define g, the ORIGINAL integrand \n    # t an (n,d) shaped np.ndarray of sample from the ORIGINAL (true) measure\n    y = t.sum(1)\n    return y # an (n,) shaped np.ndarray\ntrue_measure = qp.Gaussian(qp.Halton(5)) # LD Halton discrete distrib for QMC problem\nqp_myfun = qp.CustomFun(true_measure,myfun,parallel=False)\n</pre> def myfun(t): # define g, the ORIGINAL integrand      # t an (n,d) shaped np.ndarray of sample from the ORIGINAL (true) measure     y = t.sum(1)     return y # an (n,) shaped np.ndarray true_measure = qp.Gaussian(qp.Halton(5)) # LD Halton discrete distrib for QMC problem qp_myfun = qp.CustomFun(true_measure,myfun,parallel=False) In\u00a0[18]: Copied! <pre>x = qp_myfun.discrete_distrib.gen_samples(4) # samples from the TRANSFORMED measure\ny = qp_myfun.f(x) # evaluate the TRANSFORMED integrand at the TRANSFORMED samples\ny\n</pre> x = qp_myfun.discrete_distrib.gen_samples(4) # samples from the TRANSFORMED measure y = qp_myfun.f(x) # evaluate the TRANSFORMED integrand at the TRANSFORMED samples y Out[18]: <pre>array([1.80203064, 1.41177199, 2.76302476, 1.46425731])</pre> In\u00a0[19]: Copied! <pre>x = qp_myfun.discrete_distrib.gen_samples(2**16) # samples from the TRANSFORMED measure\ny = qp_myfun.f(x) # evaluate the TRANSFORMED integrand at the TRANSFORMED samples\nmu_hat = y.mean()\nmu_hat\n</pre> x = qp_myfun.discrete_distrib.gen_samples(2**16) # samples from the TRANSFORMED measure y = qp_myfun.f(x) # evaluate the TRANSFORMED integrand at the TRANSFORMED samples mu_hat = y.mean() mu_hat Out[19]: <pre>np.float64(-7.210079451455405e-07)</pre> In\u00a0[20]: Copied! <pre>asian_option = qp.FinancialOption(\n    sampler = qp.DigitalNetB2(52),\n    option = \"ASIAN\",\n    volatility = 1/2,\n    start_price = 30,\n    strike_price = 35,\n    interest_rate = 0.001,\n    t_final = 1,\n    call_put = 'call',\n    asian_mean = 'arithmetic')\nx = asian_option.discrete_distrib.gen_samples(2**16)\ny = asian_option.f(x)\nmu_hat = y.mean()\nmu_hat\n</pre> asian_option = qp.FinancialOption(     sampler = qp.DigitalNetB2(52),     option = \"ASIAN\",     volatility = 1/2,     start_price = 30,     strike_price = 35,     interest_rate = 0.001,     t_final = 1,     call_put = 'call',     asian_mean = 'arithmetic') x = asian_option.discrete_distrib.gen_samples(2**16) y = asian_option.f(x) mu_hat = y.mean() mu_hat Out[20]: <pre>np.float64(1.7888486351025943)</pre> In\u00a0[21]: Copied! <pre>n = 32\nkeister = qp.Keister(qp.DigitalNetB2(1))\nfig,ax = pyplot.subplots(nrows=1,ncols=2,figsize=(8,4))\nx = keister.discrete_distrib.gen_samples(n)\nt = keister.true_measure.gen_samples(n)\nf_of_x = keister.f(x).squeeze()\ng_of_t = keister.g(t).squeeze()\nassert (f_of_x==g_of_t).all()\nx_fine = np.linspace(0,1,257)[1:-1,None]\nf_of_xfine = keister.f(x_fine).squeeze()\nlb = 1.2*max(abs(t.min()),abs(t.max()))\nt_fine = np.linspace(-lb,lb,257)[:,None]\ng_of_tfine = keister.g(t_fine).squeeze()\nax[0].set_title(r'Original')\nax[0].set_xlabel(r'$T_i$'); ax[0].set_ylabel(r'$g(T_i) = g(\\Phi^{-1}(X_i))$')\nax[0].plot(t_fine.squeeze(),g_of_tfine,color=colors[0],alpha=.5)\nax[0].scatter(t.squeeze(),f_of_x,s=10,color='k')\nax[1].set_title(r'Transformed')\nax[1].set_xlabel(r'$X_i$'); ax[1].set_ylabel(r'$f(X_i)$')\nax[1].scatter(x.squeeze(),f_of_x,s=10,color='k')\nax[1].plot(x_fine.squeeze(),f_of_xfine,color=colors[1],alpha=.5);\n</pre> n = 32 keister = qp.Keister(qp.DigitalNetB2(1)) fig,ax = pyplot.subplots(nrows=1,ncols=2,figsize=(8,4)) x = keister.discrete_distrib.gen_samples(n) t = keister.true_measure.gen_samples(n) f_of_x = keister.f(x).squeeze() g_of_t = keister.g(t).squeeze() assert (f_of_x==g_of_t).all() x_fine = np.linspace(0,1,257)[1:-1,None] f_of_xfine = keister.f(x_fine).squeeze() lb = 1.2*max(abs(t.min()),abs(t.max())) t_fine = np.linspace(-lb,lb,257)[:,None] g_of_tfine = keister.g(t_fine).squeeze() ax[0].set_title(r'Original') ax[0].set_xlabel(r'$T_i$'); ax[0].set_ylabel(r'$g(T_i) = g(\\Phi^{-1}(X_i))$') ax[0].plot(t_fine.squeeze(),g_of_tfine,color=colors[0],alpha=.5) ax[0].scatter(t.squeeze(),f_of_x,s=10,color='k') ax[1].set_title(r'Transformed') ax[1].set_xlabel(r'$X_i$'); ax[1].set_ylabel(r'$f(X_i)$') ax[1].scatter(x.squeeze(),f_of_x,s=10,color='k') ax[1].plot(x_fine.squeeze(),f_of_xfine,color=colors[1],alpha=.5); In\u00a0[22]: Copied! <pre>problem_cmc = qp.FinancialOption(qp.IIDStdUniform(52),option=\"ASIAN\")\ncmc_stop_crit = qp.CubMCG(problem_cmc,abs_tol=0.025)\napprox_cmc,data_cmc = cmc_stop_crit.integrate()\ndata_cmc\n</pre> problem_cmc = qp.FinancialOption(qp.IIDStdUniform(52),option=\"ASIAN\") cmc_stop_crit = qp.CubMCG(problem_cmc,abs_tol=0.025) approx_cmc,data_cmc = cmc_stop_crit.integrate() data_cmc Out[22]: <pre>Data (Data)\n    solution        1.791\n    bound_low       1.766\n    bound_high      1.816\n    bound_diff      0.050\n    n_total         492693\n    time_integrate  1.217\nCubMCG (AbstractStoppingCriterion)\n    abs_tol         0.025\n    rel_tol         0\n    n_init          2^(10)\n    n_limit         2^(30)\n    inflate         1.200\n    alpha           0.010\n    kurtmax         1.478\nFinancialOption (AbstractIntegrand)\n    option          ASIAN\n    call_put        CALL\n    volatility      2^(-1)\n    start_price     30\n    strike_price    35\n    interest_rate   0\n    t_final         1\n    asian_mean      ARITHMETIC\nBrownianMotion (AbstractTrueMeasure)\n    time_vec        [0.019 0.038 0.058 ... 0.962 0.981 1.   ]\n    drift           0\n    mean            [0. 0. 0. ... 0. 0. 0.]\n    covariance      [[0.019 0.019 0.019 ... 0.019 0.019 0.019]\n                     [0.019 0.038 0.038 ... 0.038 0.038 0.038]\n                     [0.019 0.038 0.058 ... 0.058 0.058 0.058]\n                     ...\n                     [0.019 0.038 0.058 ... 0.962 0.962 0.962]\n                     [0.019 0.038 0.058 ... 0.962 0.981 0.981]\n                     [0.019 0.038 0.058 ... 0.962 0.981 1.   ]]\n    decomp_type     PCA\nIIDStdUniform (AbstractIIDDiscreteDistribution)\n    d               52\n    replications    1\n    entropy         97012320886264964284917995170158594466</pre> In\u00a0[23]: Copied! <pre>problem_qmc = qp.FinancialOption(qp.DigitalNetB2(52),option=\"ASIAN\")\nqmc_stop_crit = qp.CubQMCNetG(problem_qmc,abs_tol=0.025)\napprox_qmc,data_qmc = qmc_stop_crit.integrate()\ndata_qmc\n</pre> problem_qmc = qp.FinancialOption(qp.DigitalNetB2(52),option=\"ASIAN\") qmc_stop_crit = qp.CubQMCNetG(problem_qmc,abs_tol=0.025) approx_qmc,data_qmc = qmc_stop_crit.integrate() data_qmc Out[23]: <pre>Data (Data)\n    solution        1.773\n    comb_bound_low  1.749\n    comb_bound_high 1.797\n    comb_bound_diff 0.048\n    comb_flags      1\n    n_total         2^(10)\n    n               2^(10)\n    time_integrate  0.002\nCubQMCNetG (AbstractStoppingCriterion)\n    abs_tol         0.025\n    rel_tol         0\n    n_init          2^(10)\n    n_limit         2^(35)\nFinancialOption (AbstractIntegrand)\n    option          ASIAN\n    call_put        CALL\n    volatility      2^(-1)\n    start_price     30\n    strike_price    35\n    interest_rate   0\n    t_final         1\n    asian_mean      ARITHMETIC\nBrownianMotion (AbstractTrueMeasure)\n    time_vec        [0.019 0.038 0.058 ... 0.962 0.981 1.   ]\n    drift           0\n    mean            [0. 0. 0. ... 0. 0. 0.]\n    covariance      [[0.019 0.019 0.019 ... 0.019 0.019 0.019]\n                     [0.019 0.038 0.038 ... 0.038 0.038 0.038]\n                     [0.019 0.038 0.058 ... 0.058 0.058 0.058]\n                     ...\n                     [0.019 0.038 0.058 ... 0.962 0.962 0.962]\n                     [0.019 0.038 0.058 ... 0.962 0.981 0.981]\n                     [0.019 0.038 0.058 ... 0.962 0.981 1.   ]]\n    decomp_type     PCA\nDigitalNetB2 (AbstractLDDiscreteDistribution)\n    d               52\n    replications    1\n    randomize       LMS DS\n    gen_mats_source joe_kuo.6.21201.txt\n    order           RADICAL INVERSE\n    t               63\n    alpha           1\n    n_limit         2^(32)\n    entropy         273514699345978368463399322345138322896</pre> In\u00a0[24]: Copied! <pre>print('QMC took %.2f%% the time and %.2f%% the samples compared to CMC'%(\n      100*data_qmc.time_integrate/data_cmc.time_integrate,100*data_qmc.n_total/data_cmc.n_total))\n</pre> print('QMC took %.2f%% the time and %.2f%% the samples compared to CMC'%(       100*data_qmc.time_integrate/data_cmc.time_integrate,100*data_qmc.n_total/data_cmc.n_total)) <pre>QMC took 0.19% the time and 0.21% the samples compared to CMC\n</pre> In\u00a0[25]: Copied! <pre>cmc_tols = [1,.75,.5,.25,.1,.075,.05,.025]\nqmc_tols = [1,.5,.1,.05,.02,.01,.005,.002,.001]\nfig,ax = pyplot.subplots(nrows=1,ncols=2,figsize=(10,5))\nn_cmc,time_cmc = np.zeros_like(cmc_tols),np.zeros_like(cmc_tols)\nfor i,cmc_tol in enumerate(cmc_tols):\n    cmc_stop_crit = qp.CubMCG(qp.FinancialOption(qp.IIDStdUniform(52),option=\"ASIAN\"),abs_tol=cmc_tol)\n    approx_cmc,data_cmc = cmc_stop_crit.integrate()\n    n_cmc[i],time_cmc[i] = data_cmc.n_total,data_cmc.time_integrate\nax[0].plot(cmc_tols,n_cmc,'-o',color=colors[0],label=r'CMC, $\\mathcal{O}(\\varepsilon^{-2})$')\nax[1].plot(cmc_tols,time_cmc,'-o',color=colors[0])\nn_qmc,time_qmc = np.zeros_like(qmc_tols),np.zeros_like(qmc_tols)\nfor i,qmc_tol in enumerate(qmc_tols):\n    qmc_stop_crit = qp.CubQMCNetG(qp.FinancialOption(qp.DigitalNetB2(52),option=\"ASIAN\"),abs_tol=qmc_tol)\n    approx_qmc,data_qmc = qmc_stop_crit.integrate()\n    n_qmc[i],time_qmc[i] = data_qmc.n_total,data_qmc.time_integrate\nax[0].plot(qmc_tols[1:],n_qmc[1:],'-o',color=colors[1],label=r'QMC, $\\mathcal{O}(\\varepsilon^{-1})$')\nax[1].plot(qmc_tols[1:],time_qmc[1:],'-o',color=colors[1])\nax[0].set_xscale('log',base=10); ax[0].set_yscale('log',base=2)\nax[1].set_xscale('log',base=10); ax[1].set_yscale('log',base=10)\nax[0].invert_xaxis(); ax[1].invert_xaxis()\nax[0].set_xlabel(r'absolute tolerance $\\varepsilon$'); ax[1].set_xlabel(r'absolute tolerance $\\varepsilon$')\nax[0].set_ylabel(r'numer of samples $n$'); ax[1].set_ylabel('integration time')\nax[0].legend(loc='upper left');\n</pre> cmc_tols = [1,.75,.5,.25,.1,.075,.05,.025] qmc_tols = [1,.5,.1,.05,.02,.01,.005,.002,.001] fig,ax = pyplot.subplots(nrows=1,ncols=2,figsize=(10,5)) n_cmc,time_cmc = np.zeros_like(cmc_tols),np.zeros_like(cmc_tols) for i,cmc_tol in enumerate(cmc_tols):     cmc_stop_crit = qp.CubMCG(qp.FinancialOption(qp.IIDStdUniform(52),option=\"ASIAN\"),abs_tol=cmc_tol)     approx_cmc,data_cmc = cmc_stop_crit.integrate()     n_cmc[i],time_cmc[i] = data_cmc.n_total,data_cmc.time_integrate ax[0].plot(cmc_tols,n_cmc,'-o',color=colors[0],label=r'CMC, $\\mathcal{O}(\\varepsilon^{-2})$') ax[1].plot(cmc_tols,time_cmc,'-o',color=colors[0]) n_qmc,time_qmc = np.zeros_like(qmc_tols),np.zeros_like(qmc_tols) for i,qmc_tol in enumerate(qmc_tols):     qmc_stop_crit = qp.CubQMCNetG(qp.FinancialOption(qp.DigitalNetB2(52),option=\"ASIAN\"),abs_tol=qmc_tol)     approx_qmc,data_qmc = qmc_stop_crit.integrate()     n_qmc[i],time_qmc[i] = data_qmc.n_total,data_qmc.time_integrate ax[0].plot(qmc_tols[1:],n_qmc[1:],'-o',color=colors[1],label=r'QMC, $\\mathcal{O}(\\varepsilon^{-1})$') ax[1].plot(qmc_tols[1:],time_qmc[1:],'-o',color=colors[1]) ax[0].set_xscale('log',base=10); ax[0].set_yscale('log',base=2) ax[1].set_xscale('log',base=10); ax[1].set_yscale('log',base=10) ax[0].invert_xaxis(); ax[1].invert_xaxis() ax[0].set_xlabel(r'absolute tolerance $\\varepsilon$'); ax[1].set_xlabel(r'absolute tolerance $\\varepsilon$') ax[0].set_ylabel(r'numer of samples $n$'); ax[1].set_ylabel('integration time') ax[0].legend(loc='upper left'); In\u00a0[26]: Copied! <pre>qmc_stop_crit = qp.CubQMCCLT(\n    integrand = qp.CustomFun(\n        true_measure = qp.Uniform(sampler=qp.Halton(3,replications=32),lower_bound=0,upper_bound=np.pi),\n        g = lambda t: np.stack([np.cos(t).prod(-1),np.sin(t).prod(-1)],axis=0),\n        dimension_indv = 2),\n    abs_tol=.0001)\napprox,data = qmc_stop_crit.integrate()\ndata\n</pre> qmc_stop_crit = qp.CubQMCCLT(     integrand = qp.CustomFun(         true_measure = qp.Uniform(sampler=qp.Halton(3,replications=32),lower_bound=0,upper_bound=np.pi),         g = lambda t: np.stack([np.cos(t).prod(-1),np.sin(t).prod(-1)],axis=0),         dimension_indv = 2),     abs_tol=.0001) approx,data = qmc_stop_crit.integrate() data Out[26]: <pre>Data (Data)\n    solution        [1.006e-05 2.580e-01]\n    comb_bound_low  [-5.012e-05  2.579e-01]\n    comb_bound_high [7.024e-05 2.581e-01]\n    comb_bound_diff [0. 0.]\n    comb_flags      [ True  True]\n    n_total         262144\n    n               [262144 131072]\n    n_rep           [8192 4096]\n    time_integrate  0.348\nCubQMCRepStudentT (AbstractStoppingCriterion)\n    inflate         1\n    alpha           0.010\n    abs_tol         1.00e-04\n    rel_tol         0\n    n_init          2^(8)\n    n_limit         2^(30)\nCustomFun (AbstractIntegrand)\nUniform (AbstractTrueMeasure)\n    lower_bound     0\n    upper_bound     3.142\nHalton (AbstractLDDiscreteDistribution)\n    d               3\n    replications    2^(5)\n    randomize       LMS DP\n    t               63\n    n_limit         2^(32)\n    entropy         339443966251781763705539893307306305063</pre> In\u00a0[27]: Copied! <pre>class CovIntegrand(qp.integrand.Integrand):\n    def __init__(self, sampler):\n        self.sampler = sampler\n        self.true_measure = qp.Gaussian(sampler,mean=1)\n        super(CovIntegrand,self).__init__(dimension_indv=3,dimension_comb=(),parallel=False)\n    def g(self, t):\n        P = t.prod(1) # P\n        S = t.sum(1) # S\n        PS = P*S #PS\n        y = np.stack([PS,P,S],axis=0)\n        return y\n    def bound_fun(self, low, high):\n        comb_low = low[0]-max(low[1]*low[2],low[1]*high[2],high[1]*low[2],high[1]*high[2])\n        comb_high = high[0]-min(low[1]*low[2],low[1]*high[2],high[1]*low[2],high[1]*high[2])\n        return comb_low,comb_high\n    def dependency(self, comb_flag):\n        return np.tile(comb_flag,3)\napprox,data = qp.CubQMCLatticeG(CovIntegrand(qp.Lattice(10)),rel_tol=.025).integrate()\ndata\n</pre> class CovIntegrand(qp.integrand.Integrand):     def __init__(self, sampler):         self.sampler = sampler         self.true_measure = qp.Gaussian(sampler,mean=1)         super(CovIntegrand,self).__init__(dimension_indv=3,dimension_comb=(),parallel=False)     def g(self, t):         P = t.prod(1) # P         S = t.sum(1) # S         PS = P*S #PS         y = np.stack([PS,P,S],axis=0)         return y     def bound_fun(self, low, high):         comb_low = low[0]-max(low[1]*low[2],low[1]*high[2],high[1]*low[2],high[1]*high[2])         comb_high = high[0]-min(low[1]*low[2],low[1]*high[2],high[1]*low[2],high[1]*high[2])         return comb_low,comb_high     def dependency(self, comb_flag):         return np.tile(comb_flag,3) approx,data = qp.CubQMCLatticeG(CovIntegrand(qp.Lattice(10)),rel_tol=.025).integrate() data Out[27]: <pre>Data (Data)\n    solution        10.074\n    comb_bound_low  9.840\n    comb_bound_high 10.320\n    comb_bound_diff 0.480\n    comb_flags      1\n    n_total         2^(20)\n    n               [1048576 1048576 1048576]\n    time_integrate  0.686\nCubQMCLatticeG (AbstractStoppingCriterion)\n    abs_tol         0.010\n    rel_tol         0.025\n    n_init          2^(10)\n    n_limit         2^(30)\nCovIntegrand (AbstractIntegrand)\nGaussian (AbstractTrueMeasure)\n    mean            1\n    covariance      1\n    decomp_type     PCA\nLattice (AbstractLDDiscreteDistribution)\n    d               10\n    replications    1\n    randomize       SHIFT\n    gen_vec_source  kuo.lattice-33002-1024-1048576.9125.txt\n    order           RADICAL INVERSE\n    n_limit         2^(20)\n    entropy         21765430054726836764349893853647822716</pre> In\u00a0[28]: Copied! <pre>from sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neural_network import MLPClassifier\ndata = load_iris()\nog_feature_names = data[\"feature_names\"]\nfeature_names = [fn.replace('sepal ','S')\\\n    .replace('length ','L')\\\n    .replace('petal ','P')\\\n    .replace('width ','W')\\\n    .replace('(cm)','') for fn in og_feature_names]\ntarget_names = data[\"target_names\"]\nxt,xv,yt,yv = train_test_split(data[\"data\"],data[\"target\"],\n    test_size = 1/3,\n    random_state = 7)\npd.DataFrame(np.hstack([data['data'],data['target'][:,None]]),columns=og_feature_names+['species']).iloc[[0,1,90,91,140,141]]\n</pre> from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split from sklearn.neural_network import MLPClassifier data = load_iris() og_feature_names = data[\"feature_names\"] feature_names = [fn.replace('sepal ','S')\\     .replace('length ','L')\\     .replace('petal ','P')\\     .replace('width ','W')\\     .replace('(cm)','') for fn in og_feature_names] target_names = data[\"target_names\"] xt,xv,yt,yv = train_test_split(data[\"data\"],data[\"target\"],     test_size = 1/3,     random_state = 7) pd.DataFrame(np.hstack([data['data'],data['target'][:,None]]),columns=og_feature_names+['species']).iloc[[0,1,90,91,140,141]] Out[28]: sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) species 0 5.1 3.5 1.4 0.2 0.0 1 4.9 3.0 1.4 0.2 0.0 90 5.5 2.6 4.4 1.2 1.0 91 6.1 3.0 4.6 1.4 1.0 140 6.7 3.1 5.6 2.4 2.0 141 6.9 3.1 5.1 2.3 2.0 In\u00a0[29]: Copied! <pre>mlpc = MLPClassifier(random_state=7,max_iter=1024).fit(xt,yt)\nyhat = mlpc.predict(xv)\nprint(\"accuracy: %.1f%%\"%(100*(yv==yhat).mean()))\n# accuracy: 98.0%\nsampler = qp.DigitalNetB2(4,seed=7)\ntrue_measure =  qp.Uniform(sampler,\n    lower_bound = xt.min(0),\n    upper_bound = xt.max(0))\nfun = qp.CustomFun(\n    true_measure = true_measure,\n    g = lambda x: mlpc.predict_proba(x).T,\n    dimension_indv = 3)\nsi_fun = qp.SensitivityIndices(fun,indices=\"all\")\nqmc_algo = qp.CubQMCNetG(si_fun,abs_tol=.005)\nnn_sis,nn_sis_data = qmc_algo.integrate()\n</pre> mlpc = MLPClassifier(random_state=7,max_iter=1024).fit(xt,yt) yhat = mlpc.predict(xv) print(\"accuracy: %.1f%%\"%(100*(yv==yhat).mean())) # accuracy: 98.0% sampler = qp.DigitalNetB2(4,seed=7) true_measure =  qp.Uniform(sampler,     lower_bound = xt.min(0),     upper_bound = xt.max(0)) fun = qp.CustomFun(     true_measure = true_measure,     g = lambda x: mlpc.predict_proba(x).T,     dimension_indv = 3) si_fun = qp.SensitivityIndices(fun,indices=\"all\") qmc_algo = qp.CubQMCNetG(si_fun,abs_tol=.005) nn_sis,nn_sis_data = qmc_algo.integrate() <pre>accuracy: 98.0%\n</pre> In\u00a0[30]: Copied! <pre>#print(nn_sis_data.flags_indv.shape)\n#print(nn_sis_data.flags_comb.shape)\nprint('samples: 2^(%d)'%np.log2(nn_sis_data.n_total))\nprint('time: %.1e'%nn_sis_data.time_integrate)\nprint('indices:\\n%s'%nn_sis_data.integrand.indices)\n\nimport pandas as pd\n\ndf_closed = pd.DataFrame(nn_sis[0],columns=target_names,index=[str(np.where(idx)[0]) for idx in nn_sis_data.integrand.indices])\nprint('\\nClosed Indices')\nprint(df_closed)\ndf_total = pd.DataFrame(nn_sis[1],columns=target_names,index=[str(np.where(idx)[0]) for idx in nn_sis_data.integrand.indices])\nprint('\\nTotal Indices')\ndf_closed_singletons = df_closed.loc[['[%d]'%i for i in range(4)]].T\ndf_closed_singletons['sum singletons'] = df_closed_singletons[['[%d]'%i for i in range(4)]].sum(1)\ndf_closed_singletons.columns = data['feature_names']+['sum']\ndf_closed_singletons = df_closed_singletons*100\ndf_closed_singletons\n</pre> #print(nn_sis_data.flags_indv.shape) #print(nn_sis_data.flags_comb.shape) print('samples: 2^(%d)'%np.log2(nn_sis_data.n_total)) print('time: %.1e'%nn_sis_data.time_integrate) print('indices:\\n%s'%nn_sis_data.integrand.indices)  import pandas as pd  df_closed = pd.DataFrame(nn_sis[0],columns=target_names,index=[str(np.where(idx)[0]) for idx in nn_sis_data.integrand.indices]) print('\\nClosed Indices') print(df_closed) df_total = pd.DataFrame(nn_sis[1],columns=target_names,index=[str(np.where(idx)[0]) for idx in nn_sis_data.integrand.indices]) print('\\nTotal Indices') df_closed_singletons = df_closed.loc[['[%d]'%i for i in range(4)]].T df_closed_singletons['sum singletons'] = df_closed_singletons[['[%d]'%i for i in range(4)]].sum(1) df_closed_singletons.columns = data['feature_names']+['sum'] df_closed_singletons = df_closed_singletons*100 df_closed_singletons <pre>samples: 2^(15)\ntime: 6.6e-01\nindices:\n[[ True False False False]\n [False  True False False]\n [False False  True False]\n [False False False  True]\n [ True  True False False]\n [ True False  True False]\n [ True False False  True]\n [False  True  True False]\n [False  True False  True]\n [False False  True  True]\n [ True  True  True False]\n [ True  True False  True]\n [ True False  True  True]\n [False  True  True  True]]\n\nClosed Indices\n           setosa  versicolor  virginica\n[0]      0.001323    0.068645   0.077325\n[1]      0.063749    0.026565   0.004784\n[2]      0.713825    0.325072   0.497800\n[3]      0.052967    0.025579   0.120317\n[0 1]    0.063925    0.091300   0.085151\n[0 2]    0.715316    0.460314   0.637738\n[0 3]    0.053469    0.092601   0.205639\n[1 2]    0.841655    0.431035   0.513277\n[1 3]    0.110739    0.039410   0.131264\n[2 3]    0.822910    0.583282   0.703142\n[0 1 2]  0.843726    0.570076   0.658272\n[0 1 3]  0.112798    0.104804   0.215817\n[0 2 3]  0.825330    0.815263   0.945267\n[1 2 3]  0.995864    0.739588   0.728499\n\nTotal Indices\n</pre> Out[30]: sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) sum setosa 0.132343 6.374860 71.382498 5.296674 83.186375 versicolor 6.864536 2.656530 32.507230 2.557911 44.586208 virginica 7.732521 0.478364 49.779978 12.031725 70.022587 In\u00a0[31]: Copied! <pre>nindices = len(nn_sis_data.integrand.indices)\nfig,ax = pyplot.subplots(figsize=(9,5))\nticks = np.arange(nindices)\nwidth = .25\nfor i,(alpha,species) in enumerate(zip([.25,.5,.75],data['target_names'])):\n    cvals = df_closed[species].to_numpy()\n    tvals = df_total[species].to_numpy()\n    ticks_i = ticks+i*width\n    ax.bar(ticks_i,cvals,width=width,align='edge',color='k',alpha=alpha,label=species)\n    #ax.bar(ticks_i,np.flip(tvals),width=width,align='edge',bottom=1-np.flip(tvals),color=color,alpha=.1)\nax.set_xlim([0,13+3*width])\nax.set_xticks(ticks+1.5*width)\n\n# closed_labels = [r'$\\underline{s}_{\\{%s\\}}$'%(','.join([r'\\text{%s}'%feature_names[i] for i in idx])) for idx in nn_sis_data.integrand.indices]\nclosed_labels = ['\\n'.join([feature_names[i] for i in np.where(idx)[0]]) for idx in nn_sis_data.integrand.indices]\nax.set_xticklabels(closed_labels,rotation=0)\nax.set_ylim([0,1]); ax.set_yticks([0,1])\nax.grid(False)\nfor spine in ['top','right','bottom']: ax.spines[spine].set_visible(False)\nax.legend(frameon=False,loc='lower center',bbox_to_anchor=(.5,-.2),ncol=3);\nfig.tight_layout()\n</pre> nindices = len(nn_sis_data.integrand.indices) fig,ax = pyplot.subplots(figsize=(9,5)) ticks = np.arange(nindices) width = .25 for i,(alpha,species) in enumerate(zip([.25,.5,.75],data['target_names'])):     cvals = df_closed[species].to_numpy()     tvals = df_total[species].to_numpy()     ticks_i = ticks+i*width     ax.bar(ticks_i,cvals,width=width,align='edge',color='k',alpha=alpha,label=species)     #ax.bar(ticks_i,np.flip(tvals),width=width,align='edge',bottom=1-np.flip(tvals),color=color,alpha=.1) ax.set_xlim([0,13+3*width]) ax.set_xticks(ticks+1.5*width)  # closed_labels = [r'$\\underline{s}_{\\{%s\\}}$'%(','.join([r'\\text{%s}'%feature_names[i] for i in idx])) for idx in nn_sis_data.integrand.indices] closed_labels = ['\\n'.join([feature_names[i] for i in np.where(idx)[0]]) for idx in nn_sis_data.integrand.indices] ax.set_xticklabels(closed_labels,rotation=0) ax.set_ylim([0,1]); ax.set_yticks([0,1]) ax.grid(False) for spine in ['top','right','bottom']: ax.spines[spine].set_visible(False) ax.legend(frameon=False,loc='lower center',bbox_to_anchor=(.5,-.2),ncol=3); fig.tight_layout() In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"demos/talk_paper_demos/pydata_chi_2023/#monte-carlo-for-vector-functions-of-integrals","title":"Monte Carlo for Vector Functions of Integrals\u00b6","text":"<p>Demo Accompanying Aleksei Sorokin's PyData Chicago 2023 Talk</p>"},{"location":"demos/talk_paper_demos/pydata_chi_2023/#monte-carlo-problem","title":"Monte Carlo Problem\u00b6","text":"<p>$$\\text{True Mean} = \\mu = \\mathbb{E}[g(T)] = \\mathbb{E}[f(X)] = \\int_{[0,1]^d} f(x) \\mathrm{d} x \\approx \\frac{1}{n} \\sum_{i=0}^{n-1} f(X_i) = \\hat{\\mu} = \\text{Sample Mean}$$</p> <ul> <li>$T$, original measure on $\\mathcal{T}$</li> <li>$g: \\mathcal{T} \\to \\mathbb{R}$, original integrand</li> <li>$X \\sim \\mathcal{U}[0,1]^d$, transformed measure</li> <li>$f: [0,1]^d \\to \\mathbb{R}$, transformed integrand</li> </ul>"},{"location":"demos/talk_paper_demos/pydata_chi_2023/#python-setup","title":"Python Setup\u00b6","text":""},{"location":"demos/talk_paper_demos/pydata_chi_2023/#discrete-distribution","title":"Discrete Distribution\u00b6","text":"<p>Generate sampling locations $X_0,\\dots,X_{n-1} \\sim \\mathcal{U}[0,1]^d$</p>"},{"location":"demos/talk_paper_demos/pydata_chi_2023/#independent-identically-distributed-iid-points-for-crude-monte-carlo-cmc","title":"Independent Identically Distributed (IID) Points for Crude Monte Carlo (CMC)\u00b6","text":""},{"location":"demos/talk_paper_demos/pydata_chi_2023/#low-discrepancy-ld-points-for-quasi-monte-carlo-qmc","title":"Low Discrepancy (LD) Points for Quasi-Monte Carlo (QMC)\u00b6","text":""},{"location":"demos/talk_paper_demos/pydata_chi_2023/#visuals","title":"Visuals\u00b6","text":""},{"location":"demos/talk_paper_demos/pydata_chi_2023/#iid-vs-ld-points","title":"IID vs LD Points\u00b6","text":""},{"location":"demos/talk_paper_demos/pydata_chi_2023/#ld-space-filling-extensibility","title":"LD Space Filling Extensibility\u00b6","text":""},{"location":"demos/talk_paper_demos/pydata_chi_2023/#high-dimensional-pairs-plotting","title":"High Dimensional Pairs Plotting\u00b6","text":""},{"location":"demos/talk_paper_demos/pydata_chi_2023/#true-measure","title":"True Measure\u00b6","text":"<p>Define $T$, facilitate transform from original integrand $g$ to transformed integrand $f$</p>"},{"location":"demos/talk_paper_demos/pydata_chi_2023/#visuals","title":"Visuals\u00b6","text":""},{"location":"demos/talk_paper_demos/pydata_chi_2023/#some-true-measure-samplings","title":"Some True Measure Samplings\u00b6","text":""},{"location":"demos/talk_paper_demos/pydata_chi_2023/#brownian-motion","title":"Brownian Motion\u00b6","text":""},{"location":"demos/talk_paper_demos/pydata_chi_2023/#integrand","title":"Integrand\u00b6","text":"<p>Define original integrand $g$, store transformed integrand $f$</p>"},{"location":"demos/talk_paper_demos/pydata_chi_2023/#wrap-your-function-into-qmcpy","title":"Wrap your Function into QMCPy\u00b6","text":"<p>Our simple example $$g(T) = T_0+T_1+\\dots+T_{d-1}, \\qquad T \\sim \\mathcal{N}(0,I_d)$$ $$f(X) = g(\\Phi^{-1}(X)), \\qquad \\Phi \\text{ standard normal CDF}$$ $$\\mathbb{E}[f(X)] = \\mathbb{E}[g(T)] = 0$$</p>"},{"location":"demos/talk_paper_demos/pydata_chi_2023/#evalute-the-automatically-transformed-integrand","title":"Evalute the Automatically Transformed Integrand\u00b6","text":""},{"location":"demos/talk_paper_demos/pydata_chi_2023/#manual-qmc-approximation","title":"Manual QMC Approximation\u00b6","text":"<p>Note that when doing importance sampling the below doesn't work. In that case we need to take a specially weighted sum instead instead of the equally weighted sum as done below.</p>"},{"location":"demos/talk_paper_demos/pydata_chi_2023/#predefined-integrands","title":"Predefined Integrands\u00b6","text":"<p>Many more integrands detailed at https://qmcpy.readthedocs.io/en/master/algorithms.html#integrand-class</p> <p>Integrands contain their true measure definition, so the user only needs to pass in a sampler. Samplers are often just discrete distributions.</p>"},{"location":"demos/talk_paper_demos/pydata_chi_2023/#visual-transformation","title":"Visual Transformation\u00b6","text":""},{"location":"demos/talk_paper_demos/pydata_chi_2023/#stopping-criterion","title":"Stopping Criterion\u00b6","text":"<p>Adaptively increase $n$ until $\\lvert \\mu - \\hat{\\mu} \\rvert &lt; \\varepsilon$ where $\\varepsilon$ is a user defined tolerance.</p> <p>The stopping criterion should match the discrete distribution e.g. IID CMC stopping criterion for IID points, QMC Lattice stopping criterion for LD Lattice points, QMC digital net stopping criterion for LD digital net points, etc.</p>"},{"location":"demos/talk_paper_demos/pydata_chi_2023/#iid-cmc-algorithm","title":"IID CMC Algorithm\u00b6","text":""},{"location":"demos/talk_paper_demos/pydata_chi_2023/#ld-qmc-algorithm","title":"LD QMC Algorithm\u00b6","text":""},{"location":"demos/talk_paper_demos/pydata_chi_2023/#visual-cmc-vs-ld","title":"Visual CMC vs LD\u00b6","text":""},{"location":"demos/talk_paper_demos/pydata_chi_2023/#vectorized-stopping-criterion","title":"Vectorized Stopping Criterion\u00b6","text":"<p>Many more examples available at https://github.com/QMCSoftware/QMCSoftware/blob/master/demos/vectorized_qmc.ipynb</p>"},{"location":"demos/talk_paper_demos/pydata_chi_2023/#vector-of-expectations","title":"Vector of Expectations\u00b6","text":"<p>As a simple example, lets compute $\\mathbb{E}[\\cos(T_0)\\cdots\\cos(T_{d-1})]$ and $\\mathbb{E}[\\sin(T_0)\\cdots\\sin(T_{d-1})]$ where $T \\sim \\mathcal{U}[0,\\pi]^d$</p>"},{"location":"demos/talk_paper_demos/pydata_chi_2023/#covariance","title":"Covariance\u00b6","text":"<p>In a simple example, let $T \\sim \\mathcal{N}(1,I_d)$ and compute the covariance of $P = T_0\\cdots T_{d-1}$ and $S = T_0+\\dots+T_{d-1}$ so that $$\\mathrm{Cov}[P,S] = \\mathbb{E}[PS]-\\mathbb{E}[P]\\mathbb{E}[S] = \\mu_0-\\mu_1\\mu_2$$ Theoretically we have $\\mathrm{Cov}[P,S] = 2d-(1)(d) = d$</p>"},{"location":"demos/talk_paper_demos/pydata_chi_2023/#sensitiviy-indices","title":"Sensitiviy Indices\u00b6","text":"<p>See Appendix A of Art Owen's Monte Carlo Book</p> <p>In the following example, we fit a neural network to Iris flower features and try to classify the Iris species. For each set of features, the classifier provides a probability of belonging to each species, a length 3 vector. We quantify the sensitiviy of this classificaiton probability to Iris features, assuming features are uniformly distributed throughout the feature domain.</p>"},{"location":"demos/talk_paper_demos/ACMTOMS_Sorokin_2025/acm_toms_sorokin_2025/","title":"2025 ACM-TOMS Paper","text":"In\u00a0[1]: Copied! <pre>import qmcpy as qp\nimport numpy as np\nimport timeit\nfrom collections import OrderedDict\nimport os\nimport scipy.stats\nimport time\nimport torch\nimport sympy\nimport gc\nimport itertools\n</pre> import qmcpy as qp import numpy as np import timeit from collections import OrderedDict import os import scipy.stats import time import torch import sympy import gc import itertools In\u00a0[2]: Copied! <pre>import matplotlib \nfrom matplotlib import pyplot\nfrom tueplots import cycler\nfrom tueplots.bundles import probnum2025\nfrom tueplots.constants import markers\nfrom tueplots.constants.color import palettes\nmatplotlib.rcParams['figure.dpi'] = 256\n_golden = (1 + 5 ** 0.5) / 2\nMW1 = 240/72\nMW2 = 500/72\nMH1 = MW1/_golden\nMH2 = MW2/_golden\nCOLORS = palettes.tue_plot\nMARKERS = markers.o_sized\npyplot.rcParams.update(probnum2025())\npyplot.rcParams.update(cycler.cycler(color=COLORS,marker=MARKERS))\n</pre> import matplotlib  from matplotlib import pyplot from tueplots import cycler from tueplots.bundles import probnum2025 from tueplots.constants import markers from tueplots.constants.color import palettes matplotlib.rcParams['figure.dpi'] = 256 _golden = (1 + 5 ** 0.5) / 2 MW1 = 240/72 MW2 = 500/72 MH1 = MW1/_golden MH2 = MW2/_golden COLORS = palettes.tue_plot MARKERS = markers.o_sized pyplot.rcParams.update(probnum2025()) pyplot.rcParams.update(cycler.cycler(color=COLORS,marker=MARKERS)) In\u00a0[3]: Copied! <pre>%%time \nlattice = qp.Lattice(\n    dimension = 52,\n    randomize = \"shift\", # for unrandomized lattice set randomize = None\n    replications = 16, # R\n    order = \"radical inverse\", # also supports \"linear\"\n    seed = None, # pass integer seed for reproducibility\n    generating_vector = \"mps.exod2_base2_m20_CKN.txt\")\nx = lattice(2**16) # a numpy.ndarray with shape 16 x 65536 x 52\n</pre> %%time  lattice = qp.Lattice(     dimension = 52,     randomize = \"shift\", # for unrandomized lattice set randomize = None     replications = 16, # R     order = \"radical inverse\", # also supports \"linear\"     seed = None, # pass integer seed for reproducibility     generating_vector = \"mps.exod2_base2_m20_CKN.txt\") x = lattice(2**16) # a numpy.ndarray with shape 16 x 65536 x 52 <pre>CPU times: user 58.9 ms, sys: 27.4 ms, total: 86.3 ms\nWall time: 400 ms\n</pre> In\u00a0[4]: Copied! <pre>%%time\ndnb2 = qp.DigitalNetB2(\n    dimension = 52, \n    randomize = \"LMS DS\", # Matousek's LMS then a digital shift\n    # other options [\"NUS\", \"DS\", \"LMS\", None]\n    t = 64, # number of LMS bits i.e. number of rows in S_j\n    alpha = 2, # interlacing factor for higher order digital nets\n    replications = 16, # R\n    order = \"radical inverse\", # also supports \"Gray code\"\n    seed = None, # pass integer seed for reproducibility\n    generating_matrices = \"joe_kuo.6.21201.txt\")\nx = dnb2(2**16) # a numpy.ndarray with shape 16 x 65536 x 52\n</pre> %%time dnb2 = qp.DigitalNetB2(     dimension = 52,      randomize = \"LMS DS\", # Matousek's LMS then a digital shift     # other options [\"NUS\", \"DS\", \"LMS\", None]     t = 64, # number of LMS bits i.e. number of rows in S_j     alpha = 2, # interlacing factor for higher order digital nets     replications = 16, # R     order = \"radical inverse\", # also supports \"Gray code\"     seed = None, # pass integer seed for reproducibility     generating_matrices = \"joe_kuo.6.21201.txt\") x = dnb2(2**16) # a numpy.ndarray with shape 16 x 65536 x 52 <pre>CPU times: user 364 ms, sys: 88.6 ms, total: 453 ms\nWall time: 453 ms\n</pre> In\u00a0[5]: Copied! <pre>%%time \nhalton = qp.Halton(\n    dimension = 52, \n    randomize = \"LMS DP\", # Matousek's LMS then a digital permutation\n    # other options [\"LMS DS\", \"LMS\", \"DP\", \"DS\", \"NUS\", \"QRNG\", None]\n    t = 64, # number of LMS digits i.e. number of rows in S_j\n    replications = 16, # R\n    seed = None) # pass integer seed for reproducibility\nx = halton(2**10) # a numpy.ndarray with shape 16 x 1024 x 52\n</pre> %%time  halton = qp.Halton(     dimension = 52,      randomize = \"LMS DP\", # Matousek's LMS then a digital permutation     # other options [\"LMS DS\", \"LMS\", \"DP\", \"DS\", \"NUS\", \"QRNG\", None]     t = 64, # number of LMS digits i.e. number of rows in S_j     replications = 16, # R     seed = None) # pass integer seed for reproducibility x = halton(2**10) # a numpy.ndarray with shape 16 x 1024 x 52 <pre>CPU times: user 338 ms, sys: 56.7 ms, total: 394 ms\nWall time: 394 ms\n</pre> In\u00a0[6]: Copied! <pre>d = 3 # dimension \nm = 10 # will generate 2^m points\nn = 2**m # number of points\nlattice = qp.Lattice(d) # default to radical inverse order\nkernel = qp.KernelShiftInvar(\n    d, # dimension \n    alpha = [1,2,3], # per dimension smoothness parameters\n    lengthscales = [1, 1/2, 1/4]) # per dimension product weights\nx = lattice(n_min=0,n_max=n) # shape=(n,d) lattice points\ny = np.random.rand(n) # shape=(n,) random uniforms\n# fast matrix multiplication and linear system solve\nk1 = kernel(x,x[0]) # shape=(n,) first column of Gram matrix\nlam = np.sqrt(n)*qp.fftbr(k1) # vector of eigenvalues\nyt = qp.fftbr(y)\nu = qp.ifftbr(yt*lam) # fast matrix multiplication \nv = qp.ifftbr(yt/lam) # fast linear system solve\n# efficient fast transform updates\nynew = np.random.rand(n) # shape=(n,) new random uniforms\nomega = qp.omega_fftbr(m) # shape=(n,)\nytnew = qp.fftbr(ynew) # shape=(n,)\nytfull = np.concatenate([yt+omega*ytnew,yt-omega*ytnew])/np.sqrt(2) # shape=(2n,)\n</pre> d = 3 # dimension  m = 10 # will generate 2^m points n = 2**m # number of points lattice = qp.Lattice(d) # default to radical inverse order kernel = qp.KernelShiftInvar(     d, # dimension      alpha = [1,2,3], # per dimension smoothness parameters     lengthscales = [1, 1/2, 1/4]) # per dimension product weights x = lattice(n_min=0,n_max=n) # shape=(n,d) lattice points y = np.random.rand(n) # shape=(n,) random uniforms # fast matrix multiplication and linear system solve k1 = kernel(x,x[0]) # shape=(n,) first column of Gram matrix lam = np.sqrt(n)*qp.fftbr(k1) # vector of eigenvalues yt = qp.fftbr(y) u = qp.ifftbr(yt*lam) # fast matrix multiplication  v = qp.ifftbr(yt/lam) # fast linear system solve # efficient fast transform updates ynew = np.random.rand(n) # shape=(n,) new random uniforms omega = qp.omega_fftbr(m) # shape=(n,) ytnew = qp.fftbr(ynew) # shape=(n,) ytfull = np.concatenate([yt+omega*ytnew,yt-omega*ytnew])/np.sqrt(2) # shape=(2n,) In\u00a0[7]: Copied! <pre># slow matrix multiplication and linear system solve\nkmat = kernel(x[:,None,:],x[None,:,:]) # shape=(n,n)\nu_slow = kmat@y # matrix multiplication\nv_slow = np.linalg.solve(kmat,y) # solve a linear system\n# verify correctness\nassert np.allclose(u,u_slow)\nassert np.allclose(v,v_slow)\n# get next samples \nxnew = lattice(n_min=n,n_max=2*n) # shape=(n,d) new lattice points\nk1new = kernel(xnew,x[0]) # shape=(n,) new values in the first column\n# inefficient fast transform update \nk1full = np.concatenate([k1,k1new]) # shape=(2*n,) full first column\nlamfull_inefficient = np.sqrt(2*n)*qp.fftbr(k1full) # shape=(2*n,) full eigenvalues\nyfull = np.concatenate([y,ynew]) # shape=(2*n,) full random values\nytfull_inefficient = qp.fftbr(yfull) # shape=(2*n,) full transformed points\n# efficient fast transform updates\nlamnew = np.sqrt(n)*qp.fftbr(k1new) # shape=(n,) new eigenvalues\nlamfull = np.hstack([lam+omega*lamnew,lam-omega*lamnew])\n# verify correctness\nassert np.allclose(lamfull,lamfull_inefficient)\nassert np.allclose(ytfull,ytfull_inefficient)\n</pre> # slow matrix multiplication and linear system solve kmat = kernel(x[:,None,:],x[None,:,:]) # shape=(n,n) u_slow = kmat@y # matrix multiplication v_slow = np.linalg.solve(kmat,y) # solve a linear system # verify correctness assert np.allclose(u,u_slow) assert np.allclose(v,v_slow) # get next samples  xnew = lattice(n_min=n,n_max=2*n) # shape=(n,d) new lattice points k1new = kernel(xnew,x[0]) # shape=(n,) new values in the first column # inefficient fast transform update  k1full = np.concatenate([k1,k1new]) # shape=(2*n,) full first column lamfull_inefficient = np.sqrt(2*n)*qp.fftbr(k1full) # shape=(2*n,) full eigenvalues yfull = np.concatenate([y,ynew]) # shape=(2*n,) full random values ytfull_inefficient = qp.fftbr(yfull) # shape=(2*n,) full transformed points # efficient fast transform updates lamnew = np.sqrt(n)*qp.fftbr(k1new) # shape=(n,) new eigenvalues lamfull = np.hstack([lam+omega*lamnew,lam-omega*lamnew]) # verify correctness assert np.allclose(lamfull,lamfull_inefficient) assert np.allclose(ytfull,ytfull_inefficient) In\u00a0[8]: Copied! <pre>d = 3 # dimension \nm = 10 # will generate 2^m points\nn = 2**m # number of points\ndnb2 = qp.DigitalNetB2(d) # default to radical inverse order\nkernel = qp.KernelDigShiftInvar(\n    d, # dimension \n    t = dnb2.t, # number of bits in integer representation of points\n    alpha = [1,2,3], # per dimension smoothness parameters\n    lengthscales = [1, 1/2, 1/4]) # per dimension product weights\nx = dnb2(n_min=0,n_max=n) # shape=(n,d) digital net\ny = np.random.rand(n) # shape=(n,) random uniforms\n# fast matrix multiplication and linear system solve\nk1 = kernel(x,x[0]) # shape=(n,) first column of Gram matrix\nlam = np.sqrt(n)*qp.fwht(k1) # vector of eigenvalues\nyt = qp.fwht(y)\nu = qp.fwht(yt*lam) # fast matrix multiplication \nv = qp.fwht(yt/lam) # fast linear system solve\n# efficient fast transform updates\nynew = np.random.rand(n) # shape=(n,) new random uniforms\nomega = qp.omega_fwht(m) # shape=(n,)\nytnew = qp.fwht(ynew) # shape=(n,)\nytfull = np.concatenate([yt+omega*ytnew,yt-omega*ytnew])/np.sqrt(2) # shape=(2n,)\n</pre> d = 3 # dimension  m = 10 # will generate 2^m points n = 2**m # number of points dnb2 = qp.DigitalNetB2(d) # default to radical inverse order kernel = qp.KernelDigShiftInvar(     d, # dimension      t = dnb2.t, # number of bits in integer representation of points     alpha = [1,2,3], # per dimension smoothness parameters     lengthscales = [1, 1/2, 1/4]) # per dimension product weights x = dnb2(n_min=0,n_max=n) # shape=(n,d) digital net y = np.random.rand(n) # shape=(n,) random uniforms # fast matrix multiplication and linear system solve k1 = kernel(x,x[0]) # shape=(n,) first column of Gram matrix lam = np.sqrt(n)*qp.fwht(k1) # vector of eigenvalues yt = qp.fwht(y) u = qp.fwht(yt*lam) # fast matrix multiplication  v = qp.fwht(yt/lam) # fast linear system solve # efficient fast transform updates ynew = np.random.rand(n) # shape=(n,) new random uniforms omega = qp.omega_fwht(m) # shape=(n,) ytnew = qp.fwht(ynew) # shape=(n,) ytfull = np.concatenate([yt+omega*ytnew,yt-omega*ytnew])/np.sqrt(2) # shape=(2n,) In\u00a0[9]: Copied! <pre># slow matrix multiplication and linear system solve\nkmat = kernel(x[:,None,:],x[None,:,:]) # shape=(n,n)\nu_slow = kmat@y # matrix multiplication\nv_slow = np.linalg.solve(kmat,y) # solve a linear system\n# verify correctness\nassert np.allclose(u,u_slow)\nassert np.allclose(v,v_slow)\n# get next samples \nxnew = dnb2(n_min=n,n_max=2*n) # shape=(n,d) new digital net points\nk1new = kernel(xnew,x[0]) # shape=(n,) new values in the first column\n# inefficient fast transform update \nk1full = np.concatenate([k1,k1new]) # shape=(2*n,) full first column\nlamfull_inefficient = np.sqrt(2*n)*qp.fwht(k1full) # shape=(2*n,) full eigenvalues\nyfull = np.concatenate([y,ynew]) # shape=(2*n,) full random values\nytfull_inefficient = qp.fwht(yfull) # shape=(2*n,) full transformed points\n# efficient fast transform updates\nlamnew = np.sqrt(n)*qp.fwht(k1new) # shape=(n,) new eigenvalues\nlamfull = np.hstack([lam+omega*lamnew,lam-omega*lamnew])\n# verify correctness\nassert np.allclose(lamfull,lamfull_inefficient)\nassert np.allclose(ytfull,ytfull_inefficient)\n</pre> # slow matrix multiplication and linear system solve kmat = kernel(x[:,None,:],x[None,:,:]) # shape=(n,n) u_slow = kmat@y # matrix multiplication v_slow = np.linalg.solve(kmat,y) # solve a linear system # verify correctness assert np.allclose(u,u_slow) assert np.allclose(v,v_slow) # get next samples  xnew = dnb2(n_min=n,n_max=2*n) # shape=(n,d) new digital net points k1new = kernel(xnew,x[0]) # shape=(n,) new values in the first column # inefficient fast transform update  k1full = np.concatenate([k1,k1new]) # shape=(2*n,) full first column lamfull_inefficient = np.sqrt(2*n)*qp.fwht(k1full) # shape=(2*n,) full eigenvalues yfull = np.concatenate([y,ynew]) # shape=(2*n,) full random values ytfull_inefficient = qp.fwht(yfull) # shape=(2*n,) full transformed points # efficient fast transform updates lamnew = np.sqrt(n)*qp.fwht(k1new) # shape=(n,) new eigenvalues lamfull = np.hstack([lam+omega*lamnew,lam-omega*lamnew]) # verify correctness assert np.allclose(lamfull,lamfull_inefficient) assert np.allclose(ytfull,ytfull_inefficient) In\u00a0[\u00a0]: Copied! <pre>import scipy.stats\ndef gen_corner_peak_2(x):\n    d = x.shape[-1] # x.shape=(...,n,d), e.g., (n,d) or (R,n,d) \n    c_tilde = 1/np.arange(1,d+1)**2\n    c = 0.25*c_tilde/np.sum(c_tilde)\n    y = (1+np.sum(c*x,axis=-1))**(-(d+1)) \n    return y # y.shape=(...,n), e.g., (n,) or (R,n)\nR = 10 # number of randomizations\nn = 2**15 # number of points \nd = 50 # dimension\ndnb2 = qp.DigitalNetB2(dimension=d, replications=R, seed=7, alpha=3)\nx = dnb2(n) # x.shape=(R,n,d)\ny = gen_corner_peak_2(x) # y.shape=(R,n) \nmuhats = np.mean(y,axis=1) # muhats.shape=(R,)\nmuhat_aggregate = np.mean(muhats) # muhat_aggregate is a scalar \nprint(muhat_aggregate)\n\"\"\" 0.014936813948394042 \"\"\"\nalpha = 0.01 # uncertainty level\nt_star = -scipy.stats.t.ppf(alpha/2,df=R-1) # quantile of Student's t \nstdhat = np.std(muhats,ddof=1) # unbiased estimate of standard deviation\nstd_error = t_star*stdhat/np.sqrt(R)\nprint(std_error)\n\"\"\" 5.247445301861484e-07 \"\"\"\nconf_int = [muhat_aggregate-std_error,muhat_aggregate+std_error]\n</pre> import scipy.stats def gen_corner_peak_2(x):     d = x.shape[-1] # x.shape=(...,n,d), e.g., (n,d) or (R,n,d)      c_tilde = 1/np.arange(1,d+1)**2     c = 0.25*c_tilde/np.sum(c_tilde)     y = (1+np.sum(c*x,axis=-1))**(-(d+1))      return y # y.shape=(...,n), e.g., (n,) or (R,n) R = 10 # number of randomizations n = 2**15 # number of points  d = 50 # dimension dnb2 = qp.DigitalNetB2(dimension=d, replications=R, seed=7, alpha=3) x = dnb2(n) # x.shape=(R,n,d) y = gen_corner_peak_2(x) # y.shape=(R,n)  muhats = np.mean(y,axis=1) # muhats.shape=(R,) muhat_aggregate = np.mean(muhats) # muhat_aggregate is a scalar  print(muhat_aggregate) \"\"\" 0.014936813948394042 \"\"\" alpha = 0.01 # uncertainty level t_star = -scipy.stats.t.ppf(alpha/2,df=R-1) # quantile of Student's t  stdhat = np.std(muhats,ddof=1) # unbiased estimate of standard deviation std_error = t_star*stdhat/np.sqrt(R) print(std_error) \"\"\" 5.247445301861484e-07 \"\"\" conf_int = [muhat_aggregate-std_error,muhat_aggregate+std_error] <pre>0.014936813948394042\n5.247445301861484e-07\n</pre> In\u00a0[\u00a0]: Copied! <pre>def gen_corner_peak_2(x):\n    d = x.shape[-1] # x.shape=(...,n,d), e.g., (n,d) or (R,n,d) \n    c_tilde = 1/np.arange(1,d+1)**2\n    c = 0.25*c_tilde/np.sum(c_tilde)\n    y = (1+np.sum(c*x,axis=-1))**(-(d+1)) \n    return y # y.shape=(...,n), e.g., (n,) or (R,n)\nR = 10\nd = 50 \ndnb2 = qp.DigitalNetB2(dimension=d, replications=R, seed=7, alpha=3)\ntrue_measure = qp.Uniform(dnb2, lower_bound=0, upper_bound=1)\nintegrand = qp.CustomFun(true_measure=true_measure, g=gen_corner_peak_2)\n# equivalent to \n# integrand = qp.Genz(dnb2, kind_func=\"CORNER PEAK\", kind_coeff=2)\nqmc_algo = qp.CubQMCRepStudentT(integrand, abs_tol=1e-4)\nsolution,data = qmc_algo.integrate() # run adaptive QMC algorithm \nprint(solution)\n\"\"\" 0.014950908095474802 \"\"\"\nconf_int = [data.comb_bound_low,data.comb_bound_high]\nstd_error = (conf_int[1]-conf_int[0])/2\nprint(std_error)\n\"\"\" 2.7968149935497788e-05 \"\"\"\nprint(data)\n\"\"\"\nData (Data)\n    solution        0.015\n    comb_bound_low  0.015\n    comb_bound_high 0.015\n    comb_bound_diff 5.59e-05\n    comb_flags      1\n    n_total         10240\n    n               10240\n    n_rep           2^(10)\n    time_integrate  0.019\nCubQMCRepStudentT (AbstractStoppingCriterion)\n    inflate         1\n    alpha           0.010\n    abs_tol         1.00e-04\n    rel_tol         0\n    n_init          2^(8)\n    n_limit         2^(30)\nCustomFun (AbstractIntegrand)\nUniform (AbstractTrueMeasure)\n    lower_bound     0\n    upper_bound     1\nDigitalNetB2 (AbstractLDDiscreteDistribution)\n    d               50\n    replications    10\n    randomize       LMS DS\n    gen_mats_source joe_kuo.6.21201.txt\n    order           RADICAL INVERSE\n    t               63\n    alpha           3\n    n_limit         2^(32)\n    entropy         7\n\"\"\"\n</pre> def gen_corner_peak_2(x):     d = x.shape[-1] # x.shape=(...,n,d), e.g., (n,d) or (R,n,d)      c_tilde = 1/np.arange(1,d+1)**2     c = 0.25*c_tilde/np.sum(c_tilde)     y = (1+np.sum(c*x,axis=-1))**(-(d+1))      return y # y.shape=(...,n), e.g., (n,) or (R,n) R = 10 d = 50  dnb2 = qp.DigitalNetB2(dimension=d, replications=R, seed=7, alpha=3) true_measure = qp.Uniform(dnb2, lower_bound=0, upper_bound=1) integrand = qp.CustomFun(true_measure=true_measure, g=gen_corner_peak_2) # equivalent to  # integrand = qp.Genz(dnb2, kind_func=\"CORNER PEAK\", kind_coeff=2) qmc_algo = qp.CubQMCRepStudentT(integrand, abs_tol=1e-4) solution,data = qmc_algo.integrate() # run adaptive QMC algorithm  print(solution) \"\"\" 0.014950908095474802 \"\"\" conf_int = [data.comb_bound_low,data.comb_bound_high] std_error = (conf_int[1]-conf_int[0])/2 print(std_error) \"\"\" 2.7968149935497788e-05 \"\"\" print(data) \"\"\" Data (Data)     solution        0.015     comb_bound_low  0.015     comb_bound_high 0.015     comb_bound_diff 5.59e-05     comb_flags      1     n_total         10240     n               10240     n_rep           2^(10)     time_integrate  0.019 CubQMCRepStudentT (AbstractStoppingCriterion)     inflate         1     alpha           0.010     abs_tol         1.00e-04     rel_tol         0     n_init          2^(8)     n_limit         2^(30) CustomFun (AbstractIntegrand) Uniform (AbstractTrueMeasure)     lower_bound     0     upper_bound     1 DigitalNetB2 (AbstractLDDiscreteDistribution)     d               50     replications    10     randomize       LMS DS     gen_mats_source joe_kuo.6.21201.txt     order           RADICAL INVERSE     t               63     alpha           3     n_limit         2^(32)     entropy         7 \"\"\" <pre>0.014950908095474802\n2.7968149935497788e-05\nData (Data)\n    solution        0.015\n    comb_bound_low  0.015\n    comb_bound_high 0.015\n    comb_bound_diff 5.59e-05\n    comb_flags      1\n    n_total         10240\n    n               10240\n    n_rep           2^(10)\n    time_integrate  0.006\nCubQMCRepStudentT (AbstractStoppingCriterion)\n    inflate         1\n    alpha           0.010\n    abs_tol         1.00e-04\n    rel_tol         0\n    n_init          2^(8)\n    n_limit         2^(30)\nCustomFun (AbstractIntegrand)\nUniform (AbstractTrueMeasure)\n    lower_bound     0\n    upper_bound     1\nDigitalNetB2 (AbstractLDDiscreteDistribution)\n    d               50\n    replications    10\n    randomize       LMS DS\n    gen_mats_source joe_kuo.6.21201.txt\n    order           RADICAL INVERSE\n    t               63\n    alpha           3\n    n_limit         2^(32)\n    entropy         7\n</pre> Out[\u00a0]: <pre>'\\nData (Data)\\n    solution        0.015\\n    comb_bound_low  0.015\\n    comb_bound_high 0.015\\n    comb_bound_diff 5.59e-05\\n    comb_flags      1\\n    n_total         10240\\n    n               10240\\n    n_rep           2^(10)\\n    time_integrate  0.019\\nCubQMCRepStudentT (AbstractStoppingCriterion)\\n    inflate         1\\n    alpha           0.010\\n    abs_tol         1.00e-04\\n    rel_tol         0\\n    n_init          2^(8)\\n    n_limit         2^(30)\\nCustomFun (AbstractIntegrand)\\nUniform (AbstractTrueMeasure)\\n    lower_bound     0\\n    upper_bound     1\\nDigitalNetB2 (AbstractLDDiscreteDistribution)\\n    d               50\\n    replications    10\\n    randomize       LMS DS\\n    gen_mats_source joe_kuo.6.21201.txt\\n    order           RADICAL INVERSE\\n    t               63\\n    alpha           3\\n    n_limit         2^(32)\\n    entropy         7\\n'</pre> In\u00a0[33]: Copied! <pre>m = 10 # n = 2^m\nn = 2**m # number of points\nd = 2 # dimensions\n</pre> m = 10 # n = 2^m n = 2**m # number of points d = 2 # dimensions In\u00a0[34]: Copied! <pre>pointsets = OrderedDict({\n    \"IID\": (qp.IIDStdUniform(d,seed=11).gen_samples(n),{\"color\":COLORS[2]}),\n    \"Lattice Shift\": (qp.Lattice(d,seed=11).gen_samples(n),{\"color\":COLORS[5]}),\n    \"Halton LMS DP\": (qp.Halton(d,randomize=\"LMS_PERM\",seed=11).gen_samples(n),{\"color\":COLORS[1]}),\n    \"Halton NUS\": (qp.Halton(d,randomize=\"NUS\",seed=11).gen_samples(n),{\"color\":COLORS[4]}),\n    r\"DN${}_{1}$ LMS DS\": (qp.DigitalNetB2(d,randomize=\"LMS_DS\",seed=11).gen_samples(n),{\"color\":COLORS[0]}),\n    r\"DN${}_{2}$ LMS DS\": (qp.DigitalNetB2(d,randomize=\"LMS_DS\",alpha=2,seed=11).gen_samples(n),{\"color\":COLORS[0]}),\n    r\"DN${}_{3}$ LMS DS\": (qp.DigitalNetB2(d,randomize=\"LMS_DS\",alpha=3,seed=11).gen_samples(n),{\"color\":COLORS[0]}),\n    r\"DN${}_{4}$ LMS DS\": (qp.DigitalNetB2(d,randomize=\"LMS_DS\",alpha=4,seed=11).gen_samples(n),{\"color\":COLORS[0]}),\n    r\"DN${}_{1}$ NUS\": (qp.DigitalNetB2(d,randomize=\"NUS\",seed=11).gen_samples(n),{\"color\":COLORS[3]}),\n    r\"DN${}_{2}$ NUS\": (qp.DigitalNetB2(d,randomize=\"NUS\",alpha=2,seed=11).gen_samples(n),{\"color\":COLORS[3]}),\n    r\"DN${}_{3}$ NUS\": (qp.DigitalNetB2(d,randomize=\"NUS\",alpha=3,seed=11).gen_samples(n),{\"color\":COLORS[3]}),\n    r\"DN${}_{4}$ NUS\": (qp.DigitalNetB2(d,randomize=\"NUS\",alpha=4,seed=11).gen_samples(n),{\"color\":COLORS[3]}),\n})\n</pre> pointsets = OrderedDict({     \"IID\": (qp.IIDStdUniform(d,seed=11).gen_samples(n),{\"color\":COLORS[2]}),     \"Lattice Shift\": (qp.Lattice(d,seed=11).gen_samples(n),{\"color\":COLORS[5]}),     \"Halton LMS DP\": (qp.Halton(d,randomize=\"LMS_PERM\",seed=11).gen_samples(n),{\"color\":COLORS[1]}),     \"Halton NUS\": (qp.Halton(d,randomize=\"NUS\",seed=11).gen_samples(n),{\"color\":COLORS[4]}),     r\"DN${}_{1}$ LMS DS\": (qp.DigitalNetB2(d,randomize=\"LMS_DS\",seed=11).gen_samples(n),{\"color\":COLORS[0]}),     r\"DN${}_{2}$ LMS DS\": (qp.DigitalNetB2(d,randomize=\"LMS_DS\",alpha=2,seed=11).gen_samples(n),{\"color\":COLORS[0]}),     r\"DN${}_{3}$ LMS DS\": (qp.DigitalNetB2(d,randomize=\"LMS_DS\",alpha=3,seed=11).gen_samples(n),{\"color\":COLORS[0]}),     r\"DN${}_{4}$ LMS DS\": (qp.DigitalNetB2(d,randomize=\"LMS_DS\",alpha=4,seed=11).gen_samples(n),{\"color\":COLORS[0]}),     r\"DN${}_{1}$ NUS\": (qp.DigitalNetB2(d,randomize=\"NUS\",seed=11).gen_samples(n),{\"color\":COLORS[3]}),     r\"DN${}_{2}$ NUS\": (qp.DigitalNetB2(d,randomize=\"NUS\",alpha=2,seed=11).gen_samples(n),{\"color\":COLORS[3]}),     r\"DN${}_{3}$ NUS\": (qp.DigitalNetB2(d,randomize=\"NUS\",alpha=3,seed=11).gen_samples(n),{\"color\":COLORS[3]}),     r\"DN${}_{4}$ NUS\": (qp.DigitalNetB2(d,randomize=\"NUS\",alpha=4,seed=11).gen_samples(n),{\"color\":COLORS[3]}), }) In\u00a0[36]: Copied! <pre>nrows,ncols = 3,4\nassert len(pointsets)==(nrows*ncols)\nfig,ax = pyplot.subplots(nrows=nrows,ncols=ncols,figsize=(MW2,MW2/ncols*nrows),constrained_layout=False,tight_layout=False)#figsize=(ncols*3,nrows*3))\ns = 1.5\nfor i,(name,(x,pltkwargs)) in enumerate(pointsets.items()):\n    ri,ci = i//ncols,i%ncols\n    ax[ri,ci].set_title(name)\n    ax[ri,ci].scatter(x[:,0],x[:,1],s=s,marker='o',edgecolor='none',**pltkwargs)#,fillstyle='full')\n    ax[ri,ci].set_xlim([0,1]); ax[ri,ci].set_xticks([0,1])\n    ax[ri,ci].set_ylim([0,1]); ax[ri,ci].set_yticks([0,1])\n    ax[ri,ci].set_aspect(1)\nfig.savefig(\"outputs/pointsets.png\",format=\"png\",dpi=1024,transparent=True,bbox_inches=\"tight\")\n</pre> nrows,ncols = 3,4 assert len(pointsets)==(nrows*ncols) fig,ax = pyplot.subplots(nrows=nrows,ncols=ncols,figsize=(MW2,MW2/ncols*nrows),constrained_layout=False,tight_layout=False)#figsize=(ncols*3,nrows*3)) s = 1.5 for i,(name,(x,pltkwargs)) in enumerate(pointsets.items()):     ri,ci = i//ncols,i%ncols     ax[ri,ci].set_title(name)     ax[ri,ci].scatter(x[:,0],x[:,1],s=s,marker='o',edgecolor='none',**pltkwargs)#,fillstyle='full')     ax[ri,ci].set_xlim([0,1]); ax[ri,ci].set_xticks([0,1])     ax[ri,ci].set_ylim([0,1]); ax[ri,ci].set_yticks([0,1])     ax[ri,ci].set_aspect(1) fig.savefig(\"outputs/pointsets.png\",format=\"png\",dpi=1024,transparent=True,bbox_inches=\"tight\") <pre>/var/folders/xk/w1s5c54x0zv90dgqk3vpmbsw004hmz/T/ipykernel_55471/604512548.py:3: UserWarning: The Figure parameters 'tight_layout' and 'constrained_layout' cannot be used together. Please use 'layout' parameter\n  fig,ax = pyplot.subplots(nrows=nrows,ncols=ncols,figsize=(MW2,MW2/ncols*nrows),constrained_layout=False,tight_layout=False)#figsize=(ncols*3,nrows*3))\n</pre> In\u00a0[17]: Copied! <pre>reps = 5\nm_max = 20\n</pre> reps = 5 m_max = 20 In\u00a0[18]: Copied! <pre>def time_block(pointsets_fns, rds):\n    data = {}\n    for name,(generator,pltkwargs) in pointsets_fns.items():\n        data[name] = np.nan*np.empty((len(rds),m_max+1,reps),dtype=np.float64)\n        for i,(r,d) in enumerate(rds):\n            print(\"%25s (r=%4d, d=%4d): \"%(name,r,d),end=\"\",flush=True)\n            for m in range(0,m_max+1):\n                print(\"%d, \"%m,end='',flush=True)\n                for t in range(reps):\n                    gc.collect()\n                    t0 = time.process_time()\n                    x = generator(r,2**m,d)\n                    data[name][i,m,t] = time.process_time()-t0\n                    del x\n                if np.mean(data[name][i,m])&gt;=.2: break\n            print()\n        print()\n    return data \n</pre> def time_block(pointsets_fns, rds):     data = {}     for name,(generator,pltkwargs) in pointsets_fns.items():         data[name] = np.nan*np.empty((len(rds),m_max+1,reps),dtype=np.float64)         for i,(r,d) in enumerate(rds):             print(\"%25s (r=%4d, d=%4d): \"%(name,r,d),end=\"\",flush=True)             for m in range(0,m_max+1):                 print(\"%d, \"%m,end='',flush=True)                 for t in range(reps):                     gc.collect()                     t0 = time.process_time()                     x = generator(r,2**m,d)                     data[name][i,m,t] = time.process_time()-t0                     del x                 if np.mean(data[name][i,m])&gt;=.2: break             print()         print()     return data  In\u00a0[19]: Copied! <pre>pointsets_noho_fns = OrderedDict({\n    r\"DN${}_{1}$ LMS DS\": (lambda r,n,d: qp.DigitalNetB2(d,randomize=\"LMS_DS\",replications=r).gen_samples(n),{\"color\":COLORS[0],\"marker\":MARKERS[0]}),\n    \"Halton LMS DP\": (lambda r,n,d: qp.Halton(d,randomize=\"LMS_DS\",replications=r).gen_samples(n),{\"color\":COLORS[1],\"marker\":MARKERS[1]}),\n    \"IID\": (lambda r,n,d: qp.IIDStdUniform(d,replications=r).gen_samples(n),{\"color\":COLORS[2],\"marker\":MARKERS[2]}),\n    r\"DN${}_{1}$ NUS\": (lambda r,n,d: qp.DigitalNetB2(d,randomize=\"NUS\",replications=r).gen_samples(n),{\"color\":COLORS[3],\"marker\":MARKERS[3]}),\n    \"Halton NUS\": (lambda r,n,d: qp.Halton(d,randomize=\"NUS\",replications=r).gen_samples(n),{\"color\":COLORS[4],\"marker\":MARKERS[4]}),\n    \"Lattice Shift\": (lambda r,n,d: qp.Lattice(d,replications=r).gen_samples(n),{\"color\":COLORS[5],\"marker\":MARKERS[5]}),\n    r\"SciPy DN${}_{1}$ LMS DS\": (lambda r,n,d: np.stack([scipy.stats.qmc.Sobol(d=d,scramble=True,bits=63).random(n) for i in range(r)],axis=0),{\"color\":COLORS[6],\"marker\":MARKERS[6]}),\n    \"Halton DP\": (lambda r,n,d: qp.Halton(d,randomize=\"DP\",replications=r).gen_samples(n),{\"color\":COLORS[7],\"marker\":MARKERS[7]}),\n    r\"PyTorch DN${}_{1}$ LMS DS\": (lambda r,n,d: np.stack([torch.quasirandom.SobolEngine(d,scramble=True).draw(n) for i in range(r)],axis=0),{\"color\":COLORS[8],\"marker\":MARKERS[8]}),\n    \"SciPy Halton DP\": (lambda r,n,d: np.stack([scipy.stats.qmc.Halton(d=d,scramble=True).random(n) for i in range(r)],axis=0),{\"color\":COLORS[9],\"marker\":MARKERS[9]}),\n})\nrds_noho = np.array([\n    [1,1],\n    [1,100],\n    [100,1],\n    [10,10],\n])\nt_noho = time_block(pointsets_noho_fns,rds_noho) \n</pre> pointsets_noho_fns = OrderedDict({     r\"DN${}_{1}$ LMS DS\": (lambda r,n,d: qp.DigitalNetB2(d,randomize=\"LMS_DS\",replications=r).gen_samples(n),{\"color\":COLORS[0],\"marker\":MARKERS[0]}),     \"Halton LMS DP\": (lambda r,n,d: qp.Halton(d,randomize=\"LMS_DS\",replications=r).gen_samples(n),{\"color\":COLORS[1],\"marker\":MARKERS[1]}),     \"IID\": (lambda r,n,d: qp.IIDStdUniform(d,replications=r).gen_samples(n),{\"color\":COLORS[2],\"marker\":MARKERS[2]}),     r\"DN${}_{1}$ NUS\": (lambda r,n,d: qp.DigitalNetB2(d,randomize=\"NUS\",replications=r).gen_samples(n),{\"color\":COLORS[3],\"marker\":MARKERS[3]}),     \"Halton NUS\": (lambda r,n,d: qp.Halton(d,randomize=\"NUS\",replications=r).gen_samples(n),{\"color\":COLORS[4],\"marker\":MARKERS[4]}),     \"Lattice Shift\": (lambda r,n,d: qp.Lattice(d,replications=r).gen_samples(n),{\"color\":COLORS[5],\"marker\":MARKERS[5]}),     r\"SciPy DN${}_{1}$ LMS DS\": (lambda r,n,d: np.stack([scipy.stats.qmc.Sobol(d=d,scramble=True,bits=63).random(n) for i in range(r)],axis=0),{\"color\":COLORS[6],\"marker\":MARKERS[6]}),     \"Halton DP\": (lambda r,n,d: qp.Halton(d,randomize=\"DP\",replications=r).gen_samples(n),{\"color\":COLORS[7],\"marker\":MARKERS[7]}),     r\"PyTorch DN${}_{1}$ LMS DS\": (lambda r,n,d: np.stack([torch.quasirandom.SobolEngine(d,scramble=True).draw(n) for i in range(r)],axis=0),{\"color\":COLORS[8],\"marker\":MARKERS[8]}),     \"SciPy Halton DP\": (lambda r,n,d: np.stack([scipy.stats.qmc.Halton(d=d,scramble=True).random(n) for i in range(r)],axis=0),{\"color\":COLORS[9],\"marker\":MARKERS[9]}), }) rds_noho = np.array([     [1,1],     [1,100],     [100,1],     [10,10], ]) t_noho = time_block(pointsets_noho_fns,rds_noho)  <pre>        DN${}_{1}$ LMS DS (r=   1, d=   1): 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, \n        DN${}_{1}$ LMS DS (r=   1, d= 100): 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, \n        DN${}_{1}$ LMS DS (r= 100, d=   1): 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, \n        DN${}_{1}$ LMS DS (r=  10, d=  10): 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, \n\n            Halton LMS DP (r=   1, d=   1): 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, \n            Halton LMS DP (r=   1, d= 100): 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, \n            Halton LMS DP (r= 100, d=   1): 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, \n            Halton LMS DP (r=  10, d=  10): 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, \n\n                      IID (r=   1, d=   1): 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, \n                      IID (r=   1, d= 100): 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, \n                      IID (r= 100, d=   1): 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, \n                      IID (r=  10, d=  10): 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, \n\n           DN${}_{1}$ NUS (r=   1, d=   1): 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, \n           DN${}_{1}$ NUS (r=   1, d= 100): 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n           DN${}_{1}$ NUS (r= 100, d=   1): 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n           DN${}_{1}$ NUS (r=  10, d=  10): 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n\n               Halton NUS (r=   1, d=   1): 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, \n               Halton NUS (r=   1, d= 100): 0, 1, 2, 3, 4, \n               Halton NUS (r= 100, d=   1): 0, 1, 2, 3, 4, 5, \n               Halton NUS (r=  10, d=  10): 0, 1, 2, 3, 4, 5, \n\n            Lattice Shift (r=   1, d=   1): 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, \n            Lattice Shift (r=   1, d= 100): 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, \n            Lattice Shift (r= 100, d=   1): 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, \n            Lattice Shift (r=  10, d=  10): 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, \n\n  SciPy DN${}_{1}$ LMS DS (r=   1, d=   1): 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, \n  SciPy DN${}_{1}$ LMS DS (r=   1, d= 100): 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, \n  SciPy DN${}_{1}$ LMS DS (r= 100, d=   1): 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, \n  SciPy DN${}_{1}$ LMS DS (r=  10, d=  10): 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, \n\n                Halton DP (r=   1, d=   1): 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, \n                Halton DP (r=   1, d= 100): 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, \n                Halton DP (r= 100, d=   1): 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, \n                Halton DP (r=  10, d=  10): 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, \n\nPyTorch DN${}_{1}$ LMS DS (r=   1, d=   1): 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, \nPyTorch DN${}_{1}$ LMS DS (r=   1, d= 100): 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, \nPyTorch DN${}_{1}$ LMS DS (r= 100, d=   1): 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, \nPyTorch DN${}_{1}$ LMS DS (r=  10, d=  10): 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, \n\n          SciPy Halton DP (r=   1, d=   1): 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, \n          SciPy Halton DP (r=   1, d= 100): 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, \n          SciPy Halton DP (r= 100, d=   1): 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, \n          SciPy Halton DP (r=  10, d=  10): 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, \n\n</pre> In\u00a0[20]: Copied! <pre>pointsets_dnb2_lms_ds_nus_ho_fns = OrderedDict({\n    r\"DN${}_{1}$ LMS DS\": (lambda r,n,d: qp.DigitalNetB2(d,randomize=\"LMS_DS\",replications=r,alpha=1).gen_samples(n),{\"color\":COLORS[0],\"marker\":MARKERS[0]}),\n    r\"DN${}_{1}$ NUS\": (lambda r,n,d: qp.DigitalNetB2(d,randomize=\"NUS\",replications=r,alpha=1).gen_samples(n),{\"color\":COLORS[3],\"marker\":MARKERS[3]}),\n    r\"DN${}_{2}$ LMS DS\": (lambda r,n,d: qp.DigitalNetB2(d,randomize=\"LMS_DS\",replications=r,alpha=2).gen_samples(n),{\"color\":COLORS[0],\"marker\":MARKERS[1]}),\n    r\"DN${}_{2}$ NUS\": (lambda r,n,d: qp.DigitalNetB2(d,randomize=\"NUS\",replications=r,alpha=2).gen_samples(n),{\"color\":COLORS[3],\"marker\":MARKERS[4]}),\n    r\"DN${}_{3}$ LMS DS\": (lambda r,n,d: qp.DigitalNetB2(d,randomize=\"LMS_DS\",replications=r,alpha=3).gen_samples(n),{\"color\":COLORS[0],\"marker\":MARKERS[2]}),\n    r\"DN${}_{3}$ NUS\": (lambda r,n,d: qp.DigitalNetB2(d,randomize=\"NUS\",replications=r,alpha=3).gen_samples(n),{\"color\":COLORS[3],\"marker\":MARKERS[5]}),\n    r\"DN${}_{4}$ LMS DS\": (lambda r,n,d: qp.DigitalNetB2(d,randomize=\"LMS_DS\",replications=r,alpha=4).gen_samples(n),{\"color\":COLORS[0],\"marker\":MARKERS[7]}),\n    r\"DN${}_{4}$ NUS\": (lambda r,n,d: qp.DigitalNetB2(d,randomize=\"NUS\",replications=r,alpha=4).gen_samples(n),{\"color\":COLORS[3],\"marker\":MARKERS[6]}),\n})\nrds_dnb2_lms_ds_nus_ho = np.array([\n    [1,1],\n    [1,100],\n    [100,1],\n    [10,10],\n])\nt_dnb2_lms_ds_nus_ho = time_block(pointsets_dnb2_lms_ds_nus_ho_fns,rds_dnb2_lms_ds_nus_ho)\n</pre> pointsets_dnb2_lms_ds_nus_ho_fns = OrderedDict({     r\"DN${}_{1}$ LMS DS\": (lambda r,n,d: qp.DigitalNetB2(d,randomize=\"LMS_DS\",replications=r,alpha=1).gen_samples(n),{\"color\":COLORS[0],\"marker\":MARKERS[0]}),     r\"DN${}_{1}$ NUS\": (lambda r,n,d: qp.DigitalNetB2(d,randomize=\"NUS\",replications=r,alpha=1).gen_samples(n),{\"color\":COLORS[3],\"marker\":MARKERS[3]}),     r\"DN${}_{2}$ LMS DS\": (lambda r,n,d: qp.DigitalNetB2(d,randomize=\"LMS_DS\",replications=r,alpha=2).gen_samples(n),{\"color\":COLORS[0],\"marker\":MARKERS[1]}),     r\"DN${}_{2}$ NUS\": (lambda r,n,d: qp.DigitalNetB2(d,randomize=\"NUS\",replications=r,alpha=2).gen_samples(n),{\"color\":COLORS[3],\"marker\":MARKERS[4]}),     r\"DN${}_{3}$ LMS DS\": (lambda r,n,d: qp.DigitalNetB2(d,randomize=\"LMS_DS\",replications=r,alpha=3).gen_samples(n),{\"color\":COLORS[0],\"marker\":MARKERS[2]}),     r\"DN${}_{3}$ NUS\": (lambda r,n,d: qp.DigitalNetB2(d,randomize=\"NUS\",replications=r,alpha=3).gen_samples(n),{\"color\":COLORS[3],\"marker\":MARKERS[5]}),     r\"DN${}_{4}$ LMS DS\": (lambda r,n,d: qp.DigitalNetB2(d,randomize=\"LMS_DS\",replications=r,alpha=4).gen_samples(n),{\"color\":COLORS[0],\"marker\":MARKERS[7]}),     r\"DN${}_{4}$ NUS\": (lambda r,n,d: qp.DigitalNetB2(d,randomize=\"NUS\",replications=r,alpha=4).gen_samples(n),{\"color\":COLORS[3],\"marker\":MARKERS[6]}), }) rds_dnb2_lms_ds_nus_ho = np.array([     [1,1],     [1,100],     [100,1],     [10,10], ]) t_dnb2_lms_ds_nus_ho = time_block(pointsets_dnb2_lms_ds_nus_ho_fns,rds_dnb2_lms_ds_nus_ho) <pre>        DN${}_{1}$ LMS DS (r=   1, d=   1): 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, \n        DN${}_{1}$ LMS DS (r=   1, d= 100): 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, \n        DN${}_{1}$ LMS DS (r= 100, d=   1): 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, \n        DN${}_{1}$ LMS DS (r=  10, d=  10): 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, \n\n           DN${}_{1}$ NUS (r=   1, d=   1): 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, \n           DN${}_{1}$ NUS (r=   1, d= 100): 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n           DN${}_{1}$ NUS (r= 100, d=   1): 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n           DN${}_{1}$ NUS (r=  10, d=  10): 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n\n        DN${}_{2}$ LMS DS (r=   1, d=   1): 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, \n        DN${}_{2}$ LMS DS (r=   1, d= 100): 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, \n        DN${}_{2}$ LMS DS (r= 100, d=   1): 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, \n        DN${}_{2}$ LMS DS (r=  10, d=  10): 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, \n\n           DN${}_{2}$ NUS (r=   1, d=   1): 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, \n           DN${}_{2}$ NUS (r=   1, d= 100): 0, 1, 2, 3, 4, 5, 6, 7, 8, \n           DN${}_{2}$ NUS (r= 100, d=   1): 0, 1, 2, 3, 4, 5, 6, 7, 8, \n           DN${}_{2}$ NUS (r=  10, d=  10): 0, 1, 2, 3, 4, 5, 6, 7, 8, \n\n        DN${}_{3}$ LMS DS (r=   1, d=   1): 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, \n        DN${}_{3}$ LMS DS (r=   1, d= 100): 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, \n        DN${}_{3}$ LMS DS (r= 100, d=   1): 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, \n        DN${}_{3}$ LMS DS (r=  10, d=  10): 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, \n\n           DN${}_{3}$ NUS (r=   1, d=   1): 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, \n           DN${}_{3}$ NUS (r=   1, d= 100): 0, 1, 2, 3, 4, 5, 6, 7, \n           DN${}_{3}$ NUS (r= 100, d=   1): 0, 1, 2, 3, 4, 5, 6, 7, \n           DN${}_{3}$ NUS (r=  10, d=  10): 0, 1, 2, 3, 4, 5, 6, 7, \n\n        DN${}_{4}$ LMS DS (r=   1, d=   1): 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, \n        DN${}_{4}$ LMS DS (r=   1, d= 100): 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, \n        DN${}_{4}$ LMS DS (r= 100, d=   1): 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, \n        DN${}_{4}$ LMS DS (r=  10, d=  10): 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, \n\n           DN${}_{4}$ NUS (r=   1, d=   1): 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, \n           DN${}_{4}$ NUS (r=   1, d= 100): 0, 1, 2, 3, 4, 5, 6, 7, \n           DN${}_{4}$ NUS (r= 100, d=   1): 0, 1, 2, 3, 4, 5, 6, 7, \n           DN${}_{4}$ NUS (r=  10, d=  10): 0, 1, 2, 3, 4, 5, 6, 7, \n\n</pre> In\u00a0[21]: Copied! <pre>rds_ft = np.array([\n    [1,1],\n    [10,1],\n    [100,1],\n    [1000,1],\n])\nassert (rds_ft[:,1]==1).all()\n_rmax = 100\nx_fft  = np.random.rand(_rmax*2**(m_max))+1j*np.random.rand(_rmax*2**(m_max))\nx_fwht = np.random.rand(_rmax*2**(m_max))\nft_fns = OrderedDict({\n    \"FFT BR\": (lambda r,n,d: qp.fftbr(x_fft[:r*n].reshape((r,n))),{\"color\":COLORS[0],\"marker\":MARKERS[0]}),\n    \"SciPy FFT\": (lambda r,n,d: scipy.fft.fft(x_fft[:r*n].reshape((r,n))),{\"color\":COLORS[0],\"marker\":MARKERS[1]}),\n    \"IFFT BR\": (lambda r,n,d: qp.ifftbr(x_fft[:r*n].reshape((r,n))),{\"color\":COLORS[1],\"marker\":MARKERS[2]}),\n    \"SciPy IFFT\": (lambda r,n,d: scipy.fft.ifft(x_fft[:r*n].reshape((r,n))),{\"color\":COLORS[1],\"marker\":MARKERS[3]}),\n    \"FWHT\": (lambda r,n,d: qp.fwht(x_fwht[:r*n].reshape((r,n))),{\"color\":COLORS[2],\"marker\":MARKERS[4]}),\n    \"SymPy FWHT\": (lambda r,n,d: np.stack([sympy.fwht(x_fwht_i) for x_fwht_i in x_fwht[:r*n].reshape((r,n))],axis=0),{\"color\":COLORS[2],\"marker\":MARKERS[5]}),\n})\nt_ft = time_block(ft_fns,rds_ft)\n</pre> rds_ft = np.array([     [1,1],     [10,1],     [100,1],     [1000,1], ]) assert (rds_ft[:,1]==1).all() _rmax = 100 x_fft  = np.random.rand(_rmax*2**(m_max))+1j*np.random.rand(_rmax*2**(m_max)) x_fwht = np.random.rand(_rmax*2**(m_max)) ft_fns = OrderedDict({     \"FFT BR\": (lambda r,n,d: qp.fftbr(x_fft[:r*n].reshape((r,n))),{\"color\":COLORS[0],\"marker\":MARKERS[0]}),     \"SciPy FFT\": (lambda r,n,d: scipy.fft.fft(x_fft[:r*n].reshape((r,n))),{\"color\":COLORS[0],\"marker\":MARKERS[1]}),     \"IFFT BR\": (lambda r,n,d: qp.ifftbr(x_fft[:r*n].reshape((r,n))),{\"color\":COLORS[1],\"marker\":MARKERS[2]}),     \"SciPy IFFT\": (lambda r,n,d: scipy.fft.ifft(x_fft[:r*n].reshape((r,n))),{\"color\":COLORS[1],\"marker\":MARKERS[3]}),     \"FWHT\": (lambda r,n,d: qp.fwht(x_fwht[:r*n].reshape((r,n))),{\"color\":COLORS[2],\"marker\":MARKERS[4]}),     \"SymPy FWHT\": (lambda r,n,d: np.stack([sympy.fwht(x_fwht_i) for x_fwht_i in x_fwht[:r*n].reshape((r,n))],axis=0),{\"color\":COLORS[2],\"marker\":MARKERS[5]}), }) t_ft = time_block(ft_fns,rds_ft) <pre>                   FFT BR (r=   1, d=   1): 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, \n                   FFT BR (r=  10, d=   1): 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, \n                   FFT BR (r= 100, d=   1): 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, \n                   FFT BR (r=1000, d=   1): 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, \n\n                SciPy FFT (r=   1, d=   1): 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, \n                SciPy FFT (r=  10, d=   1): 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, \n                SciPy FFT (r= 100, d=   1): 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, \n                SciPy FFT (r=1000, d=   1): 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, \n\n                  IFFT BR (r=   1, d=   1): 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, \n                  IFFT BR (r=  10, d=   1): 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, \n                  IFFT BR (r= 100, d=   1): 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, \n                  IFFT BR (r=1000, d=   1): 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, \n\n               SciPy IFFT (r=   1, d=   1): 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, \n               SciPy IFFT (r=  10, d=   1): 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, \n               SciPy IFFT (r= 100, d=   1): 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, \n               SciPy IFFT (r=1000, d=   1): 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, \n\n                     FWHT (r=   1, d=   1): 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, \n                     FWHT (r=  10, d=   1): 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, \n                     FWHT (r= 100, d=   1): 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, \n                     FWHT (r=1000, d=   1): 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, \n\n               SymPy FWHT (r=   1, d=   1): 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, \n               SymPy FWHT (r=  10, d=   1): 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, \n               SymPy FWHT (r= 100, d=   1): 0, 1, 2, 3, 4, 5, 6, 7, 8, \n               SymPy FWHT (r=1000, d=   1): 0, 1, 2, 3, 4, 5, \n\n</pre> In\u00a0[45]: Copied! <pre>ncols = 4\nnrows = 3\nmvec = np.arange(0,m_max+1)\nnvec = 2**mvec\nfig = pyplot.figure(figsize=(MW2,MW2/ncols*nrows*1.2),constrained_layout=False,tight_layout=False)#,sharey=True,sharex=True)\nsubfigs = fig.subfigures(nrows=nrows,ncols=1)\nax = np.stack([subfigs[j].subplots(nrows=1,ncols=ncols,sharey=True,sharex=True) for j in range(nrows)],axis=0)\ncommonkwargs = {\"markersize\":2.5,\"linewidth\":.5}#,\"markerfacecolor\":'black',\"markeredgecolor\":'white'}\n# statfunc = lambda x: np.nanquantile(x[:,1:],q=.5,axis=1)\nstatfunc = lambda x: np.nanmean(x[:,1:],axis=1)\nfor name in list(t_noho.keys()):\n    for j in range(ncols):\n        ax[0,j].plot(nvec,statfunc(t_noho[name][j,mvec]),label=name,**pointsets_noho_fns[name][1],**commonkwargs)\nfor name in list(t_dnb2_lms_ds_nus_ho.keys()):\n    for j in range(ncols):\n        ax[1,j].plot(nvec,statfunc(t_dnb2_lms_ds_nus_ho[name][j,mvec]),label=name,**pointsets_dnb2_lms_ds_nus_ho_fns[name][1],**commonkwargs)\nfor name in list(t_ft.keys()):\n    for j in range(ncols):\n        ax[2,j].plot(nvec,statfunc(t_ft[name][j,mvec]),label=name,**ft_fns[name][1],**commonkwargs)\nsubfigs[0].legend(*ax[0,0].get_legend_handles_labels(),frameon=False,bbox_to_anchor=(.925,.14),ncol=4)#,fontsize=\"medium\")\nsubfigs[1].legend(*ax[1,0].get_legend_handles_labels(),frameon=False,bbox_to_anchor=(.82,.1),ncol=4)#,fontsize=\"medium\")\nsubfigs[2].legend(*ax[2,0].get_legend_handles_labels(),frameon=False,bbox_to_anchor=(.64,.125),ncol=3)#,fontsize=\"medium\")\nfor i in range(nrows):\n    for j in range(ncols):\n        ax[i,j].set_xscale('log',base=2)\n        ax[i,j].set_yscale('log',base=10)\n        #ax[i,j].yaxis.set_tick_params(labelleft=True)\n        ax[i,j].xaxis.set_tick_params(labelbottom=True)\n        ax[i,j].grid(True)\n        ax[i,j].set_xlim(nvec[0],nvec[-1])\n        _xmin,_xmax = ax[i,j].get_xlim()\n        _ymin,_ymax = ax[i,j].get_ylim()\n        ax[i,j].set_aspect((np.log2(_xmax)-np.log2(_xmin))/(np.log10(_ymax)-np.log10(_ymin)))\n    #ax[i,0].set_ylabel('time (sec)',fontsize=\"xx-large\")\nfor j in range(ncols):\n    ax[0,j].set_title(r\"$(R,d)=(%d,%d)$\"%(rds_noho[j,0],rds_noho[j,1]))\n    ax[1,j].set_title(r\"$(R,d)=(%d,%d)$\"%(rds_dnb2_lms_ds_nus_ho[j,0],rds_dnb2_lms_ds_nus_ho[j,1]))\n    ax[2,j].set_title(r\"$R=%d$\"%rds_ft[j,0])\nax[0,0].set_ylabel(\"popular pointsets\")\nax[1,0].set_ylabel(\"higher-order digital nets\")\nax[2,0].set_ylabel(\"fast transforms\");\nfig.text(s=r\"time (sec) vs number of points $n$\",x=.42,y=.98,fontsize=\"x-large\");\nfig.savefig(\"outputs/timing.png\",format=\"png\",dpi=1024,transparent=True,bbox_inches=\"tight\")\n</pre> ncols = 4 nrows = 3 mvec = np.arange(0,m_max+1) nvec = 2**mvec fig = pyplot.figure(figsize=(MW2,MW2/ncols*nrows*1.2),constrained_layout=False,tight_layout=False)#,sharey=True,sharex=True) subfigs = fig.subfigures(nrows=nrows,ncols=1) ax = np.stack([subfigs[j].subplots(nrows=1,ncols=ncols,sharey=True,sharex=True) for j in range(nrows)],axis=0) commonkwargs = {\"markersize\":2.5,\"linewidth\":.5}#,\"markerfacecolor\":'black',\"markeredgecolor\":'white'} # statfunc = lambda x: np.nanquantile(x[:,1:],q=.5,axis=1) statfunc = lambda x: np.nanmean(x[:,1:],axis=1) for name in list(t_noho.keys()):     for j in range(ncols):         ax[0,j].plot(nvec,statfunc(t_noho[name][j,mvec]),label=name,**pointsets_noho_fns[name][1],**commonkwargs) for name in list(t_dnb2_lms_ds_nus_ho.keys()):     for j in range(ncols):         ax[1,j].plot(nvec,statfunc(t_dnb2_lms_ds_nus_ho[name][j,mvec]),label=name,**pointsets_dnb2_lms_ds_nus_ho_fns[name][1],**commonkwargs) for name in list(t_ft.keys()):     for j in range(ncols):         ax[2,j].plot(nvec,statfunc(t_ft[name][j,mvec]),label=name,**ft_fns[name][1],**commonkwargs) subfigs[0].legend(*ax[0,0].get_legend_handles_labels(),frameon=False,bbox_to_anchor=(.925,.14),ncol=4)#,fontsize=\"medium\") subfigs[1].legend(*ax[1,0].get_legend_handles_labels(),frameon=False,bbox_to_anchor=(.82,.1),ncol=4)#,fontsize=\"medium\") subfigs[2].legend(*ax[2,0].get_legend_handles_labels(),frameon=False,bbox_to_anchor=(.64,.125),ncol=3)#,fontsize=\"medium\") for i in range(nrows):     for j in range(ncols):         ax[i,j].set_xscale('log',base=2)         ax[i,j].set_yscale('log',base=10)         #ax[i,j].yaxis.set_tick_params(labelleft=True)         ax[i,j].xaxis.set_tick_params(labelbottom=True)         ax[i,j].grid(True)         ax[i,j].set_xlim(nvec[0],nvec[-1])         _xmin,_xmax = ax[i,j].get_xlim()         _ymin,_ymax = ax[i,j].get_ylim()         ax[i,j].set_aspect((np.log2(_xmax)-np.log2(_xmin))/(np.log10(_ymax)-np.log10(_ymin)))     #ax[i,0].set_ylabel('time (sec)',fontsize=\"xx-large\") for j in range(ncols):     ax[0,j].set_title(r\"$(R,d)=(%d,%d)$\"%(rds_noho[j,0],rds_noho[j,1]))     ax[1,j].set_title(r\"$(R,d)=(%d,%d)$\"%(rds_dnb2_lms_ds_nus_ho[j,0],rds_dnb2_lms_ds_nus_ho[j,1]))     ax[2,j].set_title(r\"$R=%d$\"%rds_ft[j,0]) ax[0,0].set_ylabel(\"popular pointsets\") ax[1,0].set_ylabel(\"higher-order digital nets\") ax[2,0].set_ylabel(\"fast transforms\"); fig.text(s=r\"time (sec) vs number of points $n$\",x=.42,y=.98,fontsize=\"x-large\"); fig.savefig(\"outputs/timing.png\",format=\"png\",dpi=1024,transparent=True,bbox_inches=\"tight\") <pre>/var/folders/xk/w1s5c54x0zv90dgqk3vpmbsw004hmz/T/ipykernel_57279/3255602729.py:5: UserWarning: The Figure parameters 'tight_layout' and 'constrained_layout' cannot be used together. Please use 'layout' parameter\n  fig = pyplot.figure(figsize=(MW2,MW2/ncols*nrows*1.2),constrained_layout=False,tight_layout=False)#,sharey=True,sharex=True)\n/var/folders/xk/w1s5c54x0zv90dgqk3vpmbsw004hmz/T/ipykernel_57279/3255602729.py:10: RuntimeWarning: Mean of empty slice\n  statfunc = lambda x: np.nanmean(x[:,1:],axis=1)\n</pre> In\u00a0[24]: Copied! <pre># https://www.sfu.ca/~ssurjano/sulf.html\ndef sulfer_func(t):\n    Tr       = t[...,0]\n    fAc      = t[...,1]\n    fRs      = t[...,2]\n    beta_bar = t[...,3]\n    Psi_e    = t[...,4]\n    f_Psi_e  = t[...,5]\n    Q        = t[...,6]\n    Y        = t[...,7]\n    L        = t[...,8]\n    S0 = 1366;\n    A  = 5*10**14;\n    fact1 = (S0**2) * fAc * (Tr**2) * fRs**2 * beta_bar * Psi_e * f_Psi_e;\n    fact2 = 3*Q*Y*L / A;\n    DeltaF = -1/2 * fact1 * fact2;\n    return DeltaF\nsulfer_cf = qp.CustomFun(\n    qp.SciPyWrapper(\n        qp.IIDStdUniform(9),\n        [   scipy.stats.lognorm(scale=0.76, s=np.log(1.2)),\n            scipy.stats.lognorm(scale=0.39, s=np.log(1.1)),\n            scipy.stats.lognorm(scale=0.85, s=np.log(1.1)),\n            scipy.stats.lognorm(scale=0.3,  s=np.log(1.3)),\n            scipy.stats.lognorm(scale=5.0,  s=np.log(1.4)),\n            scipy.stats.lognorm(scale=1.7,  s=np.log(1.2)),\n            scipy.stats.lognorm(scale=71.0, s=np.log(1.15)),\n            scipy.stats.lognorm(scale=0.5,  s=np.log(1.5)),\n            scipy.stats.lognorm(scale=5.5,  s=np.log(1.5)),]),\n    sulfer_func)\n</pre> # https://www.sfu.ca/~ssurjano/sulf.html def sulfer_func(t):     Tr       = t[...,0]     fAc      = t[...,1]     fRs      = t[...,2]     beta_bar = t[...,3]     Psi_e    = t[...,4]     f_Psi_e  = t[...,5]     Q        = t[...,6]     Y        = t[...,7]     L        = t[...,8]     S0 = 1366;     A  = 5*10**14;     fact1 = (S0**2) * fAc * (Tr**2) * fRs**2 * beta_bar * Psi_e * f_Psi_e;     fact2 = 3*Q*Y*L / A;     DeltaF = -1/2 * fact1 * fact2;     return DeltaF sulfer_cf = qp.CustomFun(     qp.SciPyWrapper(         qp.IIDStdUniform(9),         [   scipy.stats.lognorm(scale=0.76, s=np.log(1.2)),             scipy.stats.lognorm(scale=0.39, s=np.log(1.1)),             scipy.stats.lognorm(scale=0.85, s=np.log(1.1)),             scipy.stats.lognorm(scale=0.3,  s=np.log(1.3)),             scipy.stats.lognorm(scale=5.0,  s=np.log(1.4)),             scipy.stats.lognorm(scale=1.7,  s=np.log(1.2)),             scipy.stats.lognorm(scale=71.0, s=np.log(1.15)),             scipy.stats.lognorm(scale=0.5,  s=np.log(1.5)),             scipy.stats.lognorm(scale=5.5,  s=np.log(1.5)),]),     sulfer_func) In\u00a0[25]: Copied! <pre># https://www.sfu.ca/~ssurjano/borehole.html \ndef borehole_func(t):\n    rw = t[...,0];\n    r  = t[...,1];\n    Tu = t[...,2];\n    Hu = t[...,3];\n    Tl = t[...,4];\n    Hl = t[...,5];\n    L  = t[...,6];\n    Kw = t[...,7];\n    frac1 = 2 * np.pi * Tu * (Hu-Hl);\n    frac2a = 2*L*Tu / (np.log(r/rw)*rw**2*Kw);\n    frac2b = Tu / Tl;\n    frac2 = np.log(r/rw) * (1+frac2a+frac2b);\n    y = frac1 / frac2;\n    return y \nborehole_cf = qp.CustomFun(\n    qp.SciPyWrapper(\n        qp.IIDStdUniform(8),\n        [   scipy.stats.norm(loc=0.10,scale=0.0161812),\n            scipy.stats.lognorm(scale=np.exp(7.71),s=1.0056),\n            scipy.stats.uniform(loc=63070,scale=115600-63070),\n            scipy.stats.uniform(loc=990,scale=1110-990),\n            scipy.stats.uniform(loc=63.1,scale=116-63.1),\n            scipy.stats.uniform(loc=700,scale=820-700),\n            scipy.stats.uniform(loc=1120,scale=1680-1120),\n            scipy.stats.uniform(loc=9855,scale=12045-9855),]),\n    borehole_func)\n</pre> # https://www.sfu.ca/~ssurjano/borehole.html  def borehole_func(t):     rw = t[...,0];     r  = t[...,1];     Tu = t[...,2];     Hu = t[...,3];     Tl = t[...,4];     Hl = t[...,5];     L  = t[...,6];     Kw = t[...,7];     frac1 = 2 * np.pi * Tu * (Hu-Hl);     frac2a = 2*L*Tu / (np.log(r/rw)*rw**2*Kw);     frac2b = Tu / Tl;     frac2 = np.log(r/rw) * (1+frac2a+frac2b);     y = frac1 / frac2;     return y  borehole_cf = qp.CustomFun(     qp.SciPyWrapper(         qp.IIDStdUniform(8),         [   scipy.stats.norm(loc=0.10,scale=0.0161812),             scipy.stats.lognorm(scale=np.exp(7.71),s=1.0056),             scipy.stats.uniform(loc=63070,scale=115600-63070),             scipy.stats.uniform(loc=990,scale=1110-990),             scipy.stats.uniform(loc=63.1,scale=116-63.1),             scipy.stats.uniform(loc=700,scale=820-700),             scipy.stats.uniform(loc=1120,scale=1680-1120),             scipy.stats.uniform(loc=9855,scale=12045-9855),]),     borehole_func) In\u00a0[26]: Copied! <pre># https://www.sfu.ca/~ssurjano/webetal96.html\nwebster_cf = qp.CustomFun(\n    qp.SciPyWrapper(\n        qp.IIDStdUniform(2),\n        [   scipy.stats.uniform(loc=1,scale=10-1),\n            scipy.stats.norm(loc=2,scale=1)]),\n    lambda x: x[:,0]**2+x[:,1]**3)\n</pre> # https://www.sfu.ca/~ssurjano/webetal96.html webster_cf = qp.CustomFun(     qp.SciPyWrapper(         qp.IIDStdUniform(2),         [   scipy.stats.uniform(loc=1,scale=10-1),             scipy.stats.norm(loc=2,scale=1)]),     lambda x: x[:,0]**2+x[:,1]**3) In\u00a0[27]: Copied! <pre>def oakley_ohagan15_func(t):\n    a1 = np.array([0.0118, 0.0456, 0.2297, 0.0393, 0.1177, 0.3865, 0.3897, 0.6061, 0.6159, 0.4005, 1.0741, 1.1474, 0.7880, 1.1242, 1.1982])\n    a2 = np.array([0.4341, 0.0887, 0.0512, 0.3233, 0.1489, 1.0360, 0.9892, 0.9672, 0.8977, 0.8083, 1.8426, 2.4712, 2.3946, 2.0045, 2.2621])\n    a3 = np.array([0.1044, 0.2057, 0.0774, 0.2730, 0.1253, 0.7526, 0.8570, 1.0331, 0.8388, 0.7970, 2.2145, 2.0382, 2.4004, 2.0541, 1.9845])\n    M = np.array([\n        [-0.022482886,  -0.18501666,  0.13418263,   0.36867264,   0.17172785,   0.13651143,  -0.44034404, -0.081422854,   0.71321025,  -0.44361072,   0.50383394, -0.024101458, -0.045939684,   0.21666181,  0.055887417],\n        [   0.25659630,  0.053792287,  0.25800381,   0.23795905,  -0.59125756, -0.081627077,  -0.28749073,   0.41581639,   0.49752241,  0.083893165,  -0.11056683,  0.033222351,  -0.13979497, -0.031020556,  -0.22318721],\n        [ -0.055999811,   0.19542252, 0.095529005,  -0.28626530,  -0.14441303,   0.22369356,   0.14527412,   0.28998481,   0.23105010,  -0.31929879,  -0.29039128,  -0.20956898,   0.43139047,  0.024429152,  0.044904409],\n        [   0.66448103,   0.43069872,  0.29924645,  -0.16202441,  -0.31479544,  -0.39026802,   0.17679822,  0.057952663,   0.17230342,   0.13466011,  -0.35275240,   0.25146896, -0.018810529,   0.36482392,  -0.32504618],\n        [  -0.12127800,   0.12463327,  0.10656519,  0.046562296,  -0.21678617,   0.19492172, -0.065521126,  0.024404669, -0.096828860,   0.19366196,   0.33354757,   0.31295994, -0.083615456,  -0.25342082,   0.37325717],\n        [  -0.28376230,  -0.32820154, -0.10496068,  -0.22073452,  -0.13708154,  -0.14426375,  -0.11503319,   0.22424151, -0.030395022,  -0.51505615,  0.017254978,  0.038957118,   0.36069184,   0.30902452,  0.050030193],\n        [ -0.077875893, 0.0037456560,  0.88685604,  -0.26590028, -0.079325357, -0.042734919,  -0.18653782,  -0.35604718,  -0.17497421,  0.088699956,   0.40025886, -0.055979693,   0.13724479,   0.21485613, -0.011265799],\n        [ -0.092294730,   0.59209563, 0.031338285, -0.033080861,  -0.24308858, -0.099798547,  0.034460195,  0.095119813,  -0.33801620, 0.0063860024,  -0.61207299,  0.081325416,   0.88683114,   0.14254905,   0.14776204],\n        [  -0.13189434,   0.52878496,  0.12652391,  0.045113625,   0.58373514,   0.37291503,   0.11395325,  -0.29479222,  -0.57014085,   0.46291592, -0.094050179,   0.13959097,  -0.38607402,  -0.44897060,  -0.14602419],\n        [  0.058107658,  -0.32289338, 0.093139162,  0.072427234,  -0.56919401,   0.52554237,   0.23656926, -0.011782016,  0.071820601,  0.078277291,  -0.13355752,   0.22722721,   0.14369455,  -0.45198935,  -0.55574794],\n        [   0.66145875,   0.34633299,  0.14098019,   0.51882591,  -0.28019898,  -0.16032260, -0.068413337,  -0.20428242,  0.069672173,   0.23112577, -0.044368579,  -0.16455425,   0.21620977, 0.0042702105, -0.087399014],\n        [   0.31599556, -0.027551859,  0.13434254,   0.13497371,  0.054005680,  -0.17374789,   0.17525393,  0.060258929,  -0.17914162,  -0.31056619,  -0.25358691,  0.025847535,  -0.43006001,  -0.62266361, -0.033996882],\n        [  -0.29038151,  0.034101270, 0.034903413,  -0.12121764,  0.026030714,  -0.33546274,  -0.41424111,  0.053248380,  -0.27099455, -0.026251302,   0.41024137,   0.26636349,   0.15582891,  -0.18666254,  0.019895831],\n        [  -0.24388652,  -0.44098852, 0.012618825,   0.24945112,  0.071101888,   0.24623792,   0.17484502, 0.0085286769,   0.25147070,  -0.14659862, -0.084625150,   0.36931333,  -0.29955293,   0.11044360,  -0.75690139],\n        [  0.041494323,  -0.25980564,  0.46402128,  -0.36112127,  -0.94980789,  -0.16504063, 0.0030943325,  0.052792942,   0.22523648,   0.38390366,   0.45562427,  -0.18631744, 0.0082333995,   0.16670803,   0.16045688]])\n    return (a1*t).sum(-1) + (a2*np.sin(t)).sum(-1) + (a3*np.cos(t)).sum(-1) + (t*np.einsum(\"ij,...j-&gt;...i\",M,t)).sum(-1)\noakley_ohagan15_cf = qp.CustomFun(\n        qp.Gaussian(qp.IIDStdUniform(15)),\n        oakley_ohagan15_func)\n</pre> def oakley_ohagan15_func(t):     a1 = np.array([0.0118, 0.0456, 0.2297, 0.0393, 0.1177, 0.3865, 0.3897, 0.6061, 0.6159, 0.4005, 1.0741, 1.1474, 0.7880, 1.1242, 1.1982])     a2 = np.array([0.4341, 0.0887, 0.0512, 0.3233, 0.1489, 1.0360, 0.9892, 0.9672, 0.8977, 0.8083, 1.8426, 2.4712, 2.3946, 2.0045, 2.2621])     a3 = np.array([0.1044, 0.2057, 0.0774, 0.2730, 0.1253, 0.7526, 0.8570, 1.0331, 0.8388, 0.7970, 2.2145, 2.0382, 2.4004, 2.0541, 1.9845])     M = np.array([         [-0.022482886,  -0.18501666,  0.13418263,   0.36867264,   0.17172785,   0.13651143,  -0.44034404, -0.081422854,   0.71321025,  -0.44361072,   0.50383394, -0.024101458, -0.045939684,   0.21666181,  0.055887417],         [   0.25659630,  0.053792287,  0.25800381,   0.23795905,  -0.59125756, -0.081627077,  -0.28749073,   0.41581639,   0.49752241,  0.083893165,  -0.11056683,  0.033222351,  -0.13979497, -0.031020556,  -0.22318721],         [ -0.055999811,   0.19542252, 0.095529005,  -0.28626530,  -0.14441303,   0.22369356,   0.14527412,   0.28998481,   0.23105010,  -0.31929879,  -0.29039128,  -0.20956898,   0.43139047,  0.024429152,  0.044904409],         [   0.66448103,   0.43069872,  0.29924645,  -0.16202441,  -0.31479544,  -0.39026802,   0.17679822,  0.057952663,   0.17230342,   0.13466011,  -0.35275240,   0.25146896, -0.018810529,   0.36482392,  -0.32504618],         [  -0.12127800,   0.12463327,  0.10656519,  0.046562296,  -0.21678617,   0.19492172, -0.065521126,  0.024404669, -0.096828860,   0.19366196,   0.33354757,   0.31295994, -0.083615456,  -0.25342082,   0.37325717],         [  -0.28376230,  -0.32820154, -0.10496068,  -0.22073452,  -0.13708154,  -0.14426375,  -0.11503319,   0.22424151, -0.030395022,  -0.51505615,  0.017254978,  0.038957118,   0.36069184,   0.30902452,  0.050030193],         [ -0.077875893, 0.0037456560,  0.88685604,  -0.26590028, -0.079325357, -0.042734919,  -0.18653782,  -0.35604718,  -0.17497421,  0.088699956,   0.40025886, -0.055979693,   0.13724479,   0.21485613, -0.011265799],         [ -0.092294730,   0.59209563, 0.031338285, -0.033080861,  -0.24308858, -0.099798547,  0.034460195,  0.095119813,  -0.33801620, 0.0063860024,  -0.61207299,  0.081325416,   0.88683114,   0.14254905,   0.14776204],         [  -0.13189434,   0.52878496,  0.12652391,  0.045113625,   0.58373514,   0.37291503,   0.11395325,  -0.29479222,  -0.57014085,   0.46291592, -0.094050179,   0.13959097,  -0.38607402,  -0.44897060,  -0.14602419],         [  0.058107658,  -0.32289338, 0.093139162,  0.072427234,  -0.56919401,   0.52554237,   0.23656926, -0.011782016,  0.071820601,  0.078277291,  -0.13355752,   0.22722721,   0.14369455,  -0.45198935,  -0.55574794],         [   0.66145875,   0.34633299,  0.14098019,   0.51882591,  -0.28019898,  -0.16032260, -0.068413337,  -0.20428242,  0.069672173,   0.23112577, -0.044368579,  -0.16455425,   0.21620977, 0.0042702105, -0.087399014],         [   0.31599556, -0.027551859,  0.13434254,   0.13497371,  0.054005680,  -0.17374789,   0.17525393,  0.060258929,  -0.17914162,  -0.31056619,  -0.25358691,  0.025847535,  -0.43006001,  -0.62266361, -0.033996882],         [  -0.29038151,  0.034101270, 0.034903413,  -0.12121764,  0.026030714,  -0.33546274,  -0.41424111,  0.053248380,  -0.27099455, -0.026251302,   0.41024137,   0.26636349,   0.15582891,  -0.18666254,  0.019895831],         [  -0.24388652,  -0.44098852, 0.012618825,   0.24945112,  0.071101888,   0.24623792,   0.17484502, 0.0085286769,   0.25147070,  -0.14659862, -0.084625150,   0.36931333,  -0.29955293,   0.11044360,  -0.75690139],         [  0.041494323,  -0.25980564,  0.46402128,  -0.36112127,  -0.94980789,  -0.16504063, 0.0030943325,  0.052792942,   0.22523648,   0.38390366,   0.45562427,  -0.18631744, 0.0082333995,   0.16670803,   0.16045688]])     return (a1*t).sum(-1) + (a2*np.sin(t)).sum(-1) + (a3*np.cos(t)).sum(-1) + (t*np.einsum(\"ij,...j-&gt;...i\",M,t)).sum(-1) oakley_ohagan15_cf = qp.CustomFun(         qp.Gaussian(qp.IIDStdUniform(15)),         oakley_ohagan15_func) In\u00a0[28]: Copied! <pre>def cbeam_func(t):\n    R = t[...,0]\n    E = t[...,1]\n    X = t[...,2]\n    Y = t[...,3]\n    L = 100;\n    D_0 = 2.2535;\n    w = 4;\n    t = 2;\n    Sterm1 = 600*Y / (w*(t**2));\n    Sterm2 = 600*X / ((w**2)*t);\n    S = Sterm1 + Sterm2;\n    Dfact1 = 4*(L**3) / (E*w*t);\n    Dfact2 = np.sqrt((Y/(t**2))**2 + (X/(w**2))**2);\n    D = Dfact1 * Dfact2\n    return D \ncbeam_cf = qp.CustomFun(\n    qp.Gaussian(qp.IIDStdUniform(4),mean=[40000,2.9e7,500,1000],covariance=[2000**2,1.45e6**2,100**2,100**2]),\n    cbeam_func)\n</pre> def cbeam_func(t):     R = t[...,0]     E = t[...,1]     X = t[...,2]     Y = t[...,3]     L = 100;     D_0 = 2.2535;     w = 4;     t = 2;     Sterm1 = 600*Y / (w*(t**2));     Sterm2 = 600*X / ((w**2)*t);     S = Sterm1 + Sterm2;     Dfact1 = 4*(L**3) / (E*w*t);     Dfact2 = np.sqrt((Y/(t**2))**2 + (X/(w**2))**2);     D = Dfact1 * Dfact2     return D  cbeam_cf = qp.CustomFun(     qp.Gaussian(qp.IIDStdUniform(4),mean=[40000,2.9e7,500,1000],covariance=[2000**2,1.45e6**2,100**2,100**2]),     cbeam_func) In\u00a0[29]: Copied! <pre>def G_func(t):\n    d = t.shape[-1]\n    a = (np.arange(1,d+1)-2)/2\n    return ((np.abs(4*t-2)+a)/(1+a)).prod(-1)\nG_cf = qp.CustomFun(\n    qp.Uniform(qp.IIDStdUniform(3)),\n    G_func)\n</pre> def G_func(t):     d = t.shape[-1]     a = (np.arange(1,d+1)-2)/2     return ((np.abs(4*t-2)+a)/(1+a)).prod(-1) G_cf = qp.CustomFun(     qp.Uniform(qp.IIDStdUniform(3)),     G_func) In\u00a0[30]: Copied! <pre>simple_func_1d = qp.CustomFun(qp.Uniform(qp.IIDStdUniform(1)),lambda x: x[...,0]*np.exp(x[...,0])-1.)\n</pre> simple_func_1d = qp.CustomFun(qp.Uniform(qp.IIDStdUniform(1)),lambda x: x[...,0]*np.exp(x[...,0])-1.) In\u00a0[31]: Copied! <pre>simple_func_2d = qp.CustomFun(qp.Uniform(qp.IIDStdUniform(2)),lambda x: x[...,1]*np.exp(x[...,0]*x[...,1])/(np.exp(1)-2)-1)\n</pre> simple_func_2d = qp.CustomFun(qp.Uniform(qp.IIDStdUniform(2)),lambda x: x[...,1]*np.exp(x[...,0]*x[...,1])/(np.exp(1)-2)-1) In\u00a0[32]: Copied! <pre>oakley_ohagan_2d = qp.CustomFun(\n        qp.Uniform(qp.IIDStdUniform(2),lower_bound=-0.01,upper_bound=0.01),\n        lambda x: 5+x[...,0]+x[...,1]+2*np.cos(x[...,0])+2*np.sin(x[...,1]))\n</pre> oakley_ohagan_2d = qp.CustomFun(         qp.Uniform(qp.IIDStdUniform(2),lower_bound=-0.01,upper_bound=0.01),         lambda x: 5+x[...,0]+x[...,1]+2*np.cos(x[...,0])+2*np.sin(x[...,1])) In\u00a0[33]: Copied! <pre>genz_oscillatory3_3d = qp.Genz(qp.IIDStdUniform(3),kind_func='oscillatory',kind_coeff=3)\ngenz_cornerpeak2_3d = qp.Genz(qp.IIDStdUniform(3),kind_func='corner-peak',kind_coeff=2)\n</pre> genz_oscillatory3_3d = qp.Genz(qp.IIDStdUniform(3),kind_func='oscillatory',kind_coeff=3) genz_cornerpeak2_3d = qp.Genz(qp.IIDStdUniform(3),kind_func='corner-peak',kind_coeff=2) In\u00a0[34]: Copied! <pre>ishigami = qp.Ishigami(qp.IIDStdUniform(3))\n</pre> ishigami = qp.Ishigami(qp.IIDStdUniform(3)) In\u00a0[35]: Copied! <pre>def convergence_block(integrands, pointsets_fns, r, m_max):\n    times = {} \n    muhathats = {} \n    rmses = {}\n    for name,integrand in integrands.items():\n        times[name] = {} \n        muhathats[name] = {} \n        rmses[name] = {} \n        for pname,(generator,pltkwargs) in pointsets_fns.items():\n            times[name][pname] = np.nan*np.empty(m_max+1,dtype=np.float64)\n            muhathats[name][pname] = np.nan*np.empty(m_max+1,dtype=np.float64)\n            rmses[name][pname] = np.nan*np.empty(m_max+1,dtype=np.float64)\n            print(\"%35s %-35s: \"%(name,pname),end=\"\",flush=True)\n            for m in range(0,m_max+1):\n                print(\"%d, \"%m,end='',flush=True)\n                t0 = timeit.default_timer()\n                x = generator(r,2**m,integrand.d)\n                times[name][pname][m] = timeit.default_timer()-t0\n                y = integrand.f(x)\n                muhats = y.mean(1)\n                muhathat = y.mean()\n                muhathats[name][pname][m] = muhathat\n                rmses[name][pname][m] = np.sqrt(np.mean((muhats-muhathat)**2/(r*(r-1))))\n            print()\n        print()\n    return times,muhathats,rmses\n</pre> def convergence_block(integrands, pointsets_fns, r, m_max):     times = {}      muhathats = {}      rmses = {}     for name,integrand in integrands.items():         times[name] = {}          muhathats[name] = {}          rmses[name] = {}          for pname,(generator,pltkwargs) in pointsets_fns.items():             times[name][pname] = np.nan*np.empty(m_max+1,dtype=np.float64)             muhathats[name][pname] = np.nan*np.empty(m_max+1,dtype=np.float64)             rmses[name][pname] = np.nan*np.empty(m_max+1,dtype=np.float64)             print(\"%35s %-35s: \"%(name,pname),end=\"\",flush=True)             for m in range(0,m_max+1):                 print(\"%d, \"%m,end='',flush=True)                 t0 = timeit.default_timer()                 x = generator(r,2**m,integrand.d)                 times[name][pname][m] = timeit.default_timer()-t0                 y = integrand.f(x)                 muhats = y.mean(1)                 muhathat = y.mean()                 muhathats[name][pname][m] = muhathat                 rmses[name][pname][m] = np.sqrt(np.mean((muhats-muhathat)**2/(r*(r-1))))             print()         print()     return times,muhathats,rmses In\u00a0[36]: Copied! <pre>funcs = OrderedDict({\n    r\"$f(x) = x e^x - 1$\": simple_func_1d,\n    r\"$f(x_1,x_2) = x_2 e^{x_1 x_2}/(e-2)-1$\": simple_func_2d,\n    r\"Oakley-O'Hagan, $d=2$\": oakley_ohagan_2d,\n    r\"G-Function, $d=%d$\"%G_cf.d: G_cf,\n    r\"Genz Oscillatory, $d=3$\": genz_oscillatory3_3d,\n    r\"Genz Corner-peak, $d=3$\": genz_cornerpeak2_3d,\n    #\"Ishigami\": ishigami, \n    # \"Sulfer\": sulfer_cf,\n    #\"Borehole\": borehole_cf,\n    # \"Webster\": webster_cf,\n    #r\"Oakley-O'Hagan with $d=15$\": oakley_ohagan15_cf,\n    # \"Cantilever Beam\": cbeam_cf,\n    # r\"Box Integral, $d=?$\": qp.BoxIntegral(qp.IIDStdUniform(4),s=-5),\n})\nseed = 7\npointsets = OrderedDict({\n    \"IID\": (lambda r,n,d: qp.IIDStdUniform(d,replications=r,seed=seed).gen_samples(n),{\"color\":COLORS[2],\"marker\":MARKERS[2]}),\n    \"Lattice Shift\": (lambda r,n,d: qp.Lattice(d,replications=r,seed=seed).gen_samples(n),{\"color\":COLORS[5],\"marker\":MARKERS[5]}),\n    r\"DN${}_{1}$ LMS DS\": (lambda r,n,d: qp.DigitalNetB2(d,randomize=\"LMS_DS\",replications=r,seed=seed).gen_samples(n),{\"color\":COLORS[0],\"marker\":MARKERS[0]}),\n    r\"DN${}_{2}$ LMS DS\": (lambda r,n,d: qp.DigitalNetB2(d,randomize=\"LMS_DS\",replications=r,seed=seed,alpha=2).gen_samples(n),{\"color\":COLORS[0],\"marker\":MARKERS[1]}),\n    r\"DN${}_{3}$ LMS DS\": (lambda r,n,d: qp.DigitalNetB2(d,randomize=\"LMS_DS\",replications=r,seed=seed,alpha=3).gen_samples(n),{\"color\":COLORS[0],\"marker\":MARKERS[2]}),\n})\nm_max = 17\nr = 500\ntimes,muhathats,rmses = convergence_block(funcs,pointsets,r=r,m_max=m_max)\n</pre> funcs = OrderedDict({     r\"$f(x) = x e^x - 1$\": simple_func_1d,     r\"$f(x_1,x_2) = x_2 e^{x_1 x_2}/(e-2)-1$\": simple_func_2d,     r\"Oakley-O'Hagan, $d=2$\": oakley_ohagan_2d,     r\"G-Function, $d=%d$\"%G_cf.d: G_cf,     r\"Genz Oscillatory, $d=3$\": genz_oscillatory3_3d,     r\"Genz Corner-peak, $d=3$\": genz_cornerpeak2_3d,     #\"Ishigami\": ishigami,      # \"Sulfer\": sulfer_cf,     #\"Borehole\": borehole_cf,     # \"Webster\": webster_cf,     #r\"Oakley-O'Hagan with $d=15$\": oakley_ohagan15_cf,     # \"Cantilever Beam\": cbeam_cf,     # r\"Box Integral, $d=?$\": qp.BoxIntegral(qp.IIDStdUniform(4),s=-5), }) seed = 7 pointsets = OrderedDict({     \"IID\": (lambda r,n,d: qp.IIDStdUniform(d,replications=r,seed=seed).gen_samples(n),{\"color\":COLORS[2],\"marker\":MARKERS[2]}),     \"Lattice Shift\": (lambda r,n,d: qp.Lattice(d,replications=r,seed=seed).gen_samples(n),{\"color\":COLORS[5],\"marker\":MARKERS[5]}),     r\"DN${}_{1}$ LMS DS\": (lambda r,n,d: qp.DigitalNetB2(d,randomize=\"LMS_DS\",replications=r,seed=seed).gen_samples(n),{\"color\":COLORS[0],\"marker\":MARKERS[0]}),     r\"DN${}_{2}$ LMS DS\": (lambda r,n,d: qp.DigitalNetB2(d,randomize=\"LMS_DS\",replications=r,seed=seed,alpha=2).gen_samples(n),{\"color\":COLORS[0],\"marker\":MARKERS[1]}),     r\"DN${}_{3}$ LMS DS\": (lambda r,n,d: qp.DigitalNetB2(d,randomize=\"LMS_DS\",replications=r,seed=seed,alpha=3).gen_samples(n),{\"color\":COLORS[0],\"marker\":MARKERS[2]}), }) m_max = 17 r = 500 times,muhathats,rmses = convergence_block(funcs,pointsets,r=r,m_max=m_max) <pre>                 $f(x) = x e^x - 1$ IID                                : 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, \n                 $f(x) = x e^x - 1$ Lattice Shift                      : 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, \n                 $f(x) = x e^x - 1$ DN${}_{1}$ LMS DS                  : 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, \n                 $f(x) = x e^x - 1$ DN${}_{2}$ LMS DS                  : 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, \n                 $f(x) = x e^x - 1$ DN${}_{3}$ LMS DS                  : 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, \n\n$f(x_1,x_2) = x_2 e^{x_1 x_2}/(e-2)-1$ IID                                : 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, \n$f(x_1,x_2) = x_2 e^{x_1 x_2}/(e-2)-1$ Lattice Shift                      : 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, \n$f(x_1,x_2) = x_2 e^{x_1 x_2}/(e-2)-1$ DN${}_{1}$ LMS DS                  : 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, \n$f(x_1,x_2) = x_2 e^{x_1 x_2}/(e-2)-1$ DN${}_{2}$ LMS DS                  : 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, \n$f(x_1,x_2) = x_2 e^{x_1 x_2}/(e-2)-1$ DN${}_{3}$ LMS DS                  : 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, \n\n              Oakley-O'Hagan, $d=2$ IID                                : 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, \n              Oakley-O'Hagan, $d=2$ Lattice Shift                      : 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, \n              Oakley-O'Hagan, $d=2$ DN${}_{1}$ LMS DS                  : 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, \n              Oakley-O'Hagan, $d=2$ DN${}_{2}$ LMS DS                  : 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, \n              Oakley-O'Hagan, $d=2$ DN${}_{3}$ LMS DS                  : 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, \n\n                  G-Function, $d=3$ IID                                : 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, \n                  G-Function, $d=3$ Lattice Shift                      : 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, \n                  G-Function, $d=3$ DN${}_{1}$ LMS DS                  : 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, \n                  G-Function, $d=3$ DN${}_{2}$ LMS DS                  : 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, \n                  G-Function, $d=3$ DN${}_{3}$ LMS DS                  : 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, \n\n            Genz Oscillatory, $d=3$ IID                                : 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, \n            Genz Oscillatory, $d=3$ Lattice Shift                      : 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, \n            Genz Oscillatory, $d=3$ DN${}_{1}$ LMS DS                  : 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, \n            Genz Oscillatory, $d=3$ DN${}_{2}$ LMS DS                  : 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, \n            Genz Oscillatory, $d=3$ DN${}_{3}$ LMS DS                  : 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, \n\n            Genz Corner-peak, $d=3$ IID                                : 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, \n            Genz Corner-peak, $d=3$ Lattice Shift                      : 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, \n            Genz Corner-peak, $d=3$ DN${}_{1}$ LMS DS                  : 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, \n            Genz Corner-peak, $d=3$ DN${}_{2}$ LMS DS                  : 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, \n            Genz Corner-peak, $d=3$ DN${}_{3}$ LMS DS                  : 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, \n\n</pre> In\u00a0[43]: Copied! <pre>nrows = 2\nncols = 3 \nfig,ax = pyplot.subplots(nrows=nrows,ncols=ncols,figsize=(MW2,MW2/ncols*nrows),sharey=False,sharex=True)\nax = np.atleast_1d(ax)\nmvec = np.arange(0,m_max+1)\nnvec = 2**mvec\ncommonkwargs = {\"markersize\":3,\"linewidth\":1}#,\"markerfacecolor\":'black',\"markeredgecolor\":'white'}\nfor i,name in enumerate(funcs.keys()):\n    i1,i2 = i//ncols,i%ncols\n    avgi = 0\n    # avgi = rmse_noho[:,mvec[0]].mean()\n    for j,(pname,(generator,pltkwargs)) in enumerate(pointsets.items()):\n        ax[i1,i2].plot(nvec,rmses[name][pname][mvec],label=pname,**pltkwargs,**commonkwargs)\n        avgi += rmses[name][pname][mvec[0]]\n    avgi /= len(pointsets)\n    for p,sp in zip([-1/2,-1.,-3/2,-5/2,-7/2],[\"-1/2\",\"-1\",\"-3/2\",\"-5/2\",\"-7/2\"]):\n        n0 = 2**mvec[0]\n        kappa = avgi/(n0**p)\n        nf = 2**mvec[-1]\n        lf = kappa*nf**p\n        ax[i1,i2].plot([nvec[0],nf],[kappa*n0**p,lf],marker=\"none\",color=\"black\",alpha=.5,**commonkwargs)\n        if i2==2:\n            ax[i1,i2].text(2*nf,lf,r\"$\\mathcal{O}(n^{%s})$\"%sp)\n    ax[i1,i2].set_yscale('log',base=10)\n    ax[i1,i2].set_title(name)\n    ax[i1,i2].grid(True) \n    ax[i1,i2].set_xlim(nvec[0],nvec[-1])\n    ax[i1,i2].set_xscale('log',base=2)\n    ax[i1,i2].xaxis.set_tick_params(labelleft=True)\nfig.legend(*ax[0,0].get_legend_handles_labels(),frameon=False,loc=\"lower center\",bbox_to_anchor=(.5,-.075),ncol=5)\nfig.suptitle(\"RMSE vs number of points $n$\",fontsize=\"large\");\nfig.savefig(\"outputs/convergence.png\",format=\"png\",dpi=1024,transparent=True,bbox_inches=\"tight\")\n</pre> nrows = 2 ncols = 3  fig,ax = pyplot.subplots(nrows=nrows,ncols=ncols,figsize=(MW2,MW2/ncols*nrows),sharey=False,sharex=True) ax = np.atleast_1d(ax) mvec = np.arange(0,m_max+1) nvec = 2**mvec commonkwargs = {\"markersize\":3,\"linewidth\":1}#,\"markerfacecolor\":'black',\"markeredgecolor\":'white'} for i,name in enumerate(funcs.keys()):     i1,i2 = i//ncols,i%ncols     avgi = 0     # avgi = rmse_noho[:,mvec[0]].mean()     for j,(pname,(generator,pltkwargs)) in enumerate(pointsets.items()):         ax[i1,i2].plot(nvec,rmses[name][pname][mvec],label=pname,**pltkwargs,**commonkwargs)         avgi += rmses[name][pname][mvec[0]]     avgi /= len(pointsets)     for p,sp in zip([-1/2,-1.,-3/2,-5/2,-7/2],[\"-1/2\",\"-1\",\"-3/2\",\"-5/2\",\"-7/2\"]):         n0 = 2**mvec[0]         kappa = avgi/(n0**p)         nf = 2**mvec[-1]         lf = kappa*nf**p         ax[i1,i2].plot([nvec[0],nf],[kappa*n0**p,lf],marker=\"none\",color=\"black\",alpha=.5,**commonkwargs)         if i2==2:             ax[i1,i2].text(2*nf,lf,r\"$\\mathcal{O}(n^{%s})$\"%sp)     ax[i1,i2].set_yscale('log',base=10)     ax[i1,i2].set_title(name)     ax[i1,i2].grid(True)      ax[i1,i2].set_xlim(nvec[0],nvec[-1])     ax[i1,i2].set_xscale('log',base=2)     ax[i1,i2].xaxis.set_tick_params(labelleft=True) fig.legend(*ax[0,0].get_legend_handles_labels(),frameon=False,loc=\"lower center\",bbox_to_anchor=(.5,-.075),ncol=5) fig.suptitle(\"RMSE vs number of points $n$\",fontsize=\"large\"); fig.savefig(\"outputs/convergence.png\",format=\"png\",dpi=1024,transparent=True,bbox_inches=\"tight\") In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"demos/talk_paper_demos/ACMTOMS_Sorokin_2025/acm_toms_sorokin_2025/#quasi-monte-carlo-generators-randomizers-and-fast-transforms","title":"Quasi-Monte Carlo Generators, Randomizers, and Fast Transforms\u00b6","text":""},{"location":"demos/talk_paper_demos/ACMTOMS_Sorokin_2025/acm_toms_sorokin_2025/#setup","title":"Setup\u00b6","text":""},{"location":"demos/talk_paper_demos/ACMTOMS_Sorokin_2025/acm_toms_sorokin_2025/#snippets","title":"Snippets\u00b6","text":""},{"location":"demos/talk_paper_demos/ACMTOMS_Sorokin_2025/acm_toms_sorokin_2025/#point-sets","title":"Point Sets\u00b6","text":""},{"location":"demos/talk_paper_demos/ACMTOMS_Sorokin_2025/acm_toms_sorokin_2025/#kernel-methods","title":"Kernel Methods\u00b6","text":""},{"location":"demos/talk_paper_demos/ACMTOMS_Sorokin_2025/acm_toms_sorokin_2025/#lattice-fftbr-ifftbr","title":"Lattice + FFTBR + IFFTBR\u00b6","text":""},{"location":"demos/talk_paper_demos/ACMTOMS_Sorokin_2025/acm_toms_sorokin_2025/#digital-net-fwht","title":"Digital Net + FWHT\u00b6","text":""},{"location":"demos/talk_paper_demos/ACMTOMS_Sorokin_2025/acm_toms_sorokin_2025/#integration","title":"Integration\u00b6","text":""},{"location":"demos/talk_paper_demos/ACMTOMS_Sorokin_2025/acm_toms_sorokin_2025/#pointsets","title":"Pointsets\u00b6","text":""},{"location":"demos/talk_paper_demos/ACMTOMS_Sorokin_2025/acm_toms_sorokin_2025/#generation-time","title":"Generation time\u00b6","text":""},{"location":"demos/talk_paper_demos/ACMTOMS_Sorokin_2025/acm_toms_sorokin_2025/#convergence","title":"Convergence\u00b6","text":""},{"location":"demos/talk_paper_demos/ACMTOMS_Sorokin_2025/acm_toms_sorokin_2025/#test-functions","title":"Test Functions\u00b6","text":""},{"location":"demos/talk_paper_demos/ACMTOMS_Sorokin_2025/acm_toms_sorokin_2025/#simulation","title":"Simulation\u00b6","text":""},{"location":"demos/talk_paper_demos/Argonne_Talk_2023_May/Argonne_2023_Talk_Figures/","title":"2023 Argonne Talk","text":"In\u00a0[1]: Copied! <pre>import matplotlib.pyplot as plt\nimport numpy as np\nimport scipy.integrate as Nintegrate\nimport qmcpy as qp\nfrom sympy import * #so that we can do symbolic integration\nimport time  #timing routines\nimport warnings  #to suppress warnings when needed\nimport pickle  #write output to a file and load it back in\nfrom copy import deepcopy\nfrom matplotlib.patches import Polygon\n\nnp.set_printoptions(threshold=10)\n\nplt.rc('font', size=16)  #set defaults so that the plots are readable\nplt.rc('font', serif = \"Computer Modern Roman\")\nplt.rc('text', usetex=True)\nplt.rc('text.latex', preamble=r'\\usepackage{amsmath,amssymb,bm}')\nplt.rc('axes', titlesize=16)\nplt.rc('axes', labelsize=16)\nplt.rc('xtick', labelsize=16)\nplt.rc('ytick', labelsize=16)\nplt.rc('legend', fontsize=16)\nplt.rc('figure', titlesize=16)\n\n#a helpful plotting method to show increasing numbers of points\ndef plot_successive_points(distrib,ld_name,first_n=64,n_cols=1,\n                           pt_clr=['tab:blue', 'tab:orange', 'k', 'tab:cyan', 'tab:purple', 'tab:orange'],\n                           xlim=[0,1],ylim=[0,1],ntitle=True,titleText='',\n                          coordlist=[[0,1]],nrep=1):\n  fig,ax = plt.subplots(nrows=1,ncols=n_cols,figsize=(5*n_cols,5.5))\n  if n_cols==1: ax = [ax]\n  last_n = first_n*(2**n_cols)\n  if isinstance(distrib, list):\n    nrep = len(distrib)\n  else:\n    nrep = 1\n  if nrep==1: points = distrib.gen_samples(n=last_n)\n  ncoord = len(coordlist)\n  for i in range(n_cols):\n    n = first_n\n    nstart = 0\n    if nrep &gt; 1:\n      points = distrib[i].gen_samples(n=last_n)\n      ax[i].scatter(points[nstart:n,0],points[nstart:n,1],color=pt_clr[0])\n      ax[i].set_xlim(xlim); ax[i].set_xticks(xlim); ax[i].set_xlabel('$x_{i,'+str(coordlist[0][0]+1)+'}$')\n      ax[i].set_ylim(ylim); ax[i].set_yticks(ylim); ax[i].set_ylabel('$x_{i,'+str(coordlist[0][1]+1)+'}$')\n    elif ncoord &gt;1:\n      ax[i].scatter(points[nstart:n,coordlist[i][0]],points[nstart:n,coordlist[i][1]],color=pt_clr[0])\n      ax[i].set_xlim(xlim); ax[i].set_xticks(xlim); ax[i].set_xlabel('$x_{i,'+str(coordlist[i][0]+1)+'}$')\n      ax[i].set_ylim(ylim); ax[i].set_yticks(ylim); ax[i].set_ylabel('$x_{i,'+str(coordlist[i][1]+1)+'}$')\n    else:\n      for j in range(i+1):\n        n = first_n*(2**j)\n        ax[i].scatter(points[nstart:n,0],points[nstart:n,1],color=pt_clr[j])\n        nstart = n\n        if ntitle == True:\n          ax[i].set_title('n = %d'%n)\n        else:\n          ax[i].set_title(titleText)\n        ax[i].set_xlabel('$x_{i,'+str(coordlist[0][0]+1)+'}$')\n        ax[i].set_ylabel('$x_{i,'+str(coordlist[0][1]+1)+'}$')\n    ax[i].set_xlim(xlim); ax[i].set_xticks(xlim)\n    ax[i].set_ylim(ylim); ax[i].set_yticks(ylim)\n    ax[i].set_aspect((xlim[1]-xlim[0])/(ylim[1]-ylim[0]))\n  if ld_name != \"\":\n    fig.suptitle('%s Points'%ld_name, y=0.87)\n  return fig\n</pre> import matplotlib.pyplot as plt import numpy as np import scipy.integrate as Nintegrate import qmcpy as qp from sympy import * #so that we can do symbolic integration import time  #timing routines import warnings  #to suppress warnings when needed import pickle  #write output to a file and load it back in from copy import deepcopy from matplotlib.patches import Polygon  np.set_printoptions(threshold=10)  plt.rc('font', size=16)  #set defaults so that the plots are readable plt.rc('font', serif = \"Computer Modern Roman\") plt.rc('text', usetex=True) plt.rc('text.latex', preamble=r'\\usepackage{amsmath,amssymb,bm}') plt.rc('axes', titlesize=16) plt.rc('axes', labelsize=16) plt.rc('xtick', labelsize=16) plt.rc('ytick', labelsize=16) plt.rc('legend', fontsize=16) plt.rc('figure', titlesize=16)  #a helpful plotting method to show increasing numbers of points def plot_successive_points(distrib,ld_name,first_n=64,n_cols=1,                            pt_clr=['tab:blue', 'tab:orange', 'k', 'tab:cyan', 'tab:purple', 'tab:orange'],                            xlim=[0,1],ylim=[0,1],ntitle=True,titleText='',                           coordlist=[[0,1]],nrep=1):   fig,ax = plt.subplots(nrows=1,ncols=n_cols,figsize=(5*n_cols,5.5))   if n_cols==1: ax = [ax]   last_n = first_n*(2**n_cols)   if isinstance(distrib, list):     nrep = len(distrib)   else:     nrep = 1   if nrep==1: points = distrib.gen_samples(n=last_n)   ncoord = len(coordlist)   for i in range(n_cols):     n = first_n     nstart = 0     if nrep &gt; 1:       points = distrib[i].gen_samples(n=last_n)       ax[i].scatter(points[nstart:n,0],points[nstart:n,1],color=pt_clr[0])       ax[i].set_xlim(xlim); ax[i].set_xticks(xlim); ax[i].set_xlabel('$x_{i,'+str(coordlist[0][0]+1)+'}$')       ax[i].set_ylim(ylim); ax[i].set_yticks(ylim); ax[i].set_ylabel('$x_{i,'+str(coordlist[0][1]+1)+'}$')     elif ncoord &gt;1:       ax[i].scatter(points[nstart:n,coordlist[i][0]],points[nstart:n,coordlist[i][1]],color=pt_clr[0])       ax[i].set_xlim(xlim); ax[i].set_xticks(xlim); ax[i].set_xlabel('$x_{i,'+str(coordlist[i][0]+1)+'}$')       ax[i].set_ylim(ylim); ax[i].set_yticks(ylim); ax[i].set_ylabel('$x_{i,'+str(coordlist[i][1]+1)+'}$')     else:       for j in range(i+1):         n = first_n*(2**j)         ax[i].scatter(points[nstart:n,0],points[nstart:n,1],color=pt_clr[j])         nstart = n         if ntitle == True:           ax[i].set_title('n = %d'%n)         else:           ax[i].set_title(titleText)         ax[i].set_xlabel('$x_{i,'+str(coordlist[0][0]+1)+'}$')         ax[i].set_ylabel('$x_{i,'+str(coordlist[0][1]+1)+'}$')     ax[i].set_xlim(xlim); ax[i].set_xticks(xlim)     ax[i].set_ylim(ylim); ax[i].set_yticks(ylim)     ax[i].set_aspect((xlim[1]-xlim[0])/(ylim[1]-ylim[0]))   if ld_name != \"\":     fig.suptitle('%s Points'%ld_name, y=0.87)   return fig In\u00a0[2]: Copied! <pre>figpath = '' #this path sends the figures to the directory that you want\n</pre> figpath = '' #this path sends the figures to the directory that you want In\u00a0[3]: Copied! <pre>x = Symbol('x')                       #do some symbolic computation \nfsim = 30*x*exp(-5*x)                 #our test function\nintf = N(integrate(fsim, (x, 0, 1)))  #and its integral\nprint(intf)\n</pre> x = Symbol('x')                       #do some symbolic computation  fsim = 30*x*exp(-5*x)                 #our test function intf = N(integrate(fsim, (x, 0, 1)))  #and its integral print(intf) <pre>1.15148678160658\n</pre> In\u00a0[4]: Copied! <pre>f = lambdify(x,fsim)                     #make it a numeric function\nxnodes = np.array([0, 0.2, 0.6, 0.8, 1]) #choose nodes to apply the trapezoidla rule\nfnodes = f(xnodes)                       #function values at those nodes\nxplot = np.linspace(0, 1, 1000)          #nodes for polotting our function\nfig,ax = plt.subplots()\nxfpatch = np.append(np.transpose(np.array([xnodes, fnodes])),[[1,0]],axis=0)  #the trapezoidal rule approximation\nax.add_patch(Polygon(xfpatch,facecolor='tab:cyan'))  #plot the trapezoidal rule\nplt.plot(xplot, f(xplot), color='tab:blue')          #and the function\nplt.scatter(xnodes, fnodes, color='tab:orange')     #and the nodes\nax.set_xlabel('$x$')\nax.set_ylabel('$f(x)$')\nxlabels = []\nfor i in range(xnodes.size): xlabels.append(f'$x_{i}$')    \nax.set_xticks(xnodes,labels = xlabels) \nplt.tight_layout() \nfig.savefig(figpath+'traprule.eps',format='eps',bbox_inches='tight')\n</pre> f = lambdify(x,fsim)                     #make it a numeric function xnodes = np.array([0, 0.2, 0.6, 0.8, 1]) #choose nodes to apply the trapezoidla rule fnodes = f(xnodes)                       #function values at those nodes xplot = np.linspace(0, 1, 1000)          #nodes for polotting our function fig,ax = plt.subplots() xfpatch = np.append(np.transpose(np.array([xnodes, fnodes])),[[1,0]],axis=0)  #the trapezoidal rule approximation ax.add_patch(Polygon(xfpatch,facecolor='tab:cyan'))  #plot the trapezoidal rule plt.plot(xplot, f(xplot), color='tab:blue')          #and the function plt.scatter(xnodes, fnodes, color='tab:orange')     #and the nodes ax.set_xlabel('$x$') ax.set_ylabel('$f(x)$') xlabels = [] for i in range(xnodes.size): xlabels.append(f'$x_{i}$')     ax.set_xticks(xnodes,labels = xlabels)  plt.tight_layout()  fig.savefig(figpath+'traprule.eps',format='eps',bbox_inches='tight') In\u00a0[5]: Copied! <pre>xdiff = np.diff(xnodes)\ntrap_rule = np.sum((fnodes[:-1]*xdiff+fnodes[1:]*xdiff)/2)\nerr_trap = intf - trap_rule \nprint(err_trap)\n</pre> xdiff = np.diff(xnodes) trap_rule = np.sum((fnodes[:-1]*xdiff+fnodes[1:]*xdiff)/2) err_trap = intf - trap_rule  print(err_trap) <pre>0.112324710648342\n</pre> In\u00a0[6]: Copied! <pre>discrepancy = np.max(xdiff**2)/8                          #quality of the nodes\nf2diffsim = diff(fsim,x,2)\nprint(f2diffsim)\nf2diff = lambdify(x,f2diffsim) \nfig,ax = plt.subplots()\nplt.plot(xplot, f2diff(xplot), color='tab:blue')\nax.set_xlabel('$x$')\nax.set_ylabel(\"$f''(x)$\");\nvariation = N(integrate(abs(f2diffsim), (x, 0, 1)))   #roughness of the integrand\nNvariation = Nintegrate.quad(lambda x: abs(f2diff(x)),0,1,epsabs = 1e-14)  #checking whether the symbolic integration is correct\nprint(abs(Nvariation[0]-variation) &lt;= 1e-10)          #do the symbolic and numerical integration methods agree\nconfound = err_trap/(discrepancy*variation)           #how unlucky the integrand is, should be between -1 and +1\nprint([confound,discrepancy, variation])              #the trapezoidal rule error is the product of these three numbers\nfig.savefig(figpath+'traprulef2.eps',format='eps',bbox_inches='tight')\n</pre> discrepancy = np.max(xdiff**2)/8                          #quality of the nodes f2diffsim = diff(fsim,x,2) print(f2diffsim) f2diff = lambdify(x,f2diffsim)  fig,ax = plt.subplots() plt.plot(xplot, f2diff(xplot), color='tab:blue') ax.set_xlabel('$x$') ax.set_ylabel(\"$f''(x)$\"); variation = N(integrate(abs(f2diffsim), (x, 0, 1)))   #roughness of the integrand Nvariation = Nintegrate.quad(lambda x: abs(f2diff(x)),0,1,epsabs = 1e-14)  #checking whether the symbolic integration is correct print(abs(Nvariation[0]-variation) &lt;= 1e-10)          #do the symbolic and numerical integration methods agree confound = err_trap/(discrepancy*variation)           #how unlucky the integrand is, should be between -1 and +1 print([confound,discrepancy, variation])              #the trapezoidal rule error is the product of these three numbers fig.savefig(figpath+'traprulef2.eps',format='eps',bbox_inches='tight') <pre>150*(5*x - 2)*exp(-5*x)\nTrue\n[0.150522653770521, 0.019999999999999997, 37.3115633543065]\n</pre> In\u00a0[7]: Copied! <pre>d = 15 #dimension\nn = 64 #number of points\nnrand = 4 #number of randomizations\ncoordlist=[[0,1],[1,2],[4,10],[13,14]]\nld = qp.Lattice(d) #define the generator\nxpts = ld.gen_samples(n) #generate points\nprint(xpts)\nfig = plot_successive_points(ld,'',first_n=4*n,n_cols=1,ntitle=False,titleText=r'Lattice $\\boldsymbol{X} \\sim \\mathcal{U}[0,1]^d$')\nplt.tight_layout() \nfig.savefig(figpath+'latticeptssingle.eps',format='eps',bbox_inches='tight')\nfig = plot_successive_points(ld,'Lattice',first_n=n,n_cols=4)\nfig.savefig(figpath+'latticeptsseq.eps',format='eps',bbox_inches='tight')\nldrand = ld.spawn(nrand) #randomize\nfig = plot_successive_points(ldrand,'Randomized Lattice',first_n=4*n,n_cols=nrand)\nfig.savefig(figpath+'latticeptsrand.eps',format='eps',bbox_inches='tight')\nfig = plot_successive_points(ld,'Projections of Lattice',first_n=4*n,n_cols=nrand,coordlist=coordlist)\nfig.savefig(figpath+'latticeptsproj.eps',format='eps',bbox_inches='tight')\n</pre> d = 15 #dimension n = 64 #number of points nrand = 4 #number of randomizations coordlist=[[0,1],[1,2],[4,10],[13,14]] ld = qp.Lattice(d) #define the generator xpts = ld.gen_samples(n) #generate points print(xpts) fig = plot_successive_points(ld,'',first_n=4*n,n_cols=1,ntitle=False,titleText=r'Lattice $\\boldsymbol{X} \\sim \\mathcal{U}[0,1]^d$') plt.tight_layout()  fig.savefig(figpath+'latticeptssingle.eps',format='eps',bbox_inches='tight') fig = plot_successive_points(ld,'Lattice',first_n=n,n_cols=4) fig.savefig(figpath+'latticeptsseq.eps',format='eps',bbox_inches='tight') ldrand = ld.spawn(nrand) #randomize fig = plot_successive_points(ldrand,'Randomized Lattice',first_n=4*n,n_cols=nrand) fig.savefig(figpath+'latticeptsrand.eps',format='eps',bbox_inches='tight') fig = plot_successive_points(ld,'Projections of Lattice',first_n=4*n,n_cols=nrand,coordlist=coordlist) fig.savefig(figpath+'latticeptsproj.eps',format='eps',bbox_inches='tight') <pre>[[0.15289077 0.1012989  0.19275833 ... 0.04742184 0.83174917 0.38357771]\n [0.65289077 0.6012989  0.69275833 ... 0.54742184 0.33174917 0.88357771]\n [0.40289077 0.8512989  0.94275833 ... 0.29742184 0.08174917 0.63357771]\n ...\n [0.88726577 0.1794239  0.89588333 ... 0.09429684 0.25362417 0.05545271]\n [0.63726577 0.4294239  0.14588333 ... 0.84429684 0.00362417 0.80545271]\n [0.13726577 0.9294239  0.64588333 ... 0.34429684 0.50362417 0.30545271]]\n</pre> In\u00a0[10]: Copied! <pre>ld = qp.Sobol(d,seed=11) #define the generator\nxpts_Sobol = ld.gen_samples(n) #generate points\nfig = plot_successive_points(ld,'',first_n=4*n,n_cols=1,ntitle=False,titleText=r\"Sobol' $\\boldsymbol{X} \\sim \\mathcal{U}[0,1]^d$\")\nplt.tight_layout() \nfig.savefig(figpath+'sobolptssingle.eps',format='eps',bbox_inches='tight')\nfig = plot_successive_points(ld,'Sobol\\'',first_n=n,n_cols=4)\nfig.savefig(figpath+'sobolptsseq.eps',format='eps',bbox_inches='tight')\nldrand = ld.spawn(nrand) #randomize\nfig = plot_successive_points(ldrand,'Randomized Sobol\\'',first_n=4*n,n_cols=nrand)\nfig.savefig(figpath+'sobolptsrand.eps',format='eps',bbox_inches='tight')\nfig = plot_successive_points(ld,'Projections of Sobol\\'',first_n=4*n,n_cols=nrand,coordlist=coordlist)\nfig.savefig(figpath+'sobolptsproj.eps',format='eps',bbox_inches='tight')\n</pre> ld = qp.Sobol(d,seed=11) #define the generator xpts_Sobol = ld.gen_samples(n) #generate points fig = plot_successive_points(ld,'',first_n=4*n,n_cols=1,ntitle=False,titleText=r\"Sobol' $\\boldsymbol{X} \\sim \\mathcal{U}[0,1]^d$\") plt.tight_layout()  fig.savefig(figpath+'sobolptssingle.eps',format='eps',bbox_inches='tight') fig = plot_successive_points(ld,'Sobol\\'',first_n=n,n_cols=4) fig.savefig(figpath+'sobolptsseq.eps',format='eps',bbox_inches='tight') ldrand = ld.spawn(nrand) #randomize fig = plot_successive_points(ldrand,'Randomized Sobol\\'',first_n=4*n,n_cols=nrand) fig.savefig(figpath+'sobolptsrand.eps',format='eps',bbox_inches='tight') fig = plot_successive_points(ld,'Projections of Sobol\\'',first_n=4*n,n_cols=nrand,coordlist=coordlist) fig.savefig(figpath+'sobolptsproj.eps',format='eps',bbox_inches='tight') In\u00a0[11]: Copied! <pre>ld = qp.Halton(d) #define the generator\nxpts_Halton = ld.gen_samples(n) #generate points\nfig = plot_successive_points(ld,'',first_n=4*n,n_cols=1,ntitle=False,titleText=r'Halton $\\boldsymbol{X} \\sim \\mathcal{U}[0,1]^d$')\nplt.tight_layout() \nfig.savefig(figpath+'haltonptssingle.eps',format='eps',bbox_inches='tight')\nfig = plot_successive_points(ld,'Halton',first_n=n,n_cols=4)\nfig.savefig(figpath+'haltonptsseq.eps',format='eps',bbox_inches='tight')\nldrand = ld.spawn(nrand) #randomize\nfig = plot_successive_points(ldrand,'Randomized Halton',first_n=4*n,n_cols=nrand)\nfig.savefig(figpath+'haltonptsrand.eps',format='eps',bbox_inches='tight')\nfig = plot_successive_points(ld,'Projections of Halton',first_n=4*n,n_cols=nrand,coordlist=coordlist)\nfig.savefig(figpath+'haltonptsproj.eps',format='eps',bbox_inches='tight')\n</pre> ld = qp.Halton(d) #define the generator xpts_Halton = ld.gen_samples(n) #generate points fig = plot_successive_points(ld,'',first_n=4*n,n_cols=1,ntitle=False,titleText=r'Halton $\\boldsymbol{X} \\sim \\mathcal{U}[0,1]^d$') plt.tight_layout()  fig.savefig(figpath+'haltonptssingle.eps',format='eps',bbox_inches='tight') fig = plot_successive_points(ld,'Halton',first_n=n,n_cols=4) fig.savefig(figpath+'haltonptsseq.eps',format='eps',bbox_inches='tight') ldrand = ld.spawn(nrand) #randomize fig = plot_successive_points(ldrand,'Randomized Halton',first_n=4*n,n_cols=nrand) fig.savefig(figpath+'haltonptsrand.eps',format='eps',bbox_inches='tight') fig = plot_successive_points(ld,'Projections of Halton',first_n=4*n,n_cols=nrand,coordlist=coordlist) fig.savefig(figpath+'haltonptsproj.eps',format='eps',bbox_inches='tight') In\u00a0[12]: Copied! <pre>iid = qp.IIDStdUniform(d) #define the generator\nxpts = ld.gen_samples(n) #generate points\nxpts\nfig = plot_successive_points(iid,'',first_n=4*n,n_cols=1,ntitle=False,titleText=r'IID $\\boldsymbol{X}\\sim\\mathcal{U}[0,1]^d$')\nplt.tight_layout() \nfig.savefig(figpath+'iidptssingle.eps',format='eps',bbox_inches='tight')\nfig = plot_successive_points(iid,'IID',first_n=n,n_cols=4)\nfig.savefig(figpath+'iidptsseq.eps',format='eps',bbox_inches='tight')\niidrand = iid.spawn(nrand) #randomize\nfig = plot_successive_points(iidrand,'IID',first_n=4*n,n_cols=nrand)\nfig.savefig(figpath+'iidptsrand.eps',format='eps',bbox_inches='tight')\nfig = plot_successive_points(iid,'Projections of IID',first_n=4*n,n_cols=nrand,coordlist=coordlist)\nfig.savefig(figpath+'iidptsproj.eps',format='eps',bbox_inches='tight')\n</pre> iid = qp.IIDStdUniform(d) #define the generator xpts = ld.gen_samples(n) #generate points xpts fig = plot_successive_points(iid,'',first_n=4*n,n_cols=1,ntitle=False,titleText=r'IID $\\boldsymbol{X}\\sim\\mathcal{U}[0,1]^d$') plt.tight_layout()  fig.savefig(figpath+'iidptssingle.eps',format='eps',bbox_inches='tight') fig = plot_successive_points(iid,'IID',first_n=n,n_cols=4) fig.savefig(figpath+'iidptsseq.eps',format='eps',bbox_inches='tight') iidrand = iid.spawn(nrand) #randomize fig = plot_successive_points(iidrand,'IID',first_n=4*n,n_cols=nrand) fig.savefig(figpath+'iidptsrand.eps',format='eps',bbox_inches='tight') fig = plot_successive_points(iid,'Projections of IID',first_n=4*n,n_cols=nrand,coordlist=coordlist) fig.savefig(figpath+'iidptsproj.eps',format='eps',bbox_inches='tight') In\u00a0[13]: Copied! <pre>with open(figpath+'iid_ld.pkl','rb') as myfile: tol_vec,n_tol,ii_iid,ld_time,ld_n,iid_time,iid_n,best_solution_i = pickle.load(myfile)\nprint(best_solution_i)\nfig,ax = plt.subplots(nrows=1,ncols=2,figsize=(13,5.5))\nax[0].scatter(tol_vec[0:n_tol],ld_time[0:n_tol],color='tab:blue'); \nax[0].plot(tol_vec[0:n_tol],[(ld_time[0]*tol_vec[0])/tol_vec[jj] for jj in range(n_tol)],color='tab:blue')\nax[0].scatter(tol_vec[ii_iid:n_tol],iid_time[ii_iid:n_tol],color='tab:orange'); \nax[0].plot(tol_vec[ii_iid:n_tol],[(iid_time[ii_iid]*(tol_vec[ii_iid]**2))/(tol_vec[jj]**2) for jj in range(ii_iid,n_tol)],color='tab:orange')\nax[0].set_ylim([0.001,1000]); ax[0].set_ylabel('Time (s)')\nax[1].scatter(tol_vec[0:n_tol],ld_n[0:n_tol],color='tab:blue'); \nax[1].plot(tol_vec[0:n_tol],[(ld_n[0]*tol_vec[0])/tol_vec[jj] for jj in range(n_tol)],color='tab:blue')\nax[1].scatter(tol_vec[ii_iid:n_tol],iid_n[ii_iid:n_tol],color='tab:orange'); \nax[1].plot(tol_vec[ii_iid:n_tol],[(iid_n[ii_iid]*(tol_vec[ii_iid]**2))/(tol_vec[jj]**2) for jj in range(ii_iid,n_tol)],color='tab:orange')\nax[1].set_ylim([1e2,1e8]); ax[1].set_ylabel('n')\nfor ii in range(2):\n  ax[ii].set_xlim([0.007,100]); ax[ii].set_xlabel('Tolerance, '+r'$\\varepsilon$')\n  ax[ii].set_xscale('log'); ax[ii].set_yscale('log')\n  ax[ii].legend(['Lattice',r'$\\mathcal{O}(\\varepsilon^{-1})$','IID',r'$\\mathcal{O}(\\varepsilon^{-2})$'],frameon=False)\n  ax[ii].set_aspect(0.65)\nax[0].set_yticks([1, 60, 3600], labels = ['1 sec', '1 min', '1 hr'])    \nfig.savefig(figpath+'iidldbeam.eps',format='eps',bbox_inches='tight')\n</pre> with open(figpath+'iid_ld.pkl','rb') as myfile: tol_vec,n_tol,ii_iid,ld_time,ld_n,iid_time,iid_n,best_solution_i = pickle.load(myfile) print(best_solution_i) fig,ax = plt.subplots(nrows=1,ncols=2,figsize=(13,5.5)) ax[0].scatter(tol_vec[0:n_tol],ld_time[0:n_tol],color='tab:blue');  ax[0].plot(tol_vec[0:n_tol],[(ld_time[0]*tol_vec[0])/tol_vec[jj] for jj in range(n_tol)],color='tab:blue') ax[0].scatter(tol_vec[ii_iid:n_tol],iid_time[ii_iid:n_tol],color='tab:orange');  ax[0].plot(tol_vec[ii_iid:n_tol],[(iid_time[ii_iid]*(tol_vec[ii_iid]**2))/(tol_vec[jj]**2) for jj in range(ii_iid,n_tol)],color='tab:orange') ax[0].set_ylim([0.001,1000]); ax[0].set_ylabel('Time (s)') ax[1].scatter(tol_vec[0:n_tol],ld_n[0:n_tol],color='tab:blue');  ax[1].plot(tol_vec[0:n_tol],[(ld_n[0]*tol_vec[0])/tol_vec[jj] for jj in range(n_tol)],color='tab:blue') ax[1].scatter(tol_vec[ii_iid:n_tol],iid_n[ii_iid:n_tol],color='tab:orange');  ax[1].plot(tol_vec[ii_iid:n_tol],[(iid_n[ii_iid]*(tol_vec[ii_iid]**2))/(tol_vec[jj]**2) for jj in range(ii_iid,n_tol)],color='tab:orange') ax[1].set_ylim([1e2,1e8]); ax[1].set_ylabel('n') for ii in range(2):   ax[ii].set_xlim([0.007,100]); ax[ii].set_xlabel('Tolerance, '+r'$\\varepsilon$')   ax[ii].set_xscale('log'); ax[ii].set_yscale('log')   ax[ii].legend(['Lattice',r'$\\mathcal{O}(\\varepsilon^{-1})$','IID',r'$\\mathcal{O}(\\varepsilon^{-2})$'],frameon=False)   ax[ii].set_aspect(0.65) ax[0].set_yticks([1, 60, 3600], labels = ['1 sec', '1 min', '1 hr'])     fig.savefig(figpath+'iidldbeam.eps',format='eps',bbox_inches='tight') <pre>[1037.12106673]\n</pre> In\u00a0[14]: Copied! <pre>with open(figpath+'ld_parallel.pkl','rb') as myfile: tol_vec,n_tol,ld_time,ld_n,ld_p_time,ld_p_n,best_solution = pickle.load(myfile)\nprint(best_solution_i)\nfig,ax = plt.subplots(nrows=1,ncols=2,figsize=(11,4))\nax[0].scatter(tol_vec[0:n_tol],ld_time[0:n_tol],color='tab:blue'); \nax[0].plot(tol_vec[0:n_tol],[(ld_time[0]*tol_vec[0])/tol_vec[jj] for jj in range(n_tol)],color='tab:blue')\nax[0].scatter(tol_vec[0:n_tol],ld_p_time[0:n_tol],color='tab:orange',marker = '+',s=100,linewidths=2); \n#ax[0].plot(tol_vec[0:n_tol],[(ld_p_time[0]*tol_vec[0])/tol_vec[jj] for jj in range(n_tol)],color='tab:orange')\nax[0].set_ylim([0.05,500]); ax[0].set_ylabel('Time')\nax[1].scatter(tol_vec[0:n_tol],ld_n[0:n_tol],color='tab:blue'); \nax[1].plot(tol_vec[0:n_tol],[(ld_n[0]*tol_vec[0])/tol_vec[jj] for jj in range(n_tol)],color='tab:blue')\nax[1].scatter(tol_vec[0:n_tol],ld_p_n[0:n_tol],color='tab:orange',marker = '+',s=100,linewidths=2); \n#ax[1].plot(tol_vec[0:n_tol],[(ld_p_n[0]*tol_vec[0])/tol_vec[jj] for jj in range(n_tol)],color='tab:orange')\nax[1].set_ylim([10,1e5]); ax[1].set_ylabel('n')\nfor ii in range(2):\n  ax[ii].set_xlim([0.007,4]); ax[ii].set_xlabel('Tolerance, '+r'$\\varepsilon$')\n  ax[ii].set_xscale('log'); ax[ii].set_yscale('log')\n  ax[ii].legend(['Lattice',r'$\\mathcal{O}(\\varepsilon^{-1})$','Lattice Parallel',r'$\\mathcal{O}(\\varepsilon^{-1})$'],frameon=False)\n  ax[ii].set_aspect(0.6)\nax[0].set_yticks([1, 60], labels = ['1 sec', '1 min'])\nfig.savefig(figpath+'ldparallelbeam.eps',format='eps',bbox_inches='tight')\n</pre> with open(figpath+'ld_parallel.pkl','rb') as myfile: tol_vec,n_tol,ld_time,ld_n,ld_p_time,ld_p_n,best_solution = pickle.load(myfile) print(best_solution_i) fig,ax = plt.subplots(nrows=1,ncols=2,figsize=(11,4)) ax[0].scatter(tol_vec[0:n_tol],ld_time[0:n_tol],color='tab:blue');  ax[0].plot(tol_vec[0:n_tol],[(ld_time[0]*tol_vec[0])/tol_vec[jj] for jj in range(n_tol)],color='tab:blue') ax[0].scatter(tol_vec[0:n_tol],ld_p_time[0:n_tol],color='tab:orange',marker = '+',s=100,linewidths=2);  #ax[0].plot(tol_vec[0:n_tol],[(ld_p_time[0]*tol_vec[0])/tol_vec[jj] for jj in range(n_tol)],color='tab:orange') ax[0].set_ylim([0.05,500]); ax[0].set_ylabel('Time') ax[1].scatter(tol_vec[0:n_tol],ld_n[0:n_tol],color='tab:blue');  ax[1].plot(tol_vec[0:n_tol],[(ld_n[0]*tol_vec[0])/tol_vec[jj] for jj in range(n_tol)],color='tab:blue') ax[1].scatter(tol_vec[0:n_tol],ld_p_n[0:n_tol],color='tab:orange',marker = '+',s=100,linewidths=2);  #ax[1].plot(tol_vec[0:n_tol],[(ld_p_n[0]*tol_vec[0])/tol_vec[jj] for jj in range(n_tol)],color='tab:orange') ax[1].set_ylim([10,1e5]); ax[1].set_ylabel('n') for ii in range(2):   ax[ii].set_xlim([0.007,4]); ax[ii].set_xlabel('Tolerance, '+r'$\\varepsilon$')   ax[ii].set_xscale('log'); ax[ii].set_yscale('log')   ax[ii].legend(['Lattice',r'$\\mathcal{O}(\\varepsilon^{-1})$','Lattice Parallel',r'$\\mathcal{O}(\\varepsilon^{-1})$'],frameon=False)   ax[ii].set_aspect(0.6) ax[0].set_yticks([1, 60], labels = ['1 sec', '1 min']) fig.savefig(figpath+'ldparallelbeam.eps',format='eps',bbox_inches='tight') <pre>[1037.12106673]\n</pre> In\u00a0[15]: Copied! <pre>fig,ax = plt.subplots(figsize=(6,3))\nax.plot(best_solution,'-')\nax.set_xlim([0,len(best_solution)-1]); ax.set_xlabel('Position')\nax.set_ylim([1050,-50]);  ax.set_ylabel('Mean Deflection');\nax.set_aspect(0.02)\nfig.suptitle('Cantilevered Beam')\nfig.savefig(figpath+'cantileveredbeamwords.eps',format='eps',bbox_inches='tight')\n</pre> fig,ax = plt.subplots(figsize=(6,3)) ax.plot(best_solution,'-') ax.set_xlim([0,len(best_solution)-1]); ax.set_xlabel('Position') ax.set_ylim([1050,-50]);  ax.set_ylabel('Mean Deflection'); ax.set_aspect(0.02) fig.suptitle('Cantilevered Beam') fig.savefig(figpath+'cantileveredbeamwords.eps',format='eps',bbox_inches='tight') <p>Below is long-running code, that we rarely wish to run</p> In\u00a0[16]: Copied! <pre>import pandas as pd\nfrom sklearn.model_selection import train_test_split\ndf = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/haberman/haberman.data',header=None)\ndf.columns = ['Age','1900 Year','Axillary Nodes','Survival Status']\ndf.loc[df['Survival Status']==2,'Survival Status'] = 0\nx,y = df[['Age','1900 Year','Axillary Nodes']],df['Survival Status']\nxt,xv,yt,yv = train_test_split(x,y,test_size=.33,random_state=7)\n</pre> import pandas as pd from sklearn.model_selection import train_test_split df = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/haberman/haberman.data',header=None) df.columns = ['Age','1900 Year','Axillary Nodes','Survival Status'] df.loc[df['Survival Status']==2,'Survival Status'] = 0 x,y = df[['Age','1900 Year','Axillary Nodes']],df['Survival Status'] xt,xv,yt,yv = train_test_split(x,y,test_size=.33,random_state=7) In\u00a0[17]: Copied! <pre>print(df.head(),'\\n')\nprint(df[['Age','1900 Year','Axillary Nodes']].describe(),'\\n')\nprint(df['Survival Status'].astype(str).describe())\nprint('\\ntrain samples: %d test samples: %d\\n'%(len(xt),len(xv)))\nprint('train positives %d   train negatives: %d'%(np.sum(yt==1),np.sum(yt==0)))\nprint(' test positives %d    test negatives: %d'%(np.sum(yv==1),np.sum(yv==0)))\nxt.head()\n</pre> print(df.head(),'\\n') print(df[['Age','1900 Year','Axillary Nodes']].describe(),'\\n') print(df['Survival Status'].astype(str).describe()) print('\\ntrain samples: %d test samples: %d\\n'%(len(xt),len(xv))) print('train positives %d   train negatives: %d'%(np.sum(yt==1),np.sum(yt==0))) print(' test positives %d    test negatives: %d'%(np.sum(yv==1),np.sum(yv==0))) xt.head() <pre>   Age  1900 Year  Axillary Nodes  Survival Status\n0   30         64               1                1\n1   30         62               3                1\n2   30         65               0                1\n3   31         59               2                1\n4   31         65               4                1 \n\n              Age   1900 Year  Axillary Nodes\ncount  306.000000  306.000000      306.000000\nmean    52.457516   62.852941        4.026144\nstd     10.803452    3.249405        7.189654\nmin     30.000000   58.000000        0.000000\n25%     44.000000   60.000000        0.000000\n50%     52.000000   63.000000        1.000000\n75%     60.750000   65.750000        4.000000\nmax     83.000000   69.000000       52.000000 \n\ncount     306\nunique      2\ntop         1\nfreq      225\nName: Survival Status, dtype: object\n\ntrain samples: 205 test samples: 101\n\ntrain positives 151   train negatives: 54\n test positives 74    test negatives: 27\n</pre> Out[17]: Age 1900 Year Axillary Nodes 46 41 58 0 199 57 64 1 115 49 64 10 128 50 61 0 249 63 63 0 In\u00a0[18]: Copied! <pre>blr = qp.BayesianLRCoeffs(\n    sampler = qp.DigitalNetB2(4,seed=1),\n    feature_array = xt, # np.ndarray of shape (n,d-1)\n    response_vector = yt, # np.ndarray of shape (n,)\n    prior_mean = 0, # normal prior mean = (0,0,...,0)\n    prior_covariance = 5) # normal prior covariance = I\nqmc_sc = qp.CubQMCNetG(blr,\n    abs_tol = .1,\n    rel_tol = .5,\n    error_fun = \"BOTH\",\n    n_limit=2**18)\nblr_coefs,blr_data = qmc_sc.integrate()\nprint(blr_data)\n# Data (Data)\n#     solution        [ 0.262 -0.043 -0.226 -1.203]\n#     comb_bound_low  [ 0.185 -0.067 -0.306 -1.656]\n#     comb_bound_high [ 0.347 -0.035 -0.163 -0.75 ]\n#     comb_bound_diff [0.162 0.031 0.143 0.906]\n#     comb_flags      [ True  True  True False]\n#     n_total         2^(18)\n#     n               [[  1024   1024   1024 262144]\n#                      [  1024   1024   1024 262144]]\n#     time_integrate  3.120\n</pre> blr = qp.BayesianLRCoeffs(     sampler = qp.DigitalNetB2(4,seed=1),     feature_array = xt, # np.ndarray of shape (n,d-1)     response_vector = yt, # np.ndarray of shape (n,)     prior_mean = 0, # normal prior mean = (0,0,...,0)     prior_covariance = 5) # normal prior covariance = I qmc_sc = qp.CubQMCNetG(blr,     abs_tol = .1,     rel_tol = .5,     error_fun = \"BOTH\",     n_limit=2**18) blr_coefs,blr_data = qmc_sc.integrate() print(blr_data) # Data (Data) #     solution        [ 0.262 -0.043 -0.226 -1.203] #     comb_bound_low  [ 0.185 -0.067 -0.306 -1.656] #     comb_bound_high [ 0.347 -0.035 -0.163 -0.75 ] #     comb_bound_diff [0.162 0.031 0.143 0.906] #     comb_flags      [ True  True  True False] #     n_total         2^(18) #     n               [[  1024   1024   1024 262144] #                      [  1024   1024   1024 262144]] #     time_integrate  3.120 <pre>Data (Data)\n    solution        [ 0.262 -0.043 -0.226 -1.203]\n    comb_bound_low  [ 0.185 -0.067 -0.306 -1.656]\n    comb_bound_high [ 0.347 -0.035 -0.163 -0.75 ]\n    comb_bound_diff [0.162 0.031 0.143 0.906]\n    comb_flags      [ True  True  True False]\n    n_total         2^(18)\n    n               [[  1024   1024   1024 262144]\n                     [  1024   1024   1024 262144]]\n    time_integrate  4.105\nCubQMCNetG (AbstractStoppingCriterion)\n    abs_tol         0.100\n    rel_tol         2^(-1)\n    n_init          2^(10)\n    n_limit         2^(18)\nBayesianLRCoeffs (AbstractIntegrand)\nGaussian (AbstractTrueMeasure)\n    mean            0\n    covariance      5\n    decomp_type     PCA\nDigitalNetB2 (AbstractLDDiscreteDistribution)\n    d               2^(2)\n    replications    1\n    randomize       LMS_DS\n    gen_mats_source joe_kuo.6.21201.txt\n    order           NATURAL\n    t               63\n    alpha           1\n    n_limit         2^(32)\n    entropy         1\n</pre> <pre>/Users/alegresor/Desktop/QMCSoftware/qmcpy/stopping_criterion/abstract_cub_qmc_ld_g.py:215: MaxSamplesWarning: \n                Already generated 262144 samples.\n                Trying to generate 262144 new samples would exceeds n_limit = 262144.\n                No more samples will be generated.\n                Note that error tolerances may not be satisfied. \n  warnings.warn(warning_s, MaxSamplesWarning)\n</pre> In\u00a0[19]: Copied! <pre>from sklearn.linear_model import LogisticRegression\ndef metrics(y,yhat):\n    y,yhat = np.array(y),np.array(yhat)\n    tp = np.sum((y==1)*(yhat==1))\n    tn = np.sum((y==0)*(yhat==0))\n    fp = np.sum((y==0)*(yhat==1))\n    fn = np.sum((y==1)*(yhat==0))\n    accuracy = (tp+tn)/(len(y))\n    precision = tp/(tp+fp)\n    recall = tp/(tp+fn)\n    return [accuracy,precision,recall]\n\nresults = pd.DataFrame({name:[] for name in ['method','Age','1900 Year','Axillary Nodes','Intercept','Accuracy','Precision','Recall']})\nfor i,l1_ratio in enumerate([0,.5,1]):\n    lr = LogisticRegression(random_state=7,penalty=\"elasticnet\",solver='saga',l1_ratio=l1_ratio).fit(xt,yt)\n    results.loc[i] = [r'Elastic-Net \\lambda=%.1f'%l1_ratio]+lr.coef_.squeeze().tolist()+[lr.intercept_.item()]+metrics(yv,lr.predict(xv))\n\nblr_predict = lambda x: 1/(1+np.exp(-np.array(x)@blr_coefs[:-1]-blr_coefs[-1]))&gt;=.5\nblr_train_accuracy = np.mean(blr_predict(xt)==yt)\nblr_test_accuracy = np.mean(blr_predict(xv)==yv)\nresults.loc[len(results)] = ['Bayesian']+blr_coefs.squeeze().tolist()+metrics(yv,blr_predict(xv))\n\nimport warnings\nwarnings.simplefilter('ignore',FutureWarning)\nresults.set_index('method',inplace=True)\nprint(results.head())\n#root: results.to_latex(root+'lr_table.tex',formatters={'%s'%tt:lambda v:'%.1f'%(100*v) for tt in ['accuracy','precision','recall']},float_format=\"%.2e\")\n</pre> from sklearn.linear_model import LogisticRegression def metrics(y,yhat):     y,yhat = np.array(y),np.array(yhat)     tp = np.sum((y==1)*(yhat==1))     tn = np.sum((y==0)*(yhat==0))     fp = np.sum((y==0)*(yhat==1))     fn = np.sum((y==1)*(yhat==0))     accuracy = (tp+tn)/(len(y))     precision = tp/(tp+fp)     recall = tp/(tp+fn)     return [accuracy,precision,recall]  results = pd.DataFrame({name:[] for name in ['method','Age','1900 Year','Axillary Nodes','Intercept','Accuracy','Precision','Recall']}) for i,l1_ratio in enumerate([0,.5,1]):     lr = LogisticRegression(random_state=7,penalty=\"elasticnet\",solver='saga',l1_ratio=l1_ratio).fit(xt,yt)     results.loc[i] = [r'Elastic-Net \\lambda=%.1f'%l1_ratio]+lr.coef_.squeeze().tolist()+[lr.intercept_.item()]+metrics(yv,lr.predict(xv))  blr_predict = lambda x: 1/(1+np.exp(-np.array(x)@blr_coefs[:-1]-blr_coefs[-1]))&gt;=.5 blr_train_accuracy = np.mean(blr_predict(xt)==yt) blr_test_accuracy = np.mean(blr_predict(xv)==yv) results.loc[len(results)] = ['Bayesian']+blr_coefs.squeeze().tolist()+metrics(yv,blr_predict(xv))  import warnings warnings.simplefilter('ignore',FutureWarning) results.set_index('method',inplace=True) print(results.head()) #root: results.to_latex(root+'lr_table.tex',formatters={'%s'%tt:lambda v:'%.1f'%(100*v) for tt in ['accuracy','precision','recall']},float_format=\"%.2e\") <pre>                              Age  1900 Year  Axillary Nodes  Intercept  \\\nmethod                                                                    \nElastic-Net \\lambda=0.0 -0.012279   0.034401       -0.115153   0.001990   \nElastic-Net \\lambda=0.5 -0.012041   0.034170       -0.114770   0.002025   \nElastic-Net \\lambda=1.0 -0.011803   0.033940       -0.114387   0.002061   \nBayesian                 0.262086  -0.043253       -0.225631  -1.202777   \n\n                         Accuracy  Precision    Recall  \nmethod                                                  \nElastic-Net \\lambda=0.0  0.742574   0.766667  0.932432  \nElastic-Net \\lambda=0.5  0.742574   0.766667  0.932432  \nElastic-Net \\lambda=1.0  0.742574   0.766667  0.932432  \nBayesian                 0.732673   0.737374  0.986486  \n</pre> In\u00a0[53]: Copied! <pre>option_parameters = {\n            'option': \"ASIAN\",\n            'volatility' : .2,\n            'start_price' : 100,\n            'strike_price' : 100,\n            'interest_rate' : 0.05}\nm_steps = 7\nnsteps = np.zeros(m_steps,dtype=int)\npca_price = np.zeros(m_steps)\nchol_price = np.zeros(m_steps)\npca_time = np.zeros(m_steps)\nchol_time = np.zeros(m_steps)\nabstol=1e-4\nfor level in range(m_steps):\n    nsteps[level] = 2**level\n    aco = qp.FinancialOption(qp.Sobol(nsteps[level]), **option_parameters, decomp_type = \"PCA\")\n    approx_solution, data = qp.CubQMCSobolG(aco, abs_tol=abstol).integrate()\n    pca_price[level] = approx_solution\n    pca_time[level] = data.time_integrate\n    print(\"Asian Option true value (%d time steps): %.5f (to within %.0e) in %.2f seconds using PCA\"%(nsteps[level], approx_solution, abstol, data.time_integrate))\n    aco = qp.FinancialOption(qp.Sobol(nsteps[level]), **option_parameters, decomp_type = \"Cholesky\")\n    approx_solution, data = qp.CubQMCSobolG(aco, abs_tol=abstol).integrate()\n    chol_price[level] = approx_solution\n    chol_time[level] = data.time_integrate\n    print(\"Asian Option true value (%d time steps): %.5f (to within %.0e) in %.2f seconds using Cholesky\"%(nsteps[level], approx_solution, abstol, data.time_integrate))\n</pre> option_parameters = {             'option': \"ASIAN\",             'volatility' : .2,             'start_price' : 100,             'strike_price' : 100,             'interest_rate' : 0.05} m_steps = 7 nsteps = np.zeros(m_steps,dtype=int) pca_price = np.zeros(m_steps) chol_price = np.zeros(m_steps) pca_time = np.zeros(m_steps) chol_time = np.zeros(m_steps) abstol=1e-4 for level in range(m_steps):     nsteps[level] = 2**level     aco = qp.FinancialOption(qp.Sobol(nsteps[level]), **option_parameters, decomp_type = \"PCA\")     approx_solution, data = qp.CubQMCSobolG(aco, abs_tol=abstol).integrate()     pca_price[level] = approx_solution     pca_time[level] = data.time_integrate     print(\"Asian Option true value (%d time steps): %.5f (to within %.0e) in %.2f seconds using PCA\"%(nsteps[level], approx_solution, abstol, data.time_integrate))     aco = qp.FinancialOption(qp.Sobol(nsteps[level]), **option_parameters, decomp_type = \"Cholesky\")     approx_solution, data = qp.CubQMCSobolG(aco, abs_tol=abstol).integrate()     chol_price[level] = approx_solution     chol_time[level] = data.time_integrate     print(\"Asian Option true value (%d time steps): %.5f (to within %.0e) in %.2f seconds using Cholesky\"%(nsteps[level], approx_solution, abstol, data.time_integrate)) <pre>Asian Option true value (1 time steps): 5.22530 (to within 1e-04) in 0.09 seconds using PCA\nAsian Option true value (1 time steps): 5.22529 (to within 1e-04) in 0.08 seconds using Cholesky\nAsian Option true value (2 time steps): 5.63592 (to within 1e-04) in 0.11 seconds using PCA\nAsian Option true value (2 time steps): 5.63593 (to within 1e-04) in 0.24 seconds using Cholesky\nAsian Option true value (4 time steps): 5.73171 (to within 1e-04) in 0.44 seconds using PCA\nAsian Option true value (4 time steps): 5.73169 (to within 1e-04) in 0.87 seconds using Cholesky\nAsian Option true value (8 time steps): 5.75525 (to within 1e-04) in 0.64 seconds using PCA\nAsian Option true value (8 time steps): 5.75517 (to within 1e-04) in 6.24 seconds using Cholesky\nAsian Option true value (16 time steps): 5.76114 (to within 1e-04) in 1.28 seconds using PCA\nAsian Option true value (16 time steps): 5.76139 (to within 1e-04) in 25.96 seconds using Cholesky\nAsian Option true value (32 time steps): 5.76260 (to within 1e-04) in 2.81 seconds using PCA\nAsian Option true value (32 time steps): 5.76245 (to within 1e-04) in 147.31 seconds using Cholesky\nAsian Option true value (64 time steps): 5.76296 (to within 1e-04) in 4.69 seconds using PCA\nAsian Option true value (64 time steps): 5.76281 (to within 1e-04) in 442.84 seconds using Cholesky\n</pre> In\u00a0[54]: Copied! <pre>fig,ax = plt.subplots(nrows=1,ncols=2,figsize=(13,4.5))\nax[0].scatter(nsteps,pca_price,color='tab:blue'); \nax[0].scatter(nsteps,chol_price,color='tab:orange'); \nax[0].set_xscale('log')\nax[1].scatter(nsteps,pca_time,color='tab:blue'); \nax[1].scatter(nsteps,chol_time,color='tab:orange'); \nax[1].set_xscale('log')\nax[1].set_yscale('log')\nax[0].set_xlabel(r'\\# time steps, $d$');\nax[1].set_xlabel(r'\\# time steps, $d$');\nax[0].set_ylabel(r'Option Price');\nax[1].set_ylabel(r'Computation Time (sec)');\nax[0].legend(['Eigenvalue','Cholesky'],frameon=False)\nax[1].legend(['Eigenvalue','Cholesky'],frameon=False)\nfig.savefig(figpath+'optionpricing.eps',format='eps',bbox_inches='tight')\n</pre> fig,ax = plt.subplots(nrows=1,ncols=2,figsize=(13,4.5)) ax[0].scatter(nsteps,pca_price,color='tab:blue');  ax[0].scatter(nsteps,chol_price,color='tab:orange');  ax[0].set_xscale('log') ax[1].scatter(nsteps,pca_time,color='tab:blue');  ax[1].scatter(nsteps,chol_time,color='tab:orange');  ax[1].set_xscale('log') ax[1].set_yscale('log') ax[0].set_xlabel(r'\\# time steps, $d$'); ax[1].set_xlabel(r'\\# time steps, $d$'); ax[0].set_ylabel(r'Option Price'); ax[1].set_ylabel(r'Computation Time (sec)'); ax[0].legend(['Eigenvalue','Cholesky'],frameon=False) ax[1].legend(['Eigenvalue','Cholesky'],frameon=False) fig.savefig(figpath+'optionpricing.eps',format='eps',bbox_inches='tight') In\u00a0[55]: Copied! <pre>qp.util.stop_notebook()\n</pre> qp.util.stop_notebook() In\u00a0[3]: Copied! <pre>import umbridge #this is the connector\n!docker run --name muqbp -d -it -p 4243:4243 linusseelinger/benchmark-muq-beam-propagation:latest #get beam example\nd = 3 #dimension of the randomness\nlb = 1 #lower bound on randomness\nub = 1.2 #upper bound on randomness\numbridge_config = {\"d\": d}\nmodel = umbridge.HTTPModel('http://localhost:4243','forward') #this is the original model\noutindex = -1 #choose last element of the vector of beam deflections\nmodeli = deepcopy(model) #and construct a model for just that deflection\nmodeli.get_output_sizes = lambda *args : [1]\nmodeli.get_output_sizes()\nmodeli.__call__ = lambda *args,**kwargs: [[model.__call__(*args,**kwargs)[0][outindex]]]\n</pre> import umbridge #this is the connector !docker run --name muqbp -d -it -p 4243:4243 linusseelinger/benchmark-muq-beam-propagation:latest #get beam example d = 3 #dimension of the randomness lb = 1 #lower bound on randomness ub = 1.2 #upper bound on randomness umbridge_config = {\"d\": d} model = umbridge.HTTPModel('http://localhost:4243','forward') #this is the original model outindex = -1 #choose last element of the vector of beam deflections modeli = deepcopy(model) #and construct a model for just that deflection modeli.get_output_sizes = lambda *args : [1] modeli.get_output_sizes() modeli.__call__ = lambda *args,**kwargs: [[model.__call__(*args,**kwargs)[0][outindex]]] <pre>docker: Error response from daemon: Conflict. The container name \"/muqbp\" is already in use by container \"d30a451e4f61fd8d33dd7bb63d9338183a4015fe9dbbb755727cf7f3596fecee\". You have to remove (or rename) that container to be able to reuse that name.\nSee 'docker run --help'.\n</pre> In\u00a0[\u00a0]: Copied! <pre>ld = qp.Uniform(qp.Lattice(d,seed=7),lower_bound=lb,upper_bound=ub) #lattice points for this problem\nld_integ = qp.UMBridgeWrapper(ld,modeli,umbridge_config,parallel=False) #integrand\niid = qp.Uniform(qp.IIDStdUniform(d),lower_bound=lb,upper_bound=ub) #iid points for this problem\niid_integ = qp.UMBridgeWrapper(iid,modeli,umbridge_config,parallel=False) #integrand\ntol = 0.01  #smallest tolerance\n\nn_tol = 14  #number of different tolerances\nii_iid = 9  #make this larger to reduce the time required by not running all cases for IID\ntol_vec = [tol*(2**ii) for ii in range(n_tol)]  #initialize vector of tolerances\nld_time = [0]*n_tol; ld_n = [0]*n_tol  #low discrepancy time and number of function values\niid_time = [0]*n_tol; iid_n = [0]*n_tol  #IID time and number of function values\nprint(f'\\nCantilever Beam\\n')\nprint('iteration ', end = '')\nfor ii in range(n_tol):\n  solution, data = qp.CubQMCLatticeG(ld_integ, abs_tol = tol_vec[ii]).integrate()\n  if ii == 0:\n    best_solution_i = solution\n  ld_time[ii] = data.time_integrate\n  ld_n[ii] = data.n_total\n  if ii &gt;= ii_iid:\n    solution, data = qp.CubMCG(iid_integ, abs_tol = tol_vec[ii]).integrate()\n    iid_time[ii] = data.time_integrate\n    iid_n[ii] = data.n_total\n  print(ii, end = ' ')\nwith open(figpath+'iid_ld.pkl','wb') as myfile:pickle.dump([tol_vec,n_tol,ii_iid,ld_time,ld_n,iid_time,iid_n,best_solution_i],myfile)\n</pre> ld = qp.Uniform(qp.Lattice(d,seed=7),lower_bound=lb,upper_bound=ub) #lattice points for this problem ld_integ = qp.UMBridgeWrapper(ld,modeli,umbridge_config,parallel=False) #integrand iid = qp.Uniform(qp.IIDStdUniform(d),lower_bound=lb,upper_bound=ub) #iid points for this problem iid_integ = qp.UMBridgeWrapper(iid,modeli,umbridge_config,parallel=False) #integrand tol = 0.01  #smallest tolerance  n_tol = 14  #number of different tolerances ii_iid = 9  #make this larger to reduce the time required by not running all cases for IID tol_vec = [tol*(2**ii) for ii in range(n_tol)]  #initialize vector of tolerances ld_time = [0]*n_tol; ld_n = [0]*n_tol  #low discrepancy time and number of function values iid_time = [0]*n_tol; iid_n = [0]*n_tol  #IID time and number of function values print(f'\\nCantilever Beam\\n') print('iteration ', end = '') for ii in range(n_tol):   solution, data = qp.CubQMCLatticeG(ld_integ, abs_tol = tol_vec[ii]).integrate()   if ii == 0:     best_solution_i = solution   ld_time[ii] = data.time_integrate   ld_n[ii] = data.n_total   if ii &gt;= ii_iid:     solution, data = qp.CubMCG(iid_integ, abs_tol = tol_vec[ii]).integrate()     iid_time[ii] = data.time_integrate     iid_n[ii] = data.n_total   print(ii, end = ' ') with open(figpath+'iid_ld.pkl','wb') as myfile:pickle.dump([tol_vec,n_tol,ii_iid,ld_time,ld_n,iid_time,iid_n,best_solution_i],myfile) In\u00a0[\u00a0]: Copied! <pre>ld_integ = qp.UMBridgeWrapper(ld,model,umbridge_config,parallel=False) #integrand\nld_integ_p = qp.UMBridgeWrapper(ld,model,umbridge_config,parallel=8) #integrand with parallel processing\n\ntol = 0.01\nn_tol = 9  #number of different tolerances\ntol_vec = [tol*(2**ii) for ii in range(n_tol)]  #initialize vector of tolerances\nld_time = [0]*n_tol; ld_n = [0]*n_tol  #low discrepancy time and number of function values\nld_p_time = [0]*n_tol; ld_p_n = [0]*n_tol  #low discrepancy time and number of function values with parallel\nprint(f'\\nCantilever Beam\\n')\nprint('iteration ', end = '')\nfor ii in range(n_tol):\n  solution, data = qp.CubQMCLatticeG(ld_integ, abs_tol = tol_vec[ii]).integrate()\n  if ii == 0:\n    best_solution = solution\n  ld_time[ii] = data.time_integrate\n  ld_n[ii] = data.n_total\n  solution, data = qp.CubQMCLatticeG(ld_integ_p, abs_tol = tol_vec[ii]).integrate()\n  ld_p_time[ii] = data.time_integrate\n  ld_p_n[ii] = data.n_total\n  print(ii, end = ' ') \nwith open(figpath+'ld_parallel.pkl','wb') as myfile:pickle.dump([tol_vec,n_tol,ld_time,ld_n,ld_p_time,ld_p_n,best_solution],myfile)\n</pre> ld_integ = qp.UMBridgeWrapper(ld,model,umbridge_config,parallel=False) #integrand ld_integ_p = qp.UMBridgeWrapper(ld,model,umbridge_config,parallel=8) #integrand with parallel processing  tol = 0.01 n_tol = 9  #number of different tolerances tol_vec = [tol*(2**ii) for ii in range(n_tol)]  #initialize vector of tolerances ld_time = [0]*n_tol; ld_n = [0]*n_tol  #low discrepancy time and number of function values ld_p_time = [0]*n_tol; ld_p_n = [0]*n_tol  #low discrepancy time and number of function values with parallel print(f'\\nCantilever Beam\\n') print('iteration ', end = '') for ii in range(n_tol):   solution, data = qp.CubQMCLatticeG(ld_integ, abs_tol = tol_vec[ii]).integrate()   if ii == 0:     best_solution = solution   ld_time[ii] = data.time_integrate   ld_n[ii] = data.n_total   solution, data = qp.CubQMCLatticeG(ld_integ_p, abs_tol = tol_vec[ii]).integrate()   ld_p_time[ii] = data.time_integrate   ld_p_n[ii] = data.n_total   print(ii, end = ' ')  with open(figpath+'ld_parallel.pkl','wb') as myfile:pickle.dump([tol_vec,n_tol,ld_time,ld_n,ld_p_time,ld_p_n,best_solution],myfile) In\u00a0[\u00a0]: Copied! <pre>!docker rm -f muqbp #shut down docker image\n</pre> !docker rm -f muqbp #shut down docker image"},{"location":"demos/talk_paper_demos/Argonne_Talk_2023_May/Argonne_2023_Talk_Figures/#argonne-lab-talk","title":"Argonne Lab Talk\u00b6","text":"<p>Computations and figures for a seminar at Argonne Lab</p> <p>presented on Thursday, May 18, 2023, slides here</p> <p>To run this notebook you need to pip install</p> <pre><code>\u2022 qmcpy\n\u2022 sympy\n\u2022 docker</code></pre>"},{"location":"demos/talk_paper_demos/Argonne_Talk_2023_May/Argonne_2023_Talk_Figures/#import-the-necessary-packages-and-set-up-plotting-routines","title":"Import the necessary packages and set up plotting routines\u00b6","text":""},{"location":"demos/talk_paper_demos/Argonne_Talk_2023_May/Argonne_2023_Talk_Figures/#set-the-path-to-save-the-figures-here","title":"Set the path to save the figures here\u00b6","text":""},{"location":"demos/talk_paper_demos/Argonne_Talk_2023_May/Argonne_2023_Talk_Figures/#trapezoidal-rule-and-its-error","title":"Trapezoidal rule and its error\u00b6","text":""},{"location":"demos/talk_paper_demos/Argonne_Talk_2023_May/Argonne_2023_Talk_Figures/#define-test-function","title":"Define test function\u00b6","text":""},{"location":"demos/talk_paper_demos/Argonne_Talk_2023_May/Argonne_2023_Talk_Figures/#plot-test-function-and-trapezoidal-rule","title":"Plot test function and trapezoidal rule\u00b6","text":""},{"location":"demos/talk_paper_demos/Argonne_Talk_2023_May/Argonne_2023_Talk_Figures/#compute-trapezoidal-rule-and-its-error","title":"Compute trapezoidal rule and its error\u00b6","text":""},{"location":"demos/talk_paper_demos/Argonne_Talk_2023_May/Argonne_2023_Talk_Figures/#compute-elements-of-the-trio-identity","title":"Compute elements of the trio identity\u00b6","text":""},{"location":"demos/talk_paper_demos/Argonne_Talk_2023_May/Argonne_2023_Talk_Figures/#plots-of-iid-and-low-discrepancy-ld-points","title":"Plots of IID and Low Discrepancy (LD) Points\u00b6","text":"<p>These plots show how LD points fill space better than IID points</p>"},{"location":"demos/talk_paper_demos/Argonne_Talk_2023_May/Argonne_2023_Talk_Figures/#lattice-points-first","title":"Lattice points first\u00b6","text":""},{"location":"demos/talk_paper_demos/Argonne_Talk_2023_May/Argonne_2023_Talk_Figures/#next-sobol-points","title":"Next Sobol' points\u00b6","text":""},{"location":"demos/talk_paper_demos/Argonne_Talk_2023_May/Argonne_2023_Talk_Figures/#also-halton-points","title":"Also Halton points\u00b6","text":""},{"location":"demos/talk_paper_demos/Argonne_Talk_2023_May/Argonne_2023_Talk_Figures/#compare-to-iid","title":"Compare to IID\u00b6","text":"<p>Note that there are more gaps and clusters</p>"},{"location":"demos/talk_paper_demos/Argonne_Talk_2023_May/Argonne_2023_Talk_Figures/#beam-example-figures","title":"Beam Example Figures\u00b6","text":"<p>Using computations done below</p>"},{"location":"demos/talk_paper_demos/Argonne_Talk_2023_May/Argonne_2023_Talk_Figures/#plot-the-time-and-sample-size-required-to-solve-for-the-deflection-of-the-end-point-using-iid-and-low-discrepancy","title":"Plot the time and sample size required to solve for the deflection of the end point using IID and low discrepancy\u00b6","text":""},{"location":"demos/talk_paper_demos/Argonne_Talk_2023_May/Argonne_2023_Talk_Figures/#plot-the-time-and-sample-size-required-to-solve-for-the-deflection-of-the-whole-beam-using-low-discrepancy-with-and-without-parallel","title":"Plot the time and sample size required to solve for the deflection of the whole beam using low discrepancy with and without parallel\u00b6","text":""},{"location":"demos/talk_paper_demos/Argonne_Talk_2023_May/Argonne_2023_Talk_Figures/#plot-of-beam-solution","title":"Plot of beam solution\u00b6","text":""},{"location":"demos/talk_paper_demos/Argonne_Talk_2023_May/Argonne_2023_Talk_Figures/#bayesian-logistic-regression","title":"Bayesian Logistic Regression\u00b6","text":"<p>This is an example where the solution is ratio of two integrals</p>"},{"location":"demos/talk_paper_demos/Argonne_Talk_2023_May/Argonne_2023_Talk_Figures/#asian-option","title":"Asian Option\u00b6","text":""},{"location":"demos/talk_paper_demos/Argonne_Talk_2023_May/Argonne_2023_Talk_Figures/#comparing-pca-and-cholesky-decompositions","title":"Comparing PCA and Cholesky Decompositions\u00b6","text":"<p>Note that the Cholesky decomposition might not even meet the error tolerance</p>"},{"location":"demos/talk_paper_demos/Argonne_Talk_2023_May/Argonne_2023_Talk_Figures/#beam-example-computations","title":"Beam Example Computations\u00b6","text":""},{"location":"demos/talk_paper_demos/Argonne_Talk_2023_May/Argonne_2023_Talk_Figures/#set-up-the-problem-using-a-docker-container-to-solve-the-ode","title":"Set up the problem using a docker container to solve the ODE\u00b6","text":"<p>To run this, you need to be running the docker application, https://www.docker.com/products/docker-desktop/</p>"},{"location":"demos/talk_paper_demos/Argonne_Talk_2023_May/Argonne_2023_Talk_Figures/#first-we-compute-the-time-required-to-solve-for-the-deflection-of-the-end-point-using-iid-and-low-discrepancy","title":"First we compute the time required to solve for the deflection of the end point using IID and low discrepancy\u00b6","text":""},{"location":"demos/talk_paper_demos/Argonne_Talk_2023_May/Argonne_2023_Talk_Figures/#next-we-compute-the-time-required-to-solve-for-the-deflection-of-the-whole-beam-using-low-discrepancy-with-and-without-parallel","title":"Next, we compute the time required to solve for the deflection of the whole beam using low discrepancy with and without parallel\u00b6","text":""},{"location":"demos/talk_paper_demos/Argonne_Talk_2023_May/Argonne_2023_Talk_Figures/#shut-down-docker","title":"Shut down docker\u00b6","text":""},{"location":"demos/talk_paper_demos/JOSS2026/joss2026/","title":"2026 JOSS Paper","text":"In\u00a0[1]: Copied! <pre>try:\n    import qmcpy \nexcept ImportError:\n    !pip install -q qmcpy\n\ntry:\n    import seaborn as sns\nexcept ImportError:\n    !pip install -q seaborn\n</pre> try:     import qmcpy  except ImportError:     !pip install -q qmcpy  try:     import seaborn as sns except ImportError:     !pip install -q seaborn In\u00a0[2]: Copied! <pre>import os\nimport matplotlib\nfrom matplotlib import pyplot \nfrom tueplots import bundles\nimport seaborn as sns\n\nmatplotlib.rcParams['figure.dpi'] = 256\npyplot.rcParams.update(bundles.probnum2025())\nCOLORS = ['xkcd:purple', 'xkcd:green', 'xkcd:blue', 'xkcd:orange']\nMARKERS = [\"*\",\"^\",\"o\",\"s\"]\nMARKERSIZE = 5\nOUTDIR = \"JOSS2026.outputs\"\nFIGWIDTH = 500/72\nassert os.path.isdir(OUTDIR)\n</pre> import os import matplotlib from matplotlib import pyplot  from tueplots import bundles import seaborn as sns  matplotlib.rcParams['figure.dpi'] = 256 pyplot.rcParams.update(bundles.probnum2025()) COLORS = ['xkcd:purple', 'xkcd:green', 'xkcd:blue', 'xkcd:orange'] MARKERS = [\"*\",\"^\",\"o\",\"s\"] MARKERSIZE = 5 OUTDIR = \"JOSS2026.outputs\" FIGWIDTH = 500/72 assert os.path.isdir(OUTDIR) In\u00a0[3]: Copied! <pre>import qmcpy as qp \nqp.__version__\n</pre> import qmcpy as qp  qp.__version__ Out[3]: <pre>'2.1'</pre> In\u00a0[4]: Copied! <pre>dnb2 = qp.DigitalNetB2(dimension = 2, randomize = \"LMS_DS\")\nx = dnb2(2**7)\nprint(x.shape)\n</pre> dnb2 = qp.DigitalNetB2(dimension = 2, randomize = \"LMS_DS\") x = dnb2(2**7) print(x.shape)  <pre>(128, 2)\n</pre> In\u00a0[5]: Copied! <pre>dnb2 = qp.DigitalNetB2(dimension = 2, replications=5, randomize = \"LMS_DS\")\nx = dnb2(2**7)\nprint(x.shape)\n</pre> dnb2 = qp.DigitalNetB2(dimension = 2, replications=5, randomize = \"LMS_DS\") x = dnb2(2**7) print(x.shape) <pre>(5, 128, 2)\n</pre> In\u00a0[6]: Copied! <pre>dnb2 = qp.DigitalNetB2( # special case of a digital net\n    dimension = 12, # monthly monitoring \n    seed = 7, # for reproducibility\n)\nasian_option = qp.FinancialOption(\n    sampler = dnb2 ,\n    option = \"ASIAN\",\n    call_put = \"CALL\",\n    asian_mean = \"GEOMETRIC\",\n    asian_mean_quadrature_rule = \"RIGHT\", \n    volatility = 0.5, \n    start_price = 30., \n    strike_price = 35., \n    interest_rate = 0.01, # 1% interest rate\n    t_final = 1, # 1 year \n)\nqmc_algorithm = qp.CubQMCNetG(\n    integrand = asian_option,\n    abs_tol = 1e-3,\n    rel_tol = 0,\n    n_init = 2**8,\n)\napprox_value,data = qmc_algorithm.integrate()\nprint(approx_value)\nprint(data)\n</pre> dnb2 = qp.DigitalNetB2( # special case of a digital net     dimension = 12, # monthly monitoring      seed = 7, # for reproducibility ) asian_option = qp.FinancialOption(     sampler = dnb2 ,     option = \"ASIAN\",     call_put = \"CALL\",     asian_mean = \"GEOMETRIC\",     asian_mean_quadrature_rule = \"RIGHT\",      volatility = 0.5,      start_price = 30.,      strike_price = 35.,      interest_rate = 0.01, # 1% interest rate     t_final = 1, # 1 year  ) qmc_algorithm = qp.CubQMCNetG(     integrand = asian_option,     abs_tol = 1e-3,     rel_tol = 0,     n_init = 2**8, ) approx_value,data = qmc_algorithm.integrate() print(approx_value) print(data) <pre>1.7665677027586677\nData (Data)\n    solution        1.767\n    comb_bound_low  1.766\n    comb_bound_high 1.767\n    comb_bound_diff 0.002\n    comb_flags      1\n    n_total         2^(15)\n    n               2^(15)\n    time_integrate  0.015\nCubQMCNetG (AbstractStoppingCriterion)\n    abs_tol         0.001\n    rel_tol         0\n    n_init          2^(8)\n    n_limit         2^(35)\nFinancialOption (AbstractIntegrand)\n    option          ASIAN\n    call_put        CALL\n    volatility      2^(-1)\n    start_price     30\n    strike_price    35\n    interest_rate   0.010\n    t_final         1\n    asian_mean      GEOMETRIC\nGeometricBrownianMotion (AbstractTrueMeasure)\n    time_vec        [0.083 0.167 0.25  ... 0.833 0.917 1.   ]\n    drift           0.010\n    diffusion       2^(-2)\n    mean_gbm        [30.025 30.05  30.075 ... 30.251 30.276 30.302]\n    covariance_gbm  [[ 18.978  18.994  19.01  ...  19.121  19.137  19.153]\n                     [ 18.994  38.42   38.452 ...  38.677  38.709  38.742]\n                     [ 19.01   38.452  58.336 ...  58.677  58.726  58.775]\n                     ...\n                     [ 19.121  38.677  58.677 ... 211.965 212.141 212.318]\n                     [ 19.137  38.709  58.726 ... 212.141 236.085 236.282]\n                     [ 19.153  38.742  58.775 ... 212.318 236.282 260.787]]\n    decomp_type     PCA\nDigitalNetB2 (AbstractLDDiscreteDistribution)\n    d               12\n    replications    1\n    randomize       LMS DS\n    gen_mats_source joe_kuo.6.21201.txt\n    order           RADICAL INVERSE\n    t               63\n    alpha           1\n    n_limit         2^(32)\n    entropy         7\n</pre> In\u00a0[7]: Copied! <pre>import numpy as np\nimport scipy.stats\n\ndef borehole(t): # t.shape == (..., n, d) for n samples in d dimensions\n    \"\"\" https://www.sfu.ca/~ssurjano/borehole.html \"\"\"\n    rw, r, Tu, Hu, Tl, Hl, L, Kw = np.moveaxis(t, -1, 0)\n    numer = 2 * np.pi * Tu * (Hu-Hl) / np.log(r / rw)\n    denom =  1 + 2 * L * Tu / (np.log(r / rw) * rw**2 * Kw) + Tu / Tl\n    y = numer / denom\n    return y\nindep_distribs = [\n    scipy.stats.norm(loc=0.10, scale=0.0161812),\n    scipy.stats.lognorm(scale=np.exp(7.71), s=1.0056),\n    scipy.stats.uniform(loc=63070, scale=115600-63070),\n    scipy.stats.uniform(loc=990, scale=1110-990),\n    scipy.stats.uniform(loc=63.1, scale=116-63.1),\n    scipy.stats.uniform(loc=700, scale=820-700),\n    scipy.stats.uniform(loc=1120, scale=1680-1120),\n    scipy.stats.uniform(loc=9855, scale=12045-9855),\n]\ndiscrete_distrib = qp.Lattice(dimension = 8, seed = 11)\ntrue_measure = qp.SciPyWrapper(discrete_distrib, indep_distribs)\nintegrand = qp.CustomFun(true_measure, borehole)\nqmc_algorithm = qp.CubQMCBayesLatticeG(\n    integrand = integrand,\n    ptransform = \"BAKER\", # periodization transform \n    abs_tol = 1e-3,\n    rel_tol = 1e-5,\n    error_fun = \"BOTH\" # abs and rel tol satisfied, default is \"EITHER\",\n)\napprox_value,data = qmc_algorithm.integrate()\nprint(approx_value)\nprint(data)\n</pre> import numpy as np import scipy.stats  def borehole(t): # t.shape == (..., n, d) for n samples in d dimensions     \"\"\" https://www.sfu.ca/~ssurjano/borehole.html \"\"\"     rw, r, Tu, Hu, Tl, Hl, L, Kw = np.moveaxis(t, -1, 0)     numer = 2 * np.pi * Tu * (Hu-Hl) / np.log(r / rw)     denom =  1 + 2 * L * Tu / (np.log(r / rw) * rw**2 * Kw) + Tu / Tl     y = numer / denom     return y indep_distribs = [     scipy.stats.norm(loc=0.10, scale=0.0161812),     scipy.stats.lognorm(scale=np.exp(7.71), s=1.0056),     scipy.stats.uniform(loc=63070, scale=115600-63070),     scipy.stats.uniform(loc=990, scale=1110-990),     scipy.stats.uniform(loc=63.1, scale=116-63.1),     scipy.stats.uniform(loc=700, scale=820-700),     scipy.stats.uniform(loc=1120, scale=1680-1120),     scipy.stats.uniform(loc=9855, scale=12045-9855), ] discrete_distrib = qp.Lattice(dimension = 8, seed = 11) true_measure = qp.SciPyWrapper(discrete_distrib, indep_distribs) integrand = qp.CustomFun(true_measure, borehole) qmc_algorithm = qp.CubQMCBayesLatticeG(     integrand = integrand,     ptransform = \"BAKER\", # periodization transform      abs_tol = 1e-3,     rel_tol = 1e-5,     error_fun = \"BOTH\" # abs and rel tol satisfied, default is \"EITHER\", ) approx_value,data = qmc_algorithm.integrate() print(approx_value) print(data) <pre>73.73882925995098\nData (Data)\n    solution        73.739\n    comb_bound_low  73.739\n    comb_bound_high 73.739\n    comb_bound_diff 2.76e-04\n    comb_flags      1\n    n_total         2^(16)\n    n               2^(16)\n    time_integrate  0.275\nCubQMCBayesLatticeG (AbstractStoppingCriterion)\n    abs_tol         0.001\n    rel_tol         1.00e-05\n    n_init          2^(8)\n    n_limit         2^(22)\n    order           2^(1)\nCustomFun (AbstractIntegrand)\nSciPyWrapper (AbstractTrueMeasure)\nLattice (AbstractLDDiscreteDistribution)\n    d               2^(3)\n    replications    1\n    randomize       SHIFT\n    gen_vec_source  kuo.lattice-33002-1024-1048576.9125.txt\n    order           RADICAL INVERSE\n    n_limit         2^(20)\n    entropy         11\n</pre> In\u00a0[8]: Copied! <pre>ns = np.array([0,2**5,2**6,2**7])\nassert len(COLORS)&gt;=(len(ns)-1) and len(MARKERS)&gt;=(len(ns)-1)\nnmax = ns.max() \ndata = [\n    (\"IID\",qp.IIDStdUniform(2,seed=7)(nmax)),\n    (\"LD Lattice Seq.\",qp.Lattice(2,seed=7)(nmax)),\n    (\"LD Digital Seq.\",qp.DigitalNetB2(2,seed=7,randomize=\"NUS\")(nmax)),\n    (\"LD Halton Seq.\",qp.Halton(2,seed=7,randomize=\"LMS_DS\")(nmax)),\n]\nnrows = 1\nncols = len(data)\nfig,ax = pyplot.subplots(nrows=nrows,ncols=ncols,sharey=True,sharex=True,figsize=(FIGWIDTH,FIGWIDTH/ncols))\nfor i,(name,x) in enumerate(data):\n    for j in range(len(ns)-1):\n        nmin = ns[j]\n        nmax = ns[j+1]\n        ax[i].scatter(x[nmin:nmax,0],x[nmin:nmax,1],marker=MARKERS[j],color=COLORS[j],s=MARKERSIZE);\n    ax[i].set_xlim([0,1]);\n    ax[i].set_ylim([0,1]);\n    ax[i].set_xticks([0,1/4,1/2,3/4,1]); ax[i].set_xticklabels([r\"$0$\",r\"$1/4$\",r\"$1/2$\",r\"$3/4$\",r\"$1$\"]);\n    ax[i].set_yticks([0,1/4,1/2,3/4,1]); ax[i].set_yticklabels([r\"$0$\",r\"$1/4$\",r\"$1/2$\",r\"$3/4$\",r\"$1$\"]);\n    ax[i].grid();\n    ax[i].set_aspect(1);\n    ax[i].set_title(name);\nfig.savefig(OUTDIR+\"/points.pdf\");\nfig.savefig(OUTDIR+\"/points.svg\",transparent=True);\nfig.savefig(OUTDIR+\"/points.png\",transparent=True);\n</pre> ns = np.array([0,2**5,2**6,2**7]) assert len(COLORS)&gt;=(len(ns)-1) and len(MARKERS)&gt;=(len(ns)-1) nmax = ns.max()  data = [     (\"IID\",qp.IIDStdUniform(2,seed=7)(nmax)),     (\"LD Lattice Seq.\",qp.Lattice(2,seed=7)(nmax)),     (\"LD Digital Seq.\",qp.DigitalNetB2(2,seed=7,randomize=\"NUS\")(nmax)),     (\"LD Halton Seq.\",qp.Halton(2,seed=7,randomize=\"LMS_DS\")(nmax)), ] nrows = 1 ncols = len(data) fig,ax = pyplot.subplots(nrows=nrows,ncols=ncols,sharey=True,sharex=True,figsize=(FIGWIDTH,FIGWIDTH/ncols)) for i,(name,x) in enumerate(data):     for j in range(len(ns)-1):         nmin = ns[j]         nmax = ns[j+1]         ax[i].scatter(x[nmin:nmax,0],x[nmin:nmax,1],marker=MARKERS[j],color=COLORS[j],s=MARKERSIZE);     ax[i].set_xlim([0,1]);     ax[i].set_ylim([0,1]);     ax[i].set_xticks([0,1/4,1/2,3/4,1]); ax[i].set_xticklabels([r\"$0$\",r\"$1/4$\",r\"$1/2$\",r\"$3/4$\",r\"$1$\"]);     ax[i].set_yticks([0,1/4,1/2,3/4,1]); ax[i].set_yticklabels([r\"$0$\",r\"$1/4$\",r\"$1/2$\",r\"$3/4$\",r\"$1$\"]);     ax[i].grid();     ax[i].set_aspect(1);     ax[i].set_title(name); fig.savefig(OUTDIR+\"/points.pdf\"); fig.savefig(OUTDIR+\"/points.svg\",transparent=True); fig.savefig(OUTDIR+\"/points.png\",transparent=True); In\u00a0[9]: Copied! <pre>tag = \"FULL\"\nforce = False\ndatapath = \"%s/%s.npy\"%(OUTDIR,tag)\nif (not os.path.isfile(datapath)) or force:\n    d = 12\n    n_init = 2**8\n    eps = 10**np.linspace(np.log10(5e-1),np.log10(1e-4),10)\n    trials = 100\n    with np.printoptions(formatter={\"float\":lambda x: \"%.1e\"%x}): print(\"eps = %s\"%str(eps))\n    problem = lambda discrete_distrib: \\\n        qp.FinancialOption(discrete_distrib,\n            option = \"ASIAN\",\n            call_put = \"CALL\",\n            asian_mean = \"GEOMETRIC\",\n            asian_mean_quadrature_rule = \"RIGHT\", \n            volatility = 0.5, \n            start_price = 30., \n            strike_price = 35., \n            interest_rate = 0.01,\n            t_final = 1)\n    exact_value = problem(qp.IIDStdUniform(d)).get_exact_value()\n    print(\"exact value = %.3f\\n\"%exact_value)\n    algorithms = {\n        \"IID MC\": [1e-2, lambda atol: qp.CubMCG(problem(qp.IIDStdUniform(d)),abs_tol=atol,rel_tol=0,n_init=n_init)], \n        \"QMC Digital Net'\": [0, lambda atol: qp.CubQMCNetG(problem(qp.DigitalNetB2(d)),abs_tol=atol,rel_tol=0,n_init=n_init)],\n        \"QMC Lattice\": [0, lambda atol: qp.CubQMCLatticeG(problem(qp.Lattice(d)),abs_tol=atol,rel_tol=0,n_init=n_init)],\n    }\n    keys = list(algorithms.keys())\n    n_totals = {}\n    times = {}\n    approxs = {}\n    for key,(eps_threshold,sc_constsruct) in algorithms.items():\n        print(key)\n        n_totals[key] = np.nan*np.ones((len(eps),trials))\n        times[key] = np.nan*np.ones((len(eps),trials))\n        approxs[key] = np.nan*np.ones((len(eps),trials))\n        for j in range(len(eps)):\n            if eps[j]&lt;eps_threshold: break\n            print(\"\\teps[j] = %.1e\"%eps[j])\n            for t in range(trials):\n                sc = sc_constsruct(eps[j])\n                sol,data = sc.integrate()\n                approxs[key][j,t] = sol\n                n_totals[key][j,t] = data.n_total \n                times[key][j,t] = data.time_integrate\n    _data = {\n        \"eps\": eps,\n        \"n_totals\": n_totals,\n        \"times\": times, \n        \"approxs\": approxs,\n        \"keys\": keys, \n        \"exact_value\": exact_value,\n    }\n    np.save(datapath,_data)\nelse:\n    _data = np.load(datapath,allow_pickle=True).item()\n    eps = _data[\"eps\"]\n    n_totals = _data[\"n_totals\"]\n    times = _data[\"times\"]\n    approxs = _data[\"approxs\"]\n    keys = _data[\"keys\"]\n    exact_value = _data[\"exact_value\"]\n</pre> tag = \"FULL\" force = False datapath = \"%s/%s.npy\"%(OUTDIR,tag) if (not os.path.isfile(datapath)) or force:     d = 12     n_init = 2**8     eps = 10**np.linspace(np.log10(5e-1),np.log10(1e-4),10)     trials = 100     with np.printoptions(formatter={\"float\":lambda x: \"%.1e\"%x}): print(\"eps = %s\"%str(eps))     problem = lambda discrete_distrib: \\         qp.FinancialOption(discrete_distrib,             option = \"ASIAN\",             call_put = \"CALL\",             asian_mean = \"GEOMETRIC\",             asian_mean_quadrature_rule = \"RIGHT\",              volatility = 0.5,              start_price = 30.,              strike_price = 35.,              interest_rate = 0.01,             t_final = 1)     exact_value = problem(qp.IIDStdUniform(d)).get_exact_value()     print(\"exact value = %.3f\\n\"%exact_value)     algorithms = {         \"IID MC\": [1e-2, lambda atol: qp.CubMCG(problem(qp.IIDStdUniform(d)),abs_tol=atol,rel_tol=0,n_init=n_init)],          \"QMC Digital Net'\": [0, lambda atol: qp.CubQMCNetG(problem(qp.DigitalNetB2(d)),abs_tol=atol,rel_tol=0,n_init=n_init)],         \"QMC Lattice\": [0, lambda atol: qp.CubQMCLatticeG(problem(qp.Lattice(d)),abs_tol=atol,rel_tol=0,n_init=n_init)],     }     keys = list(algorithms.keys())     n_totals = {}     times = {}     approxs = {}     for key,(eps_threshold,sc_constsruct) in algorithms.items():         print(key)         n_totals[key] = np.nan*np.ones((len(eps),trials))         times[key] = np.nan*np.ones((len(eps),trials))         approxs[key] = np.nan*np.ones((len(eps),trials))         for j in range(len(eps)):             if eps[j] <pre>eps = [5.0e-01 1.9e-01 7.5e-02 2.9e-02 1.1e-02 4.4e-03 1.7e-03 6.6e-04 2.6e-04\n 1.0e-04]\nexact value = 1.767\n\nIID MC\n\teps[j] = 5.0e-01\n\teps[j] = 1.9e-01\n\teps[j] = 7.5e-02\n\teps[j] = 2.9e-02\n\teps[j] = 1.1e-02\nQMC Digital Net'\n\teps[j] = 5.0e-01\n\teps[j] = 1.9e-01\n\teps[j] = 7.5e-02\n\teps[j] = 2.9e-02\n\teps[j] = 1.1e-02\n\teps[j] = 4.4e-03\n\teps[j] = 1.7e-03\n\teps[j] = 6.6e-04\n\teps[j] = 2.6e-04\n\teps[j] = 1.0e-04\nQMC Lattice\n\teps[j] = 5.0e-01\n\teps[j] = 1.9e-01\n\teps[j] = 7.5e-02\n\teps[j] = 2.9e-02\n\teps[j] = 1.1e-02\n\teps[j] = 4.4e-03\n\teps[j] = 1.7e-03\n\teps[j] = 6.6e-04\n\teps[j] = 2.6e-04\n\teps[j] = 1.0e-04\n</pre> In\u00a0[10]: Copied! <pre>nrows = 1\nncols = 3\nalpha = .25\nalpha2 = .75\nqlow = .1\nqhigh = .9\njstar = 4\nfig,ax = pyplot.subplots(nrows=nrows,ncols=ncols,figsize=(7/8*FIGWIDTH,7/8*FIGWIDTH/ncols),sharey=False,sharex=False)\nfor i,key in enumerate(keys):\n    n_total = n_totals[key] \n    time = times[key]\n    approx = approxs[key]\n    color = COLORS[i]\n    ax[0].plot(eps,np.quantile(n_total,.5,axis=1),label=key,color=color,marker=MARKERS[i],alpha=alpha2);\n    ax[0].fill_between(eps,np.quantile(n_total,qlow,axis=1),np.quantile(n_total,qhigh,axis=1),color=color,alpha=alpha);\n    ax[1].plot(eps,np.quantile(time[:,:],.5,axis=1),color=color,marker=MARKERS[i],alpha=alpha2);\n    ax[1].fill_between(eps,np.quantile(time,qlow,axis=1),np.quantile(time,qhigh,axis=1),color=color,alpha=alpha);\nerrors = np.stack([np.abs(exact_value-approxs[key][jstar]) for key in keys],axis=1)\nsns.violinplot(data=errors,ax=ax[2],log_scale=True,orient=\"h\",palette=COLORS[:len(keys)],inner=\"box\");\nfor i in range(2):\n    ax[i].set_xscale(\"log\",base=10);\n    ax[i].set_yscale(\"log\",base=10);\n    ax[i].set_xlabel(\"error tolerance\");\n    ax[i].xaxis.set_inverted(True);\nfor i in range(3):\n    for spine in [\"top\",\"right\"]: ax[i].spines[spine].set_visible(False);\nax[2].axvline(eps[jstar],color=\"xkcd:gray\",linestyle=(0,(1,1)),label=\"absolute error tolerance = %.2e\"%eps[jstar]);\nax[0].set_ylabel(\"samples\");\nax[1].set_ylabel(\"time (sec)\");\nax[2].set_xlabel(\"true error\");\nax[2].get_yaxis().set_visible(False);\nax[2].spines[\"left\"].set_visible(False);\nfig.legend(frameon=False,loc=\"upper center\",ncol=4,bbox_to_anchor=(.5,1.1));\nfig.savefig(OUTDIR+\"/stopping_crit.pdf\");\nfig.savefig(OUTDIR+\"/stopping_crit.svg\",transparent=True);\nfig.savefig(OUTDIR+\"/stopping_crit.png\",transparent=True);\n</pre> nrows = 1 ncols = 3 alpha = .25 alpha2 = .75 qlow = .1 qhigh = .9 jstar = 4 fig,ax = pyplot.subplots(nrows=nrows,ncols=ncols,figsize=(7/8*FIGWIDTH,7/8*FIGWIDTH/ncols),sharey=False,sharex=False) for i,key in enumerate(keys):     n_total = n_totals[key]      time = times[key]     approx = approxs[key]     color = COLORS[i]     ax[0].plot(eps,np.quantile(n_total,.5,axis=1),label=key,color=color,marker=MARKERS[i],alpha=alpha2);     ax[0].fill_between(eps,np.quantile(n_total,qlow,axis=1),np.quantile(n_total,qhigh,axis=1),color=color,alpha=alpha);     ax[1].plot(eps,np.quantile(time[:,:],.5,axis=1),color=color,marker=MARKERS[i],alpha=alpha2);     ax[1].fill_between(eps,np.quantile(time,qlow,axis=1),np.quantile(time,qhigh,axis=1),color=color,alpha=alpha); errors = np.stack([np.abs(exact_value-approxs[key][jstar]) for key in keys],axis=1) sns.violinplot(data=errors,ax=ax[2],log_scale=True,orient=\"h\",palette=COLORS[:len(keys)],inner=\"box\"); for i in range(2):     ax[i].set_xscale(\"log\",base=10);     ax[i].set_yscale(\"log\",base=10);     ax[i].set_xlabel(\"error tolerance\");     ax[i].xaxis.set_inverted(True); for i in range(3):     for spine in [\"top\",\"right\"]: ax[i].spines[spine].set_visible(False); ax[2].axvline(eps[jstar],color=\"xkcd:gray\",linestyle=(0,(1,1)),label=\"absolute error tolerance = %.2e\"%eps[jstar]); ax[0].set_ylabel(\"samples\"); ax[1].set_ylabel(\"time (sec)\"); ax[2].set_xlabel(\"true error\"); ax[2].get_yaxis().set_visible(False); ax[2].spines[\"left\"].set_visible(False); fig.legend(frameon=False,loc=\"upper center\",ncol=4,bbox_to_anchor=(.5,1.1)); fig.savefig(OUTDIR+\"/stopping_crit.pdf\"); fig.savefig(OUTDIR+\"/stopping_crit.svg\",transparent=True); fig.savefig(OUTDIR+\"/stopping_crit.png\",transparent=True);"},{"location":"demos/talk_paper_demos/JOSS2026/joss2026/#joss-2026","title":"JOSS 2026\u00b6","text":""},{"location":"demos/talk_paper_demos/JOSS2026/joss2026/#setup","title":"Setup\u00b6","text":""},{"location":"demos/talk_paper_demos/JOSS2026/joss2026/#listings","title":"Listings\u00b6","text":""},{"location":"demos/talk_paper_demos/JOSS2026/joss2026/#points","title":"Points\u00b6","text":""},{"location":"demos/talk_paper_demos/JOSS2026/joss2026/#convergence","title":"Convergence\u00b6","text":""},{"location":"demos/talk_paper_demos/JOSS2026/joss2026/#convergence-visualization","title":"Convergence Visualization\u00b6","text":"<p>This visualization compares three methods\u2014MC IID, QMC Digital Net, and QMC Lattice\u2014across 100 trials for each error tolerance. The figure includes three subplots:</p> <ul> <li><p>Left Panel - Sample Complexity: Illustrates the number of samples needed for each error tolerance. Solid lines indicate the median (50th percentile) across trials, while shaded areas represent the inter-decile range (10th to 90th percentiles), showing trial variability. Both axes use a logarithmic scale, with the $x$-axis reversed to highlight lower tolerances on the right. QMC methods require significantly fewer samples than Monte Carlo for equivalent tolerances.</p> </li> <li><p>Middle Panel - Computational Time: Shows the wall-clock time to achieve each error tolerance, using median values and 10th-90th percentile bands.  Both axes are log-scaled with an inverted $x$-axis. QMC methods demonstrate faster convergence than MC IID.</p> </li> <li><p>Right Panel - Error Distribution (Violin Plots): The right panel displays error distributions for a specific tolerance level (eps[jstar], the 5th tolerance). Each method's distribution is represented by a violin plot. The violin shape indicates error probability density across 100 trials; wider sections show more trials within a given error range. Inner box plots display quartiles (25th, 50th, 75th percentiles). The logarithmic $x$-axis accommodates a broad range of error magnitudes.</p> <p>These plots illustrate each method's consistency and reliability. Narrower distributions suggest more predictable performance, while the plot's position indicates the typical error size. Quasi-Monte Carlo (QMC) methods generally exhibit tighter, more consistent error distributions than Monte Carlo (MC) methods. This demonstrates superior average performance and more reliable convergence.</p> </li> </ul>"},{"location":"demos/talk_paper_demos/MCQMC2022_Article_Figures/MCQMC2022_Article_Figures/","title":"2022 MCQMC Paper 1","text":"In\u00a0[1]: Copied! <pre>import matplotlib.pyplot as plt\nimport numpy as np\nimport qmcpy as qp\nimport time  #timing routines\nimport warnings  #to suppress warnings when needed\nimport pickle  #write output to a file and load it back in\nfrom copy import deepcopy\n\nplt.rc('font', size=16)  #set defaults so that the plots are readable\nplt.rc('axes', titlesize=16)\nplt.rc('axes', labelsize=16)\nplt.rc('xtick', labelsize=16)\nplt.rc('ytick', labelsize=16)\nplt.rc('legend', fontsize=16)\nplt.rc('figure', titlesize=16)\n\n#a helpful plotting method to show increasing numbers of points\ndef plot_successive_points(distrib,ld_name,first_n=64,n_cols=1,\n                           pt_clr=['tab:blue', 'tab:green', 'k', 'tab:cyan', 'tab:purple', 'tab:orange'],\n                           xlim=[0,1],ylim=[0,1]):\n  fig,ax = plt.subplots(nrows=1,ncols=n_cols,figsize=(5*n_cols,5.5))\n  if n_cols==1: ax = [ax]\n  last_n = first_n*(2**n_cols)\n  points = distrib.gen_samples(n=last_n)\n  for i in range(n_cols):\n    n = first_n\n    nstart = 0\n    for j in range(i+1):\n      n = first_n*(2**j)\n      ax[i].scatter(points[nstart:n,0],points[nstart:n,1],color=pt_clr[j])\n      nstart = n\n    ax[i].set_title('n = %d'%n)\n    ax[i].set_xlim(xlim); ax[i].set_xticks(xlim); ax[i].set_xlabel('$x_{i,1}$')\n    ax[i].set_ylim(ylim); ax[i].set_yticks(ylim); ax[i].set_ylabel('$x_{i,2}$')\n    ax[i].set_aspect((xlim[1]-xlim[0])/(ylim[1]-ylim[0]))\n  fig.suptitle('%s Points'%ld_name, y=0.9)\n  return fig\n\nprint('QMCPy Version',qp.__version__)\n</pre> import matplotlib.pyplot as plt import numpy as np import qmcpy as qp import time  #timing routines import warnings  #to suppress warnings when needed import pickle  #write output to a file and load it back in from copy import deepcopy  plt.rc('font', size=16)  #set defaults so that the plots are readable plt.rc('axes', titlesize=16) plt.rc('axes', labelsize=16) plt.rc('xtick', labelsize=16) plt.rc('ytick', labelsize=16) plt.rc('legend', fontsize=16) plt.rc('figure', titlesize=16)  #a helpful plotting method to show increasing numbers of points def plot_successive_points(distrib,ld_name,first_n=64,n_cols=1,                            pt_clr=['tab:blue', 'tab:green', 'k', 'tab:cyan', 'tab:purple', 'tab:orange'],                            xlim=[0,1],ylim=[0,1]):   fig,ax = plt.subplots(nrows=1,ncols=n_cols,figsize=(5*n_cols,5.5))   if n_cols==1: ax = [ax]   last_n = first_n*(2**n_cols)   points = distrib.gen_samples(n=last_n)   for i in range(n_cols):     n = first_n     nstart = 0     for j in range(i+1):       n = first_n*(2**j)       ax[i].scatter(points[nstart:n,0],points[nstart:n,1],color=pt_clr[j])       nstart = n     ax[i].set_title('n = %d'%n)     ax[i].set_xlim(xlim); ax[i].set_xticks(xlim); ax[i].set_xlabel('$x_{i,1}$')     ax[i].set_ylim(ylim); ax[i].set_yticks(ylim); ax[i].set_ylabel('$x_{i,2}$')     ax[i].set_aspect((xlim[1]-xlim[0])/(ylim[1]-ylim[0]))   fig.suptitle('%s Points'%ld_name, y=0.9)   return fig  print('QMCPy Version',qp.__version__) <pre>QMCPy Version 1.6.3c\n</pre> In\u00a0[2]: Copied! <pre>figpath = '' #this path sends the figures to the desired directory\n</pre> figpath = '' #this path sends the figures to the desired directory In\u00a0[3]: Copied! <pre>d = 5 #dimension\nn = 32 #number of points\ncols = 3 #number of columns\nld = qp.Lattice(d) #define the generator\nfig = plot_successive_points(ld,'Lattice',first_n=n,n_cols=cols)\nfig.savefig(figpath+'latticepts.eps',format='eps',bbox_inches='tight')\n</pre> d = 5 #dimension n = 32 #number of points cols = 3 #number of columns ld = qp.Lattice(d) #define the generator fig = plot_successive_points(ld,'Lattice',first_n=n,n_cols=cols) fig.savefig(figpath+'latticepts.eps',format='eps',bbox_inches='tight') In\u00a0[4]: Copied! <pre>with open(figpath+'ld_parallel.pkl','rb') as myfile: tol_vec,n_tol,ld_time,ld_n,ld_p_time,ld_p_n,best_solution = pickle.load(myfile)\nprint(best_solution)\nfig,ax = plt.subplots(nrows=1,ncols=2,figsize=(11,4))\nax[0].scatter(tol_vec[0:n_tol],ld_time[0:n_tol],color='tab:blue'); \nax[0].plot(tol_vec[0:n_tol],[(ld_time[0]*tol_vec[0])/tol_vec[jj] for jj in range(n_tol)],color='tab:blue')\nax[0].scatter(tol_vec[0:n_tol],ld_p_time[0:n_tol],color='tab:orange',marker = '+',s=100,linewidths=2); \n#ax[0].plot(tol_vec[0:n_tol],[(ld_p_time[0]*tol_vec[0])/tol_vec[jj] for jj in range(n_tol)],color='tab:orange')\nax[0].set_ylim([0.05,500]); ax[0].set_ylabel('Time')\nax[1].scatter(tol_vec[0:n_tol],ld_n[0:n_tol],color='tab:blue'); \nax[1].plot(tol_vec[0:n_tol],[(ld_n[0]*tol_vec[0])/tol_vec[jj] for jj in range(n_tol)],color='tab:blue')\nax[1].scatter(tol_vec[0:n_tol],ld_p_n[0:n_tol],color='tab:orange',marker = '+',s=100,linewidths=2); \n#ax[1].plot(tol_vec[0:n_tol],[(ld_p_n[0]*tol_vec[0])/tol_vec[jj] for jj in range(n_tol)],color='tab:orange')\nax[1].set_ylim([10,1e5]); ax[1].set_ylabel('n')\nfor ii in range(2):\n  ax[ii].set_xlim([0.007,4]); ax[ii].set_xlabel('Tolerance, '+r'$\\varepsilon$')\n  ax[ii].set_xscale('log'); ax[ii].set_yscale('log')\n  ax[ii].legend(['Lattice',r'$\\mathcal{O}(\\varepsilon^{-1})$','Lattice Parallel',r'$\\mathcal{O}(\\varepsilon^{-1})$'],frameon=False)\n  ax[ii].set_aspect(0.6)\nax[0].set_yticks([1, 60], labels = ['1 sec', '1 min'])\nfig.savefig(figpath+'ldparallelbeam.eps',format='eps',bbox_inches='tight')\n</pre> with open(figpath+'ld_parallel.pkl','rb') as myfile: tol_vec,n_tol,ld_time,ld_n,ld_p_time,ld_p_n,best_solution = pickle.load(myfile) print(best_solution) fig,ax = plt.subplots(nrows=1,ncols=2,figsize=(11,4)) ax[0].scatter(tol_vec[0:n_tol],ld_time[0:n_tol],color='tab:blue');  ax[0].plot(tol_vec[0:n_tol],[(ld_time[0]*tol_vec[0])/tol_vec[jj] for jj in range(n_tol)],color='tab:blue') ax[0].scatter(tol_vec[0:n_tol],ld_p_time[0:n_tol],color='tab:orange',marker = '+',s=100,linewidths=2);  #ax[0].plot(tol_vec[0:n_tol],[(ld_p_time[0]*tol_vec[0])/tol_vec[jj] for jj in range(n_tol)],color='tab:orange') ax[0].set_ylim([0.05,500]); ax[0].set_ylabel('Time') ax[1].scatter(tol_vec[0:n_tol],ld_n[0:n_tol],color='tab:blue');  ax[1].plot(tol_vec[0:n_tol],[(ld_n[0]*tol_vec[0])/tol_vec[jj] for jj in range(n_tol)],color='tab:blue') ax[1].scatter(tol_vec[0:n_tol],ld_p_n[0:n_tol],color='tab:orange',marker = '+',s=100,linewidths=2);  #ax[1].plot(tol_vec[0:n_tol],[(ld_p_n[0]*tol_vec[0])/tol_vec[jj] for jj in range(n_tol)],color='tab:orange') ax[1].set_ylim([10,1e5]); ax[1].set_ylabel('n') for ii in range(2):   ax[ii].set_xlim([0.007,4]); ax[ii].set_xlabel('Tolerance, '+r'$\\varepsilon$')   ax[ii].set_xscale('log'); ax[ii].set_yscale('log')   ax[ii].legend(['Lattice',r'$\\mathcal{O}(\\varepsilon^{-1})$','Lattice Parallel',r'$\\mathcal{O}(\\varepsilon^{-1})$'],frameon=False)   ax[ii].set_aspect(0.6) ax[0].set_yticks([1, 60], labels = ['1 sec', '1 min']) fig.savefig(figpath+'ldparallelbeam.eps',format='eps',bbox_inches='tight') <pre>[   0.            4.09650336   15.63089718   33.92196588   58.36259037\n   88.42166624  123.63753779  163.62564457  208.07398743  256.74371736\n  309.46852588  362.17337763  410.80019812  455.47543786  496.40155276\n  533.85794874  568.20201672  599.87051305  629.38039738  657.32920342\n  684.39496877  712.15829288  742.93687638  776.09146089  811.06031655\n  847.35856113  884.57752606  922.38415394  960.52038491  998.8025606\n 1037.12106673]\n</pre> In\u00a0[5]: Copied! <pre>fig,ax = plt.subplots(figsize=(6,3))\nax.plot(best_solution,'-')\nax.set_xlim([0,len(best_solution)-1]); ax.set_xlabel('Position')\nax.set_ylim([1050,-50]);  ax.set_ylabel('Deflection');\nax.set_aspect(0.02)\nfig.suptitle('Cantilevered Beam')\nfig.savefig(figpath+'cantileveredbeamwords.eps',format='eps',bbox_inches='tight')\n</pre> fig,ax = plt.subplots(figsize=(6,3)) ax.plot(best_solution,'-') ax.set_xlim([0,len(best_solution)-1]); ax.set_xlabel('Position') ax.set_ylim([1050,-50]);  ax.set_ylabel('Deflection'); ax.set_aspect(0.02) fig.suptitle('Cantilevered Beam') fig.savefig(figpath+'cantileveredbeamwords.eps',format='eps',bbox_inches='tight') In\u00a0[6]: Copied! <pre>qp.util.stop_notebook()\n</pre> qp.util.stop_notebook() <p>Below is long-running code, that we rarely wish to run</p> In\u00a0[7]: Copied! <pre>import umbridge #this is the connector\n!docker run --name muqbp -d -it -p 4243:4243 linusseelinger/benchmark-muq-beam-propagation:latest #get beam example\nd = 3 #dimension of the randomness\nlb = 1 #lower bound on randomness\nub = 1.2 #upper bound on randomness\numbridge_config = {\"d\": d}\nmodel = umbridge.HTTPModel('http://localhost:4243','forward') #this is the original model\noutindex = -1 #choose last element of the vector of beam deflections\nmodeli = deepcopy(model) #and construct a model for just that deflection\nmodeli.get_output_sizes = lambda *args : [1]\nmodeli.get_output_sizes()\nmodeli.__call__ = lambda *args,**kwargs: [[model.__call__(*args,**kwargs)[0][outindex]]]\n</pre> import umbridge #this is the connector !docker run --name muqbp -d -it -p 4243:4243 linusseelinger/benchmark-muq-beam-propagation:latest #get beam example d = 3 #dimension of the randomness lb = 1 #lower bound on randomness ub = 1.2 #upper bound on randomness umbridge_config = {\"d\": d} model = umbridge.HTTPModel('http://localhost:4243','forward') #this is the original model outindex = -1 #choose last element of the vector of beam deflections modeli = deepcopy(model) #and construct a model for just that deflection modeli.get_output_sizes = lambda *args : [1] modeli.get_output_sizes() modeli.__call__ = lambda *args,**kwargs: [[model.__call__(*args,**kwargs)[0][outindex]]] <pre>docker: Error response from daemon: Conflict. The container name \"/muqbp\" is already in use by container \"d30a451e4f61fd8d33dd7bb63d9338183a4015fe9dbbb755727cf7f3596fecee\". You have to remove (or rename) that container to be able to reuse that name.\nSee 'docker run --help'.\n</pre> In\u00a0[\u00a0]: Copied! <pre>ld = qp.Uniform(qp.Lattice(d,seed=7),lower_bound=lb,upper_bound=ub) #lattice points for this problem\nld_integ = qp.UMBridgeWrapper(ld,modeli,umbridge_config,parallel=False) #integrand\niid = qp.Uniform(qp.IIDStdUniform(d),lower_bound=lb,upper_bound=ub) #iid points for this problem\niid_integ = qp.UMBridgeWrapper(iid,modeli,umbridge_config,parallel=False) #integrand\ntol = 0.01  #smallest tolerance\n\nn_tol = 14  #number of different tolerances\nii_iid = 9  #make this larger to reduce the time required by not running all cases for IID\ntol_vec = [tol*(2**ii) for ii in range(n_tol)]  #initialize vector of tolerances\nld_time = [0]*n_tol; ld_n = [0]*n_tol  #low discrepancy time and number of function values\niid_time = [0]*n_tol; iid_n = [0]*n_tol  #IID time and number of function values\nprint(f'\\nCantilever Beam\\n')\nprint('iteration ', end = '')\nfor ii in range(n_tol):\n  solution, data = qp.CubQMCLatticeG(ld_integ, abs_tol = tol_vec[ii]).integrate()\n  if ii == 0:\n    best_solution_i = solution\n  ld_time[ii] = data.time_integrate\n  ld_n[ii] = data.n_total\n  if ii &gt;= ii_iid:\n    solution, data = qp.CubMCG(iid_integ, abs_tol = tol_vec[ii]).integrate()\n    iid_time[ii] = data.time_integrate\n    iid_n[ii] = data.n_total\n  print(ii, end = ' ')\nwith open(figpath+'iid_ld.pkl','wb') as myfile:pickle.dump([tol_vec,n_tol,ii_iid,ld_time,ld_n,iid_time,iid_n,best_solution_i],myfile)\n</pre> ld = qp.Uniform(qp.Lattice(d,seed=7),lower_bound=lb,upper_bound=ub) #lattice points for this problem ld_integ = qp.UMBridgeWrapper(ld,modeli,umbridge_config,parallel=False) #integrand iid = qp.Uniform(qp.IIDStdUniform(d),lower_bound=lb,upper_bound=ub) #iid points for this problem iid_integ = qp.UMBridgeWrapper(iid,modeli,umbridge_config,parallel=False) #integrand tol = 0.01  #smallest tolerance  n_tol = 14  #number of different tolerances ii_iid = 9  #make this larger to reduce the time required by not running all cases for IID tol_vec = [tol*(2**ii) for ii in range(n_tol)]  #initialize vector of tolerances ld_time = [0]*n_tol; ld_n = [0]*n_tol  #low discrepancy time and number of function values iid_time = [0]*n_tol; iid_n = [0]*n_tol  #IID time and number of function values print(f'\\nCantilever Beam\\n') print('iteration ', end = '') for ii in range(n_tol):   solution, data = qp.CubQMCLatticeG(ld_integ, abs_tol = tol_vec[ii]).integrate()   if ii == 0:     best_solution_i = solution   ld_time[ii] = data.time_integrate   ld_n[ii] = data.n_total   if ii &gt;= ii_iid:     solution, data = qp.CubMCG(iid_integ, abs_tol = tol_vec[ii]).integrate()     iid_time[ii] = data.time_integrate     iid_n[ii] = data.n_total   print(ii, end = ' ') with open(figpath+'iid_ld.pkl','wb') as myfile:pickle.dump([tol_vec,n_tol,ii_iid,ld_time,ld_n,iid_time,iid_n,best_solution_i],myfile) In\u00a0[\u00a0]: Copied! <pre>ld_integ = qp.UMBridgeWrapper(ld,model,umbridge_config,parallel=False) #integrand\nld_integ_p = qp.UMBridgeWrapper(ld,model,umbridge_config,parallel=8) #integrand with parallel processing\n\ntol = 0.01\nn_tol = 9  #number of different tolerances\ntol_vec = [tol*(2**ii) for ii in range(n_tol)]  #initialize vector of tolerances\nld_time = [0]*n_tol; ld_n = [0]*n_tol  #low discrepancy time and number of function values\nld_p_time = [0]*n_tol; ld_p_n = [0]*n_tol  #low discrepancy time and number of function values with parallel\nprint(f'\\nCantilever Beam\\n')\nprint('iteration ', end = '')\nfor ii in range(n_tol):\n  solution, data = qp.CubQMCLatticeG(ld_integ, abs_tol = tol_vec[ii]).integrate()\n  if ii == 0:\n    best_solution = solution\n  ld_time[ii] = data.time_integrate\n  ld_n[ii] = data.n_total\n  solution, data = qp.CubQMCLatticeG(ld_integ_p, abs_tol = tol_vec[ii]).integrate()\n  ld_p_time[ii] = data.time_integrate\n  ld_p_n[ii] = data.n_total\n  print(ii, end = ' ') \nwith open(figpath+'ld_parallel.pkl','wb') as myfile:pickle.dump([tol_vec,n_tol,ld_time,ld_n,ld_p_time,ld_p_n,best_solution],myfile)\n</pre> ld_integ = qp.UMBridgeWrapper(ld,model,umbridge_config,parallel=False) #integrand ld_integ_p = qp.UMBridgeWrapper(ld,model,umbridge_config,parallel=8) #integrand with parallel processing  tol = 0.01 n_tol = 9  #number of different tolerances tol_vec = [tol*(2**ii) for ii in range(n_tol)]  #initialize vector of tolerances ld_time = [0]*n_tol; ld_n = [0]*n_tol  #low discrepancy time and number of function values ld_p_time = [0]*n_tol; ld_p_n = [0]*n_tol  #low discrepancy time and number of function values with parallel print(f'\\nCantilever Beam\\n') print('iteration ', end = '') for ii in range(n_tol):   solution, data = qp.CubQMCLatticeG(ld_integ, abs_tol = tol_vec[ii]).integrate()   if ii == 0:     best_solution = solution   ld_time[ii] = data.time_integrate   ld_n[ii] = data.n_total   solution, data = qp.CubQMCLatticeG(ld_integ_p, abs_tol = tol_vec[ii]).integrate()   ld_p_time[ii] = data.time_integrate   ld_p_n[ii] = data.n_total   print(ii, end = ' ')  with open(figpath+'ld_parallel.pkl','wb') as myfile:pickle.dump([tol_vec,n_tol,ld_time,ld_n,ld_p_time,ld_p_n,best_solution],myfile) In\u00a0[\u00a0]: Copied! <pre>!docker rm -f muqbp #shut down docker image\n</pre> !docker rm -f muqbp #shut down docker image"},{"location":"demos/talk_paper_demos/MCQMC2022_Article_Figures/MCQMC2022_Article_Figures/#challenges-in-developing-great-qmc-software","title":"Challenges in Developing Great QMC Software\u00b6","text":"<p>Computations and Figures for the MCQMC 2022 Article: Challenges in Developing Great Quasi-Monte Carlo Software</p>"},{"location":"demos/talk_paper_demos/MCQMC2022_Article_Figures/MCQMC2022_Article_Figures/#import-the-necessary-packages-and-set-up-plotting-routines","title":"Import the necessary packages and set up plotting routines\u00b6","text":""},{"location":"demos/talk_paper_demos/MCQMC2022_Article_Figures/MCQMC2022_Article_Figures/#make-sure-that-you-have-the-relevant-path-to-store-the-figures","title":"Make sure that you have the relevant path to store the figures\u00b6","text":""},{"location":"demos/talk_paper_demos/MCQMC2022_Article_Figures/MCQMC2022_Article_Figures/#here-are-some-plots-of-low-discrepancy-ld-lattice-points","title":"Here are some plots of Low Discrepancy (LD) Lattice Points\u00b6","text":""},{"location":"demos/talk_paper_demos/MCQMC2022_Article_Figures/MCQMC2022_Article_Figures/#beam-example-plots","title":"Beam Example Plots\u00b6","text":""},{"location":"demos/talk_paper_demos/MCQMC2022_Article_Figures/MCQMC2022_Article_Figures/#plot-the-time-and-sample-size-required-to-solve-for-the-deflection-of-the-whole-beam-using-low-discrepancy-with-and-without-parallel","title":"Plot the time and sample size required to solve for the deflection of the whole beam using low discrepancy with and without parallel\u00b6","text":""},{"location":"demos/talk_paper_demos/MCQMC2022_Article_Figures/MCQMC2022_Article_Figures/#plot-of-beam-solution","title":"Plot of beam solution\u00b6","text":""},{"location":"demos/talk_paper_demos/MCQMC2022_Article_Figures/MCQMC2022_Article_Figures/#beam-example-computations","title":"Beam Example Computations\u00b6","text":"<p>To run this, you need to be running the <code>docker</code> application, https://www.docker.com/products/docker-desktop/</p>"},{"location":"demos/talk_paper_demos/MCQMC2022_Article_Figures/MCQMC2022_Article_Figures/#set-up-the-problem-using-a-docker-container-to-solve-the-ode","title":"Set up the problem using a docker container to solve the ODE\u00b6","text":""},{"location":"demos/talk_paper_demos/MCQMC2022_Article_Figures/MCQMC2022_Article_Figures/#first-we-compute-the-time-required-to-solve-for-the-deflection-of-the-end-point-using-iid-and-low-discrepancy","title":"First we compute the time required to solve for the deflection of the end point using IID and low discrepancy\u00b6","text":""},{"location":"demos/talk_paper_demos/MCQMC2022_Article_Figures/MCQMC2022_Article_Figures/#next-we-compute-the-time-required-to-solve-for-the-deflection-of-the-whole-beam-using-low-discrepancy-with-and-without-parallel","title":"Next, we compute the time required to solve for the deflection of the whole beam using low discrepancy with and without parallel\u00b6","text":""},{"location":"demos/talk_paper_demos/MCQMC_Tutorial_2020/MCQMC_2020_QMC_Software_Tutorial/","title":"2020 MCQMC Software Tutorial","text":"In\u00a0[1]: Copied! <pre>import qmcpy  #we import the environment at the start to use it\nimport numpy as np  #basic numerical routines in Python\nimport time  #timing routines\nimport warnings  #to suppress warnings when needed\nimport torch  #only needed for PyTorch Sobol' backend\nfrom torch.quasirandom import SobolEngine\nfrom matplotlib import pyplot;  #plotting\n\npyplot.rc('font', size=16)  #set defaults so that the plots are readable\npyplot.rc('axes', titlesize=16)\npyplot.rc('axes', labelsize=16)\npyplot.rc('xtick', labelsize=16)\npyplot.rc('ytick', labelsize=16)\npyplot.rc('legend', fontsize=16)\npyplot.rc('figure', titlesize=16)\n\n#a helpful plotting method to show increasing numbers of points\ndef plot_successive_points(distrib,ld_name,first_n=64,n_cols=1,pt_clr='bgkcmy',\n                           xlim=[0,1],ylim=[0,1],coord1 = 0,coord2 = 1):\n  fig,ax = pyplot.subplots(nrows=1,ncols=n_cols,figsize=(5*n_cols,5.5))\n  if n_cols==1: ax = [ax]\n  last_n = first_n*(2**n_cols)\n  points = distrib.gen_samples(n=last_n)\n  for i in range(n_cols):\n    n = first_n\n    nstart = 0\n    for j in range(i+1):\n      n = first_n*(2**j)\n      ax[i].scatter(points[nstart:n,coord1],points[nstart:n,coord2],color=pt_clr[j])\n      nstart = n\n    ax[i].set_title('n = %d'%n)\n    ax[i].set_xlim(xlim); ax[i].set_xticks(xlim); ax[i].set_xlabel('$x_{i,%d}$'%(coord1+1))\n    ax[i].set_ylim(ylim); ax[i].set_yticks(ylim); ax[i].set_ylabel('$x_{i,%d}$'%(coord2+1))\n    ax[i].set_aspect((xlim[1]-xlim[0])/(ylim[1]-ylim[0]))\n  fig.suptitle('%s Points'%ld_name)\nprint('QMCPy Version',qmcpy.__version__)\n</pre> import qmcpy  #we import the environment at the start to use it import numpy as np  #basic numerical routines in Python import time  #timing routines import warnings  #to suppress warnings when needed import torch  #only needed for PyTorch Sobol' backend from torch.quasirandom import SobolEngine from matplotlib import pyplot;  #plotting  pyplot.rc('font', size=16)  #set defaults so that the plots are readable pyplot.rc('axes', titlesize=16) pyplot.rc('axes', labelsize=16) pyplot.rc('xtick', labelsize=16) pyplot.rc('ytick', labelsize=16) pyplot.rc('legend', fontsize=16) pyplot.rc('figure', titlesize=16)  #a helpful plotting method to show increasing numbers of points def plot_successive_points(distrib,ld_name,first_n=64,n_cols=1,pt_clr='bgkcmy',                            xlim=[0,1],ylim=[0,1],coord1 = 0,coord2 = 1):   fig,ax = pyplot.subplots(nrows=1,ncols=n_cols,figsize=(5*n_cols,5.5))   if n_cols==1: ax = [ax]   last_n = first_n*(2**n_cols)   points = distrib.gen_samples(n=last_n)   for i in range(n_cols):     n = first_n     nstart = 0     for j in range(i+1):       n = first_n*(2**j)       ax[i].scatter(points[nstart:n,coord1],points[nstart:n,coord2],color=pt_clr[j])       nstart = n     ax[i].set_title('n = %d'%n)     ax[i].set_xlim(xlim); ax[i].set_xticks(xlim); ax[i].set_xlabel('$x_{i,%d}$'%(coord1+1))     ax[i].set_ylim(ylim); ax[i].set_yticks(ylim); ax[i].set_ylabel('$x_{i,%d}$'%(coord2+1))     ax[i].set_aspect((xlim[1]-xlim[0])/(ylim[1]-ylim[0]))   fig.suptitle('%s Points'%ld_name) print('QMCPy Version',qmcpy.__version__) <pre>QMCPy Version 1.6.3.2a\n</pre> In\u00a0[2]: Copied! <pre>lattice = qmcpy.Lattice(dimension=2)  #define a discrete LD distribution based on an integration lattice\nprint(lattice)  #print the properties of the lattice object\nn = 16  #number of points to generate\npoints = lattice.gen_samples(n)  #construct some points\nprint(f'\\nLD Lattice Points with shape {points.shape}\\n'+str(points))  #these points have 15 significant digit precision but only three digits are shown\nplot_successive_points(lattice,'Lattice',n)\n</pre> lattice = qmcpy.Lattice(dimension=2)  #define a discrete LD distribution based on an integration lattice print(lattice)  #print the properties of the lattice object n = 16  #number of points to generate points = lattice.gen_samples(n)  #construct some points print(f'\\nLD Lattice Points with shape {points.shape}\\n'+str(points))  #these points have 15 significant digit precision but only three digits are shown plot_successive_points(lattice,'Lattice',n) <pre>Lattice (AbstractLDDiscreteDistribution)\n    d               2^(1)\n    replications    1\n    randomize       SHIFT\n    gen_vec_source  kuo.lattice-33002-1024-1048576.9125.txt\n    order           RADICAL INVERSE\n    n_limit         2^(20)\n    entropy         264030601762852985433978628854999910438\n\nLD Lattice Points with shape (16, 2)\n[[0.14448802 0.23129441]\n [0.64448802 0.73129441]\n [0.39448802 0.98129441]\n [0.89448802 0.48129441]\n [0.26948802 0.60629441]\n [0.76948802 0.10629441]\n [0.51948802 0.35629441]\n [0.01948802 0.85629441]\n [0.20698802 0.91879441]\n [0.70698802 0.41879441]\n [0.45698802 0.66879441]\n [0.95698802 0.16879441]\n [0.33198802 0.29379441]\n [0.83198802 0.79379441]\n [0.58198802 0.04379441]\n [0.08198802 0.54379441]]\n</pre> <p>Rerunning the commands above yields a different sequence of points because these points are randomly shifted modulo 1.</p> <p>We may also construct a subsequence of points in the middle of the sequence. Note that the points below match those above.</p> In\u00a0[3]: Copied! <pre>more_points = lattice.gen_samples(n_min=4,n_max=n)  #get more points\nprint('LD Lattice Points with shape',more_points.shape,'\\n'+str(more_points))\n</pre> more_points = lattice.gen_samples(n_min=4,n_max=n)  #get more points print('LD Lattice Points with shape',more_points.shape,'\\n'+str(more_points)) <pre>LD Lattice Points with shape (12, 2) \n[[0.26948802 0.60629441]\n [0.76948802 0.10629441]\n [0.51948802 0.35629441]\n [0.01948802 0.85629441]\n [0.20698802 0.91879441]\n [0.70698802 0.41879441]\n [0.45698802 0.66879441]\n [0.95698802 0.16879441]\n [0.33198802 0.29379441]\n [0.83198802 0.79379441]\n [0.58198802 0.04379441]\n [0.08198802 0.54379441]]\n</pre> <p>Each $d$-dimensional point is one row in the array.</p> <p>As we increase the number of points, they fill $[0,1]^d$ evenly.  The next points are placed in between the existing points.  Here we illustrate with $d=2$.</p> In\u00a0[4]: Copied! <pre>plot_successive_points(lattice,'Lattice',n_cols=5)\n</pre> plot_successive_points(lattice,'Lattice',n_cols=5) In\u00a0[5]: Copied! <pre>iid = qmcpy.IIDStdUniform(2)  #standard uniform IID random vector generator from NumPy\nprint(iid)  #print the properties of iid\nplot_successive_points(iid,'IID Uniform',n_cols=5)\n</pre> iid = qmcpy.IIDStdUniform(2)  #standard uniform IID random vector generator from NumPy print(iid)  #print the properties of iid plot_successive_points(iid,'IID Uniform',n_cols=5) <pre>IIDStdUniform (AbstractIIDDiscreteDistribution)\n    d               2^(1)\n    replications    1\n    entropy         151929946340103232908357805579611369175\n</pre> In\u00a0[6]: Copied! <pre>d = 16  #dimension\nn = 2  #number of points\nlattice = qmcpy.Lattice(d)  #define a discrete LD distribution based on an integration lattice\nlattice_pts = lattice.gen_samples(n)  #the first parameter in the .gen_samples method is the number of points\niid = qmcpy.IIDStdUniform(d)\niid_pts = iid.gen_samples(n)\nfig,ax = pyplot.subplots(nrows=1,ncols=2,figsize=(10,5.5))\nax[0].scatter(lattice_pts[0,0:d],lattice_pts[1,0:d],color='b')\nax[0].set_title('Transposed Lattice Points')\nax[1].scatter(iid_pts[0,0:d],iid_pts[1,0:d],color='b')\nax[1].set_title('Transposed IID Points')\nfor ii in range(2):\n  ax[ii].set_xlim([0,1]); ax[ii].set_xticks([0,1]); ax[ii].set_xlabel('$x_{1,j}$')\n  ax[ii].set_ylim([0,1]); ax[ii].set_yticks([0,1]); ax[ii].set_ylabel('$x_{2,j}$')\n  ax[ii].set_aspect(1)\n</pre> d = 16  #dimension n = 2  #number of points lattice = qmcpy.Lattice(d)  #define a discrete LD distribution based on an integration lattice lattice_pts = lattice.gen_samples(n)  #the first parameter in the .gen_samples method is the number of points iid = qmcpy.IIDStdUniform(d) iid_pts = iid.gen_samples(n) fig,ax = pyplot.subplots(nrows=1,ncols=2,figsize=(10,5.5)) ax[0].scatter(lattice_pts[0,0:d],lattice_pts[1,0:d],color='b') ax[0].set_title('Transposed Lattice Points') ax[1].scatter(iid_pts[0,0:d],iid_pts[1,0:d],color='b') ax[1].set_title('Transposed IID Points') for ii in range(2):   ax[ii].set_xlim([0,1]); ax[ii].set_xticks([0,1]); ax[ii].set_xlabel('$x_{1,j}$')   ax[ii].set_ylim([0,1]); ax[ii].set_yticks([0,1]); ax[ii].set_ylabel('$x_{2,j}$')   ax[ii].set_aspect(1) In\u00a0[7]: Copied! <pre>sobol = qmcpy.Sobol(2)  #Sobol LD generator\nprint(sobol) #note below that the default is qrng from Hofert and Lemieux\npoints = sobol.gen_samples(16)\nprint(f'\\nLD Sobol\\' Points with shape {points.shape}\\n'+str(points))\nplot_successive_points(sobol,'Sobol\\'',n_cols=5)\n</pre> sobol = qmcpy.Sobol(2)  #Sobol LD generator print(sobol) #note below that the default is qrng from Hofert and Lemieux points = sobol.gen_samples(16) print(f'\\nLD Sobol\\' Points with shape {points.shape}\\n'+str(points)) plot_successive_points(sobol,'Sobol\\'',n_cols=5) <pre>DigitalNetB2 (AbstractLDDiscreteDistribution)\n    d               2^(1)\n    replications    1\n    randomize       LMS DS\n    gen_mats_source joe_kuo.6.21201.txt\n    order           RADICAL INVERSE\n    t               63\n    alpha           1\n    n_limit         2^(32)\n    entropy         73208827127383995610457310988530317873\n\nLD Sobol' Points with shape (16, 2)\n[[0.9431598  0.47365992]\n [0.22125498 0.72515188]\n [0.66129464 0.90348679]\n [0.37738574 0.15491697]\n [0.79964553 0.60309908]\n [0.02104669 0.35362196]\n [0.58028993 0.0173281 ]\n [0.36467173 0.76778926]\n [0.88650748 0.81477058]\n [0.16853181 0.06430855]\n [0.72964997 0.30712722]\n [0.44965495 0.55660491]\n [0.87190807 0.20193054]\n [0.0894106  0.95049973]\n [0.52754482 0.67862835]\n [0.30804326 0.4271372 ]]\n</pre> In\u00a0[8]: Copied! <pre>halton = qmcpy.Halton(2)\nprint(halton)\npoints = halton.gen_samples(20)\nprint(f'\\nLD Halton Points with shape {points.shape}\\n'+str(points))\nplot_successive_points(halton,'Halton',n_cols=5,first_n=60)\n</pre> halton = qmcpy.Halton(2) print(halton) points = halton.gen_samples(20) print(f'\\nLD Halton Points with shape {points.shape}\\n'+str(points)) plot_successive_points(halton,'Halton',n_cols=5,first_n=60) <pre>Halton (AbstractLDDiscreteDistribution)\n    d               2^(1)\n    replications    1\n    randomize       LMS DP\n    t               63\n    n_limit         2^(32)\n    entropy         240669245053839331683025566616811162761\n\nLD Halton Points with shape (20, 2)\n[[0.44785589 0.67105156]\n [0.81034548 0.6477606 ]\n [0.01802002 0.17997437]\n [0.72376384 0.96444929]\n [0.31082674 0.49671198]\n [0.94638376 0.02481055]\n [0.22522047 0.82620954]\n [0.51752374 0.3543589 ]\n [0.40526643 0.33101523]\n [0.85320912 0.7104405 ]\n [0.06844815 0.57598763]\n [0.67312161 0.21524872]\n [0.35369039 0.89273277]\n [0.90379429 0.53194055]\n [0.17457824 0.06420577]\n [0.56795187 0.86102595]\n [0.49980629 0.39323972]\n [0.75841033 0.25883637]\n [0.03822453 0.74815281]\n [0.70357459 0.60958533]]\n</pre> In\u00a0[9]: Copied! <pre>ld = qmcpy.Lattice(64)  #define a discrete LD distribution\nprint(ld)  #print the properties of the lattice object\nstart_time = time.time()  #time now\npoints = ld.gen_samples(2**20)  #construct some points\nend_time = time.time()  #time after points are constructed\nprint(f'\\nLD Points with shape {points.shape}\\n'+str(points))\nprint(f'\\nTime to construct points is %.1e seconds'%(end_time - start_time))\n</pre> ld = qmcpy.Lattice(64)  #define a discrete LD distribution print(ld)  #print the properties of the lattice object start_time = time.time()  #time now points = ld.gen_samples(2**20)  #construct some points end_time = time.time()  #time after points are constructed print(f'\\nLD Points with shape {points.shape}\\n'+str(points)) print(f'\\nTime to construct points is %.1e seconds'%(end_time - start_time)) <pre>Lattice (AbstractLDDiscreteDistribution)\n    d               2^(6)\n    replications    1\n    randomize       SHIFT\n    gen_vec_source  kuo.lattice-33002-1024-1048576.9125.txt\n    order           RADICAL INVERSE\n    n_limit         2^(20)\n    entropy         323672018533134169508871551556781883883\n\nLD Points with shape (1048576, 64)\n[[4.84051442e-01 1.74980418e-01 6.35194475e-01 ... 7.14805235e-01\n  1.59923415e-01 7.06555592e-02]\n [9.84051442e-01 6.74980418e-01 1.35194475e-01 ... 2.14805235e-01\n  6.59923415e-01 5.70655559e-01]\n [7.34051442e-01 9.24980418e-01 3.85194475e-01 ... 9.64805235e-01\n  9.09923415e-01 8.20655559e-01]\n ...\n [2.34050488e-01 2.50775592e-01 6.81364710e-01 ... 9.84270681e-01\n  1.84181733e-02 2.36378406e-01]\n [9.84050488e-01 5.00775592e-01 9.31364710e-01 ... 7.34270681e-01\n  2.68418173e-01 4.86378406e-01]\n [4.84050488e-01 7.75592029e-04 4.31364710e-01 ... 2.34270681e-01\n  7.68418173e-01 9.86378406e-01]]\n\nTime to construct points is 1.7e-01 seconds\n</pre> In\u00a0[10]: Copied! <pre>ld = qmcpy.Sobol(2)  #Sobol' points\nanother_unif_ld = qmcpy.Uniform(ld, lower_bound=[-2,2], upper_bound=[2,4])  #define the desired probability distribution with sobol as input\npoints = another_unif_ld.gen_samples(2**8)\nprint(another_unif_ld)\nprint(f'\\nUniform LD points with shape {points.shape}\\n'+str(points))  #these points have 15 significant digit precision but only three digits are shown\nplot_successive_points(another_unif_ld,'Uniform LD',first_n=2**8,xlim=[-2,2],ylim=[2,4])\n</pre> ld = qmcpy.Sobol(2)  #Sobol' points another_unif_ld = qmcpy.Uniform(ld, lower_bound=[-2,2], upper_bound=[2,4])  #define the desired probability distribution with sobol as input points = another_unif_ld.gen_samples(2**8) print(another_unif_ld) print(f'\\nUniform LD points with shape {points.shape}\\n'+str(points))  #these points have 15 significant digit precision but only three digits are shown plot_successive_points(another_unif_ld,'Uniform LD',first_n=2**8,xlim=[-2,2],ylim=[2,4]) <pre>Uniform (AbstractTrueMeasure)\n    lower_bound     [-2  2]\n    upper_bound     [2 4]\n\nUniform LD points with shape (256, 2)\n[[ 1.63211619e+00  3.73527085e+00]\n [-1.96458235e+00  2.76718358e+00]\n [ 9.09260722e-01  2.10405185e+00]\n [-7.49905356e-01  3.38671429e+00]\n [ 1.09655014e+00  2.74751185e+00]\n [-1.31071479e+00  3.77735132e+00]\n [ 3.18856288e-01  3.10066974e+00]\n [-2.58190328e-02  2.38125511e+00]\n [ 1.85902575e+00  2.35110377e+00]\n [-1.55013461e+00  3.13179771e+00]\n [ 5.74189117e-01  3.99793234e+00]\n [-7.72499888e-01  2.52790732e+00]\n [ 1.38496040e+00  3.35363327e+00]\n [-1.20972064e+00  2.13615638e+00]\n [ 1.70346353e-01  2.98483491e+00]\n [-4.86928113e-01  3.51664289e+00]\n [ 1.59319338e+00  2.83569329e+00]\n [-1.75352515e+00  3.67851923e+00]\n [ 8.70309343e-01  3.45102593e+00]\n [-5.38819580e-01  2.04362477e+00]\n [ 1.18259209e+00  3.83803777e+00]\n [-1.47465285e+00  2.68294084e+00]\n [ 4.04926810e-01  2.43774730e+00]\n [-1.89785667e-01  3.03241946e+00]\n [ 1.92921095e+00  3.18830328e+00]\n [-1.72992369e+00  2.28284011e+00]\n [ 6.44406709e-01  2.58858045e+00]\n [-9.52321360e-01  3.93337465e+00]\n [ 1.33015606e+00  2.20048515e+00]\n [-1.01455070e+00  3.29318905e+00]\n [ 1.15509619e-01  3.58513540e+00]\n [-2.91725784e-01  2.92810048e+00]\n [ 1.69674328e+00  2.02283906e+00]\n [-1.89601564e+00  3.49030058e+00]\n [ 9.81610664e-01  3.63771572e+00]\n [-6.73680637e-01  2.85592312e+00]\n [ 1.04382486e+00  3.02634606e+00]\n [-1.35944513e+00  2.49563675e+00]\n [ 2.58530228e-01  2.62559582e+00]\n [-8.23294418e-02  3.84563622e+00]\n [ 1.78281433e+00  3.87505519e+00]\n [-1.62246743e+00  2.59520034e+00]\n [ 5.05639523e-01  2.27578809e+00]\n [-8.37113668e-01  3.24521810e+00]\n [ 1.44145742e+00  2.88827178e+00]\n [-1.14941179e+00  3.60634367e+00]\n [ 2.19059476e-01  3.27338160e+00]\n [-4.34216234e-01  2.24073455e+00]\n [ 1.51690842e+00  3.42208176e+00]\n [-1.82581517e+00  2.07936273e+00]\n [ 8.01804452e-01  2.79132255e+00]\n [-6.03508819e-01  3.69837261e+00]\n [ 1.23904058e+00  2.43523923e+00]\n [-1.41426473e+00  3.09068726e+00]\n [ 4.53717300e-01  3.78885312e+00]\n [-1.37120398e-01  2.69407406e+00]\n [ 1.99388680e+00  2.53843444e+00]\n [-1.66143601e+00  3.94351623e+00]\n [ 7.16679522e-01  3.18480345e+00]\n [-8.76049791e-01  2.34014643e+00]\n [ 1.27750410e+00  3.54175642e+00]\n [-1.06332406e+00  2.94891535e+00]\n [ 5.51386166e-02  2.17250235e+00]\n [-3.48160972e-01  3.32991865e+00]\n [ 1.68443108e+00  2.66428490e+00]\n [-1.96890853e+00  3.82156387e+00]\n [ 8.98955486e-01  3.04917923e+00]\n [-6.91855546e-01  2.45623134e+00]\n [ 1.08720775e+00  3.66175516e+00]\n [-1.25561124e+00  2.81720496e+00]\n [ 3.72134031e-01  2.06227642e+00]\n [-3.30914651e-02  3.46749554e+00]\n [ 1.83223923e+00  3.31185587e+00]\n [-1.50856227e+00  2.21693948e+00]\n [ 6.09782398e-01  2.91132119e+00]\n [-7.93551486e-01  3.56666242e+00]\n [ 1.42346943e+00  2.29961511e+00]\n [-1.23176561e+00  3.20677197e+00]\n [ 1.46475575e-01  3.91470354e+00]\n [-4.46349142e-01  2.57212184e+00]\n [ 1.59862091e+00  3.76507331e+00]\n [-1.80474448e+00  2.73253306e+00]\n [ 8.13112855e-01  2.39554271e+00]\n [-5.27659039e-01  3.11375194e+00]\n [ 1.12638706e+00  2.75289168e+00]\n [-1.46640614e+00  3.72218437e+00]\n [ 4.11345801e-01  3.39898800e+00]\n [-2.43918834e-01  2.11902634e+00]\n [ 1.88678700e+00  2.14844526e+00]\n [-1.70399443e+00  3.36859247e+00]\n [ 6.64358817e-01  3.50233576e+00]\n [-9.89012298e-01  2.97176378e+00]\n [ 1.35305252e+00  3.14610054e+00]\n [-1.05220258e+00  2.36417062e+00]\n [ 7.60300175e-02  2.51561415e+00]\n [-2.66757468e-01  3.98296885e+00]\n [ 1.73720513e+00  3.07783324e+00]\n [-1.92001307e+00  2.42066099e+00]\n [ 9.59692714e-01  2.70882325e+00]\n [-6.35054132e-01  3.80142033e+00]\n [ 1.02289814e+00  2.06461688e+00]\n [-1.32373269e+00  3.40951789e+00]\n [ 2.99983303e-01  3.71122997e+00]\n [-1.09240950e-01  2.80590412e+00]\n [ 1.77542454e+00  2.96178788e+00]\n [-1.56931658e+00  3.55632271e+00]\n [ 5.60869853e-01  3.31515758e+00]\n [-8.46338810e-01  2.15995383e+00]\n [ 1.49963234e+00  3.95828065e+00]\n [-1.15959764e+00  2.55098630e+00]\n [ 2.14614269e-01  2.32727723e+00]\n [-3.82026098e-01  3.17024050e+00]\n [ 1.54175770e+00  2.47744239e+00]\n [-1.86541953e+00  3.00935717e+00]\n [ 7.64277676e-01  3.86181997e+00]\n [-5.80492975e-01  2.64448040e+00]\n [ 1.20247642e+00  3.47412016e+00]\n [-1.39419538e+00  2.00395782e+00]\n [ 4.79529198e-01  2.87412082e+00]\n [-1.79671249e-01  3.65470795e+00]\n [ 1.93963436e+00  3.62455655e+00]\n [-1.65514201e+00  2.90524872e+00]\n [ 7.25051103e-01  2.22453890e+00]\n [-9.32135663e-01  3.25451570e+00]\n [ 1.28879166e+00  2.61139932e+00]\n [-1.12040307e+00  3.89392442e+00]\n [ 3.80216143e-03  3.22700857e+00]\n [-3.42860104e-01  2.25881449e+00]\n [ 1.64846946e+00  2.25745798e+00]\n [-1.94236157e+00  3.22540792e+00]\n [ 9.33914805e-01  3.90407310e+00]\n [-7.19383823e-01  2.62179213e+00]\n [ 1.12267729e+00  3.25998869e+00]\n [-1.28264265e+00  2.22976775e+00]\n [ 3.37659195e-01  2.89097689e+00]\n [-5.07107836e-03  3.61052886e+00]\n [ 1.86025008e+00  3.64141262e+00]\n [-1.54305808e+00  2.86106964e+00]\n [ 5.82737638e-01  2.01040743e+00]\n [-7.58099114e-01  3.48032563e+00]\n [ 1.39594306e+00  2.65365246e+00]\n [-1.19677768e+00  3.87123616e+00]\n [ 1.73028253e-01  3.00702416e+00]\n [-4.82285964e-01  2.47486523e+00]\n [ 1.56267932e+00  3.15718932e+00]\n [-1.77818702e+00  2.31398191e+00]\n [ 8.48096027e-01  2.55719177e+00]\n [-5.55180645e-01  3.96473026e+00]\n [ 1.16183658e+00  2.16937002e+00]\n [-1.49344805e+00  3.32432963e+00]\n [ 3.76847111e-01  3.55374555e+00]\n [-2.15905118e-01  2.95945487e+00]\n [ 1.91480262e+00  2.80430347e+00]\n [-1.73846451e+00  3.70987346e+00]\n [ 6.37322628e-01  3.41991071e+00]\n [-9.53537988e-01  2.07476556e+00]\n [ 1.32552137e+00  3.80664918e+00]\n [-1.01724039e+00  2.71429624e+00]\n [ 1.02574124e-01  2.40663330e+00]\n [-3.02716229e-01  3.06356141e+00]\n [ 1.70919046e+00  3.96891344e+00]\n [-1.88551344e+00  2.50131460e+00]\n [ 9.86733664e-01  2.36943290e+00]\n [-6.70502686e-01  3.15160697e+00]\n [ 1.05042069e+00  2.98213078e+00]\n [-1.35871681e+00  3.51245863e+00]\n [ 2.73426806e-01  3.36702716e+00]\n [-7.33003155e-02  2.14712409e+00]\n [ 1.81138234e+00  2.11648441e+00]\n [-1.59585973e+00  3.39669021e+00]\n [ 5.25906716e-01  3.73157487e+00]\n [-8.18806722e-01  2.76203804e+00]\n [ 1.46415899e+00  3.11999072e+00]\n [-1.13256241e+00  2.40202564e+00]\n [ 2.49085294e-01  2.71945429e+00]\n [-4.10042667e-01  3.75175039e+00]\n [ 1.51373826e+00  2.56982405e+00]\n [-1.83094564e+00  3.91216161e+00]\n [ 7.91310047e-01  3.21591834e+00]\n [-6.15963474e-01  2.30900561e+00]\n [ 1.23000375e+00  3.57314535e+00]\n [-1.42915375e+00  2.91755998e+00]\n [ 4.52981281e-01  2.20361656e+00]\n [-1.43708670e-01  3.29877709e+00]\n [ 1.97557215e+00  3.45319600e+00]\n [-1.68169565e+00  2.04822102e+00]\n [ 6.90064120e-01  2.82271138e+00]\n [-9.04610239e-01  3.66701745e+00]\n [ 1.25333832e+00  2.46635421e+00]\n [-1.09335735e+00  3.05954623e+00]\n [ 3.82970322e-02  3.82024270e+00]\n [-3.70870007e-01  2.66271959e+00]\n [ 1.65976941e+00  3.34321421e+00]\n [-1.99942245e+00  2.18555377e+00]\n [ 8.82594568e-01  2.94246598e+00]\n [-7.14068659e-01  3.53555119e+00]\n [ 1.06841247e+00  2.33097414e+00]\n [-1.27636677e+00  3.17538702e+00]\n [ 3.46014554e-01  3.94584901e+00]\n [-6.11712500e-02  2.54101136e+00]\n [ 1.82369833e+00  2.69543034e+00]\n [-1.52297062e+00  3.79045353e+00]\n [ 6.08565743e-01  3.08053835e+00]\n [-8.00635651e-01  2.42484617e+00]\n [ 1.42077994e+00  3.69289986e+00]\n [-1.23640015e+00  2.78609394e+00]\n [ 1.35485274e-01  2.09363479e+00]\n [-4.59284431e-01  3.43610969e+00]\n [ 1.62084184e+00  2.24233496e+00]\n [-1.78839100e+00  3.27473788e+00]\n [ 8.43634601e-01  3.59595062e+00]\n [-5.03004805e-01  2.87812287e+00]\n [ 1.15445917e+00  3.23998949e+00]\n [-1.44027908e+00  2.27031534e+00]\n [ 4.32093663e-01  2.60922826e+00]\n [-2.25115961e-01  3.88932725e+00]\n [ 1.89386349e+00  3.85868764e+00]\n [-1.70277019e+00  2.63889138e+00]\n [ 6.78759497e-01  2.48943152e+00]\n [-9.80463810e-01  3.01989669e+00]\n [ 1.36599563e+00  2.84650669e+00]\n [-1.04121972e+00  3.62854343e+00]\n [ 8.06723779e-02  3.49287749e+00]\n [-2.64075414e-01  2.02517184e+00]\n [ 1.73207452e+00  2.93064218e+00]\n [-1.92318343e+00  3.58743295e+00]\n [ 9.47237855e-01  3.28379830e+00]\n [-6.45548684e-01  2.19133855e+00]\n [ 1.00800914e+00  3.92713610e+00]\n [-1.33276944e+00  2.58209776e+00]\n [ 2.93395118e-01  2.29591913e+00]\n [-1.09976944e-01  3.20162644e+00]\n [ 1.75516493e+00  3.04647510e+00]\n [-1.58763115e+00  2.45204708e+00]\n [ 5.32309489e-01  2.67767879e+00]\n [-8.72954185e-01  3.83253158e+00]\n [ 1.46959891e+00  2.03325752e+00]\n [-1.18376362e+00  3.44090282e+00]\n [ 1.91905028e-01  3.68008429e+00]\n [-3.98867827e-01  2.83701422e+00]\n [ 1.55225969e+00  3.53094266e+00]\n [-1.85297249e+00  2.99889055e+00]\n [ 7.67455475e-01  2.13065019e+00]\n [-5.75370188e-01  3.34837122e+00]\n [ 1.20320483e+00  2.51778422e+00]\n [-1.38759952e+00  3.98756510e+00]\n [ 4.88558359e-01  3.13311863e+00]\n [-1.64774579e-01  2.35266884e+00]\n [ 1.96624215e+00  2.38355266e+00]\n [-1.62657397e+00  3.10321143e+00]\n [ 7.43358081e-01  3.76820472e+00]\n [-9.11868376e-01  2.73812111e+00]\n [ 1.30564082e+00  3.38023160e+00]\n [-1.09770165e+00  2.09781330e+00]\n [ 2.79755744e-02  2.78050674e+00]\n [-3.12834497e-01  3.74834986e+00]]\n</pre> In\u00a0[11]: Copied! <pre>ld = qmcpy.Lattice(2)\ngaussian_ld = qmcpy.Gaussian(ld, mean=[3,2], covariance=[[9,5], [5,4]])  #specify the desired mean and covariance of your multivariate Gaussian distribution\npoints = gaussian_ld.gen_samples(2**8)\nprint(gaussian_ld)\nprint(f'\\nGaussian LD Points with shape {points.shape}\\n'+str(points))  #these points have 15 significant digit precision but only three digits are shown\nplot_successive_points(gaussian_ld,'Gaussian LD',first_n=2**8,xlim=[-6,12],ylim=[-2,6])\n</pre> ld = qmcpy.Lattice(2) gaussian_ld = qmcpy.Gaussian(ld, mean=[3,2], covariance=[[9,5], [5,4]])  #specify the desired mean and covariance of your multivariate Gaussian distribution points = gaussian_ld.gen_samples(2**8) print(gaussian_ld) print(f'\\nGaussian LD Points with shape {points.shape}\\n'+str(points))  #these points have 15 significant digit precision but only three digits are shown plot_successive_points(gaussian_ld,'Gaussian LD',first_n=2**8,xlim=[-6,12],ylim=[-2,6]) <pre>Gaussian (AbstractTrueMeasure)\n    mean            [3 2]\n    covariance      [[9 5]\n                     [5 4]]\n    decomp_type     PCA\n\nGaussian LD Points with shape (256, 2)\n[[ 9.80784089e-01 -6.70135637e-01]\n [ 4.29201337e+00  3.08939243e+00]\n [ 2.70351262e+00  1.39177419e+00]\n [ 6.94591370e+00  5.61220091e+00]\n [ 1.22732660e+00  1.58259254e+00]\n [ 5.96795261e+00  2.99596433e+00]\n [ 2.44027019e+00  3.89861289e+00]\n [-1.57840943e+00 -8.93333455e-01]\n [ 1.08644957e+00  5.76156485e-01]\n [ 4.28113054e+00  4.32931865e+00]\n [ 2.76553638e+00  2.33245354e+00]\n [ 1.16726000e+01  6.26695439e+00]\n [ 2.91426798e+00 -1.81254438e-02]\n [ 6.31198394e+00  4.15908938e+00]\n [ 4.19815770e+00  2.11946301e+00]\n [-8.47601153e-01  5.25690347e-01]\n [ 3.23117324e-01  1.13251342e+00]\n [ 5.01299750e+00  2.51783050e+00]\n [ 3.94190142e+00 -8.97084397e-02]\n [ 8.38405732e+00  5.35168099e+00]\n [ 1.93590871e+00  1.01037649e+00]\n [ 5.35428481e+00  4.79291114e+00]\n [ 3.50884070e+00  2.69727756e+00]\n [-3.92563166e-01 -1.34090316e+00]\n [ 1.99818505e+00 -2.64924500e-01]\n [ 5.17839575e+00  3.54721126e+00]\n [ 3.44389373e+00  1.75332272e+00]\n [-3.09434454e+00 -7.35422900e-01]\n [ 2.01945669e+00  1.96944279e+00]\n [ 7.27802715e+00  3.68449383e+00]\n [ 3.36065324e+00  4.02606972e+00]\n [ 4.54772993e-02  2.20186863e-02]\n [-2.28034432e-01  1.66370309e+00]\n [ 4.64142815e+00  2.81801881e+00]\n [ 3.08866057e+00  1.03037222e+00]\n [ 7.63842422e+00  5.39260651e+00]\n [ 1.59034291e+00  1.28519127e+00]\n [ 6.56705459e+00  2.41716598e+00]\n [ 3.12967941e+00  3.04594991e+00]\n [-9.96559899e-01 -1.04047120e+00]\n [ 1.47195482e+00  2.74291161e-01]\n [ 4.78855644e+00  3.83706943e+00]\n [ 3.10372116e+00  2.04433326e+00]\n [-4.83428748e+00 -8.83988370e-02]\n [ 1.59676521e+00  2.38507554e+00]\n [ 6.76249366e+00  3.94719157e+00]\n [ 4.63717928e+00  1.68283168e+00]\n [-3.75332096e-01  2.50487752e-01]\n [ 7.19791850e-01  8.36442410e-01]\n [ 5.50904053e+00  2.02427292e+00]\n [ 2.37115736e+00  2.71142598e+00]\n [ 9.38353714e+00  5.47267375e+00]\n [ 2.31380818e+00  6.78318397e-01]\n [ 5.86980213e+00  4.39978668e+00]\n [ 3.84776228e+00  2.41617606e+00]\n [-1.57086820e+00  1.16086154e+00]\n [ 7.57617121e-01  2.04616706e+00]\n [ 5.55504055e+00  3.29274496e+00]\n [ 3.84864655e+00  1.35854687e+00]\n [-2.23685566e+00 -8.06858572e-01]\n [ 2.36515462e+00  1.67575215e+00]\n [ 8.12666627e+00  2.95547973e+00]\n [ 3.91586794e+00  3.41017200e+00]\n [ 4.67532378e-01 -2.38064730e-01]\n [ 1.50066305e+00 -1.32000733e+00]\n [ 4.46684609e+00  2.95268125e+00]\n [ 2.88485110e+00  1.22944002e+00]\n [ 7.29676633e+00  5.47080847e+00]\n [ 1.41414900e+00  1.42604782e+00]\n [ 6.21779887e+00  2.78437733e+00]\n [ 2.88390396e+00  3.31198873e+00]\n [-1.28308860e+00 -9.56198304e-01]\n [ 1.27325597e+00  4.36135487e-01]\n [ 4.56530836e+00  4.03249049e+00]\n [ 2.93710416e+00  2.18437228e+00]\n [-3.91402699e+00 -3.85694469e+00]\n [ 1.25186889e+00  2.80783663e+00]\n [ 6.53317939e+00  4.05377234e+00]\n [ 4.39651028e+00  1.93491215e+00]\n [-6.00491974e-01  3.75421729e-01]\n [ 5.28836279e-01  9.74250689e-01]\n [ 5.23007762e+00  2.32004358e+00]\n [ 2.09144320e+00  3.03428837e+00]\n [ 8.83218247e+00  5.38570864e+00]\n [ 2.11653447e+00  8.58389504e-01]\n [ 5.63166058e+00  4.56081781e+00]\n [ 3.67960417e+00  2.55428135e+00]\n [ 2.44456679e-02 -1.70939455e+00]\n [ 2.40267088e-01  2.73233648e+00]\n [ 5.36542734e+00  3.42017714e+00]\n [ 3.63105061e+00  1.58044418e+00]\n [-2.62790196e+00 -7.73512510e-01]\n [ 2.19607272e+00  1.81680154e+00]\n [ 7.60289295e+00  3.47033475e+00]\n [ 3.68784258e+00  3.63726842e+00]\n [ 2.53096150e-01 -9.91643665e-02]\n [ 8.71882533e-02  1.33595653e+00]\n [ 4.82112334e+00  2.67683927e+00]\n [ 3.34744682e+00  7.42028224e-01]\n [ 7.99446374e+00  5.35498027e+00]\n [ 1.76250063e+00  1.14945454e+00]\n [ 4.91904817e+00  5.28729484e+00]\n [ 3.32911627e+00  2.85538786e+00]\n [-7.06939793e-01 -1.15867333e+00]\n [ 1.69770894e+00  6.62606218e-02]\n [ 4.98849897e+00  3.68238565e+00]\n [ 3.27079641e+00  1.90363422e+00]\n [-3.70119871e+00 -6.58423391e-01]\n [ 1.82629835e+00  2.14819152e+00]\n [ 7.00694737e+00  3.82962630e+00]\n [ 5.03330779e+00  1.18022534e+00]\n [-1.62133607e-01  1.35816015e-01]\n [ 9.03807980e-01  7.06671980e-01]\n [ 6.48347952e+00  6.05492508e-01]\n [ 2.58162056e+00  2.50049634e+00]\n [ 1.01484810e+01  5.66443364e+00]\n [ 2.54910402e+00  4.35658204e-01]\n [ 6.09307251e+00  4.27122471e+00]\n [ 4.01857154e+00  2.27451894e+00]\n [-1.13898675e+00  7.36126324e-01]\n [ 1.01838301e+00  1.77331681e+00]\n [ 5.75302301e+00  3.15585439e+00]\n [ 4.15171717e+00  9.98632717e-01]\n [-1.89251109e+00 -8.45125326e-01]\n [ 2.53253640e+00  1.53691311e+00]\n [ 6.52451339e+00  5.90564465e+00]\n [ 4.11120689e+00  3.23730652e+00]\n [ 7.00349184e-01 -4.12794172e-01]\n [ 3.12903043e-01  5.06784577e-01]\n [ 5.23701863e+00  1.63323025e+00]\n [ 2.10370540e+00  2.42787169e+00]\n [ 7.81321429e+00  4.41672992e+00]\n [ 2.01401963e+00  3.82761606e-01]\n [ 5.39515676e+00  4.01847859e+00]\n [ 3.55624162e+00  2.15847632e+00]\n [-2.26869882e+00  4.36029399e-01]\n [ 4.45694705e-01  1.69437255e+00]\n [ 5.16895788e+00  2.97393410e+00]\n [ 3.59252940e+00  1.05911050e+00]\n [-6.23113932e+00 -3.35709699e+00]\n [ 2.06985359e+00  1.41634830e+00]\n [ 5.04691692e+00  6.33090977e+00]\n [ 3.61755087e+00  3.12698675e+00]\n [-4.38881857e-02 -6.48615372e-01]\n [ 1.10896483e+00 -5.12705353e-02]\n [ 4.43661688e+00  3.52690279e+00]\n [ 2.81934141e+00  1.79160221e+00]\n [ 7.64277320e+00  6.91022546e+00]\n [ 1.31890045e+00  2.07900114e+00]\n [ 6.20664465e+00  3.52121851e+00]\n [ 4.38023257e+00  1.35390774e+00]\n [-9.94371673e-01 -2.11710203e-01]\n [ 1.26109624e+00  1.00440968e+00]\n [ 6.26466286e+00  1.87693426e+00]\n [ 2.85587338e+00  2.76970758e+00]\n [-2.16989369e+00 -1.85498381e+00]\n [ 2.81967871e+00  7.41299630e-01]\n [ 6.72004106e+00  4.74064806e+00]\n [ 4.32142525e+00  2.54165766e+00]\n [-6.25766108e-01  1.21636543e+00]\n [ 6.98113320e-01  2.56831018e-01]\n [ 3.97847564e+00  3.96525834e+00]\n [ 2.48328928e+00  2.07511098e+00]\n [ 8.53730344e+00  4.19817722e+00]\n [ 2.81632848e+00 -6.26914449e-01]\n [ 5.79718770e+00  3.76389093e+00]\n [ 3.90988176e+00  1.84955683e+00]\n [-1.53008891e+00  1.27647173e-03]\n [ 8.94860040e-01  1.28696485e+00]\n [ 5.57540068e+00  2.64795152e+00]\n [ 2.29902873e+00  3.41115686e+00]\n [-3.22882722e+00 -1.99053931e+00]\n [ 2.41449031e+00  1.12931673e+00]\n [ 6.18606224e+00  5.01705735e+00]\n [ 3.97887355e+00  2.81703294e+00]\n [ 5.49044003e-01 -1.12896364e+00]\n [ 1.72428793e+00 -7.04494923e-01]\n [ 4.80648617e+00  3.23968068e+00]\n [ 3.16414175e+00  1.49316535e+00]\n [ 9.04343526e+00  6.65396255e+00]\n [ 1.72445071e+00  1.70107574e+00]\n [ 6.68233403e+00  3.20032331e+00]\n [ 3.12924643e+00  3.64602194e+00]\n [-5.17875678e-01 -4.04146572e-01]\n [ 1.61627185e+00  7.31596185e-01]\n [ 4.93439318e+00  4.38816869e+00]\n [ 3.22120292e+00  2.43896543e+00]\n [-1.29967617e+00 -2.05532926e+00]\n [ 1.38198489e+00  3.33261337e+00]\n [ 7.23904439e+00  4.56714696e+00]\n [ 4.69376994e+00  2.22292626e+00]\n [-9.71168169e-02  7.77271632e-01]\n [ 5.05714388e-01  3.83725512e-01]\n [ 3.55065517e+00  4.50880565e+00]\n [ 2.30390827e+00  2.23482021e+00]\n [ 8.14586073e+00  4.32739486e+00]\n [ 2.27527539e+00  1.05013998e-01]\n [ 5.59813573e+00  3.88528002e+00]\n [ 3.72748544e+00  2.01280341e+00]\n [-1.85110408e+00  1.56525580e-01]\n [ 6.89612337e-01  1.46063091e+00]\n [ 5.36201580e+00  2.82592661e+00]\n [ 3.96851925e+00  5.80277017e-01]\n [-4.05584301e+00 -2.25960929e+00]\n [ 2.23951448e+00  1.27754109e+00]\n [ 5.85828948e+00  5.27641679e+00]\n [ 3.80395141e+00  2.96223566e+00]\n [ 2.18955674e-01 -8.29581008e-01]\n [ 1.35717483e+00 -2.80033648e-01]\n [ 4.62550037e+00  3.37579336e+00]\n [ 2.98772871e+00  1.64894810e+00]\n [ 8.34861301e+00  6.62139654e+00]\n [ 1.53567911e+00  1.86793380e+00]\n [ 6.43010523e+00  3.38005596e+00]\n [ 5.07249565e+00  3.67437441e-01]\n [-7.52399767e-01 -3.05940995e-01]\n [ 1.43733165e+00  8.71143648e-01]\n [ 4.59764615e+00  4.75166677e+00]\n [ 3.04633166e+00  2.59159720e+00]\n [-1.73719364e+00 -1.90314909e+00]\n [ 3.11675256e+00  3.93446854e-01]\n [ 6.97647726e+00  4.64733947e+00]\n [ 4.50007427e+00  2.39381590e+00]\n [-3.33246279e-01  9.54386555e-01]\n [ 8.96106436e-01  1.16728837e-01]\n [ 4.22937758e+00  3.70978664e+00]\n [ 2.65294752e+00  1.93083775e+00]\n [ 9.06609324e+00  3.92140543e+00]\n [ 1.02842147e+00  2.40812980e+00]\n [ 5.99815714e+00  3.64536435e+00]\n [ 4.11596980e+00  1.64850390e+00]\n [-1.25047317e+00 -1.13258988e-01]\n [ 1.08225390e+00  1.13985760e+00]\n [ 5.83394485e+00  2.40004612e+00]\n [ 2.63087555e+00  3.00391129e+00]\n [-2.64764975e+00 -1.88023932e+00]\n [ 2.60263516e+00  9.59032325e-01]\n [ 6.46121587e+00  4.85729863e+00]\n [ 4.14956870e+00  2.67972035e+00]\n [-1.22132384e+00  1.96165234e+00]\n [ 7.63974718e-02  2.12831078e+00]\n [ 4.98587165e+00  3.10853071e+00]\n [ 3.35827152e+00  1.30862368e+00]\n [ 1.00344906e+01  6.96091443e+00]\n [ 1.89998430e+00  1.55460747e+00]\n [ 7.00270854e+00  2.91919965e+00]\n [ 3.40598744e+00  3.33335666e+00]\n [-2.84451575e-01 -5.14498298e-01]\n [ 1.80459108e+00  5.75090634e-01]\n [ 5.17975110e+00  4.17678917e+00]\n [ 3.38925189e+00  2.29768250e+00]\n [-6.80226625e-01 -2.57026989e+00]\n [ 1.85459196e+00  2.69966047e+00]\n [ 7.51512076e+00  4.49278295e+00]\n [ 4.91960836e+00  2.00141816e+00]\n [ 1.14246901e-01  6.34410412e-01]]\n</pre> <p>Transformations of low discrepancy sequences often have good properties, but may not be low discrepancy themselves, depending on your definition of discrepancy as shown by Yiou Li and Lulu Kang.</p> <p>Pause for questions</p> In\u00a0[12]: Copied! <pre>d = 5  #coded as parameters so that\ntol = 1e-3  #you can change here and propagate them through this example\nlattice = qmcpy.Lattice(d)\ngaussian_lattice = qmcpy.Gaussian(lattice, mean = 0, covariance = 1/2)  #mean and covariance of the distribution identified above\n</pre> d = 5  #coded as parameters so that tol = 1e-3  #you can change here and propagate them through this example lattice = qmcpy.Lattice(d) gaussian_lattice = qmcpy.Gaussian(lattice, mean = 0, covariance = 1/2)  #mean and covariance of the distribution identified above In\u00a0[13]: Copied! <pre>keister = qmcpy.Keister(gaussian_lattice) #transform the original integrand to the eventual one\n</pre> keister = qmcpy.Keister(gaussian_lattice) #transform the original integrand to the eventual one In\u00a0[14]: Copied! <pre>keister_lattice_gauss_g = qmcpy.CubQMCLatticeG(keister, abs_tol = tol)  #using Tony's stopping criterion\n</pre> keister_lattice_gauss_g = qmcpy.CubQMCLatticeG(keister, abs_tol = tol)  #using Tony's stopping criterion <p>Invoking the <code>integrate</code> method returns the numerical solution and a data object. Printing the data object provides a neat summary of the integration problem. For details of the output fields, see the online, searchable QMCPy Documentation at https://qmcpy.readthedocs.io/.</p> In\u00a0[15]: Copied! <pre>solution, data = keister_lattice_gauss_g.integrate()\nprint(data)\n</pre> solution, data = keister_lattice_gauss_g.integrate() print(data) <pre>Data (Data)\n    solution        1.135\n    comb_bound_low  1.135\n    comb_bound_high 1.136\n    comb_bound_diff 0.001\n    comb_flags      1\n    n_total         2^(17)\n    n               2^(17)\n    time_integrate  0.042\nCubQMCLatticeG (AbstractStoppingCriterion)\n    abs_tol         0.001\n    rel_tol         0\n    n_init          2^(10)\n    n_limit         2^(30)\nKeister (AbstractIntegrand)\nGaussian (AbstractTrueMeasure)\n    mean            0\n    covariance      2^(-1)\n    decomp_type     PCA\n    transform       Gaussian (AbstractTrueMeasure)\n                        mean            0\n                        covariance      2^(-1)\n                        decomp_type     PCA\nLattice (AbstractLDDiscreteDistribution)\n    d               5\n    replications    1\n    randomize       SHIFT\n    gen_vec_source  kuo.lattice-33002-1024-1048576.9125.txt\n    order           RADICAL INVERSE\n    n_limit         2^(20)\n    entropy         38563684764170563855999902143753109741\n</pre> In\u00a0[16]: Copied! <pre>warnings.simplefilter(\"ignore\")\nkeister_lattice_gauss_g_small_n = qmcpy.CubQMCLatticeG(keister, abs_tol = 1e-6, n_limit = 2**12)  #the default n_max is 2**35\nsolution, data = keister_lattice_gauss_g_small_n.integrate()\nprint(data)\n</pre> warnings.simplefilter(\"ignore\") keister_lattice_gauss_g_small_n = qmcpy.CubQMCLatticeG(keister, abs_tol = 1e-6, n_limit = 2**12)  #the default n_max is 2**35 solution, data = keister_lattice_gauss_g_small_n.integrate() print(data) <pre>Data (Data)\n    solution        1.129\n    comb_bound_low  1.116\n    comb_bound_high 1.142\n    comb_bound_diff 0.026\n    comb_flags      0\n    n_total         2^(12)\n    n               2^(12)\n    time_integrate  0.003\nCubQMCLatticeG (AbstractStoppingCriterion)\n    abs_tol         1.00e-06\n    rel_tol         0\n    n_init          2^(10)\n    n_limit         2^(12)\nKeister (AbstractIntegrand)\nGaussian (AbstractTrueMeasure)\n    mean            0\n    covariance      2^(-1)\n    decomp_type     PCA\n    transform       Gaussian (AbstractTrueMeasure)\n                        mean            0\n                        covariance      2^(-1)\n                        decomp_type     PCA\nLattice (AbstractLDDiscreteDistribution)\n    d               5\n    replications    1\n    randomize       SHIFT\n    gen_vec_source  kuo.lattice-33002-1024-1048576.9125.txt\n    order           RADICAL INVERSE\n    n_limit         2^(20)\n    entropy         38563684764170563855999902143753109741\n</pre> In\u00a0[17]: Copied! <pre>d = 5; tol = 2.5e-3  #re-construct the example\n#d = 7; tol = 3e-3  #if you change the dimension\n#d = 10; tol = 3e-2  #you may also wish to change the tolerance, since the value of the integral changes\nld_keister = qmcpy.Keister(qmcpy.Gaussian(qmcpy.Lattice(d), mean = 0, covariance = 1/2))  #mean and covariance of the distribution identified above\niid_keister =  qmcpy.Keister(qmcpy.Gaussian(qmcpy.IIDStdUniform(d), mean = 0, covariance = 1/2))\n\nn_tol = 7\nii_iid = 2  #make this larger to reduce the time required\ntol_vec = [tol*2**(ii) for ii in range(n_tol)]  #initialize\nld_time = [0]*n_tol; ld_n = [0]*n_tol  #low discrepancy time and number of function values\niid_time = [0]*n_tol; iid_n = [0]*n_tol  #IID time and number of function values\nfor ii in range(n_tol):\n  solution, data = qmcpy.CubQMCLatticeG(ld_keister, abs_tol = tol_vec[ii]).integrate()\n  if ii == 0:\n    print(f'\\nKeister integral = {solution}\\n')\n  ld_time[ii] = data.time_integrate\n  ld_n[ii] = data.n_total\n  if ii &gt;= ii_iid:\n    solution, data = qmcpy.CubMCG(iid_keister, abs_tol = tol_vec[ii]).integrate()\n    iid_time[ii] = data.time_integrate\n    iid_n[ii] = data.n_total\nfig,ax = pyplot.subplots(nrows=1,ncols=2,figsize=(13,5.5))\nax[0].scatter(tol_vec[0:n_tol],ld_time[0:n_tol],color='b');\nax[0].plot(tol_vec[0:n_tol],[(ld_time[0]*tol_vec[0])/tol_vec[jj] for jj in range(n_tol)],color='b')\nax[0].scatter(tol_vec[ii_iid:n_tol],iid_time[ii_iid:n_tol],color='g');\nax[0].plot(tol_vec[ii_iid:n_tol],[(iid_time[ii_iid]*(tol_vec[ii_iid]**2))/(tol_vec[jj]**2) for jj in range(ii_iid,n_tol)],color='g')\nax[0].set_ylim([0.001,1000]); ax[0].set_ylabel('Time (s)')\nax[1].scatter(tol_vec[0:n_tol],ld_n[0:n_tol],color='b');\nax[1].plot(tol_vec[0:n_tol],[(ld_n[0]*tol_vec[0])/tol_vec[jj] for jj in range(n_tol)],color='b')\nax[1].scatter(tol_vec[ii_iid:n_tol],iid_n[ii_iid:n_tol],color='g');\nax[1].plot(tol_vec[ii_iid:n_tol],[(iid_n[ii_iid]*(tol_vec[ii_iid]**2))/(tol_vec[jj]**2) for jj in range(ii_iid,n_tol)],color='g')\nax[1].set_ylim([1e2,1e8]); ax[1].set_ylabel('n')\nfor ii in range(2):\n  ax[ii].set_xlim([tol,100*tol]); ax[ii].set_xlabel('Tolerance, '+r'$\\varepsilon$')\n  ax[ii].set_xscale('log'); ax[ii].set_yscale('log')\n  ax[ii].legend([r'$\\mathcal{O}(\\varepsilon^{-1})$',r'$\\mathcal{O}(\\varepsilon^{-2})$','LD','IID'],frameon=False)\n  ax[ii].set_aspect(0.35)\n</pre> d = 5; tol = 2.5e-3  #re-construct the example #d = 7; tol = 3e-3  #if you change the dimension #d = 10; tol = 3e-2  #you may also wish to change the tolerance, since the value of the integral changes ld_keister = qmcpy.Keister(qmcpy.Gaussian(qmcpy.Lattice(d), mean = 0, covariance = 1/2))  #mean and covariance of the distribution identified above iid_keister =  qmcpy.Keister(qmcpy.Gaussian(qmcpy.IIDStdUniform(d), mean = 0, covariance = 1/2))  n_tol = 7 ii_iid = 2  #make this larger to reduce the time required tol_vec = [tol*2**(ii) for ii in range(n_tol)]  #initialize ld_time = [0]*n_tol; ld_n = [0]*n_tol  #low discrepancy time and number of function values iid_time = [0]*n_tol; iid_n = [0]*n_tol  #IID time and number of function values for ii in range(n_tol):   solution, data = qmcpy.CubQMCLatticeG(ld_keister, abs_tol = tol_vec[ii]).integrate()   if ii == 0:     print(f'\\nKeister integral = {solution}\\n')   ld_time[ii] = data.time_integrate   ld_n[ii] = data.n_total   if ii &gt;= ii_iid:     solution, data = qmcpy.CubMCG(iid_keister, abs_tol = tol_vec[ii]).integrate()     iid_time[ii] = data.time_integrate     iid_n[ii] = data.n_total fig,ax = pyplot.subplots(nrows=1,ncols=2,figsize=(13,5.5)) ax[0].scatter(tol_vec[0:n_tol],ld_time[0:n_tol],color='b'); ax[0].plot(tol_vec[0:n_tol],[(ld_time[0]*tol_vec[0])/tol_vec[jj] for jj in range(n_tol)],color='b') ax[0].scatter(tol_vec[ii_iid:n_tol],iid_time[ii_iid:n_tol],color='g'); ax[0].plot(tol_vec[ii_iid:n_tol],[(iid_time[ii_iid]*(tol_vec[ii_iid]**2))/(tol_vec[jj]**2) for jj in range(ii_iid,n_tol)],color='g') ax[0].set_ylim([0.001,1000]); ax[0].set_ylabel('Time (s)') ax[1].scatter(tol_vec[0:n_tol],ld_n[0:n_tol],color='b'); ax[1].plot(tol_vec[0:n_tol],[(ld_n[0]*tol_vec[0])/tol_vec[jj] for jj in range(n_tol)],color='b') ax[1].scatter(tol_vec[ii_iid:n_tol],iid_n[ii_iid:n_tol],color='g'); ax[1].plot(tol_vec[ii_iid:n_tol],[(iid_n[ii_iid]*(tol_vec[ii_iid]**2))/(tol_vec[jj]**2) for jj in range(ii_iid,n_tol)],color='g') ax[1].set_ylim([1e2,1e8]); ax[1].set_ylabel('n') for ii in range(2):   ax[ii].set_xlim([tol,100*tol]); ax[ii].set_xlabel('Tolerance, '+r'$\\varepsilon$')   ax[ii].set_xscale('log'); ax[ii].set_yscale('log')   ax[ii].legend([r'$\\mathcal{O}(\\varepsilon^{-1})$',r'$\\mathcal{O}(\\varepsilon^{-2})$','LD','IID'],frameon=False)   ax[ii].set_aspect(0.35) <pre>Keister integral = 1.1356452963607024\n\n</pre> In\u00a0[18]: Copied! <pre>lattice_rep = qmcpy.Lattice(d,replications=15)\ngaussian_lattice_rep = qmcpy.Gaussian(lattice_rep, mean = 0, covariance = 1/2)\nkeister_rep = qmcpy.Keister(gaussian_lattice_rep)\nkeister_lattice_gauss_CLT = qmcpy.CubQMCCLT(keister_rep, abs_tol = tol)  #using a CLT stopping criterion with random replications\nsolution, data = keister_lattice_gauss_CLT.integrate()\nprint(data)\n</pre> lattice_rep = qmcpy.Lattice(d,replications=15) gaussian_lattice_rep = qmcpy.Gaussian(lattice_rep, mean = 0, covariance = 1/2) keister_rep = qmcpy.Keister(gaussian_lattice_rep) keister_lattice_gauss_CLT = qmcpy.CubQMCCLT(keister_rep, abs_tol = tol)  #using a CLT stopping criterion with random replications solution, data = keister_lattice_gauss_CLT.integrate() print(data) <pre>Data (Data)\n    solution        1.135\n    comb_bound_low  1.134\n    comb_bound_high 1.136\n    comb_bound_diff 0.002\n    comb_flags      1\n    n_total         122880\n    n               122880\n    n_rep           2^(13)\n    time_integrate  0.029\nCubQMCRepStudentT (AbstractStoppingCriterion)\n    inflate         1\n    alpha           0.010\n    abs_tol         0.003\n    rel_tol         0\n    n_init          2^(8)\n    n_limit         2^(30)\nKeister (AbstractIntegrand)\nGaussian (AbstractTrueMeasure)\n    mean            0\n    covariance      2^(-1)\n    decomp_type     PCA\n    transform       Gaussian (AbstractTrueMeasure)\n                        mean            0\n                        covariance      2^(-1)\n                        decomp_type     PCA\nLattice (AbstractLDDiscreteDistribution)\n    d               5\n    replications    15\n    randomize       SHIFT\n    gen_vec_source  kuo.lattice-33002-1024-1048576.9125.txt\n    order           RADICAL INVERSE\n    n_limit         2^(20)\n    entropy         275034244784380212109446453686580990022\n</pre> <p>This answer agrees with the one above.</p> In\u00a0[19]: Copied! <pre>def my_Keister(x):  #this could be a functional of a solution to a PDE with random coefficients\n                    #or anything that you would like\n    \"\"\"\n    x: nxd numpy ndarray\n       n samples\n       d dimensions\n\n    returns n-vector of the Kesiter function\n    evaluated at the n input samples\n    \"\"\"\n    d = x.shape[1]\n    norm = np.sqrt((x**2).sum(1))\n    k = np.cos(norm)*np.exp(-norm**2)\n    return k  #size n vector\n\nld = qmcpy.Sobol(d)  #choose the LD points\nlebesgue = qmcpy.Lebesgue(qmcpy.Gaussian(ld))  #now choose the Lebesgue distribution\nf = qmcpy.CustomFun(lebesgue, g=my_Keister)\nkeister_lebesgue_ld_g = qmcpy.CubQMCSobolG(f, abs_tol = tol)  #the stopping criterion does need to match the LD points\nsolution, data = keister_lebesgue_ld_g.integrate()\nprint(data)\n</pre> def my_Keister(x):  #this could be a functional of a solution to a PDE with random coefficients                     #or anything that you would like     \"\"\"     x: nxd numpy ndarray        n samples        d dimensions      returns n-vector of the Kesiter function     evaluated at the n input samples     \"\"\"     d = x.shape[1]     norm = np.sqrt((x**2).sum(1))     k = np.cos(norm)*np.exp(-norm**2)     return k  #size n vector  ld = qmcpy.Sobol(d)  #choose the LD points lebesgue = qmcpy.Lebesgue(qmcpy.Gaussian(ld))  #now choose the Lebesgue distribution f = qmcpy.CustomFun(lebesgue, g=my_Keister) keister_lebesgue_ld_g = qmcpy.CubQMCSobolG(f, abs_tol = tol)  #the stopping criterion does need to match the LD points solution, data = keister_lebesgue_ld_g.integrate() print(data) <pre>Data (Data)\n    solution        1.136\n    comb_bound_low  1.133\n    comb_bound_high 1.138\n    comb_bound_diff 0.004\n    comb_flags      1\n    n_total         2^(16)\n    n               2^(16)\n    time_integrate  0.030\nCubQMCNetG (AbstractStoppingCriterion)\n    abs_tol         0.003\n    rel_tol         0\n    n_init          2^(10)\n    n_limit         2^(35)\nCustomFun (AbstractIntegrand)\nLebesgue (AbstractTrueMeasure)\n    transform       Gaussian (AbstractTrueMeasure)\n                        mean            0\n                        covariance      1\n                        decomp_type     PCA\nDigitalNetB2 (AbstractLDDiscreteDistribution)\n    d               5\n    replications    1\n    randomize       LMS DS\n    gen_mats_source joe_kuo.6.21201.txt\n    order           RADICAL INVERSE\n    t               63\n    alpha           1\n    n_limit         2^(32)\n    entropy         91580573475794728146679265749810645603\n</pre> <p>This answer agrees with the answers above for the same integration problem.</p> <p>The initial <code>DiscreteDistribution</code> does not need to mimic the standard uniform distribution as this next example shows.</p> In\u00a0[20]: Copied! <pre>iid = qmcpy.IIDStdUniform(d)  #choose the LD points\nlebesgue = qmcpy.Lebesgue(qmcpy.Gaussian(iid))  #now choose the Lebesgue distribution\nf = qmcpy.CustomFun(lebesgue, g=my_Keister)\nkeister_lebesgue_ld_g = qmcpy.CubMCCLT(f, abs_tol = 10*tol)  #the stopping criterion does need to match the points\nsolution, data = keister_lebesgue_ld_g.integrate()\nprint(data)\n</pre> iid = qmcpy.IIDStdUniform(d)  #choose the LD points lebesgue = qmcpy.Lebesgue(qmcpy.Gaussian(iid))  #now choose the Lebesgue distribution f = qmcpy.CustomFun(lebesgue, g=my_Keister) keister_lebesgue_ld_g = qmcpy.CubMCCLT(f, abs_tol = 10*tol)  #the stopping criterion does need to match the points solution, data = keister_lebesgue_ld_g.integrate() print(data) <pre>Data (Data)\n    solution        1.127\n    bound_low       1.099\n    bound_high      1.155\n    bound_diff      0.056\n    n_total         1694184\n    time_integrate  0.361\nCubMCCLT (AbstractStoppingCriterion)\n    abs_tol         0.025\n    rel_tol         0\n    n_init          2^(10)\n    n_limit         2^(30)\n    inflate         1.200\n    alpha           0.010\nCustomFun (AbstractIntegrand)\nLebesgue (AbstractTrueMeasure)\n    transform       Gaussian (AbstractTrueMeasure)\n                        mean            0\n                        covariance      1\n                        decomp_type     PCA\nIIDStdUniform (AbstractIIDDiscreteDistribution)\n    d               5\n    replications    1\n    entropy         110067297374675390351789060258764096093\n</pre> In\u00a0[21]: Copied! <pre>abs_tol = .05  #a nickel\nn_time_steps = 64  #being a power of 2 will help for multi-level\n\noptions = {  #there should be nothing magic about these choices\n    'interest_rate': .05,\n    'volatility': .5,\n    'start_price': 30,\n    'strike_price': 30\n}\n</pre> abs_tol = .05  #a nickel n_time_steps = 64  #being a power of 2 will help for multi-level  options = {  #there should be nothing magic about these choices     'interest_rate': .05,     'volatility': .5,     'start_price': 30,     'strike_price': 30 } In\u00a0[22]: Copied! <pre>iidBrownian = qmcpy.BrownianMotion(qmcpy.IIDStdUniform(n_time_steps))\npayoff = qmcpy.AsianOption(iidBrownian, **options)\nIIDstop = qmcpy.CubMCG(payoff,abs_tol=abs_tol)\nprice,data = IIDstop.integrate()\nprint(data)\n</pre> iidBrownian = qmcpy.BrownianMotion(qmcpy.IIDStdUniform(n_time_steps)) payoff = qmcpy.AsianOption(iidBrownian, **options) IIDstop = qmcpy.CubMCG(payoff,abs_tol=abs_tol) price,data = IIDstop.integrate() print(data) <pre>Data (Data)\n    solution        3.686\n    bound_low       3.636\n    bound_high      3.736\n    bound_diff      0.100\n    n_total         225848\n    time_integrate  0.834\nCubMCG (AbstractStoppingCriterion)\n    abs_tol         0.050\n    rel_tol         0\n    n_init          2^(10)\n    n_limit         2^(30)\n    inflate         1.200\n    alpha           0.010\n    kurtmax         1.478\nAsianOption (AbstractIntegrand)\n    option          ASIAN\n    call_put        CALL\n    volatility      2^(-1)\n    start_price     30\n    strike_price    30\n    interest_rate   0.050\n    t_final         1\n    asian_mean      ARITHMETIC\nBrownianMotion (AbstractTrueMeasure)\n    time_vec        [0.016 0.031 0.047 ... 0.969 0.984 1.   ]\n    drift           0\n    mean            [0. 0. 0. ... 0. 0. 0.]\n    covariance      [[0.016 0.016 0.016 ... 0.016 0.016 0.016]\n                     [0.016 0.031 0.031 ... 0.031 0.031 0.031]\n                     [0.016 0.031 0.047 ... 0.047 0.047 0.047]\n                     ...\n                     [0.016 0.031 0.047 ... 0.969 0.969 0.969]\n                     [0.016 0.031 0.047 ... 0.969 0.984 0.984]\n                     [0.016 0.031 0.047 ... 0.969 0.984 1.   ]]\n    decomp_type     PCA\n    transform       BrownianMotion (AbstractTrueMeasure)\n                        time_vec        [0.016 0.031 0.047 ... 0.969 0.984 1.   ]\n                        drift           0\n                        mean            [0. 0. 0. ... 0. 0. 0.]\n                        covariance      [[0.016 0.016 0.016 ... 0.016 0.016 0.016]\n                                         [0.016 0.031 0.031 ... 0.031 0.031 0.031]\n                                         [0.016 0.031 0.047 ... 0.047 0.047 0.047]\n                                         ...\n                                         [0.016 0.031 0.047 ... 0.969 0.969 0.969]\n                                         [0.016 0.031 0.047 ... 0.969 0.984 0.984]\n                                         [0.016 0.031 0.047 ... 0.969 0.984 1.   ]]\n                        decomp_type     PCA\nIIDStdUniform (AbstractIIDDiscreteDistribution)\n    d               2^(6)\n    replications    1\n    entropy         14445467058155448935378672762555839489\n</pre> In\u00a0[23]: Copied! <pre>giles_MLMC_stop = qmcpy.CubMCML(payoff,abs_tol=abs_tol)\nsolution,data = giles_MLMC_stop.integrate()\nprint(data)\n</pre> giles_MLMC_stop = qmcpy.CubMCML(payoff,abs_tol=abs_tol) solution,data = giles_MLMC_stop.integrate() print(data) <pre>Data (Data)\n    solution        3.681\n    n_total         430326\n    levels          3\n    n_level         [341502  61469  27319]\n    mean_level      [3.589 0.071 0.021]\n    var_level       [39.742  2.575  0.665]\n    cost_per_sample [2. 4. 8.]\n    alpha           1.769\n    beta            1.953\n    gamma           1.000\n    time_integrate  0.077\nCubMLMC (AbstractStoppingCriterion)\n    rmse_tol        0.019\n    n_init          2^(8)\n    levels_min      2^(1)\n    levels_max      10\n    theta           2^(-1)\nAsianOption (AbstractIntegrand)\n    option          ASIAN\n    call_put        CALL\n    volatility      2^(-1)\n    start_price     30\n    strike_price    30\n    interest_rate   0.050\n    t_final         1\n    asian_mean      ARITHMETIC\nBrownianMotion (AbstractTrueMeasure)\n    time_vec        [0.016 0.031 0.047 ... 0.969 0.984 1.   ]\n    drift           0\n    mean            [0. 0. 0. ... 0. 0. 0.]\n    covariance      [[0.016 0.016 0.016 ... 0.016 0.016 0.016]\n                     [0.016 0.031 0.031 ... 0.031 0.031 0.031]\n                     [0.016 0.031 0.047 ... 0.047 0.047 0.047]\n                     ...\n                     [0.016 0.031 0.047 ... 0.969 0.969 0.969]\n                     [0.016 0.031 0.047 ... 0.969 0.984 0.984]\n                     [0.016 0.031 0.047 ... 0.969 0.984 1.   ]]\n    decomp_type     PCA\n    transform       BrownianMotion (AbstractTrueMeasure)\n                        time_vec        [0.016 0.031 0.047 ... 0.969 0.984 1.   ]\n                        drift           0\n                        mean            [0. 0. 0. ... 0. 0. 0.]\n                        covariance      [[0.016 0.016 0.016 ... 0.016 0.016 0.016]\n                                         [0.016 0.031 0.031 ... 0.031 0.031 0.031]\n                                         [0.016 0.031 0.047 ... 0.047 0.047 0.047]\n                                         ...\n                                         [0.016 0.031 0.047 ... 0.969 0.969 0.969]\n                                         [0.016 0.031 0.047 ... 0.969 0.984 0.984]\n                                         [0.016 0.031 0.047 ... 0.969 0.984 1.   ]]\n                        decomp_type     PCA\nIIDStdUniform (AbstractIIDDiscreteDistribution)\n    d               2^(6)\n    replications    1\n    entropy         14445467058155448935378672762555839489\n</pre> In\u00a0[24]: Copied! <pre>sobol_brownian = qmcpy.BrownianMotion(qmcpy.Sobol(n_time_steps))\nintegrand = qmcpy.AsianOption(sobol_brownian, **options)\nstopping_criterion = qmcpy.CubQMCSobolG(integrand,abs_tol = abs_tol)\nsolution,data = stopping_criterion.integrate()\nprint(data)\n</pre> sobol_brownian = qmcpy.BrownianMotion(qmcpy.Sobol(n_time_steps)) integrand = qmcpy.AsianOption(sobol_brownian, **options) stopping_criterion = qmcpy.CubQMCSobolG(integrand,abs_tol = abs_tol) solution,data = stopping_criterion.integrate() print(data) <pre>Data (Data)\n    solution        3.697\n    comb_bound_low  3.672\n    comb_bound_high 3.721\n    comb_bound_diff 0.049\n    comb_flags      1\n    n_total         2^(10)\n    n               2^(10)\n    time_integrate  0.003\nCubQMCNetG (AbstractStoppingCriterion)\n    abs_tol         0.050\n    rel_tol         0\n    n_init          2^(10)\n    n_limit         2^(35)\nAsianOption (AbstractIntegrand)\n    option          ASIAN\n    call_put        CALL\n    volatility      2^(-1)\n    start_price     30\n    strike_price    30\n    interest_rate   0.050\n    t_final         1\n    asian_mean      ARITHMETIC\nBrownianMotion (AbstractTrueMeasure)\n    time_vec        [0.016 0.031 0.047 ... 0.969 0.984 1.   ]\n    drift           0\n    mean            [0. 0. 0. ... 0. 0. 0.]\n    covariance      [[0.016 0.016 0.016 ... 0.016 0.016 0.016]\n                     [0.016 0.031 0.031 ... 0.031 0.031 0.031]\n                     [0.016 0.031 0.047 ... 0.047 0.047 0.047]\n                     ...\n                     [0.016 0.031 0.047 ... 0.969 0.969 0.969]\n                     [0.016 0.031 0.047 ... 0.969 0.984 0.984]\n                     [0.016 0.031 0.047 ... 0.969 0.984 1.   ]]\n    decomp_type     PCA\n    transform       BrownianMotion (AbstractTrueMeasure)\n                        time_vec        [0.016 0.031 0.047 ... 0.969 0.984 1.   ]\n                        drift           0\n                        mean            [0. 0. 0. ... 0. 0. 0.]\n                        covariance      [[0.016 0.016 0.016 ... 0.016 0.016 0.016]\n                                         [0.016 0.031 0.031 ... 0.031 0.031 0.031]\n                                         [0.016 0.031 0.047 ... 0.047 0.047 0.047]\n                                         ...\n                                         [0.016 0.031 0.047 ... 0.969 0.969 0.969]\n                                         [0.016 0.031 0.047 ... 0.969 0.984 0.984]\n                                         [0.016 0.031 0.047 ... 0.969 0.984 1.   ]]\n                        decomp_type     PCA\nDigitalNetB2 (AbstractLDDiscreteDistribution)\n    d               2^(6)\n    replications    1\n    randomize       LMS DS\n    gen_mats_source joe_kuo.6.21201.txt\n    order           RADICAL INVERSE\n    t               63\n    alpha           1\n    n_limit         2^(32)\n    entropy         267658111069601541207388819540918772545\n</pre> In\u00a0[25]: Copied! <pre>abs_tol = .001\nn_time_steps = 64\n\noptions = {\n    'interest_rate': .05,\n    'volatility': .5,\n    'start_price': 30,\n    'strike_price': 40  #a larger strike price than before\n}\n</pre> abs_tol = .001 n_time_steps = 64  options = {     'interest_rate': .05,     'volatility': .5,     'start_price': 30,     'strike_price': 40  #a larger strike price than before } <p>First we price it as above using single level QMC.</p> In\u00a0[26]: Copied! <pre>sobol_brownian = qmcpy.BrownianMotion(qmcpy.Sobol(n_time_steps))\nintegrand = qmcpy.AsianOption(sobol_brownian, **options)\nstopping_criterion = qmcpy.CubQMCSobolG(integrand,abs_tol = abs_tol)\nsolution,data = stopping_criterion.integrate()\nprint(data)\n</pre> sobol_brownian = qmcpy.BrownianMotion(qmcpy.Sobol(n_time_steps)) integrand = qmcpy.AsianOption(sobol_brownian, **options) stopping_criterion = qmcpy.CubQMCSobolG(integrand,abs_tol = abs_tol) solution,data = stopping_criterion.integrate() print(data) <pre>Data (Data)\n    solution        1.018\n    comb_bound_low  1.017\n    comb_bound_high 1.019\n    comb_bound_diff 0.002\n    comb_flags      1\n    n_total         2^(15)\n    n               2^(15)\n    time_integrate  0.118\nCubQMCNetG (AbstractStoppingCriterion)\n    abs_tol         0.001\n    rel_tol         0\n    n_init          2^(10)\n    n_limit         2^(35)\nAsianOption (AbstractIntegrand)\n    option          ASIAN\n    call_put        CALL\n    volatility      2^(-1)\n    start_price     30\n    strike_price    40\n    interest_rate   0.050\n    t_final         1\n    asian_mean      ARITHMETIC\nBrownianMotion (AbstractTrueMeasure)\n    time_vec        [0.016 0.031 0.047 ... 0.969 0.984 1.   ]\n    drift           0\n    mean            [0. 0. 0. ... 0. 0. 0.]\n    covariance      [[0.016 0.016 0.016 ... 0.016 0.016 0.016]\n                     [0.016 0.031 0.031 ... 0.031 0.031 0.031]\n                     [0.016 0.031 0.047 ... 0.047 0.047 0.047]\n                     ...\n                     [0.016 0.031 0.047 ... 0.969 0.969 0.969]\n                     [0.016 0.031 0.047 ... 0.969 0.984 0.984]\n                     [0.016 0.031 0.047 ... 0.969 0.984 1.   ]]\n    decomp_type     PCA\n    transform       BrownianMotion (AbstractTrueMeasure)\n                        time_vec        [0.016 0.031 0.047 ... 0.969 0.984 1.   ]\n                        drift           0\n                        mean            [0. 0. 0. ... 0. 0. 0.]\n                        covariance      [[0.016 0.016 0.016 ... 0.016 0.016 0.016]\n                                         [0.016 0.031 0.031 ... 0.031 0.031 0.031]\n                                         [0.016 0.031 0.047 ... 0.047 0.047 0.047]\n                                         ...\n                                         [0.016 0.031 0.047 ... 0.969 0.969 0.969]\n                                         [0.016 0.031 0.047 ... 0.969 0.984 0.984]\n                                         [0.016 0.031 0.047 ... 0.969 0.984 1.   ]]\n                        decomp_type     PCA\nDigitalNetB2 (AbstractLDDiscreteDistribution)\n    d               2^(6)\n    replications    1\n    randomize       LMS DS\n    gen_mats_source joe_kuo.6.21201.txt\n    order           RADICAL INVERSE\n    t               63\n    alpha           1\n    n_limit         2^(32)\n    entropy         68741035845447190293894879002444698271\n</pre> <p>Next, we introduce an upward drift in the Brownian motion, which produces more stock price paths with positive payoffs.  This produces a smaller varation in the integrand and a generally faster run time.  (There still remains the question of how to automatically choose an optimal drift.)</p> In\u00a0[27]: Copied! <pre>sobol_drift_brownian = qmcpy.BrownianMotion(qmcpy.Sobol(n_time_steps), drift = 0.5)\nintegrand = qmcpy.AsianOption(sobol_drift_brownian, **options)\nstopping_criterion = qmcpy.CubQMCSobolG(integrand,abs_tol = abs_tol)\nsolution,data = stopping_criterion.integrate()\nprint(data)\n</pre> sobol_drift_brownian = qmcpy.BrownianMotion(qmcpy.Sobol(n_time_steps), drift = 0.5) integrand = qmcpy.AsianOption(sobol_drift_brownian, **options) stopping_criterion = qmcpy.CubQMCSobolG(integrand,abs_tol = abs_tol) solution,data = stopping_criterion.integrate() print(data) <pre>Data (Data)\n    solution        1.018\n    comb_bound_low  1.018\n    comb_bound_high 1.019\n    comb_bound_diff 0.001\n    comb_flags      1\n    n_total         2^(15)\n    n               2^(15)\n    time_integrate  0.115\nCubQMCNetG (AbstractStoppingCriterion)\n    abs_tol         0.001\n    rel_tol         0\n    n_init          2^(10)\n    n_limit         2^(35)\nAsianOption (AbstractIntegrand)\n    option          ASIAN\n    call_put        CALL\n    volatility      2^(-1)\n    start_price     30\n    strike_price    40\n    interest_rate   0.050\n    t_final         1\n    asian_mean      ARITHMETIC\nBrownianMotion (AbstractTrueMeasure)\n    time_vec        [0.016 0.031 0.047 ... 0.969 0.984 1.   ]\n    drift           0\n    mean            [0. 0. 0. ... 0. 0. 0.]\n    covariance      [[0.016 0.016 0.016 ... 0.016 0.016 0.016]\n                     [0.016 0.031 0.031 ... 0.031 0.031 0.031]\n                     [0.016 0.031 0.047 ... 0.047 0.047 0.047]\n                     ...\n                     [0.016 0.031 0.047 ... 0.969 0.969 0.969]\n                     [0.016 0.031 0.047 ... 0.969 0.984 0.984]\n                     [0.016 0.031 0.047 ... 0.969 0.984 1.   ]]\n    decomp_type     PCA\n    transform       BrownianMotion (AbstractTrueMeasure)\n                        time_vec        [0.016 0.031 0.047 ... 0.969 0.984 1.   ]\n                        drift           2^(-1)\n                        mean            [0.008 0.016 0.023 ... 0.484 0.492 0.5  ]\n                        covariance      [[0.016 0.016 0.016 ... 0.016 0.016 0.016]\n                                         [0.016 0.031 0.031 ... 0.031 0.031 0.031]\n                                         [0.016 0.031 0.047 ... 0.047 0.047 0.047]\n                                         ...\n                                         [0.016 0.031 0.047 ... 0.969 0.969 0.969]\n                                         [0.016 0.031 0.047 ... 0.969 0.984 0.984]\n                                         [0.016 0.031 0.047 ... 0.969 0.984 1.   ]]\n                        decomp_type     PCA\nDigitalNetB2 (AbstractLDDiscreteDistribution)\n    d               2^(6)\n    replications    1\n    randomize       LMS DS\n    gen_mats_source joe_kuo.6.21201.txt\n    order           RADICAL INVERSE\n    t               63\n    alpha           1\n    n_limit         2^(32)\n    entropy         195386499743864987571337198121595404839\n</pre> <p>Pause for questions</p> In\u00a0[28]: Copied! <pre>d=4; n=8\nx_gail = qmcpy.Lattice(d,order='linear',randomize=False).gen_samples(n,warn=False)\nprint('GAIL Samples')\nfor i in range(n): print(x_gail[i])\nx_mps = qmcpy.Lattice(d,order='natural',randomize=False).gen_samples(n,warn=False)\nprint('\\n\\nMPS Samples')\nfor i in range(n): print(x_mps[i])\nfig,ax = pyplot.subplots(nrows=1,ncols=2,figsize=(10,5.5))\nax[0].scatter(x_gail[0:n,0],x_gail[0:n,1],color='b')\nax[0].set_title('GAIL backend')\nax[1].scatter(x_mps[0:n,0],x_mps[0:n,1],color='b')\nax[1].set_title('MPS backend')\nfor ii in range(2):\n  ax[ii].set_xlim([0,1]); ax[ii].set_xticks([0,1]); ax[ii].set_xlabel('$x_{i,1}$')\n  ax[ii].set_ylim([0,1]); ax[ii].set_yticks([0,1]); ax[ii].set_ylabel('$x_{i,2}$')\n  ax[ii].set_aspect(1)\n</pre> d=4; n=8 x_gail = qmcpy.Lattice(d,order='linear',randomize=False).gen_samples(n,warn=False) print('GAIL Samples') for i in range(n): print(x_gail[i]) x_mps = qmcpy.Lattice(d,order='natural',randomize=False).gen_samples(n,warn=False) print('\\n\\nMPS Samples') for i in range(n): print(x_mps[i]) fig,ax = pyplot.subplots(nrows=1,ncols=2,figsize=(10,5.5)) ax[0].scatter(x_gail[0:n,0],x_gail[0:n,1],color='b') ax[0].set_title('GAIL backend') ax[1].scatter(x_mps[0:n,0],x_mps[0:n,1],color='b') ax[1].set_title('MPS backend') for ii in range(2):   ax[ii].set_xlim([0,1]); ax[ii].set_xticks([0,1]); ax[ii].set_xlabel('$x_{i,1}$')   ax[ii].set_ylim([0,1]); ax[ii].set_yticks([0,1]); ax[ii].set_ylabel('$x_{i,2}$')   ax[ii].set_aspect(1) <pre>GAIL Samples\n[0. 0. 0. 0.]\n[0.125 0.375 0.375 0.875]\n[0.25 0.75 0.75 0.75]\n[0.375 0.125 0.125 0.625]\n[0.5 0.5 0.5 0.5]\n[0.625 0.875 0.875 0.375]\n[0.75 0.25 0.25 0.25]\n[0.875 0.625 0.625 0.125]\n\n\nMPS Samples\n[0. 0. 0. 0.]\n[0.5 0.5 0.5 0.5]\n[0.25 0.75 0.75 0.75]\n[0.75 0.25 0.25 0.25]\n[0.125 0.375 0.375 0.875]\n[0.625 0.875 0.875 0.375]\n[0.375 0.125 0.125 0.625]\n[0.875 0.625 0.625 0.125]\n</pre> In\u00a0[29]: Copied! <pre>warnings.simplefilter('ignore')  #turn off warnings which stop execution in Colab\nn = 16\nldA = qmcpy.Lattice(2)\nldB = qmcpy.Lattice(2, randomize = False)\npointsA = ldA.gen_samples(n)  #construct some points\npointsB = ldB.gen_samples(n)  #construct some points\nprint(f'\\nRandomized LD Points with shape {pointsA.shape}\\n'+str(pointsA))  #these points have 15 significant digit precision but only three digits are shown\nprint(f'\\nNonrandomized LD Points with shape {pointsB.shape}\\n'+str(pointsB))  #these points have 15 significant digit precision but only three digits are shown\nfig,ax = pyplot.subplots(nrows=1,ncols=2,figsize=(10,5.5))\nax[0].scatter(pointsA[0:n,0],pointsA[0:n,1],color='b')\nax[1].scatter(pointsB[0:n,0],pointsB[0:n,1],color='b')\nfor ii in range(2):\n   ax[ii].set_xlim([0,1]); ax[ii].set_xticks([0,1]); ax[ii].set_xlabel('$x_{i,1}$')\n   ax[ii].set_ylim([0,1]); ax[ii].set_yticks([0,1]); ax[ii].set_ylabel('$x_{i,2}$')\n   ax[ii].set_aspect(1)\n</pre> warnings.simplefilter('ignore')  #turn off warnings which stop execution in Colab n = 16 ldA = qmcpy.Lattice(2) ldB = qmcpy.Lattice(2, randomize = False) pointsA = ldA.gen_samples(n)  #construct some points pointsB = ldB.gen_samples(n)  #construct some points print(f'\\nRandomized LD Points with shape {pointsA.shape}\\n'+str(pointsA))  #these points have 15 significant digit precision but only three digits are shown print(f'\\nNonrandomized LD Points with shape {pointsB.shape}\\n'+str(pointsB))  #these points have 15 significant digit precision but only three digits are shown fig,ax = pyplot.subplots(nrows=1,ncols=2,figsize=(10,5.5)) ax[0].scatter(pointsA[0:n,0],pointsA[0:n,1],color='b') ax[1].scatter(pointsB[0:n,0],pointsB[0:n,1],color='b') for ii in range(2):    ax[ii].set_xlim([0,1]); ax[ii].set_xticks([0,1]); ax[ii].set_xlabel('$x_{i,1}$')    ax[ii].set_ylim([0,1]); ax[ii].set_yticks([0,1]); ax[ii].set_ylabel('$x_{i,2}$')    ax[ii].set_aspect(1) <pre>Randomized LD Points with shape (16, 2)\n[[0.00944244 0.29799738]\n [0.50944244 0.79799738]\n [0.25944244 0.04799738]\n [0.75944244 0.54799738]\n [0.13444244 0.67299738]\n [0.63444244 0.17299738]\n [0.38444244 0.42299738]\n [0.88444244 0.92299738]\n [0.07194244 0.98549738]\n [0.57194244 0.48549738]\n [0.32194244 0.73549738]\n [0.82194244 0.23549738]\n [0.19694244 0.36049738]\n [0.69694244 0.86049738]\n [0.44694244 0.11049738]\n [0.94694244 0.61049738]]\n\nNonrandomized LD Points with shape (16, 2)\n[[0.     0.    ]\n [0.5    0.5   ]\n [0.25   0.75  ]\n [0.75   0.25  ]\n [0.125  0.375 ]\n [0.625  0.875 ]\n [0.375  0.125 ]\n [0.875  0.625 ]\n [0.0625 0.6875]\n [0.5625 0.1875]\n [0.3125 0.4375]\n [0.8125 0.9375]\n [0.1875 0.0625]\n [0.6875 0.5625]\n [0.4375 0.8125]\n [0.9375 0.3125]]\n</pre> In\u00a0[30]: Copied! <pre>warnings.simplefilter('ignore')  #turn off warnings which stop execution in Colab\nld = qmcpy.Lattice(6,randomize=False)\npoints = ld.gen_samples(4)  #construct some points\nprint(f'\\nLD Points with shape {points.shape}\\n'+str(points))  #these points have 15 significant digit precision but only three digits are shown\n</pre> warnings.simplefilter('ignore')  #turn off warnings which stop execution in Colab ld = qmcpy.Lattice(6,randomize=False) points = ld.gen_samples(4)  #construct some points print(f'\\nLD Points with shape {points.shape}\\n'+str(points))  #these points have 15 significant digit precision but only three digits are shown <pre>LD Points with shape (4, 6)\n[[0.   0.   0.   0.   0.   0.  ]\n [0.5  0.5  0.5  0.5  0.5  0.5 ]\n [0.25 0.75 0.75 0.75 0.25 0.75]\n [0.75 0.25 0.25 0.25 0.75 0.25]]\n</pre> In\u00a0[31]: Copied! <pre>warnings.simplefilter('ignore')  #turn off warnings which stop execution in Colab\nld = qmcpy.Lattice(6,randomize=False)\nunif_points = ld.gen_samples(2)  #construct some points\nprint(f'\\nLD Points with shape {unif_points.shape}\\n'+str(unif_points))\nld_gauss = qmcpy.Gaussian(ld)\ngauss_points = ld_gauss.gen_samples(2)\nprint(f'\\nLD Points with shape {gauss_points.shape}\\n'+str(gauss_points))\n</pre> warnings.simplefilter('ignore')  #turn off warnings which stop execution in Colab ld = qmcpy.Lattice(6,randomize=False) unif_points = ld.gen_samples(2)  #construct some points print(f'\\nLD Points with shape {unif_points.shape}\\n'+str(unif_points)) ld_gauss = qmcpy.Gaussian(ld) gauss_points = ld_gauss.gen_samples(2) print(f'\\nLD Points with shape {gauss_points.shape}\\n'+str(gauss_points)) <pre>LD Points with shape (2, 6)\n[[0.  0.  0.  0.  0.  0. ]\n [0.5 0.5 0.5 0.5 0.5 0.5]]\n\nLD Points with shape (2, 6)\n[[nan nan nan nan nan nan]\n [ 0.  0.  0.  0.  0.  0.]]\n</pre> In\u00a0[32]: Copied! <pre>solution_G_fixed = qmcpy.CubQMCSobolG(qmcpy.Keister(qmcpy.Gaussian(qmcpy.Sobol(2,seed=47),covariance=1/2))).integrate()[0]\nsolution_CLT_fixed = qmcpy.CubQMCCLT(qmcpy.Keister(qmcpy.Gaussian(qmcpy.Sobol(2,seed=47,replications=15),covariance=1/2))).integrate()[0]\nsolution_G = qmcpy.CubQMCSobolG(qmcpy.Keister(qmcpy.Gaussian(qmcpy.Sobol(2),covariance=1/2))).integrate()[0]\nsolution_CLT = qmcpy.CubQMCCLT(qmcpy.Keister(qmcpy.Gaussian(qmcpy.Sobol(2,replications=15),covariance=1/2))).integrate()[0]\nprint(f'Solution for GAIL stopping criterion with FIXED seed {solution_G_fixed}')\nprint(f'Solution for CLT stopping criterion with FIXED seed {solution_CLT_fixed}')\nprint(f'Solution for GAIL stopping criterion {solution_G}')\nprint(f'Solution for CLT stopping criterion {solution_CLT}')\n</pre> solution_G_fixed = qmcpy.CubQMCSobolG(qmcpy.Keister(qmcpy.Gaussian(qmcpy.Sobol(2,seed=47),covariance=1/2))).integrate()[0] solution_CLT_fixed = qmcpy.CubQMCCLT(qmcpy.Keister(qmcpy.Gaussian(qmcpy.Sobol(2,seed=47,replications=15),covariance=1/2))).integrate()[0] solution_G = qmcpy.CubQMCSobolG(qmcpy.Keister(qmcpy.Gaussian(qmcpy.Sobol(2),covariance=1/2))).integrate()[0] solution_CLT = qmcpy.CubQMCCLT(qmcpy.Keister(qmcpy.Gaussian(qmcpy.Sobol(2,replications=15),covariance=1/2))).integrate()[0] print(f'Solution for GAIL stopping criterion with FIXED seed {solution_G_fixed}') print(f'Solution for CLT stopping criterion with FIXED seed {solution_CLT_fixed}') print(f'Solution for GAIL stopping criterion {solution_G}') print(f'Solution for CLT stopping criterion {solution_CLT}') <pre>Solution for GAIL stopping criterion with FIXED seed 1.8088685362448778\nSolution for CLT stopping criterion with FIXED seed 1.806594507374441\nSolution for GAIL stopping criterion 1.807944316997558\nSolution for CLT stopping criterion 1.80672801265591\n</pre> In\u00a0[33]: Copied! <pre>help(qmcpy.Sobol)\ndir(qmcpy.Sobol)\n</pre> help(qmcpy.Sobol) dir(qmcpy.Sobol) <pre>Help on class DigitalNetB2 in module qmcpy.discrete_distribution.digital_net_b2.digital_net_b2:\n\nclass DigitalNetB2(qmcpy.discrete_distribution.abstract_discrete_distribution.AbstractLDDiscreteDistribution)\n |  DigitalNetB2(dimension=1, replications=None, seed=None, randomize='LMS DS', generating_matrices='joe_kuo.6.21201.txt', order='RADICAL INVERSE', t=63, alpha=1, msb=None, _verbose=False, graycode=None, t_max=None, t_lms=None)\n |\n |  Low discrepancy digital net in base 2.\n |\n |  Note:\n |      - Digital net sample sizes should be powers of $2$ e.g. $1$, $2$, $4$, $8$, $16$, $\\dots$.\n |      - The first point of an unrandomized digital nets is the origin.\n |      - `Sobol` is an alias for `DigitalNetB2`.\n |      - To use higher order digital nets, either:\n |\n |          - Pass in `generating_matrices` *without* interlacing and supply `alpha`&gt;1 to apply interlacing, or\n |          - Pass in `generating_matrices` *with* interlacing and set `alpha=1` to avoid additional interlacing\n |\n |          i.e. do *not* pass in interlaced `generating_matrices` and set `alpha&gt;1`, this will apply additional interlacing.\n |\n |  Examples:\n |      &gt;&gt;&gt; discrete_distrib = DigitalNetB2(2,seed=7)\n |      &gt;&gt;&gt; discrete_distrib(4)\n |      array([[0.72162356, 0.914955  ],\n |             [0.16345554, 0.42964856],\n |             [0.98676255, 0.03436384],\n |             [0.42956655, 0.55876342]])\n |      &gt;&gt;&gt; discrete_distrib(1) # first point in the sequence\n |      array([[0.72162356, 0.914955  ]])\n |      &gt;&gt;&gt; discrete_distrib\n |      DigitalNetB2 (AbstractLDDiscreteDistribution)\n |          d               2^(1)\n |          replications    1\n |          randomize       LMS DS\n |          gen_mats_source joe_kuo.6.21201.txt\n |          order           RADICAL INVERSE\n |          t               63\n |          alpha           1\n |          n_limit         2^(32)\n |          entropy         7\n |\n |      Replications of independent randomizations\n |\n |      &gt;&gt;&gt; x = DigitalNetB2(dimension=3,seed=7,replications=2)(4)\n |      &gt;&gt;&gt; x.shape\n |      (2, 4, 3)\n |      &gt;&gt;&gt; x\n |      array([[[0.24653277, 0.1821862 , 0.74732591],\n |              [0.68152903, 0.66169442, 0.42891961],\n |              [0.48139855, 0.79818233, 0.08201287],\n |              [0.91541325, 0.29520621, 0.77495809]],\n |      &lt;BLANKLINE&gt;\n |             [[0.44876891, 0.85899604, 0.50549679],\n |              [0.53635924, 0.04353443, 0.33564946],\n |              [0.23214143, 0.29281506, 0.06841036],\n |              [0.75295715, 0.60241448, 0.76962976]]])\n |\n |      Different orderings (avoid warnings that the first point is the origin)\n |\n |      &gt;&gt;&gt; DigitalNetB2(dimension=2,randomize=False,order=\"GRAY\")(n_min=2,n_max=4,warn=False)\n |      array([[0.75, 0.25],\n |             [0.25, 0.75]])\n |      &gt;&gt;&gt; DigitalNetB2(dimension=2,randomize=False,order=\"RADICAL INVERSE\")(n_min=2,n_max=4,warn=False)\n |      array([[0.25, 0.75],\n |             [0.75, 0.25]])\n |\n |      Generating matrices from [https://github.com/QMCSoftware/LDData/tree/main/dnet](https://github.com/QMCSoftware/LDData/tree/main/dnet)\n |\n |      &gt;&gt;&gt; DigitalNetB2(dimension=3,randomize=False,generating_matrices=\"mps.nx_s5_alpha2_m32.txt\")(8,warn=False)\n |      array([[0.        , 0.        , 0.        ],\n |             [0.75841841, 0.45284834, 0.48844557],\n |             [0.57679828, 0.13226272, 0.10061957],\n |             [0.31858402, 0.32113875, 0.39369111],\n |             [0.90278927, 0.45867532, 0.01803333],\n |             [0.14542431, 0.02548793, 0.4749614 ],\n |             [0.45587539, 0.33081476, 0.11474426],\n |             [0.71318879, 0.15377192, 0.37629925]])\n |\n |      All randomizations\n |\n |      &gt;&gt;&gt; DigitalNetB2(dimension=3,randomize='LMS DS',seed=5)(8)\n |      array([[0.69346401, 0.20118185, 0.64779396],\n |             [0.43998032, 0.90102467, 0.0936172 ],\n |             [0.86663563, 0.60910036, 0.26043276],\n |             [0.11327376, 0.30772653, 0.93959283],\n |             [0.62102883, 0.79169756, 0.77051637],\n |             [0.37451038, 0.1231324 , 0.46634012],\n |             [0.94785596, 0.38577413, 0.13377215],\n |             [0.20121617, 0.71843325, 0.56293458]])\n |      &gt;&gt;&gt; DigitalNetB2(dimension=3,randomize='LMS',seed=5)(8,warn=False)\n |      array([[0.        , 0.        , 0.        ],\n |             [0.75446077, 0.83265937, 0.69584079],\n |             [0.42329494, 0.65793842, 0.90427279],\n |             [0.67763292, 0.48937304, 0.33344964],\n |             [0.18550714, 0.97332905, 0.3772791 ],\n |             [0.93104851, 0.17195496, 0.82311652],\n |             [0.26221346, 0.31742386, 0.53093284],\n |             [0.50787715, 0.5172669 , 0.2101083 ]])\n |      &gt;&gt;&gt; DigitalNetB2(dimension=3,randomize='DS',seed=5)(8)\n |      array([[0.68383949, 0.04047995, 0.42903182],\n |             [0.18383949, 0.54047995, 0.92903182],\n |             [0.93383949, 0.79047995, 0.67903182],\n |             [0.43383949, 0.29047995, 0.17903182],\n |             [0.55883949, 0.66547995, 0.05403182],\n |             [0.05883949, 0.16547995, 0.55403182],\n |             [0.80883949, 0.41547995, 0.80403182],\n |             [0.30883949, 0.91547995, 0.30403182]])\n |      &gt;&gt;&gt; DigitalNetB2(dimension=3,randomize='OWEN',seed=5)(8)\n |      array([[0.33595486, 0.05834975, 0.30066401],\n |             [0.89110875, 0.84905188, 0.81833285],\n |             [0.06846074, 0.59997956, 0.67064205],\n |             [0.6693703 , 0.25824002, 0.10469644],\n |             [0.44586618, 0.99161977, 0.1873488 ],\n |             [0.84245267, 0.16445553, 0.56544372],\n |             [0.18546359, 0.44859876, 0.97389524],\n |             [0.61215442, 0.64341386, 0.44529863]])\n |\n |      Higher order net without randomization\n |\n |      &gt;&gt;&gt; DigitalNetB2(dimension=3,randomize='FALSE',seed=7,alpha=2)(4,warn=False)\n |      array([[0.    , 0.    , 0.    ],\n |             [0.75  , 0.75  , 0.75  ],\n |             [0.4375, 0.9375, 0.1875],\n |             [0.6875, 0.1875, 0.9375]])\n |\n |\n |      Higher order nets with randomizations and replications\n |\n |      &gt;&gt;&gt; DigitalNetB2(dimension=3,randomize='LMS DS',seed=7,replications=2,alpha=2)(4,warn=False)\n |      array([[[0.42955149, 0.89149058, 0.43867111],\n |              [0.68701828, 0.07601148, 0.51312447],\n |              [0.10088033, 0.16293661, 0.25144138],\n |              [0.85846252, 0.87103178, 0.70041789]],\n |      &lt;BLANKLINE&gt;\n |             [[0.27151905, 0.42406763, 0.21917369],\n |              [0.55035224, 0.67864387, 0.90033876],\n |              [0.19356758, 0.57589964, 0.00347701],\n |              [0.97235125, 0.32168581, 0.86920948]]])\n |      &gt;&gt;&gt; DigitalNetB2(dimension=3,randomize='LMS',seed=7,replications=2,alpha=2)(4,warn=False)\n |      array([[[0.        , 0.        , 0.        ],\n |              [0.75817062, 0.96603053, 0.94947625],\n |              [0.45367986, 0.80295638, 0.18778553],\n |              [0.71171791, 0.2295424 , 0.76175441]],\n |      &lt;BLANKLINE&gt;\n |             [[0.        , 0.        , 0.        ],\n |              [0.78664636, 0.75470215, 0.86876474],\n |              [0.45336727, 0.99953621, 0.22253579],\n |              [0.73996397, 0.24544824, 0.9008679 ]]])\n |      &gt;&gt;&gt; DigitalNetB2(dimension=3,randomize='DS',seed=7,replications=2,alpha=2)(4)\n |      array([[[0.04386058, 0.58727432, 0.3691824 ],\n |              [0.79386058, 0.33727432, 0.6191824 ],\n |              [0.48136058, 0.39977432, 0.4316824 ],\n |              [0.73136058, 0.64977432, 0.6816824 ]],\n |      &lt;BLANKLINE&gt;\n |             [[0.65212985, 0.69669968, 0.10605352],\n |              [0.40212985, 0.44669968, 0.85605352],\n |              [0.83962985, 0.25919968, 0.16855352],\n |              [0.08962985, 0.50919968, 0.91855352]]])\n |      &gt;&gt;&gt; DigitalNetB2(dimension=3,randomize='OWEN',seed=7,replications=2,alpha=2)(4)\n |      array([[[0.46368517, 0.03964427, 0.62172094],\n |              [0.7498683 , 0.76141348, 0.4243043 ],\n |              [0.01729754, 0.97968459, 0.65963223],\n |              [0.75365329, 0.1903774 , 0.34141493]],\n |      &lt;BLANKLINE&gt;\n |             [[0.52252547, 0.5679709 , 0.05949112],\n |              [0.27248656, 0.36488289, 0.81844058],\n |              [0.94219959, 0.39172304, 0.20285965],\n |              [0.19716391, 0.64741585, 0.92494554]]])\n |\n |  **References:**\n |\n |  1.  Marius Hofert and Christiane Lemieux.\n |      qrng: (Randomized) Quasi-Random Number Generators (2019).\n |      R package version 0.0-7.\n |      [https://CRAN.R-project.org/package=qrng](https://CRAN.R-project.org/package=qrng).\n |\n |  2.  Faure, Henri, and Christiane Lemieux.\n |      Implementation of Irreducible Sobol' Sequences in Prime Power Bases.\n |      Mathematics and Computers in Simulation 161 (2019): 13-22. Crossref. Web.\n |\n |  3.  F.Y. Kuo, D. Nuyens.\n |      Application of quasi-Monte Carlo methods to elliptic PDEs with random diffusion coefficients \\- a survey of analysis and implementation.\n |      Foundations of Computational Mathematics, 16(6):1631-1696, 2016.\n |      [https://link.springer.com/article/10.1007/s10208-016-9329-5](https://link.springer.com/article/10.1007/s10208-016-9329-5).\n |\n |  4.  D. Nuyens.\n |      The Magic Point Shop of QMC point generators and generating vectors.\n |      MATLAB and Python software, 2018.\n |      [https://people.cs.kuleuven.be/~dirk.nuyens/](https://people.cs.kuleuven.be/~dirk.nuyens/).\n |\n |  5.  R. Cools, F.Y. Kuo, D. Nuyens.\n |      Constructing embedded lattice rules for multivariate integration.\n |      SIAM J. Sci. Comput., 28(6), 2162-2188.\n |\n |  6.  I.M. Sobol', V.I. Turchaninov, Yu.L. Levitan, B.V. Shukhman.\n |      Quasi-Random Sequence Generators.\n |      Keldysh Institute of Applied Mathematics.\n |      Russian Academy of Sciences, Moscow (1992).\n |\n |  7.  Sobol, Ilya &amp; Asotsky, Danil &amp; Kreinin, Alexander &amp; Kucherenko, Sergei. (2011).\n |      Construction and Comparison of High-Dimensional Sobol' Generators. Wilmott. 2011.\n |      [10.1002/wilm.10056](https://onlinelibrary.wiley.com/doi/abs/10.1002/wilm.10056).\n |\n |  8.  Paul Bratley and Bennett L. Fox.\n |      Algorithm 659: Implementing Sobol's quasirandom sequence generator.\n |      ACM Trans. Math. Softw. 14, 1 (March 1988), 88-100. 1988.\n |      [https://doi.org/10.1145/42288.214372](https://doi.org/10.1145/42288.2143720).\n |\n |  Method resolution order:\n |      DigitalNetB2\n |      qmcpy.discrete_distribution.abstract_discrete_distribution.AbstractLDDiscreteDistribution\n |      qmcpy.discrete_distribution.abstract_discrete_distribution.AbstractDiscreteDistribution\n |      builtins.object\n |\n |  Methods defined here:\n |\n |  __init__(self, dimension=1, replications=None, seed=None, randomize='LMS DS', generating_matrices='joe_kuo.6.21201.txt', order='RADICAL INVERSE', t=63, alpha=1, msb=None, _verbose=False, graycode=None, t_max=None, t_lms=None)\n |      Args:\n |          dimension (Union[int,np.ndarray]): Dimension of the generator.\n |\n |              - If an `int` is passed in, use generating vector components at indices 0,...,`dimension`-1.\n |              - If an `np.ndarray` is passed in, use generating vector components at these indices.\n |\n |          replications (int): Number of independent randomizations of a pointset.\n |          seed (Union[None,int,np.random.SeedSeq): Seed the random number generator for reproducibility.\n |          randomize (str): Options are\n |\n |              - `'LMS DS'`: Linear matrix scramble with digital shift.\n |              - `'LMS'`: Linear matrix scramble only.\n |              - `'DS'`: Digital shift only.\n |              - `'NUS'`: Nested uniform scrambling. Also known as Owen scrambling.\n |              - `'FALSE'`: No randomization. In this case the first point will be the origin.\n |\n |          generating_matrices (Union[str,np.ndarray,int]: Specify the generating matrices.\n |\n |              - A `str` should be the name (or path) of a file from the LDData repo at [https://github.com/QMCSoftware/LDData/tree/main/dnet](https://github.com/QMCSoftware/LDData/tree/main/dnet).\n |              - An `np.ndarray` of integers with shape $(d,m_\\mathrm{max})$ or $(r,d,m_\\mathrm{max})$ where $d$ is the number of dimensions, $r$ is the number of replications, and $2^{m_\\mathrm{max}}$ is the maximum number of supported points. Setting `msb=False` will flip the bits of ints in the generating matrices.\n |\n |          order (str): `'RADICAL INVERSE'`, or `'GRAY'` ordering. See the doctest example above.\n |          t (int): Number of bits in integer represetation of points *after* randomization. The number of bits in the generating matrices is inferred based on the largest value.\n |          alpha (int): Interlacing factor for higher order nets.\n |              When `alpha`&gt;1, interlacing is performed regardless of the generating matrices,\n |              i.e., for `alpha`&gt;1 do *not* pass in generating matrices which are already interlaced.\n |              The Note for this class contains more info.\n |          msb (bool): Flag for Most Significant Bit (MSB) vs Least Significant Bit (LSB) integer representations in generating matrices. If `msb=False` (LSB order), then integers in generating matrices will be bit-reversed.\n |          _verbose (bool): If `True`, print linear matrix scrambling matrices.\n |\n |  ----------------------------------------------------------------------\n |  Methods inherited from qmcpy.discrete_distribution.abstract_discrete_distribution.AbstractLDDiscreteDistribution:\n |\n |  __repr__(self)\n |      Return repr(self).\n |\n |  ----------------------------------------------------------------------\n |  Methods inherited from qmcpy.discrete_distribution.abstract_discrete_distribution.AbstractDiscreteDistribution:\n |\n |  __call__(self, n=None, n_min=None, n_max=None, return_binary=False, warn=True)\n |      - If just `n` is supplied, generate samples from the sequence at indices 0,...,`n`-1.\n |      - If `n_min` and `n_max` are supplied, generate samples from the sequence at indices `n_min`,...,`n_max`-1.\n |      - If `n` and `n_min` are supplied, then generate samples from the sequence at indices `n`,...,`n_min`-1.\n |\n |      Args:\n |          n (Union[None,int]): Number of points to generate.\n |          n_min (Union[None,int]): Starting index of sequence.\n |          n_max (Union[None,int]): Final index of sequence.\n |          return_binary (bool): Only used for `DigitalNetB2`.\n |              If `True`, *only* return the integer representation `x_integer` of base 2 digital net.\n |          warn (bool): If `False`, disable warnings when generating samples.\n |\n |      Returns:\n |          x (np.ndarray): Samples from the sequence.\n |\n |              - If `replications` is `None` then this will be of size (`n_max`-`n_min`) $\\times$ `dimension`\n |              - If `replications` is a positive int, then `x` will be of size `replications` $\\times$ (`n_max`-`n_min`) $\\times$ `dimension`\n |\n |              Note that if `return_binary=True` then `x` is returned where `x` are integer representations of the digital net points.\n |\n |  gen_samples(self, n=None, n_min=None, n_max=None, return_binary=False, warn=True)\n |\n |  pdf(self, x)\n |\n |  spawn(self, s=1, dimensions=None)\n |      Spawn new instances of the current discrete distribution but with new seeds and dimensions.\n |      Used by multi-level QMC algorithms which require different seeds and dimensions on each level.\n |\n |      Note:\n |          Use `replications` instead of using `spawn` when possible, e.g., when spawning copies which all have the same dimension.\n |\n |      Args:\n |          s (int): Number of copies to spawn\n |          dimensions (np.ndarray): Length `s` array of dimensions for each copy. Defaults to the current dimension.\n |\n |      Returns:\n |          spawned_discrete_distribs (list): Discrete distributions with new seeds and dimensions.\n |\n |  ----------------------------------------------------------------------\n |  Data descriptors inherited from qmcpy.discrete_distribution.abstract_discrete_distribution.AbstractDiscreteDistribution:\n |\n |  __dict__\n |      dictionary for instance variables\n |\n |  __weakref__\n |      list of weak references to the object\n\n</pre> Out[33]: <pre>['__call__',\n '__class__',\n '__delattr__',\n '__dict__',\n '__dir__',\n '__doc__',\n '__eq__',\n '__format__',\n '__ge__',\n '__getattribute__',\n '__getstate__',\n '__gt__',\n '__hash__',\n '__init__',\n '__init_subclass__',\n '__le__',\n '__lt__',\n '__module__',\n '__ne__',\n '__new__',\n '__reduce__',\n '__reduce_ex__',\n '__repr__',\n '__setattr__',\n '__sizeof__',\n '__str__',\n '__subclasshook__',\n '__weakref__',\n '_gen_samples',\n '_spawn',\n 'gen_samples',\n 'pdf',\n 'spawn']</pre> In\u00a0[34]: Copied! <pre>sc = qmcpy.CubQMCLatticeG(qmcpy.Keister(qmcpy.Gaussian(qmcpy.Lattice(5,seed=7),covariance=1/2)),abs_tol=.01)\nprint(sc.integrate()[1],'\\n\\n')\nsc.set_tolerance(abs_tol=.001)  #changing the tolerance\nprint(sc.integrate()[1],'\\n\\n')\n</pre> sc = qmcpy.CubQMCLatticeG(qmcpy.Keister(qmcpy.Gaussian(qmcpy.Lattice(5,seed=7),covariance=1/2)),abs_tol=.01) print(sc.integrate()[1],'\\n\\n') sc.set_tolerance(abs_tol=.001)  #changing the tolerance print(sc.integrate()[1],'\\n\\n') <pre>Data (Data)\n    solution        1.135\n    comb_bound_low  1.129\n    comb_bound_high 1.142\n    comb_bound_diff 0.012\n    comb_flags      1\n    n_total         2^(13)\n    n               2^(13)\n    time_integrate  0.007\nCubQMCLatticeG (AbstractStoppingCriterion)\n    abs_tol         0.010\n    rel_tol         0\n    n_init          2^(10)\n    n_limit         2^(30)\nKeister (AbstractIntegrand)\nGaussian (AbstractTrueMeasure)\n    mean            0\n    covariance      2^(-1)\n    decomp_type     PCA\n    transform       Gaussian (AbstractTrueMeasure)\n                        mean            0\n                        covariance      2^(-1)\n                        decomp_type     PCA\nLattice (AbstractLDDiscreteDistribution)\n    d               5\n    replications    1\n    randomize       SHIFT\n    gen_vec_source  kuo.lattice-33002-1024-1048576.9125.txt\n    order           RADICAL INVERSE\n    n_limit         2^(20)\n    entropy         7 \n\n\nData (Data)\n    solution        1.136\n    comb_bound_low  1.135\n    comb_bound_high 1.136\n    comb_bound_diff 0.001\n    comb_flags      1\n    n_total         2^(17)\n    n               2^(17)\n    time_integrate  0.040\nCubQMCLatticeG (AbstractStoppingCriterion)\n    abs_tol         0.001\n    rel_tol         0\n    n_init          2^(10)\n    n_limit         2^(30)\nKeister (AbstractIntegrand)\nGaussian (AbstractTrueMeasure)\n    mean            0\n    covariance      2^(-1)\n    decomp_type     PCA\n    transform       Gaussian (AbstractTrueMeasure)\n                        mean            0\n                        covariance      2^(-1)\n                        decomp_type     PCA\nLattice (AbstractLDDiscreteDistribution)\n    d               5\n    replications    1\n    randomize       SHIFT\n    gen_vec_source  kuo.lattice-33002-1024-1048576.9125.txt\n    order           RADICAL INVERSE\n    n_limit         2^(20)\n    entropy         7 \n\n\n</pre> In\u00a0[35]: Copied! <pre>sc = qmcpy.CubQMCLatticeG(qmcpy.Keister(qmcpy.Gaussian(qmcpy.Lattice(5,seed=7),covariance=1/2)),abs_tol=.01)  #zero relative tolerance by default\nprint(sc.integrate()[1],'\\n\\n')\nsc.set_tolerance(abs_tol=0,rel_tol=0.005)  #nonzero relative tolerance and zero absolute tolerance\nprint(sc.integrate()[1])\n</pre> sc = qmcpy.CubQMCLatticeG(qmcpy.Keister(qmcpy.Gaussian(qmcpy.Lattice(5,seed=7),covariance=1/2)),abs_tol=.01)  #zero relative tolerance by default print(sc.integrate()[1],'\\n\\n') sc.set_tolerance(abs_tol=0,rel_tol=0.005)  #nonzero relative tolerance and zero absolute tolerance print(sc.integrate()[1]) <pre>Data (Data)\n    solution        1.135\n    comb_bound_low  1.129\n    comb_bound_high 1.142\n    comb_bound_diff 0.012\n    comb_flags      1\n    n_total         2^(13)\n    n               2^(13)\n    time_integrate  0.004\nCubQMCLatticeG (AbstractStoppingCriterion)\n    abs_tol         0.010\n    rel_tol         0\n    n_init          2^(10)\n    n_limit         2^(30)\nKeister (AbstractIntegrand)\nGaussian (AbstractTrueMeasure)\n    mean            0\n    covariance      2^(-1)\n    decomp_type     PCA\n    transform       Gaussian (AbstractTrueMeasure)\n                        mean            0\n                        covariance      2^(-1)\n                        decomp_type     PCA\nLattice (AbstractLDDiscreteDistribution)\n    d               5\n    replications    1\n    randomize       SHIFT\n    gen_vec_source  kuo.lattice-33002-1024-1048576.9125.txt\n    order           RADICAL INVERSE\n    n_limit         2^(20)\n    entropy         7 \n\n\nData (Data)\n    solution        1.135\n    comb_bound_low  1.131\n    comb_bound_high 1.139\n    comb_bound_diff 0.008\n    comb_flags      1\n    n_total         2^(14)\n    n               2^(14)\n    time_integrate  0.006\nCubQMCLatticeG (AbstractStoppingCriterion)\n    abs_tol         0\n    rel_tol         0.005\n    n_init          2^(10)\n    n_limit         2^(30)\nKeister (AbstractIntegrand)\nGaussian (AbstractTrueMeasure)\n    mean            0\n    covariance      2^(-1)\n    decomp_type     PCA\n    transform       Gaussian (AbstractTrueMeasure)\n                        mean            0\n                        covariance      2^(-1)\n                        decomp_type     PCA\nLattice (AbstractLDDiscreteDistribution)\n    d               5\n    replications    1\n    randomize       SHIFT\n    gen_vec_source  kuo.lattice-33002-1024-1048576.9125.txt\n    order           RADICAL INVERSE\n    n_limit         2^(20)\n    entropy         7\n</pre>"},{"location":"demos/talk_paper_demos/MCQMC_Tutorial_2020/MCQMC_2020_QMC_Software_Tutorial/#quasi-monte-carlo-qmc-software-in-qmcpy","title":"Quasi-Monte Carlo (QMC) Software in QMCPy\u00b6","text":"<p>A tutorial based on this notebook is at https://media.ed.ac.uk/playlist/dedicated/51612401/1_0z0wec2z/1_2k12mwiw.</p> <p>As an example of available QMC software, we introduce the Python library QMCPy.  This Jupyter notebook saves your typing.</p> <p>QMCPy is a community effort.  This early release includes contributions from</p> <ul> <li>Mike Giles MLMC and MLQMC software</li> <li>Marius Hofert and Christiane Lemieux's QRNG</li> <li>Pierre L'Ecuyer's Lattice Builder</li> <li>Dirk Nuyens's Magic Point Shop (MPS)</li> <li>Art Owen's Halton sequences</li> <li>PyTorch</li> <li>Guaranteed Automatic Integration Library (GAIL)</li> </ul> <p>and depends on the NumPy and SciPy Python packages.</p> <p>View the companion pdf slides at https://speakerdeck.com/fjhickernell/quasi-monte-carlo-software and the introductory QMCPy blog at https://qmcpy.wordpress.com.</p>"},{"location":"demos/talk_paper_demos/MCQMC_Tutorial_2020/MCQMC_2020_QMC_Software_Tutorial/#installation","title":"Installation\u00b6","text":"<p>QMCPy can be installed with <code>pip install qmcpy</code> or cloned from the  QMCSoftware GitHub repository.</p>"},{"location":"demos/talk_paper_demos/MCQMC_Tutorial_2020/MCQMC_2020_QMC_Software_Tutorial/#generating-low-discrepancy-ld-points-via-a-discretedistribution-object","title":"Generating low discrepancy (LD) points via a <code>DiscreteDistribution</code> object\u00b6","text":"<p>We generate some points used for quasi-Monte Carlo methods.  These points are called low discrepancy (LD for short) and are created as an instance of a <code>DiscreteDistribution</code> class.</p>"},{"location":"demos/talk_paper_demos/MCQMC_Tutorial_2020/MCQMC_2020_QMC_Software_Tutorial/#integration-lattices","title":"Integration lattices\u00b6","text":"<p>Here are some (randomly shifted) integration lattice points. This is a two step procees:</p> <p>i) construct the <code>DiscreteDistribution</code> object <code>lattice</code>, and then</p> <p>ii) construct a number of points from the sequence.</p> <p>The structure of these points favors <code>n</code> that is a power of 2.</p>"},{"location":"demos/talk_paper_demos/MCQMC_Tutorial_2020/MCQMC_2020_QMC_Software_Tutorial/#iid-uniform-points-do-not-fill-space-as-well","title":"IID uniform points do not fill space as well\u00b6","text":"<p>Contrast this with independent and identically distributed (IID) points.  Although successive points fill the square, they do so without knowledge of the others and produce clusters and gaps.  Think of it this way</p> <ul> <li>LD = evenly spread</li> <li>IID = points do not know about each other</li> </ul> <p>(Since the first parameter in the <code>DiscreteDistribution</code> object is the dimension, you need not identify it.)</p>"},{"location":"demos/talk_paper_demos/MCQMC_Tutorial_2020/MCQMC_2020_QMC_Software_Tutorial/#rows-and-columns-are-not-interchangeable-for-ld","title":"Rows and columns are not interchangeable for LD\u00b6","text":"<p>For LD sequences we must differentiate between the cooordinates of the point (column) and which point (row). The transpose of an LD array is not LD.  This differs from IID multivariate points with IID marginals.</p> <p>In the example below we reverse the roles of the rows and columns.  The transposed lattice points do not fill space at all, while the transposed IID points are as good as the originals.</p>"},{"location":"demos/talk_paper_demos/MCQMC_Tutorial_2020/MCQMC_2020_QMC_Software_Tutorial/#sobol-points","title":"Sobol' points\u00b6","text":"<p>Another LD sequence is the Sobol' points.  Again, new points fill in the gaps between existing points. Since these are randomly digitally shifted Sobol' points, rerunning this command gives a different set of points.  For Sobol' points as well, <code>n</code> should normally be a power of 2.</p>"},{"location":"demos/talk_paper_demos/MCQMC_Tutorial_2020/MCQMC_2020_QMC_Software_Tutorial/#halton-points","title":"Halton Points\u00b6","text":"<p>A third kind of LD sequence is Halton points, which are also randomized. Again, new points fill in the gaps between existing points.  For Halton points there are no favored numbers of points.</p>"},{"location":"demos/talk_paper_demos/MCQMC_Tutorial_2020/MCQMC_2020_QMC_Software_Tutorial/#timing","title":"Timing\u00b6","text":"<p>The time to generate LD points depends on several factors, including the hardware and the particular generator.  One can generate one million points in 64 dimensions in a matter of seconds, somewhat slower than IID uniform points.  You may replace the <code>DiscreteDistribution</code> object in the first line of code below to test the timings of other kinds of points.  Halton seems to be the slowest (probably because the backend is implemented in Python rather than C).</p>"},{"location":"demos/talk_paper_demos/MCQMC_Tutorial_2020/MCQMC_2020_QMC_Software_Tutorial/#transform-ld-sequences-to-mimic-other-distributions-via-a-truemeasure-object","title":"Transform LD sequences to mimic other distributions via a <code>TrueMeasure</code> object\u00b6","text":"<p>LD sequences mimic $\\mathcal{U}[0,1]^d$ by design.  If you want to mimic another distribution, LD points can be transformed accordingly.  This is done via a <code>TrueMeasure</code> object, which takes a <code>DiscreteDistribution</code> object as input. Choose the <code>TrueMeasure</code> according to the probability distribution that you wish to mimic, and input the LD <code>DiscreteDistribution</code> object that you wish to use.</p>"},{"location":"demos/talk_paper_demos/MCQMC_Tutorial_2020/MCQMC_2020_QMC_Software_Tutorial/#uniform-on-boldsymbolaboldsymbolb","title":"Uniform on $[\\boldsymbol{a},\\boldsymbol{b}]$\u00b6","text":"<p>An affine transformation can be used to turn $\\mathcal{U}[0,1]^d$ points into $\\mathcal{U}[\\boldsymbol{a},\\boldsymbol{b}]$ points.  The <code>TrueMeasure</code> object <code>qmcpy.Uniform</code> performs the transformation automatically.  Then we generate points as before, but now they mimic our new distribution.</p>"},{"location":"demos/talk_paper_demos/MCQMC_Tutorial_2020/MCQMC_2020_QMC_Software_Tutorial/#gaussian","title":"Gaussian\u00b6","text":"<p>A similar process can be followed for constructing points that mimic a multivariate Gaussian distribution. The transformation used here assigns the directions with the larger variance to the lower numbered coordinates of the LD sequence via principal component analysis (PCA) or equivalently an eigvenvector-eigenvalue decomposition of the covariance matrix.  This takes advantage of the fact that the lower numbered coordinates of an LD sequence tend to have better evenness.</p>"},{"location":"demos/talk_paper_demos/MCQMC_Tutorial_2020/MCQMC_2020_QMC_Software_Tutorial/#integration","title":"Integration\u00b6","text":"<p>Cubature\u2014the approximation of multivariate integrals\u2014is an important application area for QMC. To solve this problem we need the <code>Integrand</code> and <code>StoppingCriterion</code> classes.</p> <p>Consider the followng $d$-variate integral due to Keister: \\begin{equation*} \\mu = \\int_{\\mathbb{R}^d} \\cos(\\lVert\\boldsymbol{t}\\rVert) \\exp( - \\lVert \\boldsymbol{t} \\rVert^2) \\,  \\mathrm{d} \\boldsymbol{t}, \\end{equation*} where $\\lVert \\cdot \\rVert$ is the Euclidean norm.  To approximate this integral via (Q)MC methods, it needs to be written as the integral with respect to a probability measure, e.g., \\begin{equation*} \\mu = \\int_{\\mathbb{R}^d} \\underbrace{\\pi^{d/2} \\cos(\\lVert\\boldsymbol{t}\\rVert)}_{g(\\boldsymbol{t})} \\; \\underbrace{\\pi^{-d/2} \\exp( - \\lVert \\boldsymbol{t} \\rVert^2) \\,  \\mathrm{d} \\boldsymbol{t}}_{\\mathcal{N}(\\boldsymbol{0}_d,\\mathsf{I}_d/2) \\text{ measure}}. \\end{equation*} Using transformation techniques highlighted above, this integral can be further transformed to an integral over the unit cube, which is suitable for certain stopping criteria: \\begin{equation*} \\mu = \\int_{[0,1]^d} \\underbrace{\\pi^{d/2}  \\cos\\left(\\sqrt{ \\frac 12 \\sum_{j=1}^d \\bigl[\\Phi^{-1}(x_j)\\bigr]^2}\\right)}_{f(\\boldsymbol{x})}  \\, \\rm d \\boldsymbol{x}. \\end{equation*}</p> <p>Although it may seem counter-intuitive, we set up our numerical problem by</p> <ul> <li>first choosing the <code>DiscreteDistribution</code> object,</li> <li>next the <code>TrueMeasure</code> object,</li> <li>thirdly choosing <code>Integrand</code> object, and</li> <li>finally the <code>StoppingCriterion</code> object.</li> </ul>"},{"location":"demos/talk_paper_demos/MCQMC_Tutorial_2020/MCQMC_2020_QMC_Software_Tutorial/#integrand-objects","title":"<code>Integrand</code> objects\u00b6","text":"<p>The <code>TrueMeasure</code> object becomes input to an <code>Integrand</code> object, which transforms our original integrand, $g$, to our eventual integrand, $f$.  This transformation relies on the <code>TrueMeasure</code> object along with its corresponding <code>DiscreteDistribution</code> object.  The object <code>qmcpy.Keister</code> has already been coded as a use case in <code>qmcpy</code>.</p>"},{"location":"demos/talk_paper_demos/MCQMC_Tutorial_2020/MCQMC_2020_QMC_Software_Tutorial/#stoppingcriterion-objects-and-the-integrate-method","title":"<code>StoppingCriterion</code> objects and the <code>integrate</code> method\u00b6","text":"<p>Determining the sample size needed requires a stopping criterion, which typically depends on the error tolerance, <code>abs_tol</code>.  The stopping criterion attempts to produce the answer satisfying the error tolerance with not much more work than is truly needed. There are several <code>StoppingCriterion</code> objects available, but they tend to work for specific LD sequences.  This one comes from Tony Jim\u00e9nez Rugama.  It takes as its input the <code>Integrand</code> object, which carries information about the <code>TrueMeasure</code> object and its <code>DiscreteDistribution</code> object.</p>"},{"location":"demos/talk_paper_demos/MCQMC_Tutorial_2020/MCQMC_2020_QMC_Software_Tutorial/#fixed-sample-budget-computation","title":"Fixed sample budget computation\u00b6","text":"<p>If you are not concerned about meeting an error tolerance but can only afford <code>n_max</code> function values, then you can set an <code>abs_tol</code> small enough and set <code>n_max</code> to your desired sample size.  You will get an error bound.</p>"},{"location":"demos/talk_paper_demos/MCQMC_Tutorial_2020/MCQMC_2020_QMC_Software_Tutorial/#qmc-is-much-faster-than-mc-qmc-cost-is-mostly-dimension-dependent","title":"QMC is much faster than MC. (Q)MC cost is mostly dimension dependent\u00b6","text":"<p>Run this next code block to see how the run time and the number of function evaluations increase as the tolerance decreases. QMC using LD sequences uses much less time and much fewer function values than MC using IID sequences.</p> <p>Tensor product rules have a time or function value cost that is $\\mathcal{O}(\\varepsilon^{-d/r})$, where $\\varepsilon$ is the error tolerance and $r$ is bounded above by both the smoothness of the integrand and the quality of the algorithm.  Such rules have a curse of dimensionality because their cost blows up exponentially with dimension.</p> <p>Unlike tensor product cubature rules, the cost of (Q)MC cubature is essentially dimension independent: $\\mathcal{O}(\\varepsilon^{-2})$ for IID MC and typically $\\mathcal{O}(\\varepsilon^{-1-\\delta})$ for QMC.  Although Q(MC) is not particularly fast, its performance usually does not degrade as the number of variables of in the integrand increases.</p>"},{"location":"demos/talk_paper_demos/MCQMC_Tutorial_2020/MCQMC_2020_QMC_Software_Tutorial/#alternatives-for-stoppingcriterion","title":"Alternatives for <code>StoppingCriterion</code>\u00b6","text":"<p>Other <code>StoppingCriterion</code> objects are available.  Most are tied to particular <code>DiscreteDistribution</code> objects.  For LD points one can use replications and the Central Limit Theorem (CLT).</p>"},{"location":"demos/talk_paper_demos/MCQMC_Tutorial_2020/MCQMC_2020_QMC_Software_Tutorial/#alternatives-for-truemeasure","title":"Alternatives for <code>TrueMeasure</code>\u00b6","text":"<p>The Keister integrand may also be solved using a Lebesgue <code>TrueMeasure</code> object: $$\\mu = \\int_{\\mathbb{R}^d} \\underbrace{\\cos(\\lVert\\boldsymbol{t}\\rVert) \\exp( - \\lVert\\boldsymbol{t}\\rVert^2)}_{g(\\boldsymbol{t})} \\,  \\underbrace{\\mathrm{d} \\boldsymbol{t}}_{\\text{Lebesgue measure}}$$ The <code>TrueMeasure</code> object contains the appropriate information so that the <code>Integrand</code> object can obtain the correct eventual integrand, $f$, in terms of the original integrand, $g$.</p> <p>Since <code>TrueMeasure</code> is not a probability measure, so it cannot be mimicked, but it can be used to solve the integration problem.  The <code>Integrand</code> object maps the problem using an affine transformation for finite boxes and an inverse normal distribution function transformation for $\\mathbb{R}^d$.</p> <p>The code below also shows how to take our own integrand defined with a simple input and output, and turn it into a <code>qmcpy</code> ready integrand using the <code>qmcpy.CustomFun</code> object.</p>"},{"location":"demos/talk_paper_demos/MCQMC_Tutorial_2020/MCQMC_2020_QMC_Software_Tutorial/#multi-level-qmc","title":"Multi-level (Q)MC\u00b6","text":"<p>When the dimesion of the multivariate integral is high, multi-level (Quasi-)Monte Carlo (ML(Q)MC) methods may save computation time.  The cost of one integrand value depends on the number of input variables, $d$, which corresponds to the dimension of our integration problem.  ML(Q)MC methods allow us attain our accuracy requirements by evaluating low dimensional integrands many times and high dimensional integrands much fewer times.</p> <p>High or infinte dimesional integration problems arise when computing the  expectations of quantities coming stochastic differential equations (SDEs).  These problems arise in finance applications. The dimension of the integrand typically refers to the number of time steps used to discretize the SDE.</p> <p>Here are some parameters for the Asian option examples below.  Changing them here allows you to compare the run times of these examples in a fair way.</p>"},{"location":"demos/talk_paper_demos/MCQMC_Tutorial_2020/MCQMC_2020_QMC_Software_Tutorial/#single-level-mc","title":"Single Level MC\u00b6","text":"<p>The vanilla way to solve this problem is IID Monte Carlo.  The fair price of the option is an expectation or integral, and the dimension is the number of time steps of used to discretize the Brownian motion that drives the SDE describing the price of the underlying asset.  The number of time steps should be fairly large.</p> <p>This paper by Lan Jiang and collaborators describes the stopping criterion.</p>"},{"location":"demos/talk_paper_demos/MCQMC_Tutorial_2020/MCQMC_2020_QMC_Software_Tutorial/#adaptive-multilevel-mc-from-mike-giles","title":"Adaptive Multilevel MC from Mike Giles\u00b6","text":"<p>Mike Giles and his collaborators have developed several ML(Q)MC algorithms.   The ML IID MC algorithm and stopping criterion implemented here are from this paper and this code. The answer is expected to be different than above, even with the same parameters, as the <code>MLCallOptions</code> uses a different discretization. The algorithm considers the SDE for logarithm of the stock price, which allows exact time stepping for constant interest rates and volatirilities, while Giles uses a Milstein discretization for the SDE for the stock price iteself.</p>"},{"location":"demos/talk_paper_demos/MCQMC_Tutorial_2020/MCQMC_2020_QMC_Software_Tutorial/#single-level-qmc-baseline","title":"Single Level QMC Baseline\u00b6","text":"<p>Tony Jim\u00e9nez's stopping criterion for cubature via Sobol' sequences in  this paper does not yet work for multi-level problems.  Here it is treating the option pricing problem as a high dimensional integral.</p>"},{"location":"demos/talk_paper_demos/MCQMC_Tutorial_2020/MCQMC_2020_QMC_Software_Tutorial/#importance-sampling","title":"Importance Sampling\u00b6","text":"<p>For the option pricing problem, we may add a drift to the Brownian motion as an example of importance sampling.</p> <p>First, we need to change our problem to one for which importance sampling can show some benefit.  We consider an out-of-the-money call option.</p>"},{"location":"demos/talk_paper_demos/MCQMC_Tutorial_2020/MCQMC_2020_QMC_Software_Tutorial/#under-the-hood","title":"Under the hood\u00b6","text":"<p>The structure of <code>qmcpy</code> is that there are five major classes:</p> <ul> <li><code>DiscreteDistribution</code> used to generate LD sequences, primarily on $[0,1]^d$</li> <li><code>TrueMeasure</code> for using these LD sequences to mimic other distributions and to define integrals with respect to other measures</li> <li><code>Integrand</code> to define the integrand for the multivariate integration problems</li> <li><code>StoppingCriterion</code> to determine when the desired accuracy has been reached</li> <li><code>AccumulateData</code> the invisible class used to keep track of important data as you continue to sample</li> </ul> <p>We look at some of the important parameters that the corresponding objects have.</p>"},{"location":"demos/talk_paper_demos/MCQMC_Tutorial_2020/MCQMC_2020_QMC_Software_Tutorial/#ld-sequence-generators","title":"LD sequence generators\u00b6","text":"<p>The LD generators (<code>DiscreteDistribution</code> objects) implemented here are drawn from several sources, which are denoted <code>backend</code> (first listed is the default):</p> <ul> <li>Sobol: QRNG, MPS, &amp; PyTorch</li> <li>Lattice: GAIL &amp; MPS, with default generating vectors from Lattice Builder</li> <li>Halton: Art Owen's &amp; QRNG</li> <li>Korobov: QRNG</li> </ul> <p>We illustrate some of the features of these varous backends and some of the other parameters that you can set.</p>"},{"location":"demos/talk_paper_demos/MCQMC_Tutorial_2020/MCQMC_2020_QMC_Software_Tutorial/#different-lattice-backends-and-generators","title":"Different lattice backends and generators\u00b6","text":"<p>The <code>qmcpy.Lattice</code> generator using the GAIL and MPS <code>backends</code> with the same generating vectors yield the same points but in a different order.  The <code>stopping criterion</code> <code>qmcpy.CubLatticeG</code> requires the GAIL order, but not all stopping criteria do.</p>"},{"location":"demos/talk_paper_demos/MCQMC_Tutorial_2020/MCQMC_2020_QMC_Software_Tutorial/#default-lds-are-randomized","title":"Default LDs are randomized\u00b6","text":"<p>LDs can be purely deterministic or they can be randomized.  For lattices this corresponds to a random shift modulo one.  For Sobol' sequences this corresponds to a random digital shift.  (PyTorch also uses random linear scrambling.)</p> <p>This randomization is turned on by default, but can also be turned off. With randomization off, the points will always look the same. Below the first sequence of points is randomized and so is different every time the code is run.  The second sequence is not randomized and stay the same.</p> <p>Turning off randomization throws a warning, which stops execution in Colab so we disable the warnings.</p>"},{"location":"demos/talk_paper_demos/MCQMC_Tutorial_2020/MCQMC_2020_QMC_Software_Tutorial/#unrandomized-lds-start-with-boldsymbol0","title":"Unrandomized LDs start with $\\boldsymbol{0}$\u00b6","text":"<p>By definition, without randomization the first point in all the popluar LD sequences is the origin.  You can try.</p>"},{"location":"demos/talk_paper_demos/MCQMC_Tutorial_2020/MCQMC_2020_QMC_Software_Tutorial/#coordinate-values-of-zero-and-one-may-be-problemetic-if-transformed-to-mathbbrd","title":"Coordinate values of zero and one may be problemetic if transformed to $\\mathbb{R}^d$\u00b6","text":"<p>If a coordinate value of a point constructed on $[0,1]^d$ is zero or one, then then $0$ is mapped to $-\\infty$ and $1$ is mapped to $\\infty$ when these points are transformed to $\\mathbb{R}^d$, as is done when solving problems with Gaussian distributions. In Python, these may turn out to be <code>nan</code>.  For many applications, infinities or <code>nan</code> will be troublesome.</p>"},{"location":"demos/talk_paper_demos/MCQMC_Tutorial_2020/MCQMC_2020_QMC_Software_Tutorial/#replications-with-a-fixed-seed","title":"Replications with a fixed seed\u00b6","text":"<p>For debugging purposes, you may wish to fix the seed of your randomized points.  This gives the advantage of an unchanging answer while avoiding the boundaries of the unit cube.  When rerunning the code below, the answers are unchanged iff the seed is fixed. For Tony's stopping criterion from GAIL, the points are randomized, but the algorithm is deterministic. For the fixed-multilevel CLT stopping criterion, the points and the algorithm are both random.</p>"},{"location":"demos/talk_paper_demos/MCQMC_Tutorial_2020/MCQMC_2020_QMC_Software_Tutorial/#help","title":"Help\u00b6","text":"<p>You can obtain help on any object by the <code>help(...)</code> or <code>dir(...)</code> commands.</p>"},{"location":"demos/talk_paper_demos/MCQMC_Tutorial_2020/MCQMC_2020_QMC_Software_Tutorial/#stopping-criteria","title":"Stopping criteria\u00b6","text":"<p>The stopping criteria implemented come from several sources:</p> <ul> <li>There are Central Limit Theorem (CLT) criteria for IID and LD sampling</li> <li>Rigorous stopping criteria imported from GAIL have been implemented for<ul> <li>IID sampling</li> <li>Lattice sampling</li> <li>Sobol' sampling</li> </ul> </li> <li>Multilevel stopping criteria due to Giles are given for IID and LD sampling</li> </ul>"},{"location":"demos/talk_paper_demos/MCQMC_Tutorial_2020/MCQMC_2020_QMC_Software_Tutorial/#changing-the-tolerance","title":"Changing the tolerance\u00b6","text":"<p>If you want to change the error tolerance, without creating a new <code>StoppingCriterion</code> object, there is a <code>set_tolerance</code> method.</p>"},{"location":"demos/talk_paper_demos/MCQMC_Tutorial_2020/MCQMC_2020_QMC_Software_Tutorial/#relative-error-tolerances","title":"Relative error tolerances\u00b6","text":"<p>For some problems a relative error tolerance may make more sense.  By default <code>rel_tol</code> is zero.  The stopping criterion stops when either of the two tolerances is met.</p>"},{"location":"demos/talk_paper_demos/MCQMC_Tutorial_2020/MCQMC_2020_QMC_Software_Tutorial/#use-cases","title":"Use cases\u00b6","text":"<p>Only a few use cases have been coded so far.  These include</p> <ul> <li><code>qmcpy.keister</code> Keiser's example</li> <li>European and Asian option pricing (Single and Multi-level)</li> <li>Computing Q-Noisy Expected Improvement (qEI) for Bayesian Optimization, see the blog at https://qmcpy.wpcomstaging.com/2020/07/19/qei-with-qmcpy/ and the Colaboratory notebook at https://drive.google.com/drive/folders/1EOREUL7lx4hytuZRQXB50UV8aE9JYN_a</li> <li>Importance Sampling</li> <li>Acceptance Rejection Sampling</li> <li>Custom Sampling by Inverse CDF Transform</li> </ul> <p><code>qmcpy</code> would benefit from contributions.</p>"},{"location":"demos/talk_paper_demos/MCQMC_Tutorial_2020/MCQMC_2020_QMC_Software_Tutorial/#acknowledgements-and-contributing","title":"Acknowledgements and Contributing\u00b6","text":"<p>The <code>qmcpy</code> code has primarily been written by Aleksei Sorokin (BS/MS expected in 2021), with the generous financial support of SigOpt https://sigopt.com.  However, much of the code is adapted from that of our friends (see above)</p> <p>We hope that QMCPy will become supported by the community.  Your contribution will add value to QMCPy, while allowing you to take advantage of the contribution of others.</p> <p>We highlight the value of community owned software.</p>"},{"location":"demos/talk_paper_demos/MCQMC_Tutorial_2020/MCQMC_2020_QMC_Software_Tutorial/#benefitting-from-each-others-work-is-easier-when-we-share","title":"Benefitting from each other's work is easier when we share\u00b6","text":"<ul> <li><p>Most of us are very good at just one or two things:</p> <ul> <li>LD sequence generators</li> <li>Increasing efficiency (e.g., MLMC)</li> <li>Stopping criteria</li> <li>Realistic use cases</li> </ul> <p>Having a shared software library let's us take advantage of the best</p> </li> <li><p>Provides a consistent interface for different pieces from different places</p> </li> <li><p>Supports reproducible computational research</p> </li> <li><p>Tedious stuff only needs to be figured out once</p> </li> </ul>"},{"location":"demos/talk_paper_demos/MCQMC_Tutorial_2020/MCQMC_2020_QMC_Software_Tutorial/#a-community-helps-find-and-correct-code-errors","title":"A community helps find and correct code errors\u00b6","text":"<p>By having more eyes on code than just the developer's we are more likely to spot errors or idiosyncracies.  Here are two examples.</p>"},{"location":"demos/talk_paper_demos/MCQMC_Tutorial_2020/MCQMC_2020_QMC_Software_Tutorial/#matlabs-sobol-generator","title":"MATLAB's Sobol' generator\u00b6","text":"<p>Several years ago Llu\u00eds Antoni Jim\u00e9nez Rugama discovered that the scrambling of the Sobol' generators implemented in MATLAB's Statistics Toolbox was wrong.  After reporting the problem to the developers, it was corrected in MATLAB 2017a</p>"},{"location":"demos/talk_paper_demos/MCQMC_Tutorial_2020/MCQMC_2020_QMC_Software_Tutorial/#pytorchs-sobol-generator","title":"PyTorch's Sobol' generator\u00b6","text":"<p>PyTorch is a popular Python library with its own Sobol generator.  However, it should be used with care.</p> <ul> <li>As noted above, the first point is skipped.</li> <li>We found that unless you specify double precision, you get points that have 1 as a coordinate far too often.  The developer seemed unaware of this.</li> </ul> <p>These issues have been reported at https://github.com/pytorch/pytorch/issues/32047.</p>"},{"location":"demos/talk_paper_demos/MCQMC_Tutorial_2020/MCQMC_2020_QMC_Software_Tutorial/#how-you-can-contribute","title":"How you can contribute\u00b6","text":"<p>After trying QMCPy out help us out by</p> <p>Easy.</p> <p>Submit your bugs and feature requests as issues to https://github.com/QMCSoftware/QMCSoftware/issues</p> <p>Moderately Difficult.</p> <p>Ask your students or collaborators to try QMCPy for their own work and submit their bugs and feature requests</p> <p>Heroic.</p> <p>Add a feature or use case and make a pull request at  https://github.com/QMCSoftware/QMCSoftware/pulls so that we can included it in our next release</p> <p>Questions?  Email us at qmc-software@googlegroups.com</p>"},{"location":"demos/talk_paper_demos/MCQMC_Tutorial_2020/MCQMC_2020_QMC_Software_Tutorial/#references","title":"References\u00b6","text":"<ol> <li><p>S.-C. T. Choi, Y. Ding, F. J. Hickernell, L. Jiang, Ll. A. Jimenez Rugama, D. Li, Jagadeeswaran R., X. Tong, K. Zhang, Y. Zhang, and X. Zhou, GAIL: Guaranteed Automatic Integration Library (Version 2.3.1) [MATLAB Software], 2020. Available from <code>http://gailgithub.github.io/GAIL_Dev/</code></p> </li> <li><p>H. Faure and C. Lemieux. \u201cImplementation of Irreducible Sobol\u2019 Sequences in Prime Power Bases,\u201d Mathematics and Computers in Simulation 161 (2019): 13\u201322.</p> </li> <li><p>M. B. Giles. \"Multi-level Monte Carlo path simulation,\" Operations Research, 56(3):607-617, 2008. <code>http://people.maths.ox.ac.uk/~gilesm/files/OPRE_2008.pdf</code>.</p> </li> <li><p>M. B. Giles. \"Improved multilevel Monte Carlo convergence using the Milstein scheme,\" 343-358, in Monte Carlo and Quasi-Monte Carlo Methods 2006, Springer, 2008. <code>http://people.maths.ox.ac.uk/~gilesm/files/mcqmc06.pdf</code>.</p> </li> <li><p>M. B. Giles and B. J. Waterhouse. \"Multilevel quasi-Monte Carlo path simulation,\" pp.165-181 in Advanced Financial Modelling, in Radon Series on Computational and Applied Mathematics, de Gruyter, 2009. <code>http://people.maths.ox.ac.uk/~gilesm/files/radon.pdf</code></p> </li> <li><p>F. J. Hickernell, L. Jiang, Y. Liu, and A. B. Owen, \"Guaranteed conservative fixed width confidence intervals via Monte Carlo sampling,\" Monte Carlo and Quasi-Monte Carlo Methods 2012 (J. Dick, F.Y. Kuo, G. W. Peters, and I. H. Sloan, eds.), pp. 105-128, Springer-Verlag, Berlin, 2014. DOI: 10.1007/978-3-642-41095-6_5</p> </li> <li><p>F. J. Hickernell and Lluis Antoni Jimenez Rugama, \"Reliable adaptive cubature using digital sequences,\" Monte Carlo and Quasi-Monte Carlo Methods: MCQMC, Leuven, Belgium, April 2014 (R. Cools and D. Nuyens, eds.), Springer Proceedings in Mathematics and Statistics, vol. 163, Springer-Verlag, Berlin, 2016, arXiv:1410.8615 [math.NA], pp. 367-383.</p> </li> <li><p>M. Hofert and C. Lemieux (2019). qrng: (Randomized) Quasi-Random Number Generators. R package version 0.0-7. <code>https://CRAN.R-project.org/package=qrng</code>.</p> </li> <li><p>Ll. A. Jimenez Rugama and F. J. Hickernell, \"Adaptive multidimensional integration based on rank-1 lattices,\" Monte Carlo and Quasi-Monte Carlo  Methods: MCQMC, Leuven, Belgium, April 2014 (R. Cools and D. Nuyens, eds.), Springer Proceedings in Mathematics and Statistics, vol. 163, Springer-Verlag, Berlin, 2016, arXiv:1411.1966, pp. 407-422.</p> </li> <li><p>B. D. Keister, Multidimensional Quadrature Algorithms,  'Computers in Physics', 10, pp. 119-122, 1996.</p> </li> <li><p>F. Y. Kuo and D. Nuyens. \"Application of quasi-Monte Carlo methods to elliptic PDEs with random diffusion coefficients - a survey of analysis and implementation,\" Foundations of Computational Mathematics, 16(6):1631-1696, 2016. (springer link, arxiv link)</p> </li> <li><p>P. L\u2019Ecuyer and D. Munger, \"LatticeBuilder: A General Software Tool for Constructing Rank-1 Lattice Rules,\" ACM Transactions on Mathematical Software. 42 (2015). 10.1145/2754929.</p> </li> <li><p>Y. Li, L. Kang, L., and F. J. Hickernell, Is a Transformed Low Discrepancy Design Also Low Discrepancy? in Contemporary Experimental Design, Multivariate Analysis and Data Mining, Festschrift in Honour of Professor Kai-Tai Fang (J. Fan and J. Pan, eds.), p. 69\u201392, 2020, https://arxiv.org/abs/2004.09887.</p> </li> <li><p>A. B. Owen, \"A randomized Halton algorithm in R,\" 2017. arXiv:1706.02808 [stat.CO]</p> </li> </ol>"},{"location":"demos/talk_paper_demos/Parslfest_2025/","title":"ParslFest 2025: Accelerating QMCPy Notebook Tests with Parsl","text":""},{"location":"demos/talk_paper_demos/Parslfest_2025/#parslfest-2025-accelerating-qmcpy-notebook-tests-with-parsl","title":"ParslFest 2025: Accelerating QMCPy Notebook Tests with Parsl","text":"<p>This directory contains demonstration notebooks for the ParslFest 2025 presentation on using Parsl to parallelize QMCPy notebook tests.</p> <p>Presentation: Accelerating QMCPy Notebook Tests with Parsl</p>"},{"location":"demos/talk_paper_demos/Parslfest_2025/#files","title":"Files","text":"<ul> <li><code>01_sequential.ipynb</code> - Runs QMCPy notebook tests sequentially without parallelization to establish a baseline execution time</li> <li><code>02_parallel.ipynb</code> - Runs the same notebook tests in parallel using Parsl with configurable worker count to measure speedup</li> <li><code>02_parallel_mac.ipynb</code> - macOS-specific version using ThreadPoolExecutor for compatibility</li> <li><code>03_visualize_speedup.ipynb</code> - Visualizes and analyzes the speedup achieved by parallel execution across different worker configurations</li> <li><code>Makefile</code> - Automates execution of all notebooks sequentially with multiple worker configurations</li> <li><code>.gitignore</code> - Git ignore rules for output files and temporary data</li> <li><code>output/</code> - Directory containing execution timing results, speedup metrics (CSV files), and plots (PNG files)</li> <li><code>runinfo/</code> - Directory containing Parsl execution logs and runtime information</li> <li><code>Parslfest_2025.deck</code> -  presentation deck </li> <li><code>Parslfest_2025.pdf</code> - Exported PDF of the presentation slides</li> </ul>"},{"location":"demos/talk_paper_demos/Parslfest_2025/#quick-start","title":"Quick Start","text":""},{"location":"demos/talk_paper_demos/Parslfest_2025/#automated-execution-recommended","title":"Automated Execution (Recommended)","text":"<p>Run all notebooks automatically with different worker configurations:</p> <pre><code>make clean &amp;&amp; make all\n</code></pre> <p>This will: 1. Clean previous outputs 2. Run <code>01_sequential.ipynb</code> to establish baseline 3. Run <code>02_parallel.ipynb</code> with 2, 4, 8, and 16 workers using <code>PARSL_MAX_WORKERS</code> environment variable 4. Run <code>03_visualize_speedup.ipynb</code> to generate plots and analysis</p> <p>Individual targets: </p><pre><code>make sequential   # Run only sequential baseline\nmake parallel     # Run parallel tests with 2,4,8,16 workers\nmake visualize    # Generate plots from existing data\nmake clean        # Remove all output files\n</code></pre><p></p>"},{"location":"demos/talk_paper_demos/Parslfest_2025/#manual-execution","title":"Manual Execution","text":"<p>Important: Run the notebooks in order (01 \u2192 02 \u2192 03).</p>"},{"location":"demos/talk_paper_demos/Parslfest_2025/#1-sequential-baseline","title":"1. Sequential Baseline","text":"<p>Run <code>01_sequential.ipynb</code> to establish the sequential execution time baseline. This generates: - <code>output/sequential_time.csv</code> - Total execution time - <code>output/sequential_output_time.csv</code> - Individual test times - <code>output/sequential_output_memory.csv</code> - Memory usage per test</p>"},{"location":"demos/talk_paper_demos/Parslfest_2025/#2-parallel-execution","title":"2. Parallel Execution","text":"<p>Run <code>02_parallel.ipynb</code> to execute tests in parallel with Parsl. </p> <p>Environment variable control: </p><pre><code># Set PARSL_MAX_WORKERS before running:\nimport os\nos.environ['PARSL_MAX_WORKERS'] = '4'  # Use 4 workers\n</code></pre><p></p> <p>Each run creates <code>output/parallel_times_{N}.csv</code> and <code>output/parallel_output_{N}.txt</code>.</p>"},{"location":"demos/talk_paper_demos/Parslfest_2025/#3-visualization","title":"3. Visualization","text":"<p>Run <code>03_visualize_speedup.ipynb</code> to generate plots comparing sequential vs parallel execution times and compute speedup ratios.</p> <p>Maximum theoretical speedup: 5.5x (limited by the longest single test: <code>tb_iris</code> at 138s).</p>"},{"location":"demos/talk_paper_demos/Parslfest_2025/#assumptions","title":"Assumptions","text":"<ol> <li>Reproducible runtimes: Test execution times are consistent across runs.</li> <li>Independent tests: Each notebook test can run independently without shared state.</li> <li>Single-threaded tests: Individual tests do not benefit from multiple cores internally.</li> <li>No I/O contention: Parallel tests do not compete for disk or network resources.</li> <li>Accurate estimates: Runtime estimates in <code>test_runtimes.py</code> reflect actual execution times.</li> </ol>"},{"location":"demos/talk_paper_demos/Parslfest_2025/#updating-runtime-estimates","title":"Updating Runtime Estimates","text":"<p>After running sequential tests, update <code>test_runtimes.py</code> with new timing data:</p> <pre><code>make booktests_no_docker  # Run sequential tests, note the timing output\n# Then update TEST_RUNTIMES dict in test_runtimes.py\n</code></pre>"},{"location":"demos/talk_paper_demos/Parslfest_2025/01_sequential/","title":"Sequential Implementation","text":"In\u00a0[1]: Copied! <pre>import os\nimport re\nfrom util import setup_environment, run_make_command, parse_total_time\n\n# Configuration flags: set force_compute=False to reuse existing outputs; set True to force re-run\nforce_compute = True\nis_debug = False  \n\nprint(f\"{os.getcwd() = }\")\noutput_dir = setup_environment()\n\nif force_compute:  # Clean local only files if force_compute is set\n    !(cd ../../.. &amp;&amp; make clean_local_only_files)\n    !rm -fr output/*.* runinfo/  # remove output directory contents\n</pre> import os import re from util import setup_environment, run_make_command, parse_total_time  # Configuration flags: set force_compute=False to reuse existing outputs; set True to force re-run force_compute = True is_debug = False    print(f\"{os.getcwd() = }\") output_dir = setup_environment()  if force_compute:  # Clean local only files if force_compute is set     !(cd ../../.. &amp;&amp; make clean_local_only_files)     !rm -fr output/*.* runinfo/  # remove output directory contents <pre>os.getcwd() = '/Users/terrya/Documents/ProgramData/QMCSoftware_resume/demos/talk_paper_demos/Parslfest_2025'\nrm -fr test/booktests/.ipynb_checkpoints/\nchmod +x scripts/find_local_only_folders.sh &gt; /dev/null 2&gt;&amp;1\nfor f in ; do \\\n\t\trm -f \"$f\"; &gt; /dev/null 2&gt;&amp;1; \\\n\tdone\n</pre> In\u00a0[2]: Copied! <pre>out_path = os.path.join(output_dir, \"sequential_output.csv\")\nif (not os.path.exists(out_path)) or force_compute:\n    run_make_command(\"booktests_no_docker\", out_path, is_debug=is_debug)\n</pre> out_path = os.path.join(output_dir, \"sequential_output.csv\") if (not os.path.exists(out_path)) or force_compute:     run_make_command(\"booktests_no_docker\", out_path, is_debug=is_debug) Out[2]: <pre>True</pre> In\u00a0[3]: Copied! <pre>import pandas as pd\n\nseq_path = os.path.join(output_dir, \"sequential_output.csv\")\nsequential_time = parse_total_time(seq_path)\n\nif sequential_time &gt; 0:  # save time to sequential_time.csv\n    seq_time_path = os.path.join(output_dir, \"sequential_time.csv\")\n    with open(seq_time_path, \"w\") as f:\n        _ = f.write(f\"{sequential_time:.2f}\\n\")\n    print(f\"Sequential time: {sequential_time:.2f} seconds\")\nelse:\n    print(\"Warning: Could not parse sequential time from output\")\n\n# Parse individual test memory and time\nwith open(seq_path, \"r\") as f:\n    text = f.read()\ntest_pattern = re.compile(r\"(test_\\w+)\\s+\\([^)]+\\)\\s+\\.\\.\\.\\s+Memory used:\\s*([\\d\\.]+)\\s*GB\\.\\s*Test time:\\s*([\\d\\.]+)\\s*s\")\ntest_matches = test_pattern.findall(text)\n\nif test_matches:  # Write time data to CSV files\n    seq_time_detail_path = os.path.join(output_dir, \"sequential_output_time.csv\")\n    rows = [{\"Notebook\": t[0].replace(\"_notebook\", \"\"), \"Memory_GB\": float(t[1]), \"Time_s\": float(t[2])} \n            for t in test_matches]\n    df = pd.DataFrame(rows).sort_values(by=\"Time_s\", ascending=False)\n    df.to_csv(seq_time_detail_path, index=False)\n    print(f\"Parsed {len(test_matches)} test results\")\nelse:\n    print(\"Warning: Could not parse individual test results\")\n</pre> import pandas as pd  seq_path = os.path.join(output_dir, \"sequential_output.csv\") sequential_time = parse_total_time(seq_path)  if sequential_time &gt; 0:  # save time to sequential_time.csv     seq_time_path = os.path.join(output_dir, \"sequential_time.csv\")     with open(seq_time_path, \"w\") as f:         _ = f.write(f\"{sequential_time:.2f}\\n\")     print(f\"Sequential time: {sequential_time:.2f} seconds\") else:     print(\"Warning: Could not parse sequential time from output\")  # Parse individual test memory and time with open(seq_path, \"r\") as f:     text = f.read() test_pattern = re.compile(r\"(test_\\w+)\\s+\\([^)]+\\)\\s+\\.\\.\\.\\s+Memory used:\\s*([\\d\\.]+)\\s*GB\\.\\s*Test time:\\s*([\\d\\.]+)\\s*s\") test_matches = test_pattern.findall(text)  if test_matches:  # Write time data to CSV files     seq_time_detail_path = os.path.join(output_dir, \"sequential_output_time.csv\")     rows = [{\"Notebook\": t[0].replace(\"_notebook\", \"\"), \"Memory_GB\": float(t[1]), \"Time_s\": float(t[2])}              for t in test_matches]     df = pd.DataFrame(rows).sort_values(by=\"Time_s\", ascending=False)     df.to_csv(seq_time_detail_path, index=False)     print(f\"Parsed {len(test_matches)} test results\") else:     print(\"Warning: Could not parse individual test results\") <pre>Sequential time: 482.21 seconds\nParsed 26 test results\n</pre> In\u00a0[4]: Copied! <pre># free memory\nimport gc\ngc.collect();\n</pre> # free memory import gc gc.collect();"},{"location":"demos/talk_paper_demos/Parslfest_2025/01_sequential/#sequential-implementation","title":"Sequential Implementation\u00b6","text":"<p>Requirements:</p> <ul> <li>QMCPy: <code>pip install qmcpy==2.1</code></li> <li>LaTeX: <code>sudo apt update &amp;&amp; sudo apt install -y texlive-full</code></li> <li>testbook : <code>pip install testbook==0.4.2</code></li> <li>Parsl: <code>pip install parsl==2025.7.28</code></li> </ul> <p>This notebook can be run interactively or in command line mode. To run in command line mode, use:</p> <pre>    jupyter nbconvert --to notebook --execute demos/talk_paper_demos/Parslfest_2025/01_sequential.ipynb \\\n  --ExecutePreprocessor.kernel_name=qmcpy --ExecutePreprocessor.timeout=3600 --inplace\n</pre> <p>Our presentation slides for ParslFest are available at Figma.</p>"},{"location":"demos/talk_paper_demos/Parslfest_2025/01_sequential/#1-sequential-execution","title":"1. Sequential Execution\u00b6","text":""},{"location":"demos/talk_paper_demos/Parslfest_2025/02_parallel/","title":"Parallel Implementation","text":"In\u00a0[1]: Copied! <pre>try:\n    import parsl as pl\nexcept ModuleNotFoundError:\n    !pip install -q parsl\n</pre> try:     import parsl as pl except ModuleNotFoundError:     !pip install -q parsl In\u00a0[2]: Copied! <pre>import os\nimport parsl as pl\nfrom util import setup_environment, find_repo_root, run_make_command, parse_total_time\n\n# Configuration flags\nforce_compute = True\nis_debug = False\nworkers = 2\n\noutput_dir = setup_environment()\n</pre> import os import parsl as pl from util import setup_environment, find_repo_root, run_make_command, parse_total_time  # Configuration flags force_compute = True is_debug = False workers = 2  output_dir = setup_environment() In\u00a0[3]: Copied! <pre>from parsl.config import Config\nfrom parsl.executors import ThreadPoolExecutor\n\n# Prefer explicit PARSL_MAX_WORKERS from environment when provided by the caller\n_env_workers = os.environ.get('PARSL_MAX_WORKERS')\nif _env_workers:\n    try:\n        max_workers = int(_env_workers)\n    except ValueError:\n        max_workers = None\nelse:\n    max_workers = None\n\nif not max_workers:  # Default fallback based on CPU count (at least 1, cap to cpu_count-1)\n    max_workers = min(workers, max(workers,  os.cpu_count()-1))\n\n# Use ThreadPoolExecutor (works reliably on macOS and Linux)\nconfig = Config(executors=[ThreadPoolExecutor(max_threads=max_workers, label=\"local_threads\")])\n\n# Ensure clean state: clear any existing Parsl config from previous runs\npl.clear()\n\n# Now load the config\npl.load(config)\nprint(f\"Parsl loaded with {max_workers} workers (PARSL_MAX_WORKERS env={os.environ.get('PARSL_MAX_WORKERS')})\")\n</pre> from parsl.config import Config from parsl.executors import ThreadPoolExecutor  # Prefer explicit PARSL_MAX_WORKERS from environment when provided by the caller _env_workers = os.environ.get('PARSL_MAX_WORKERS') if _env_workers:     try:         max_workers = int(_env_workers)     except ValueError:         max_workers = None else:     max_workers = None  if not max_workers:  # Default fallback based on CPU count (at least 1, cap to cpu_count-1)     max_workers = min(workers, max(workers,  os.cpu_count()-1))  # Use ThreadPoolExecutor (works reliably on macOS and Linux) config = Config(executors=[ThreadPoolExecutor(max_threads=max_workers, label=\"local_threads\")])  # Ensure clean state: clear any existing Parsl config from previous runs pl.clear()  # Now load the config pl.load(config) print(f\"Parsl loaded with {max_workers} workers (PARSL_MAX_WORKERS env={os.environ.get('PARSL_MAX_WORKERS')})\") Out[3]: <pre>&lt;parsl.dataflow.dflow.DataFlowKernel at 0x10b9a8cb0&gt;</pre> <pre>Parsl loaded with 2 workers (PARSL_MAX_WORKERS env=None)\n</pre> In\u00a0[4]: Copied! <pre>import parsl_test_runner\nimport inspect\n\n# See only functions\nprint(\"Functions:\")\nfunctions = inspect.getmembers(parsl_test_runner, inspect.isfunction)\nfor name, func in functions:\n    print(f\"- {name}\")\nprint(\"\\n\" + \"=\"*50)\n\n# Get help on specific function\nprint(\"Help for execute_parallel_tests:\")\nhelp(parsl_test_runner.execute_parallel_tests)\n</pre> import parsl_test_runner import inspect  # See only functions print(\"Functions:\") functions = inspect.getmembers(parsl_test_runner, inspect.isfunction) for name, func in functions:     print(f\"- {name}\") print(\"\\n\" + \"=\"*50)  # Get help on specific function print(\"Help for execute_parallel_tests:\") help(parsl_test_runner.execute_parallel_tests) <pre>Functions:\n- bash_app\n- execute_parallel_tests\n- generate_summary_report\n- main\n\n==================================================\nHelp for execute_parallel_tests:\nHelp on function execute_parallel_tests in module parsl_test_runner:\n\nexecute_parallel_tests()\n    Execute all testbook tests in parallel using Parsl\n\n</pre> In\u00a0[5]: Copied! <pre># Verify Parsl configuration\nprint(f\"Max workers configured: {max_workers}\")\nprint(f\"Active Parsl DFK: {pl.dfk()}\")\nprint(f\"Executors: {[executor.label for executor in pl.dfk().executors.values()]}\")\nif hasattr(config, 'executors'):\n    for executor in config.executors:\n        if hasattr(executor, 'max_workers_per_node'):\n            print(f\"Executor '{executor.label}' max_workers_per_node: {executor.max_workers_per_node}\")\n</pre> # Verify Parsl configuration print(f\"Max workers configured: {max_workers}\") print(f\"Active Parsl DFK: {pl.dfk()}\") print(f\"Executors: {[executor.label for executor in pl.dfk().executors.values()]}\") if hasattr(config, 'executors'):     for executor in config.executors:         if hasattr(executor, 'max_workers_per_node'):             print(f\"Executor '{executor.label}' max_workers_per_node: {executor.max_workers_per_node}\") <pre>Max workers configured: 2\nActive Parsl DFK: &lt;parsl.dataflow.dflow.DataFlowKernel object at 0x10b9a8cb0&gt;\nExecutors: ['local_threads', '_parsl_internal']\n</pre> In\u00a0[6]: Copied! <pre>import uuid\n\nexecution_id = str(uuid.uuid4())[:8]\nprint(f\"=== EXECUTION ID: {execution_id} ===\")\nprint(f\"Starting parallel test execution with {max_workers} workers...\")\n\npar_fname = os.path.join(output_dir, f\"parallel_times_{max_workers}.csv\")\npar_output = os.path.join(output_dir, f\"parallel_output_{max_workers}.txt\")\n\nif (not os.path.exists(par_fname)) or force_compute:\n    env = os.environ.copy()\n    env['PARSL_MAX_WORKERS'] = str(max_workers)\n    run_make_command(\"booktests_parallel_no_docker\", par_output, is_debug=is_debug, env=env)\n    \n    parallel_time = parse_total_time(par_output, r\"Total test time: ([\\d\\.]+)s\")\n    print(f\"\\n=== RESULTS FOR EXECUTION {execution_id} ===\")\n    print(f\"Parallel time: {parallel_time:.2f} seconds\")\n\n    with open(par_fname, \"w\") as f:\n        _ = f.write(f\"workers,time\\n{max_workers},{parallel_time:.2f}\\n\")\n    \n    print(f\"=== END EXECUTION {execution_id} ===\")\n</pre> import uuid  execution_id = str(uuid.uuid4())[:8] print(f\"=== EXECUTION ID: {execution_id} ===\") print(f\"Starting parallel test execution with {max_workers} workers...\")  par_fname = os.path.join(output_dir, f\"parallel_times_{max_workers}.csv\") par_output = os.path.join(output_dir, f\"parallel_output_{max_workers}.txt\")  if (not os.path.exists(par_fname)) or force_compute:     env = os.environ.copy()     env['PARSL_MAX_WORKERS'] = str(max_workers)     run_make_command(\"booktests_parallel_no_docker\", par_output, is_debug=is_debug, env=env)          parallel_time = parse_total_time(par_output, r\"Total test time: ([\\d\\.]+)s\")     print(f\"\\n=== RESULTS FOR EXECUTION {execution_id} ===\")     print(f\"Parallel time: {parallel_time:.2f} seconds\")      with open(par_fname, \"w\") as f:         _ = f.write(f\"workers,time\\n{max_workers},{parallel_time:.2f}\\n\")          print(f\"=== END EXECUTION {execution_id} ===\") <pre>=== EXECUTION ID: 0a17146f ===\nStarting parallel test execution with 2 workers...\n</pre> Out[6]: <pre>True</pre> <pre>=== RESULTS FOR EXECUTION 0a17146f ===\nParallel time: 322.83 seconds\n=== END EXECUTION 0a17146f ===\n</pre> In\u00a0[7]: Copied! <pre>!date\n!ls -ltr output\n</pre> !date !ls -ltr output <pre>Thu Dec  4 15:04:12 CST 2025\ntotal 368\n-rw-r--r--@ 1 terrya  staff    6267 Dec  4 14:56 sequential_output.csv\n-rw-r--r--@ 1 terrya  staff       7 Dec  4 14:56 sequential_time.csv\n-rw-r--r--@ 1 terrya  staff    6380 Dec  4 14:56 01_sequential_output.ipynb\n-rw-r--r--@ 1 terrya  staff  152935 Dec  4 15:00 parallel_times_speedup.png\n-rw-r--r--@ 1 terrya  staff    4501 Dec  4 15:04 parallel_output_2.txt\n-rw-r--r--@ 1 terrya  staff      22 Dec  4 15:04 parallel_times_2.csv\n</pre> In\u00a0[8]: Copied! <pre>import platform\n\nif platform.system().lower() == 'linux':\n    !uname -a\n    !nproc --all\n    !awk '/MemTotal/ {printf \"%.2f GB\\n\", $2/1024/1024}' /proc/meminfo\n</pre> import platform  if platform.system().lower() == 'linux':     !uname -a     !nproc --all     !awk '/MemTotal/ {printf \"%.2f GB\\n\", $2/1024/1024}' /proc/meminfo"},{"location":"demos/talk_paper_demos/Parslfest_2025/02_parallel/#parslfest-2025","title":"ParslFest 2025\u00b6","text":""},{"location":"demos/talk_paper_demos/Parslfest_2025/02_parallel/#accelerating-qmcpy-notebook-tests-with-parsl","title":"Accelerating QMCPy Notebook Tests with Parsl\u00b6","text":"<p>Joshua Herman, Brandon Sharp, and Sou-Cheng Choi, QMCPy Developers</p> <p>Aug 28 -- 29, 2025</p> <p>Updated: Dec 3, 2025</p> <p>Requirements:</p> <ul> <li>testbook : <code>pip install testbook==0.4.2</code></li> <li>Parsl: <code>pip install parsl==2025.7.28</code></li> </ul>"},{"location":"demos/talk_paper_demos/Parslfest_2025/02_parallel/#2-parsl","title":"2. Parsl\u00b6","text":"<ol> <li>Install and Configure Parsl</li> <li>Run the tests in parallel with Parsl</li> </ol>"},{"location":"demos/talk_paper_demos/Parslfest_2025/02_parallel/#21-configure-parsl","title":"2.1 Configure Parsl\u00b6","text":""},{"location":"demos/talk_paper_demos/Parslfest_2025/02_parallel/#22-create-a-parsl-test-runner","title":"2.2 Create a Parsl Test Runner\u00b6","text":""},{"location":"demos/talk_paper_demos/Parslfest_2025/02_parallel/#23-run-the-notebooks-in-parallel-with-parsl","title":"2.3 Run the Notebooks in Parallel with Parsl\u00b6","text":""},{"location":"demos/talk_paper_demos/Parslfest_2025/03_visualize_speedup/","title":"Speedup Visualization","text":"In\u00a0[1]: Copied! <pre>import os\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport re\nfrom util import setup_environment\n\n# Configuration flags\nforce_compute = True\nis_debug = False\n\noutput_dir = setup_environment()\n</pre> import os import matplotlib.pyplot as plt import pandas as pd import re from util import setup_environment  # Configuration flags force_compute = True is_debug = False  output_dir = setup_environment() In\u00a0[2]: Copied! <pre># Read the sequential test results from CSV files generated by 01_sequential.ipynb\nseq_time_detail_path = os.path.join(output_dir, \"sequential_output_time.csv\")\nseq_memory_path = os.path.join(output_dir, \"sequential_output_memory.csv\")\n\n# Read the CSV files\nif os.path.exists(seq_time_detail_path):\n    df = pd.read_csv(seq_time_detail_path)\n    df[\"Time_s\"] = df[\"Time_s\"].astype(float)\n    df[\"Memory_GB\"] = df[\"Memory_GB\"].astype(float)\n    \n    # Prepare dataframes sorted by time and memory\n    df_time = df.sort_values(by=\"Time_s\", ascending=False)\n    df_memory = df.sort_values(by=\"Memory_GB\", ascending=False)\n    \n    fig, axes = plt.subplots(1, 2, figsize=(12, 5), dpi=120)\n    \n    # Plot descending run time\n    _ = axes[0].bar(df_time['Notebook'], df_time['Time_s'], color='skyblue', alpha=0.5)\n    _ = axes[0].set_title(\"Test Time (s) Descending\")\n    _ = axes[0].set_xlabel(\"Notebook\")\n    _ = axes[0].set_ylabel(\"Test Time (s)\")\n    _ = axes[0].tick_params(axis='x', rotation=90)\n    \n    # Plot descending memory\n    _ = axes[1].bar(df_memory['Notebook'], df_memory['Memory_GB'], color='salmon', alpha=0.5)\n    _ = axes[1].set_title(\"Memory Used (GB) Descending\")\n    _ = axes[1].set_xlabel(\"Notebook\")\n    _ = axes[1].set_ylabel(\"Memory Used (GB)\")\n    _ = axes[1].tick_params(axis='x', rotation=90)\n    \n    plt.tight_layout()\n    plt.show();\n    \n    # save png\n    fig.savefig(os.path.join(output_dir, \"sequential_output.png\"))\n    print(f\"Generated sequential_output.png with {len(df)} test results\")\nelse:\n    print(f\"Warning: {seq_time_detail_path} not found. Run 01_sequential.ipynb first.\")\n</pre> # Read the sequential test results from CSV files generated by 01_sequential.ipynb seq_time_detail_path = os.path.join(output_dir, \"sequential_output_time.csv\") seq_memory_path = os.path.join(output_dir, \"sequential_output_memory.csv\")  # Read the CSV files if os.path.exists(seq_time_detail_path):     df = pd.read_csv(seq_time_detail_path)     df[\"Time_s\"] = df[\"Time_s\"].astype(float)     df[\"Memory_GB\"] = df[\"Memory_GB\"].astype(float)          # Prepare dataframes sorted by time and memory     df_time = df.sort_values(by=\"Time_s\", ascending=False)     df_memory = df.sort_values(by=\"Memory_GB\", ascending=False)          fig, axes = plt.subplots(1, 2, figsize=(12, 5), dpi=120)          # Plot descending run time     _ = axes[0].bar(df_time['Notebook'], df_time['Time_s'], color='skyblue', alpha=0.5)     _ = axes[0].set_title(\"Test Time (s) Descending\")     _ = axes[0].set_xlabel(\"Notebook\")     _ = axes[0].set_ylabel(\"Test Time (s)\")     _ = axes[0].tick_params(axis='x', rotation=90)          # Plot descending memory     _ = axes[1].bar(df_memory['Notebook'], df_memory['Memory_GB'], color='salmon', alpha=0.5)     _ = axes[1].set_title(\"Memory Used (GB) Descending\")     _ = axes[1].set_xlabel(\"Notebook\")     _ = axes[1].set_ylabel(\"Memory Used (GB)\")     _ = axes[1].tick_params(axis='x', rotation=90)          plt.tight_layout()     plt.show();          # save png     fig.savefig(os.path.join(output_dir, \"sequential_output.png\"))     print(f\"Generated sequential_output.png with {len(df)} test results\") else:     print(f\"Warning: {seq_time_detail_path} not found. Run 01_sequential.ipynb first.\") <pre>Warning: output/sequential_output_time.csv not found. Run 01_sequential.ipynb first.\n</pre> <p>Compute speedup = (sequential no-Parsl time) / (parallel Parsl time)</p> In\u00a0[3]: Copied! <pre># Read sequential time from sequential_time.csv\nseq_time_path = os.path.join(output_dir, \"sequential_time.csv\")\nif os.path.exists(seq_time_path):\n    with open(seq_time_path, \"r\") as f:\n        sequential_time = float(f.read().strip())\nelse:\n    # Fallback: try to extract from sequential_output.csv\n    match = re.search(r\"Ran \\d+ tests? in ([\\d\\.]+)s\", text)\n    if match:\n        sequential_time = float(match.group(1))\n    else:\n        sequential_time = 0.0\n\n# Collect all dataframes first, then concatenate once\ndfs = []\nfor worker in range(1, 17):\n    fname  = f\"parallel_times_{worker}.csv\"\n    if not os.path.exists(os.path.join(output_dir, fname)):\n        continue\n\n    # read CSV of parallel times if it exists\n    df_tmp = pd.read_csv(output_dir+os.sep+fname)\n    dfs.append(df_tmp)\n\n# Concatenate all at once (avoids FutureWarning about empty DataFrame concat)\nif dfs:\n    df = pd.concat(dfs, ignore_index=True)\n    df[\"speedup\"] = sequential_time / df[\"time\"]      # Compute speedup\nelse:\n    df = pd.DataFrame(columns=[\"workers\", \"time\", \"speedup\"])\n    \nprint(df)\n\n# [1, sequential_time, 1.0] to df2 (sequential baseline)\ndf2 = pd.DataFrame({\n    \"workers\": [1],\n    \"time\": [sequential_time],\n    \"speedup\": [1.0]\n})\n\ntimes = df[\"time\"] if \"time\" in df.columns else pd.Series([])\nworkers = df[\"workers\"] if \"workers\" in df.columns else pd.Series([])\nspeedup = df[\"speedup\"] if \"speedup\" in df.columns else pd.Series([])\n\nfig, axes = plt.subplots(1, 2, figsize=(10, 4), dpi=150)\n\n# Execution times\n# Only plot if both workers and times arrays are non-empty and have the same length\nif workers.size &gt; 0 and times.size &gt; 0 and workers.shape == times.shape:\n    axes[0].plot(workers, times, marker=\"o\", lw=2, color=\"steelblue\", label=\"Parsl\", alpha=0.5);\nelse:\n    axes[0].text(0.5, 0.5, \"No data to plot\", ha=\"center\", va=\"center\", fontsize=12)\n# Add No Parsl baseline point\naxes[0].plot(df2[\"workers\"], df2[\"time\"], marker=\"s\", markersize=10, color=\"darkorange\", label=\"No Parsl\", alpha=0.5);\naxes[0].set_xlabel(\"Number of Workers\")\naxes[0].set_ylabel(\"Time (s)\")\naxes[0].set_title(\"Execution Time vs Workers\")\naxes[0].set_ylim(bottom=0)  # Start y-axis at 0\naxes[0].legend()\naxes[0].grid(True, linestyle=\"--\", alpha=0.4)\naxes[0].xaxis.set_major_locator(plt.MaxNLocator(integer=True))\n\n# Speedup\n# Only plot if both workers and speedup arrays are non-empty and have the same length\nif workers.size &gt; 0 and speedup.size &gt; 0 and workers.shape == speedup.shape:\n    axes[1].plot(workers, speedup, marker=\"o\", lw=2, color=\"darkgreen\", label=\"Parsl\", alpha=0.5);\nelse:\n    axes[1].text(0.5, 0.5, \"No data to plot\", ha=\"center\", va=\"center\", fontsize=12)\n# Add No Parsl baseline point\naxes[1].plot(df2[\"workers\"], df2[\"speedup\"], marker=\"s\", markersize=10, color=\"darkorange\", label=\"No Parsl\", alpha=0.5);\naxes[1].set_xlabel(\"Number of Workers\")\naxes[1].set_ylabel(\"Speedup (sequential_time / time)\")\naxes[1].set_title(\"Speedup vs Workers\")\naxes[1].legend()\naxes[1].grid(True, linestyle=\"--\", alpha=0.4)\naxes[1].xaxis.set_major_locator(plt.MaxNLocator(integer=True))\n\n# Set top of speedup y-axis to the maximum speedup rounded to integer (minimum 1)\nimport math\nmax_speed = 1.0\ntry:\n    # consider both measured speedups and the sequential baseline\n    measured_max = df[\"speedup\"].max() if (\"speedup\" in df.columns and not df[\"speedup\"].empty) else 0.0\n    baseline = df2[\"speedup\"].max() if not df2[\"speedup\"].empty else 1.0\n    max_speed = max(measured_max, baseline, 1.0)\nexcept Exception:\n    max_speed = 1.0\n\ntop = max(1, int(math.ceil(float(max_speed))))\naxes[1].set_ylim(0, top)\n\nplt.tight_layout()\noutname = os.path.join(output_dir, \"parallel_times_speedup.png\")\nplt.savefig(outname, dpi=300, bbox_inches=\"tight\")\nprint(f\"Saved plot to {outname}\")\nplt.show();\n</pre> # Read sequential time from sequential_time.csv seq_time_path = os.path.join(output_dir, \"sequential_time.csv\") if os.path.exists(seq_time_path):     with open(seq_time_path, \"r\") as f:         sequential_time = float(f.read().strip()) else:     # Fallback: try to extract from sequential_output.csv     match = re.search(r\"Ran \\d+ tests? in ([\\d\\.]+)s\", text)     if match:         sequential_time = float(match.group(1))     else:         sequential_time = 0.0  # Collect all dataframes first, then concatenate once dfs = [] for worker in range(1, 17):     fname  = f\"parallel_times_{worker}.csv\"     if not os.path.exists(os.path.join(output_dir, fname)):         continue      # read CSV of parallel times if it exists     df_tmp = pd.read_csv(output_dir+os.sep+fname)     dfs.append(df_tmp)  # Concatenate all at once (avoids FutureWarning about empty DataFrame concat) if dfs:     df = pd.concat(dfs, ignore_index=True)     df[\"speedup\"] = sequential_time / df[\"time\"]      # Compute speedup else:     df = pd.DataFrame(columns=[\"workers\", \"time\", \"speedup\"])      print(df)  # [1, sequential_time, 1.0] to df2 (sequential baseline) df2 = pd.DataFrame({     \"workers\": [1],     \"time\": [sequential_time],     \"speedup\": [1.0] })  times = df[\"time\"] if \"time\" in df.columns else pd.Series([]) workers = df[\"workers\"] if \"workers\" in df.columns else pd.Series([]) speedup = df[\"speedup\"] if \"speedup\" in df.columns else pd.Series([])  fig, axes = plt.subplots(1, 2, figsize=(10, 4), dpi=150)  # Execution times # Only plot if both workers and times arrays are non-empty and have the same length if workers.size &gt; 0 and times.size &gt; 0 and workers.shape == times.shape:     axes[0].plot(workers, times, marker=\"o\", lw=2, color=\"steelblue\", label=\"Parsl\", alpha=0.5); else:     axes[0].text(0.5, 0.5, \"No data to plot\", ha=\"center\", va=\"center\", fontsize=12) # Add No Parsl baseline point axes[0].plot(df2[\"workers\"], df2[\"time\"], marker=\"s\", markersize=10, color=\"darkorange\", label=\"No Parsl\", alpha=0.5); axes[0].set_xlabel(\"Number of Workers\") axes[0].set_ylabel(\"Time (s)\") axes[0].set_title(\"Execution Time vs Workers\") axes[0].set_ylim(bottom=0)  # Start y-axis at 0 axes[0].legend() axes[0].grid(True, linestyle=\"--\", alpha=0.4) axes[0].xaxis.set_major_locator(plt.MaxNLocator(integer=True))  # Speedup # Only plot if both workers and speedup arrays are non-empty and have the same length if workers.size &gt; 0 and speedup.size &gt; 0 and workers.shape == speedup.shape:     axes[1].plot(workers, speedup, marker=\"o\", lw=2, color=\"darkgreen\", label=\"Parsl\", alpha=0.5); else:     axes[1].text(0.5, 0.5, \"No data to plot\", ha=\"center\", va=\"center\", fontsize=12) # Add No Parsl baseline point axes[1].plot(df2[\"workers\"], df2[\"speedup\"], marker=\"s\", markersize=10, color=\"darkorange\", label=\"No Parsl\", alpha=0.5); axes[1].set_xlabel(\"Number of Workers\") axes[1].set_ylabel(\"Speedup (sequential_time / time)\") axes[1].set_title(\"Speedup vs Workers\") axes[1].legend() axes[1].grid(True, linestyle=\"--\", alpha=0.4) axes[1].xaxis.set_major_locator(plt.MaxNLocator(integer=True))  # Set top of speedup y-axis to the maximum speedup rounded to integer (minimum 1) import math max_speed = 1.0 try:     # consider both measured speedups and the sequential baseline     measured_max = df[\"speedup\"].max() if (\"speedup\" in df.columns and not df[\"speedup\"].empty) else 0.0     baseline = df2[\"speedup\"].max() if not df2[\"speedup\"].empty else 1.0     max_speed = max(measured_max, baseline, 1.0) except Exception:     max_speed = 1.0  top = max(1, int(math.ceil(float(max_speed)))) axes[1].set_ylim(0, top)  plt.tight_layout() outname = os.path.join(output_dir, \"parallel_times_speedup.png\") plt.savefig(outname, dpi=300, bbox_inches=\"tight\") print(f\"Saved plot to {outname}\") plt.show(); <pre>   workers    time   speedup\n0        2  322.83  1.451693\nSaved plot to output/parallel_times_speedup.png\n</pre> In\u00a0[4]: Copied! <pre>!ls -ltr output/\n!date\n</pre> !ls -ltr output/ !date <pre>total 328\n-rw-r--r--@ 1 terrya  staff    6380 Dec  4 16:21 01_sequential_output.ipynb\n-rw-r--r--@ 1 terrya  staff    4501 Dec  4 16:21 parallel_output_2.txt\n-rw-r--r--@ 1 terrya  staff      22 Dec  4 16:21 parallel_times_2.csv\n-rw-r--r--@ 1 terrya  staff    6267 Dec  4 16:21 sequential_output.csv\n-rw-r--r--@ 1 terrya  staff       7 Dec  4 16:21 sequential_time.csv\n-rw-r--r--@ 1 terrya  staff  122313 Dec  4 16:27 parallel_times_speedup.png\nThu Dec  4 16:27:14 CST 2025\nThu Dec  4 16:27:14 CST 2025\n</pre>"},{"location":"demos/talk_paper_demos/Parslfest_2025/util/","title":"Util","text":"In\u00a0[\u00a0]: Copied! <pre>\"\"\"\nUtility functions for ParslFest 2025 notebooks.\nContains common helper functions used across multiple notebooks.\n\"\"\"\n</pre> \"\"\" Utility functions for ParslFest 2025 notebooks. Contains common helper functions used across multiple notebooks. \"\"\" In\u00a0[\u00a0]: Copied! <pre>import os\nimport sys\nimport subprocess\nimport re\n</pre> import os import sys import subprocess import re In\u00a0[\u00a0]: Copied! <pre>def find_repo_root(start=None):\n    \"\"\"\n    Find the repository root by looking for pyproject.toml.\n\n    Args:\n        start: Starting directory (default: current working directory)\n\n    Returns:\n        str: Path to the repository root\n\n    Raises:\n        FileNotFoundError: If repository root not found\n    \"\"\"\n    if start is None:\n        start = os.getcwd()\n\n    cur = start\n    while True:\n        if os.path.exists(os.path.join(cur, \"pyproject.toml\")):\n            return cur\n        parent = os.path.dirname(cur)\n        if parent == cur:\n            raise FileNotFoundError(\"Repository root not found (no pyproject.toml)\")\n        cur = parent\n</pre> def find_repo_root(start=None):     \"\"\"     Find the repository root by looking for pyproject.toml.      Args:         start: Starting directory (default: current working directory)      Returns:         str: Path to the repository root      Raises:         FileNotFoundError: If repository root not found     \"\"\"     if start is None:         start = os.getcwd()      cur = start     while True:         if os.path.exists(os.path.join(cur, \"pyproject.toml\")):             return cur         parent = os.path.dirname(cur)         if parent == cur:             raise FileNotFoundError(\"Repository root not found (no pyproject.toml)\")         cur = parent In\u00a0[\u00a0]: Copied! <pre>def setup_environment(output_dir=\"output\"):\n    \"\"\"\n    Common setup: add booktests to path and create output directory.\n\n    Args:\n        output_dir: Directory for output files (default: \"output\")\n\n    Returns:\n        str: The output directory path\n    \"\"\"\n    sys.path.append(os.path.join(find_repo_root(), \"test\", \"booktests\"))\n    os.makedirs(output_dir, exist_ok=True)\n    return output_dir\n</pre> def setup_environment(output_dir=\"output\"):     \"\"\"     Common setup: add booktests to path and create output directory.      Args:         output_dir: Directory for output files (default: \"output\")      Returns:         str: The output directory path     \"\"\"     sys.path.append(os.path.join(find_repo_root(), \"test\", \"booktests\"))     os.makedirs(output_dir, exist_ok=True)     return output_dir In\u00a0[\u00a0]: Copied! <pre>def run_make_command(cmd, output_file, is_debug=False, tests=None, env=None):\n    \"\"\"\n    Run a make command and capture output to a file.\n\n    Args:\n        cmd: Base make command (e.g., \"booktests_no_docker\")\n        output_file: Path to write output\n        is_debug: If True, use debug test set\n        tests: Custom test string (overrides is_debug default)\n        env: Environment variables dict (optional)\n\n    Returns:\n        bool: True if command succeeded\n    \"\"\"\n    is_linux = sys.platform.startswith(\"Linux\")\n\n    if tests is None and is_debug:\n        tests = \"tb_quickstart tb_qmcpy_intro tb_lattice_random_generator\"\n\n    if tests:\n        make_cmd = [\"make\", cmd, f\"TESTS={tests}\"]\n    else:\n        make_cmd = [\"make\", cmd]\n\n    if is_linux:\n        make_cmd = [\"taskset\", \"-c\", \"0\"] + make_cmd\n\n    repo_root = find_repo_root()\n\n    with open(output_file, \"wb\") as out_f:\n        try:\n            subprocess.run(\n                make_cmd,\n                cwd=repo_root,\n                stdout=out_f,\n                stderr=subprocess.STDOUT,\n                check=True,\n                env=env,\n            )\n            return True\n        except subprocess.CalledProcessError:\n            return False\n</pre> def run_make_command(cmd, output_file, is_debug=False, tests=None, env=None):     \"\"\"     Run a make command and capture output to a file.      Args:         cmd: Base make command (e.g., \"booktests_no_docker\")         output_file: Path to write output         is_debug: If True, use debug test set         tests: Custom test string (overrides is_debug default)         env: Environment variables dict (optional)      Returns:         bool: True if command succeeded     \"\"\"     is_linux = sys.platform.startswith(\"Linux\")      if tests is None and is_debug:         tests = \"tb_quickstart tb_qmcpy_intro tb_lattice_random_generator\"      if tests:         make_cmd = [\"make\", cmd, f\"TESTS={tests}\"]     else:         make_cmd = [\"make\", cmd]      if is_linux:         make_cmd = [\"taskset\", \"-c\", \"0\"] + make_cmd      repo_root = find_repo_root()      with open(output_file, \"wb\") as out_f:         try:             subprocess.run(                 make_cmd,                 cwd=repo_root,                 stdout=out_f,                 stderr=subprocess.STDOUT,                 check=True,                 env=env,             )             return True         except subprocess.CalledProcessError:             return False In\u00a0[\u00a0]: Copied! <pre>def parse_total_time(output_file, pattern=r\"Ran \\d+ tests? in ([\\d\\.]+)s\"):\n    \"\"\"\n    Parse total time from test output file.\n\n    Args:\n        output_file: Path to output file\n        pattern: Regex pattern to match time\n\n    Returns:\n        float: Parsed time or 0.0 if not found\n    \"\"\"\n    with open(output_file, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n        text = f.read()\n        match = re.search(pattern, text)\n        return float(match.group(1)) if match else 0.0\n</pre> def parse_total_time(output_file, pattern=r\"Ran \\d+ tests? in ([\\d\\.]+)s\"):     \"\"\"     Parse total time from test output file.      Args:         output_file: Path to output file         pattern: Regex pattern to match time      Returns:         float: Parsed time or 0.0 if not found     \"\"\"     with open(output_file, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:         text = f.read()         match = re.search(pattern, text)         return float(match.group(1)) if match else 0.0"},{"location":"demos/talk_paper_demos/Parslfest_2025/output/01_sequential_output/","title":"Sequential Output","text":"In\u00a0[1]: Copied! <pre>import os\nimport re\nfrom util import setup_environment, run_make_command, parse_total_time\n\n# Configuration flags: set force_compute=False to reuse existing outputs; set True to force re-run\nforce_compute = True\nis_debug = False  \n\nprint(f\"{os.getcwd() = }\")\noutput_dir = setup_environment()\n\nif force_compute:  # Clean local only files if force_compute is set\n    !(cd ../../.. &amp;&amp; make clean_local_only_files)\n    !rm -fr output/*.* runinfo/  # remove output directory contents\n</pre> import os import re from util import setup_environment, run_make_command, parse_total_time  # Configuration flags: set force_compute=False to reuse existing outputs; set True to force re-run force_compute = True is_debug = False    print(f\"{os.getcwd() = }\") output_dir = setup_environment()  if force_compute:  # Clean local only files if force_compute is set     !(cd ../../.. &amp;&amp; make clean_local_only_files)     !rm -fr output/*.* runinfo/  # remove output directory contents <pre>os.getcwd() = '/Users/terrya/Documents/ProgramData/QMCSoftware_resume/demos/talk_paper_demos/Parslfest_2025'\n</pre> <pre>rm -fr test/booktests/.ipynb_checkpoints/\nchmod +x scripts/find_local_only_folders.sh &gt; /dev/null 2&gt;&amp;1\nfor f in ; do \\\n\t\trm -f \"$f\"; &gt; /dev/null 2&gt;&amp;1; \\\n\tdone\n</pre> In\u00a0[2]: Copied! <pre>out_path = os.path.join(output_dir, \"sequential_output.csv\")\nif (not os.path.exists(out_path)) or force_compute:\n    run_make_command(\"booktests_no_docker\", out_path, is_debug=is_debug)\n</pre> out_path = os.path.join(output_dir, \"sequential_output.csv\") if (not os.path.exists(out_path)) or force_compute:     run_make_command(\"booktests_no_docker\", out_path, is_debug=is_debug) Out[2]: <pre>True</pre> In\u00a0[3]: Copied! <pre>import pandas as pd\n\nseq_path = os.path.join(output_dir, \"sequential_output.csv\")\nsequential_time = parse_total_time(seq_path)\n\nif sequential_time &gt; 0:  # save time to sequential_time.csv\n    seq_time_path = os.path.join(output_dir, \"sequential_time.csv\")\n    with open(seq_time_path, \"w\") as f:\n        _ = f.write(f\"{sequential_time:.2f}\\n\")\n    print(f\"Sequential time: {sequential_time:.2f} seconds\")\nelse:\n    print(\"Warning: Could not parse sequential time from output\")\n\n# Parse individual test memory and time\nwith open(seq_path, \"r\") as f:\n    text = f.read()\ntest_pattern = re.compile(r\"(test_\\w+)\\s+\\([^)]+\\)\\s+\\.\\.\\.\\s+Memory used:\\s*([\\d\\.]+)\\s*GB\\.\\s*Test time:\\s*([\\d\\.]+)\\s*s\")\ntest_matches = test_pattern.findall(text)\n\nif test_matches:  # Write time data to CSV files\n    seq_time_detail_path = os.path.join(output_dir, \"sequential_output_time.csv\")\n    rows = [{\"Notebook\": t[0].replace(\"_notebook\", \"\"), \"Memory_GB\": float(t[1]), \"Time_s\": float(t[2])} \n            for t in test_matches]\n    df = pd.DataFrame(rows).sort_values(by=\"Time_s\", ascending=False)\n    df.to_csv(seq_time_detail_path, index=False)\n    print(f\"Parsed {len(test_matches)} test results\")\nelse:\n    print(\"Warning: Could not parse individual test results\")\n</pre> import pandas as pd  seq_path = os.path.join(output_dir, \"sequential_output.csv\") sequential_time = parse_total_time(seq_path)  if sequential_time &gt; 0:  # save time to sequential_time.csv     seq_time_path = os.path.join(output_dir, \"sequential_time.csv\")     with open(seq_time_path, \"w\") as f:         _ = f.write(f\"{sequential_time:.2f}\\n\")     print(f\"Sequential time: {sequential_time:.2f} seconds\") else:     print(\"Warning: Could not parse sequential time from output\")  # Parse individual test memory and time with open(seq_path, \"r\") as f:     text = f.read() test_pattern = re.compile(r\"(test_\\w+)\\s+\\([^)]+\\)\\s+\\.\\.\\.\\s+Memory used:\\s*([\\d\\.]+)\\s*GB\\.\\s*Test time:\\s*([\\d\\.]+)\\s*s\") test_matches = test_pattern.findall(text)  if test_matches:  # Write time data to CSV files     seq_time_detail_path = os.path.join(output_dir, \"sequential_output_time.csv\")     rows = [{\"Notebook\": t[0].replace(\"_notebook\", \"\"), \"Memory_GB\": float(t[1]), \"Time_s\": float(t[2])}              for t in test_matches]     df = pd.DataFrame(rows).sort_values(by=\"Time_s\", ascending=False)     df.to_csv(seq_time_detail_path, index=False)     print(f\"Parsed {len(test_matches)} test results\") else:     print(\"Warning: Could not parse individual test results\") <pre>Sequential time: 468.65 seconds\nWarning: Could not parse individual test results\n</pre> In\u00a0[4]: Copied! <pre># free memory\nimport gc\ngc.collect();\n</pre> # free memory import gc gc.collect();"},{"location":"demos/talk_paper_demos/Parslfest_2025/output/01_sequential_output/#sequential-output","title":"Sequential Output\u00b6","text":"<p>Requirements:</p> <ul> <li>QMCPy: <code>pip install qmcpy==2.1</code></li> <li>LaTeX: <code>sudo apt update &amp;&amp; sudo apt install -y texlive-full</code></li> <li>testbook : <code>pip install testbook==0.4.2</code></li> <li>Parsl: <code>pip install parsl==2025.7.28</code></li> </ul> <p>This notebook can be run interactively or in command line mode. To run in command line mode, use:</p> <pre>    jupyter nbconvert --to notebook --execute demos/talk_paper_demos/Parslfest_2025/01_sequential.ipynb \\\n  --ExecutePreprocessor.kernel_name=qmcpy --ExecutePreprocessor.timeout=3600 --inplace\n</pre> <p>Our presentation slides for ParslFest are available at Figma.</p>"},{"location":"demos/talk_paper_demos/Parslfest_2025/output/01_sequential_output/#1-sequential-execution","title":"1. Sequential Execution\u00b6","text":""},{"location":"demos/talk_paper_demos/ProbFailureSorokinRao/prob_failure_gp_ci/","title":"2023 Probability of Failure Paper","text":"In\u00a0[1]: Copied! <pre>import qmcpy as qp\nimport numpy as np\nimport scipy.stats\nimport gpytorch\nimport torch\nimport os\nimport warnings\nimport pandas as pd\nfrom gpytorch.utils.warnings import NumericalWarning\nwarnings.filterwarnings(\"ignore\")\npd.set_option(\n    'display.max_rows', None,\n    'display.max_columns', None,\n    'display.width', 1000,\n    'display.colheader_justify', 'center',\n    'display.precision',2,\n    'display.float_format',lambda x:'%.1e'%x)\nfrom matplotlib import pyplot,gridspec\npyplot.style.use('seaborn-v0_8-whitegrid')\npyplot.rcParams['font.size'] = 25\npyplot.rcParams['legend.fontsize'] = 25\npyplot.rcParams['lines.linewidth'] = 5\npyplot.rcParams['lines.markersize'] = 10\n</pre> import qmcpy as qp import numpy as np import scipy.stats import gpytorch import torch import os import warnings import pandas as pd from gpytorch.utils.warnings import NumericalWarning warnings.filterwarnings(\"ignore\") pd.set_option(     'display.max_rows', None,     'display.max_columns', None,     'display.width', 1000,     'display.colheader_justify', 'center',     'display.precision',2,     'display.float_format',lambda x:'%.1e'%x) from matplotlib import pyplot,gridspec pyplot.style.use('seaborn-v0_8-whitegrid') pyplot.rcParams['font.size'] = 25 pyplot.rcParams['legend.fontsize'] = 25 pyplot.rcParams['lines.linewidth'] = 5 pyplot.rcParams['lines.markersize'] = 10 In\u00a0[2]: Copied! <pre>gpytorch_use_gpu = torch.cuda.is_available()\ngpytorch_use_gpu\n</pre> gpytorch_use_gpu = torch.cuda.is_available() gpytorch_use_gpu Out[2]: <pre>False</pre> In\u00a0[3]: Copied! <pre>cols = 2\nnticks = 129\nnp.random.seed(7); torch.manual_seed(1)\nci_percentage = .95\nbeta = scipy.stats.norm.ppf(np.mean([ci_percentage,1]))\n\nf = lambda x: x*np.sin(4*np.pi*x)\nx = qp.Lattice(1,seed=7).gen_samples(4).squeeze()\ny = f(x)\n\ngp = qp.util.ExactGPyTorchRegressionModel(\n    x_t = x[:,None],\n    y_t = y,\n    prior_mean = gpytorch.means.ZeroMean(),\n    prior_cov = gpytorch.kernels.ScaleKernel(gpytorch.kernels.MaternKernel(nu=2.5)),\n    likelihood = gpytorch.likelihoods.GaussianLikelihood(noise_constraint = gpytorch.constraints.Interval(1e-12,1e-8))) \ngp.fit(\n    optimizer = torch.optim.Adam(gp.parameters(),lr=0.1),\n    mll = gpytorch.mlls.ExactMarginalLogLikelihood(gp.likelihood,gp),\n    training_iter = 100)\n\nxticks = np.linspace(0,1,nticks)\nyticks = f(xticks)\nyhatticks_mean,yhatticks_std = gp.predict(xticks[:,None])\nf_preds = gp(torch.from_numpy(xticks[:,None]))\nf_samples = f_preds.sample(sample_shape=torch.Size((cols,))).numpy()\n\nfig = pyplot.figure(tight_layout=True)\ngs = gridspec.GridSpec(2,cols)\nax_top = fig.add_subplot(gs[0,:])\nscatter_data = ax_top.scatter(x,y,color='r')\nplot_yticks, = ax_top.plot(xticks,yticks,color='c')\nplot_post_mean, = ax_top.plot(xticks,yhatticks_mean,color='k')\nplot_gp_ci = ax_top.fill_between(xticks,yhatticks_mean+beta*yhatticks_std,yhatticks_mean-beta*yhatticks_std,color='k',alpha=.25)\nax_top.axhline(y=0,color='lightgreen')\nfor spine in ['top','bottom']: ax_top.spines[spine].set_visible(False)\nax_top.set_xlim([0,1]); ax_top.set_xticks([]); ax_top.set_yticks([])\nfor c in range(cols):\n    f_sample = f_samples[c]\n    ax = fig.add_subplot(gs[1,c]) if c==0 else fig.add_subplot(gs[1,c],sharey=ax)\n    ax.scatter(x,y,color='r')\n    ax.plot(xticks,yhatticks_mean,color='k')\n    ax.set_xlim([0,1]); ax.set_xticks([])\n    plot_draw, = ax.plot(xticks,f_sample,c='b')\n    ymin,ymax = ax.get_ylim()#; ax.set_aspect(1/(ymax-ymin))\n    yminticks,ymaxticks = np.tile(ymin,nticks),np.tile(ymax,nticks)\n    tp = (f_sample&gt;=0)*(yhatticks_mean&gt;=0)\n    tn = (f_sample&lt;0)*(yhatticks_mean&lt;0)\n    fp = (f_sample&lt;0)*(yhatticks_mean&gt;=0)\n    fn = (f_sample&gt;=0)*(yhatticks_mean&lt;0)\n    plot_tp, = ax.plot(xticks,np.ma.masked_where(~tp,ymaxticks),color='m',linewidth=5)\n    plot_fp, = ax.plot(xticks,np.ma.masked_where(~fp,ymaxticks),color='orange',linewidth=5)\n    plot_tn, = ax.plot(xticks,np.ma.masked_where(~tn,yminticks),color='m',linewidth=5)\n    plot_fn, = ax.plot(xticks,np.ma.masked_where(~fn,yminticks),color='orange',linewidth=5)\n    ax.set_ylim([ymin-(ymax-ymin)*.05,ymax+(ymax-ymin)*.05])\n    for spine in ['top','bottom']: ax.spines[spine].set_visible(False)\n    ax.set_yticks([])\n    plot_failure_threshold = ax.axhline(y=0,color='lightgreen')\nfig.legend([scatter_data,plot_yticks,plot_post_mean,plot_gp_ci],\n           ['data','simulation','posterior mean','posterior 95% CI'],\n           ncol=4,\n           frameon=False,\n           loc='upper center',\n           bbox_to_anchor=(.5,1.075),\n           prop={'size':11})\nfig.legend([plot_failure_threshold,plot_draw,plot_tp,plot_fp,],\n           ['failure threshold','sample path','True','False'],\n           ncol=5,\n           frameon=False,\n           loc='upper center',\n           bbox_to_anchor=(.5,0),#.58),\n           prop={'size':11})\nfig.savefig(\"outputs/TP_FP_TN_FN.png\",dpi=256,transparent=True,bbox_inches=\"tight\")\n</pre> cols = 2 nticks = 129 np.random.seed(7); torch.manual_seed(1) ci_percentage = .95 beta = scipy.stats.norm.ppf(np.mean([ci_percentage,1]))  f = lambda x: x*np.sin(4*np.pi*x) x = qp.Lattice(1,seed=7).gen_samples(4).squeeze() y = f(x)  gp = qp.util.ExactGPyTorchRegressionModel(     x_t = x[:,None],     y_t = y,     prior_mean = gpytorch.means.ZeroMean(),     prior_cov = gpytorch.kernels.ScaleKernel(gpytorch.kernels.MaternKernel(nu=2.5)),     likelihood = gpytorch.likelihoods.GaussianLikelihood(noise_constraint = gpytorch.constraints.Interval(1e-12,1e-8)))  gp.fit(     optimizer = torch.optim.Adam(gp.parameters(),lr=0.1),     mll = gpytorch.mlls.ExactMarginalLogLikelihood(gp.likelihood,gp),     training_iter = 100)  xticks = np.linspace(0,1,nticks) yticks = f(xticks) yhatticks_mean,yhatticks_std = gp.predict(xticks[:,None]) f_preds = gp(torch.from_numpy(xticks[:,None])) f_samples = f_preds.sample(sample_shape=torch.Size((cols,))).numpy()  fig = pyplot.figure(tight_layout=True) gs = gridspec.GridSpec(2,cols) ax_top = fig.add_subplot(gs[0,:]) scatter_data = ax_top.scatter(x,y,color='r') plot_yticks, = ax_top.plot(xticks,yticks,color='c') plot_post_mean, = ax_top.plot(xticks,yhatticks_mean,color='k') plot_gp_ci = ax_top.fill_between(xticks,yhatticks_mean+beta*yhatticks_std,yhatticks_mean-beta*yhatticks_std,color='k',alpha=.25) ax_top.axhline(y=0,color='lightgreen') for spine in ['top','bottom']: ax_top.spines[spine].set_visible(False) ax_top.set_xlim([0,1]); ax_top.set_xticks([]); ax_top.set_yticks([]) for c in range(cols):     f_sample = f_samples[c]     ax = fig.add_subplot(gs[1,c]) if c==0 else fig.add_subplot(gs[1,c],sharey=ax)     ax.scatter(x,y,color='r')     ax.plot(xticks,yhatticks_mean,color='k')     ax.set_xlim([0,1]); ax.set_xticks([])     plot_draw, = ax.plot(xticks,f_sample,c='b')     ymin,ymax = ax.get_ylim()#; ax.set_aspect(1/(ymax-ymin))     yminticks,ymaxticks = np.tile(ymin,nticks),np.tile(ymax,nticks)     tp = (f_sample&gt;=0)*(yhatticks_mean&gt;=0)     tn = (f_sample&lt;0)*(yhatticks_mean&lt;0)     fp = (f_sample&lt;0)*(yhatticks_mean&gt;=0)     fn = (f_sample&gt;=0)*(yhatticks_mean&lt;0)     plot_tp, = ax.plot(xticks,np.ma.masked_where(~tp,ymaxticks),color='m',linewidth=5)     plot_fp, = ax.plot(xticks,np.ma.masked_where(~fp,ymaxticks),color='orange',linewidth=5)     plot_tn, = ax.plot(xticks,np.ma.masked_where(~tn,yminticks),color='m',linewidth=5)     plot_fn, = ax.plot(xticks,np.ma.masked_where(~fn,yminticks),color='orange',linewidth=5)     ax.set_ylim([ymin-(ymax-ymin)*.05,ymax+(ymax-ymin)*.05])     for spine in ['top','bottom']: ax.spines[spine].set_visible(False)     ax.set_yticks([])     plot_failure_threshold = ax.axhline(y=0,color='lightgreen') fig.legend([scatter_data,plot_yticks,plot_post_mean,plot_gp_ci],            ['data','simulation','posterior mean','posterior 95% CI'],            ncol=4,            frameon=False,            loc='upper center',            bbox_to_anchor=(.5,1.075),            prop={'size':11}) fig.legend([plot_failure_threshold,plot_draw,plot_tp,plot_fp,],            ['failure threshold','sample path','True','False'],            ncol=5,            frameon=False,            loc='upper center',            bbox_to_anchor=(.5,0),#.58),            prop={'size':11}) fig.savefig(\"outputs/TP_FP_TN_FN.png\",dpi=256,transparent=True,bbox_inches=\"tight\") In\u00a0[4]: Copied! <pre>mcispfgp = qp.PFGPCI(\n    integrand = qp.Sin1d(qp.DigitalNetB2(1,seed=17),k=3),\n    failure_threshold = 0,\n    failure_above_threshold=True,\n    abs_tol = 1e-2,\n    alpha = 1e-1,\n    n_init = 4,\n    init_samples = None,\n    batch_sampler = qp.PFSampleErrorDensityAR(verbose=True),\n    n_batch = 4,\n    n_limit = 20,\n    n_approx = 2**18,\n    gpytorch_prior_mean = gpytorch.means.ZeroMean(),\n    gpytorch_prior_cov = gpytorch.kernels.ScaleKernel(\n        gpytorch.kernels.MaternKernel(nu=2.5,\n            lengthscale_constraint = gpytorch.constraints.Interval(.01,.1)\n            ),\n        outputscale_constraint = gpytorch.constraints.Interval(1e-3,10)\n        ),\n    gpytorch_likelihood = gpytorch.likelihoods.GaussianLikelihood(noise_constraint = gpytorch.constraints.Interval(1e-12,1e-8)),\n    gpytorch_marginal_log_likelihood_func = lambda likelihood,gpyt_model: gpytorch.mlls.ExactMarginalLogLikelihood(likelihood,gpyt_model),\n    torch_optimizer_func = lambda gpyt_model: torch.optim.Adam(gpyt_model.parameters(),lr=0.1),\n    gpytorch_train_iter = 100,\n    gpytorch_use_gpu = False,\n    verbose = 50,\n    n_ref_approx = 2**22,\n    seed_ref_approx = None)\nsolution,data = mcispfgp.integrate(seed=7,refit=False)\nprint(data)\ndf = pd.DataFrame(data.get_results_dict())\nprint(\"\\nIteration Summary\")\nprint(df)\nfig = data.plot()\ndf.to_csv(\"outputs/Sin1D_df.csv\",index=False)\nfig.savefig(\"outputs/Sin1D_fig.png\",dpi=256,transparent=True,bbox_inches=\"tight\")\n</pre> mcispfgp = qp.PFGPCI(     integrand = qp.Sin1d(qp.DigitalNetB2(1,seed=17),k=3),     failure_threshold = 0,     failure_above_threshold=True,     abs_tol = 1e-2,     alpha = 1e-1,     n_init = 4,     init_samples = None,     batch_sampler = qp.PFSampleErrorDensityAR(verbose=True),     n_batch = 4,     n_limit = 20,     n_approx = 2**18,     gpytorch_prior_mean = gpytorch.means.ZeroMean(),     gpytorch_prior_cov = gpytorch.kernels.ScaleKernel(         gpytorch.kernels.MaternKernel(nu=2.5,             lengthscale_constraint = gpytorch.constraints.Interval(.01,.1)             ),         outputscale_constraint = gpytorch.constraints.Interval(1e-3,10)         ),     gpytorch_likelihood = gpytorch.likelihoods.GaussianLikelihood(noise_constraint = gpytorch.constraints.Interval(1e-12,1e-8)),     gpytorch_marginal_log_likelihood_func = lambda likelihood,gpyt_model: gpytorch.mlls.ExactMarginalLogLikelihood(likelihood,gpyt_model),     torch_optimizer_func = lambda gpyt_model: torch.optim.Adam(gpyt_model.parameters(),lr=0.1),     gpytorch_train_iter = 100,     gpytorch_use_gpu = False,     verbose = 50,     n_ref_approx = 2**22,     seed_ref_approx = None) solution,data = mcispfgp.integrate(seed=7,refit=False) print(data) df = pd.DataFrame(data.get_results_dict()) print(\"\\nIteration Summary\") print(df) fig = data.plot() df.to_csv(\"outputs/Sin1D_df.csv\",index=False) fig.savefig(\"outputs/Sin1D_fig.png\",dpi=256,transparent=True,bbox_inches=\"tight\") <pre>reference approximation with d=1: 0.5000002384185791\nbatch 0\n\tgpytorch model fitting\n\t\titer 50  of 100\n\t\t\tlikelihood.noise_covar.raw_noise.................. -8.37e-02\n\t\t\tcovar_module.raw_outputscale...................... -3.03e+00\n\t\t\tcovar_module.base_kernel.raw_lengthscale.......... 5.02e+00\n\t\titer 100 of 100\n\t\t\tlikelihood.noise_covar.raw_noise.................. -9.22e-02\n\t\t\tcovar_module.raw_outputscale...................... -2.96e+00\n\t\t\tcovar_module.base_kernel.raw_lengthscale.......... 6.27e+00\nbatch 1\n\tAR sampling with efficiency 2.8e-01, expect 14 draws: 12, 15, 18, 21, 24, \nbatch 2\n\tAR sampling with efficiency 1.7e-01, expect 23 draws: 16, 24, 28, \nbatch 3\n\tAR sampling with efficiency 9.4e-02, expect 42 draws: 28, \nbatch 4\n\tAR sampling with efficiency 6.0e-02, expect 66 draws: 48, 72, \nPFGPCIData (Data)\n    solution        0.501\n    error_bound     0.111\n    bound_low       0.390\n    bound_high      0.611\n    n_total         20\n    time_integrate  0.363\nPFGPCI (AbstractStoppingCriterion)\n    abs_tol         0.010\n    n_init          2^(2)\n    n_limit         20\n    n_batch         2^(2)\nSin1d (AbstractIntegrand)\nUniform (AbstractTrueMeasure)\n    lower_bound     0\n    upper_bound     18.850\nDigitalNetB2 (AbstractLDDiscreteDistribution)\n    d               1\n    replications    1\n    randomize       LMS DS\n    gen_mats_source joe_kuo.6.21201.txt\n    order           RADICAL INVERSE\n    t               63\n    alpha           1\n    n_limit         2^(32)\n    entropy         17\n\nIteration Summary\n   iter  n_sum  n_batch  error_bounds  ci_low  ci_high  solutions  solutions_ref  error_ref  in_ci\n0    0     4       4       5.4e-01    0.0e+00 1.0e+00   4.6e-01      5.0e-01      3.9e-02    True \n1    1     8       4       5.0e-01    0.0e+00 1.0e+00   5.0e-01      5.0e-01      4.2e-03    True \n2    2    12       4       4.7e-01    3.5e-02 9.8e-01   5.1e-01      5.0e-01      6.4e-03    True \n3    3    16       4       3.0e-01    2.1e-01 8.1e-01   5.1e-01      5.0e-01      9.4e-03    True \n4    4    20       4       1.1e-01    3.9e-01 6.1e-01   5.0e-01      5.0e-01      5.2e-04    True \n</pre> In\u00a0[5]: Copied! <pre>mcispfgp = qp.PFGPCI(\n    integrand = qp.Multimodal2d(qp.DigitalNetB2(2,seed=17)),\n    failure_threshold = 0,\n    failure_above_threshold=True,\n    abs_tol = 1e-2,\n    alpha = 1e-1,\n    n_init = 64,\n    init_samples = None,\n    batch_sampler = qp.PFSampleErrorDensityAR(verbose=True),\n    n_batch = 16,\n    n_limit = 128,\n    n_approx = 2**18,\n    gpytorch_prior_mean = gpytorch.means.ZeroMean(),\n    gpytorch_prior_cov = gpytorch.kernels.ScaleKernel(\n        gpytorch.kernels.MaternKernel(nu=1.5,\n            lengthscale_constraint = gpytorch.constraints.Interval(.1,1)\n            ),\n        outputscale_constraint = gpytorch.constraints.Interval(1e-3,.5)\n        ),\n    gpytorch_likelihood = gpytorch.likelihoods.GaussianLikelihood(noise_constraint = gpytorch.constraints.Interval(1e-12,1e-8)),\n    gpytorch_marginal_log_likelihood_func = lambda likelihood,gpyt_model: gpytorch.mlls.ExactMarginalLogLikelihood(likelihood,gpyt_model),\n    torch_optimizer_func = lambda gpyt_model: torch.optim.Adam(gpyt_model.parameters(),lr=0.1),\n    gpytorch_train_iter = 800,\n    gpytorch_use_gpu = gpytorch_use_gpu,\n    verbose = 200,\n    n_ref_approx = 2**22,\n    seed_ref_approx = None)\nsolution,data = mcispfgp.integrate(seed=7,refit=False)\nprint(data)\ndf = pd.DataFrame(data.get_results_dict())\nprint(\"\\nIteration Summary\")\nprint(df)\nfig = data.plot()\ndf.to_csv(\"outputs/Multimodal2D_df.csv\",index=False)\nfig.savefig(\"outputs/Multimodal2D_fig.png\",dpi=256,transparent=True,bbox_inches=\"tight\")\n</pre> mcispfgp = qp.PFGPCI(     integrand = qp.Multimodal2d(qp.DigitalNetB2(2,seed=17)),     failure_threshold = 0,     failure_above_threshold=True,     abs_tol = 1e-2,     alpha = 1e-1,     n_init = 64,     init_samples = None,     batch_sampler = qp.PFSampleErrorDensityAR(verbose=True),     n_batch = 16,     n_limit = 128,     n_approx = 2**18,     gpytorch_prior_mean = gpytorch.means.ZeroMean(),     gpytorch_prior_cov = gpytorch.kernels.ScaleKernel(         gpytorch.kernels.MaternKernel(nu=1.5,             lengthscale_constraint = gpytorch.constraints.Interval(.1,1)             ),         outputscale_constraint = gpytorch.constraints.Interval(1e-3,.5)         ),     gpytorch_likelihood = gpytorch.likelihoods.GaussianLikelihood(noise_constraint = gpytorch.constraints.Interval(1e-12,1e-8)),     gpytorch_marginal_log_likelihood_func = lambda likelihood,gpyt_model: gpytorch.mlls.ExactMarginalLogLikelihood(likelihood,gpyt_model),     torch_optimizer_func = lambda gpyt_model: torch.optim.Adam(gpyt_model.parameters(),lr=0.1),     gpytorch_train_iter = 800,     gpytorch_use_gpu = gpytorch_use_gpu,     verbose = 200,     n_ref_approx = 2**22,     seed_ref_approx = None) solution,data = mcispfgp.integrate(seed=7,refit=False) print(data) df = pd.DataFrame(data.get_results_dict()) print(\"\\nIteration Summary\") print(df) fig = data.plot() df.to_csv(\"outputs/Multimodal2D_df.csv\",index=False) fig.savefig(\"outputs/Multimodal2D_fig.png\",dpi=256,transparent=True,bbox_inches=\"tight\") <pre>reference approximation with d=2: 0.30207324028015137\nbatch 0\n\tgpytorch model fitting\n\t\titer 200 of 800\n\t\t\tlikelihood.noise_covar.raw_noise.................. 1.45e+00\n\t\t\tcovar_module.raw_outputscale...................... 2.79e+00\n\t\t\tcovar_module.base_kernel.raw_lengthscale.......... -3.02e+00\n\t\titer 400 of 800\n\t\t\tlikelihood.noise_covar.raw_noise.................. 1.48e+00\n\t\t\tcovar_module.raw_outputscale...................... 3.54e+00\n\t\t\tcovar_module.base_kernel.raw_lengthscale.......... -3.33e+00\n\t\titer 600 of 800\n\t\t\tlikelihood.noise_covar.raw_noise.................. 1.52e+00\n\t\t\tcovar_module.raw_outputscale...................... 4.07e+00\n\t\t\tcovar_module.base_kernel.raw_lengthscale.......... -3.46e+00\n\t\titer 800 of 800\n\t\t\tlikelihood.noise_covar.raw_noise.................. 1.56e+00\n\t\t\tcovar_module.raw_outputscale...................... 4.48e+00\n\t\t\tcovar_module.base_kernel.raw_lengthscale.......... -3.51e+00\nbatch 1\n\tAR sampling with efficiency 4.9e-02, expect 323 draws: 224, 364, 392, 406, \nbatch 2\n\tAR sampling with efficiency 3.3e-02, expect 485 draws: 336, 609, 735, 798, 819, 840, 861, 882, \nbatch 3\n\tAR sampling with efficiency 2.1e-02, expect 764 draws: 528, 891, 1089, \nbatch 4\n\tAR sampling with efficiency 2.3e-02, expect 692 draws: 480, 660, 750, 810, \nPFGPCIData (Data)\n    solution        0.300\n    error_bound     0.090\n    bound_low       0.210\n    bound_high      0.390\n    n_total         2^(7)\n    time_integrate  1.557\nPFGPCI (AbstractStoppingCriterion)\n    abs_tol         0.010\n    n_init          2^(6)\n    n_limit         2^(7)\n    n_batch         2^(4)\nMultimodal2d (AbstractIntegrand)\nUniform (AbstractTrueMeasure)\n    lower_bound     [-4 -3]\n    upper_bound     [7 8]\nDigitalNetB2 (AbstractLDDiscreteDistribution)\n    d               2^(1)\n    replications    1\n    randomize       LMS DS\n    gen_mats_source joe_kuo.6.21201.txt\n    order           RADICAL INVERSE\n    t               63\n    alpha           1\n    n_limit         2^(32)\n    entropy         17\n\nIteration Summary\n   iter  n_sum  n_batch  error_bounds  ci_low  ci_high  solutions  solutions_ref  error_ref  in_ci\n0    0     64     64       2.5e-01    2.7e-02 5.2e-01   2.7e-01      3.0e-01      2.8e-02    True \n1    1     80     16       1.6e-01    1.2e-01 4.5e-01   2.9e-01      3.0e-01      1.5e-02    True \n2    2     96     16       1.0e-01    1.8e-01 3.9e-01   2.9e-01      3.0e-01      1.3e-02    True \n3    3    112     16       1.2e-01    1.8e-01 4.1e-01   3.0e-01      3.0e-01      6.1e-03    True \n4    4    128     16       9.0e-02    2.1e-01 3.9e-01   3.0e-01      3.0e-01      2.4e-03    True \n</pre> In\u00a0[6]: Copied! <pre>mcispfgp = qp.PFGPCI(\n    integrand = qp.FourBranch2d(qp.DigitalNetB2(2,seed=17)),\n    failure_threshold = 0,\n    failure_above_threshold=True,\n    abs_tol = 1e-2,\n    alpha = 1e-1,\n    n_init = 64,\n    init_samples = None,\n    batch_sampler = qp.PFSampleErrorDensityAR(verbose=True),\n    n_batch = 12,\n    n_limit = 200,\n    n_approx = 2**18,\n    gpytorch_prior_mean = gpytorch.means.ZeroMean(),\n    gpytorch_prior_cov = gpytorch.kernels.ScaleKernel(\n        gpytorch.kernels.MaternKernel(nu=1.5,\n            lengthscale_constraint = gpytorch.constraints.Interval(.5,1)\n            ),\n        outputscale_constraint = gpytorch.constraints.Interval(1e-8,.5)\n        ),\n    gpytorch_likelihood = gpytorch.likelihoods.GaussianLikelihood(noise_constraint = gpytorch.constraints.Interval(1e-12,1e-8)),\n    gpytorch_marginal_log_likelihood_func = lambda likelihood,gpyt_model: gpytorch.mlls.ExactMarginalLogLikelihood(likelihood,gpyt_model),\n    torch_optimizer_func = lambda gpyt_model: torch.optim.Adam(gpyt_model.parameters(),lr=0.1),\n    gpytorch_train_iter = 800,\n    gpytorch_use_gpu = gpytorch_use_gpu,\n    verbose = 200,\n    n_ref_approx = 2**22,\n    seed_ref_approx = None)\nsolution,data = mcispfgp.integrate(seed=7,refit=False)\nprint(data)\ndf = pd.DataFrame(data.get_results_dict())\nprint(\"\\nIteration Summary\")\nprint(df)\nfig = data.plot()\ndf.to_csv(\"outputs/FourBranch2D_df.csv\",index=False)\nfig.savefig(\"outputs/FourBranch2D_fig.png\",dpi=256,transparent=True,bbox_inches=\"tight\")\n</pre> mcispfgp = qp.PFGPCI(     integrand = qp.FourBranch2d(qp.DigitalNetB2(2,seed=17)),     failure_threshold = 0,     failure_above_threshold=True,     abs_tol = 1e-2,     alpha = 1e-1,     n_init = 64,     init_samples = None,     batch_sampler = qp.PFSampleErrorDensityAR(verbose=True),     n_batch = 12,     n_limit = 200,     n_approx = 2**18,     gpytorch_prior_mean = gpytorch.means.ZeroMean(),     gpytorch_prior_cov = gpytorch.kernels.ScaleKernel(         gpytorch.kernels.MaternKernel(nu=1.5,             lengthscale_constraint = gpytorch.constraints.Interval(.5,1)             ),         outputscale_constraint = gpytorch.constraints.Interval(1e-8,.5)         ),     gpytorch_likelihood = gpytorch.likelihoods.GaussianLikelihood(noise_constraint = gpytorch.constraints.Interval(1e-12,1e-8)),     gpytorch_marginal_log_likelihood_func = lambda likelihood,gpyt_model: gpytorch.mlls.ExactMarginalLogLikelihood(likelihood,gpyt_model),     torch_optimizer_func = lambda gpyt_model: torch.optim.Adam(gpyt_model.parameters(),lr=0.1),     gpytorch_train_iter = 800,     gpytorch_use_gpu = gpytorch_use_gpu,     verbose = 200,     n_ref_approx = 2**22,     seed_ref_approx = None) solution,data = mcispfgp.integrate(seed=7,refit=False) print(data) df = pd.DataFrame(data.get_results_dict()) print(\"\\nIteration Summary\") print(df) fig = data.plot() df.to_csv(\"outputs/FourBranch2D_df.csv\",index=False) fig.savefig(\"outputs/FourBranch2D_fig.png\",dpi=256,transparent=True,bbox_inches=\"tight\") <pre>reference approximation with d=2: 0.20871806144714355\nbatch 0\n\tgpytorch model fitting\n\t\titer 200 of 800\n\t\t\tlikelihood.noise_covar.raw_noise.................. 2.88e+00\n\t\t\tcovar_module.raw_outputscale...................... 3.85e+00\n\t\t\tcovar_module.base_kernel.raw_lengthscale.......... -4.50e+00\n\t\titer 400 of 800\n\t\t\tlikelihood.noise_covar.raw_noise.................. 3.63e+00\n\t\t\tcovar_module.raw_outputscale...................... 4.75e+00\n\t\t\tcovar_module.base_kernel.raw_lengthscale.......... -5.44e+00\n\t\titer 600 of 800\n\t\t\tlikelihood.noise_covar.raw_noise.................. 4.16e+00\n\t\t\tcovar_module.raw_outputscale...................... 5.33e+00\n\t\t\tcovar_module.base_kernel.raw_lengthscale.......... -6.04e+00\n\t\titer 800 of 800\n\t\t\tlikelihood.noise_covar.raw_noise.................. 4.58e+00\n\t\t\tcovar_module.raw_outputscale...................... 5.77e+00\n\t\t\tcovar_module.base_kernel.raw_lengthscale.......... -6.48e+00\nbatch 1\n\tAR sampling with efficiency 8.2e-03, expect 1470 draws: 1020, 1360, 1530, 1615, \nbatch 2\n\tAR sampling with efficiency 5.1e-03, expect 2344 draws: 1632, 2312, 2856, 3400, 3944, 4080, 4216, 4352, 4488, 4624, \nbatch 3\n\tAR sampling with efficiency 3.5e-03, expect 3474 draws: 2412, 3216, 3618, 3819, 4020, \nbatch 4\n\tAR sampling with efficiency 2.5e-03, expect 4789 draws: 3324, 5540, 6925, \nbatch 5\n\tAR sampling with efficiency 2.1e-03, expect 5641 draws: 3912, 5868, 6194, 6520, 6846, \nPFGPCIData (Data)\n    solution        0.209\n    error_bound     0.008\n    bound_low       0.202\n    bound_high      0.217\n    n_total         124\n    time_integrate  1.850\nPFGPCI (AbstractStoppingCriterion)\n    abs_tol         0.010\n    n_init          2^(6)\n    n_limit         200\n    n_batch         12\nFourBranch2d (AbstractIntegrand)\nUniform (AbstractTrueMeasure)\n    lower_bound     -8\n    upper_bound     2^(3)\nDigitalNetB2 (AbstractLDDiscreteDistribution)\n    d               2^(1)\n    replications    1\n    randomize       LMS DS\n    gen_mats_source joe_kuo.6.21201.txt\n    order           RADICAL INVERSE\n    t               63\n    alpha           1\n    n_limit         2^(32)\n    entropy         17\n\nIteration Summary\n   iter  n_sum  n_batch  error_bounds  ci_low  ci_high  solutions  solutions_ref  error_ref  in_ci\n0    0     64     64       4.1e-02    1.7e-01 2.5e-01   2.1e-01      2.1e-01      3.0e-03    True \n1    1     76     12       2.6e-02    1.9e-01 2.4e-01   2.1e-01      2.1e-01      5.4e-03    True \n2    2     88     12       1.7e-02    1.9e-01 2.3e-01   2.1e-01      2.1e-01      1.9e-03    True \n3    3    100     12       1.3e-02    2.0e-01 2.2e-01   2.1e-01      2.1e-01      1.0e-03    True \n4    4    112     12       1.1e-02    2.0e-01 2.2e-01   2.1e-01      2.1e-01      9.1e-05    True \n5    5    124     12       7.6e-03    2.0e-01 2.2e-01   2.1e-01      2.1e-01      5.5e-04    True \n</pre> In\u00a0[16]: Copied! <pre>mcispfgp = qp.PFGPCI(\n    integrand = qp.Ishigami(qp.DigitalNetB2(3,seed=17)),\n    failure_threshold = 0,\n    failure_above_threshold=False,\n    abs_tol = 1e-2,\n    alpha = 1e-1,\n    n_init = 128,\n    init_samples = None,\n    batch_sampler = qp.PFSampleErrorDensityAR(verbose=True),\n    n_batch = 16,\n    n_limit = 256,\n    n_approx = 2**18,\n    gpytorch_prior_mean = gpytorch.means.ZeroMean(),\n    gpytorch_prior_cov = gpytorch.kernels.ScaleKernel(\n        gpytorch.kernels.MaternKernel(nu=2.5,\n            lengthscale_constraint = gpytorch.constraints.Interval(.5,1)\n            ),\n        outputscale_constraint = gpytorch.constraints.Interval(1e-8,.5)\n        ),\n    gpytorch_likelihood = gpytorch.likelihoods.GaussianLikelihood(noise_constraint = gpytorch.constraints.Interval(1e-12,1e-8)),\n    gpytorch_marginal_log_likelihood_func = lambda likelihood,gpyt_model: gpytorch.mlls.ExactMarginalLogLikelihood(likelihood,gpyt_model),\n    torch_optimizer_func = lambda gpyt_model: torch.optim.Adam(gpyt_model.parameters(),lr=0.1),\n    gpytorch_train_iter = 800,\n    gpytorch_use_gpu = gpytorch_use_gpu,\n    verbose = 200,\n    n_ref_approx = 2**22,\n    seed_ref_approx = None)\nsolution,data = mcispfgp.integrate(seed=7,refit=False)\nprint(data)\ndf = pd.DataFrame(data.get_results_dict())\nprint(\"\\nIteration Summary\")\nprint(df)\npyplot.rcParams['font.size'] = 15\npyplot.rcParams['legend.fontsize'] = 15\nfig = data.plot()\nfig.tight_layout()\ndf.to_csv(\"outputs/Ishigami3D_df.csv\",index=False)\nfig.savefig(\"outputs/Ishigami3D_fig.png\",dpi=256,transparent=True,bbox_inches=\"tight\")\n</pre> mcispfgp = qp.PFGPCI(     integrand = qp.Ishigami(qp.DigitalNetB2(3,seed=17)),     failure_threshold = 0,     failure_above_threshold=False,     abs_tol = 1e-2,     alpha = 1e-1,     n_init = 128,     init_samples = None,     batch_sampler = qp.PFSampleErrorDensityAR(verbose=True),     n_batch = 16,     n_limit = 256,     n_approx = 2**18,     gpytorch_prior_mean = gpytorch.means.ZeroMean(),     gpytorch_prior_cov = gpytorch.kernels.ScaleKernel(         gpytorch.kernels.MaternKernel(nu=2.5,             lengthscale_constraint = gpytorch.constraints.Interval(.5,1)             ),         outputscale_constraint = gpytorch.constraints.Interval(1e-8,.5)         ),     gpytorch_likelihood = gpytorch.likelihoods.GaussianLikelihood(noise_constraint = gpytorch.constraints.Interval(1e-12,1e-8)),     gpytorch_marginal_log_likelihood_func = lambda likelihood,gpyt_model: gpytorch.mlls.ExactMarginalLogLikelihood(likelihood,gpyt_model),     torch_optimizer_func = lambda gpyt_model: torch.optim.Adam(gpyt_model.parameters(),lr=0.1),     gpytorch_train_iter = 800,     gpytorch_use_gpu = gpytorch_use_gpu,     verbose = 200,     n_ref_approx = 2**22,     seed_ref_approx = None) solution,data = mcispfgp.integrate(seed=7,refit=False) print(data) df = pd.DataFrame(data.get_results_dict()) print(\"\\nIteration Summary\") print(df) pyplot.rcParams['font.size'] = 15 pyplot.rcParams['legend.fontsize'] = 15 fig = data.plot() fig.tight_layout() df.to_csv(\"outputs/Ishigami3D_df.csv\",index=False) fig.savefig(\"outputs/Ishigami3D_fig.png\",dpi=256,transparent=True,bbox_inches=\"tight\") <pre>reference approximation with d=3: 0.1623830795288086\nbatch 0\n\tgpytorch model fitting\n\t\titer 200 of 800\n\t\t\tlikelihood.noise_covar.raw_noise.................. 2.21e+00\n\t\t\tcovar_module.raw_outputscale...................... 3.42e+00\n\t\t\tcovar_module.base_kernel.raw_lengthscale.......... -4.08e+00\n\t\titer 400 of 800\n\t\t\tlikelihood.noise_covar.raw_noise.................. 2.70e+00\n\t\t\tcovar_module.raw_outputscale...................... 4.25e+00\n\t\t\tcovar_module.base_kernel.raw_lengthscale.......... -4.98e+00\n\t\titer 600 of 800\n\t\t\tlikelihood.noise_covar.raw_noise.................. 3.12e+00\n\t\t\tcovar_module.raw_outputscale...................... 4.81e+00\n\t\t\tcovar_module.base_kernel.raw_lengthscale.......... -5.56e+00\n\t\titer 800 of 800\n\t\t\tlikelihood.noise_covar.raw_noise.................. 3.48e+00\n\t\t\tcovar_module.raw_outputscale...................... 5.23e+00\n\t\t\tcovar_module.base_kernel.raw_lengthscale.......... -5.99e+00\nbatch 1\n\tAR sampling with efficiency 6.2e-03, expect 2589 draws: 1792, 2240, 2576, \nbatch 2\n\tAR sampling with efficiency 5.1e-03, expect 3135 draws: 2176, 2992, 3672, 4080, 4352, 4624, 4896, \nbatch 3\n\tAR sampling with efficiency 4.3e-03, expect 3700 draws: 2560, 3040, \nbatch 4\n\tAR sampling with efficiency 3.8e-03, expect 4245 draws: 2944, 4600, 4968, \nbatch 5\n\tAR sampling with efficiency 3.3e-03, expect 4897 draws: 3392, 4876, 5936, 6360, 6572, \nbatch 6\n\tAR sampling with efficiency 2.9e-03, expect 5482 draws: 3808, 4284, \nbatch 7\n\tAR sampling with efficiency 2.6e-03, expect 6246 draws: 4336, 5691, \nbatch 8\n\tAR sampling with efficiency 2.3e-03, expect 6824 draws: 4736, 5328, \nPFGPCIData (Data)\n    solution        0.162\n    error_bound     0.010\n    bound_low       0.152\n    bound_high      0.172\n    n_total         2^(8)\n    time_integrate  4.859\nPFGPCI (AbstractStoppingCriterion)\n    abs_tol         0.010\n    n_init          2^(7)\n    n_limit         2^(8)\n    n_batch         2^(4)\nIshigami (AbstractIntegrand)\nUniform (AbstractTrueMeasure)\n    lower_bound     -3.142\n    upper_bound     3.142\nDigitalNetB2 (AbstractLDDiscreteDistribution)\n    d               3\n    replications    1\n    randomize       LMS DS\n    gen_mats_source joe_kuo.6.21201.txt\n    order           RADICAL INVERSE\n    t               63\n    alpha           1\n    n_limit         2^(32)\n    entropy         17\n\nIteration Summary\n   iter  n_sum  n_batch  error_bounds  ci_low  ci_high  solutions  solutions_ref  error_ref  in_ci\n0    0    128     128      3.1e-02    1.4e-01 2.0e-01   1.7e-01      1.6e-01      8.6e-03    True \n1    1    144      16      2.6e-02    1.4e-01 2.0e-01   1.7e-01      1.6e-01      7.7e-03    True \n2    2    160      16      2.2e-02    1.5e-01 1.9e-01   1.7e-01      1.6e-01      5.7e-03    True \n3    3    176      16      1.9e-02    1.5e-01 1.8e-01   1.7e-01      1.6e-01      3.0e-03    True \n4    4    192      16      1.6e-02    1.5e-01 1.8e-01   1.7e-01      1.6e-01      2.7e-03    True \n5    5    208      16      1.5e-02    1.5e-01 1.8e-01   1.6e-01      1.6e-01      1.8e-03    True \n6    6    224      16      1.3e-02    1.5e-01 1.8e-01   1.6e-01      1.6e-01      1.4e-03    True \n7    7    240      16      1.2e-02    1.5e-01 1.7e-01   1.6e-01      1.6e-01      8.7e-04    True \n8    8    256      16      1.0e-02    1.5e-01 1.7e-01   1.6e-01      1.6e-01      3.5e-04    True \n</pre> In\u00a0[\u00a0]: Copied! <pre>mcispfgp = qp.PFGPCI(\n    integrand = qp.Hartmann6d(qp.DigitalNetB2(6,seed=17)),\n    failure_threshold = -2,\n    failure_above_threshold=False,\n    abs_tol = 2.5e-3,\n    alpha = 1e-1,\n    n_init = 512,\n    init_samples = None,\n    batch_sampler = qp.PFSampleErrorDensityAR(verbose=True),\n    n_batch = 64,\n    n_limit = 2500,\n    n_approx = 2**18,\n    gpytorch_prior_mean = gpytorch.means.ZeroMean(),\n    gpytorch_prior_cov = gpytorch.kernels.ScaleKernel(gpytorch.kernels.MaternKernel(nu=2.5)),\n    gpytorch_likelihood = gpytorch.likelihoods.GaussianLikelihood(noise_constraint = gpytorch.constraints.Interval(1e-12,1e-8)),\n    gpytorch_marginal_log_likelihood_func = lambda likelihood,gpyt_model: gpytorch.mlls.ExactMarginalLogLikelihood(likelihood,gpyt_model),\n    torch_optimizer_func = lambda gpyt_model: torch.optim.Adam(gpyt_model.parameters(),lr=0.1),\n    gpytorch_train_iter = 150,\n    gpytorch_use_gpu = gpytorch_use_gpu,\n    verbose = 50,\n    n_ref_approx = 2**23,\n    seed_ref_approx = None)\nsolution,data = mcispfgp.integrate(seed=7,refit=False)\nprint(data)\ndf = pd.DataFrame(data.get_results_dict())\nprint(\"\\nIteration Summary\")\nprint(df)\npyplot.rcParams['font.size'] = 15\npyplot.rcParams['legend.fontsize'] = 15\nfig = data.plot()\nfig.tight_layout()\ndf.to_csv(\"outputs/Hartmann6D_df.csv\",index=False)\nfig.savefig(\"outputs/Hartmann6D_fig.png\",dpi=256,transparent=True,bbox_inches=\"tight\")\n</pre> mcispfgp = qp.PFGPCI(     integrand = qp.Hartmann6d(qp.DigitalNetB2(6,seed=17)),     failure_threshold = -2,     failure_above_threshold=False,     abs_tol = 2.5e-3,     alpha = 1e-1,     n_init = 512,     init_samples = None,     batch_sampler = qp.PFSampleErrorDensityAR(verbose=True),     n_batch = 64,     n_limit = 2500,     n_approx = 2**18,     gpytorch_prior_mean = gpytorch.means.ZeroMean(),     gpytorch_prior_cov = gpytorch.kernels.ScaleKernel(gpytorch.kernels.MaternKernel(nu=2.5)),     gpytorch_likelihood = gpytorch.likelihoods.GaussianLikelihood(noise_constraint = gpytorch.constraints.Interval(1e-12,1e-8)),     gpytorch_marginal_log_likelihood_func = lambda likelihood,gpyt_model: gpytorch.mlls.ExactMarginalLogLikelihood(likelihood,gpyt_model),     torch_optimizer_func = lambda gpyt_model: torch.optim.Adam(gpyt_model.parameters(),lr=0.1),     gpytorch_train_iter = 150,     gpytorch_use_gpu = gpytorch_use_gpu,     verbose = 50,     n_ref_approx = 2**23,     seed_ref_approx = None) solution,data = mcispfgp.integrate(seed=7,refit=False) print(data) df = pd.DataFrame(data.get_results_dict()) print(\"\\nIteration Summary\") print(df) pyplot.rcParams['font.size'] = 15 pyplot.rcParams['legend.fontsize'] = 15 fig = data.plot() fig.tight_layout() df.to_csv(\"outputs/Hartmann6D_df.csv\",index=False) fig.savefig(\"outputs/Hartmann6D_fig.png\",dpi=256,transparent=True,bbox_inches=\"tight\") <pre>reference approximation with d=6: 0.007385849952697754\nbatch 0\n\tgpytorch model fitting\n\t\titer 50  of 150\n\t\t\tlikelihood.noise_covar.raw_noise.................. 1.28e+00\n\t\t\tcovar_module.raw_outputscale...................... 5.62e-01\n\t\t\tcovar_module.base_kernel.raw_lengthscale.......... 1.90e-01\n\t\titer 100 of 150\n\t\t\tlikelihood.noise_covar.raw_noise.................. 2.28e+00\n\t\t\tcovar_module.raw_outputscale...................... 5.14e-01\n\t\t\tcovar_module.base_kernel.raw_lengthscale.......... 1.83e-01\n\t\titer 150 of 150\n\t\t\tlikelihood.noise_covar.raw_noise.................. 2.84e+00\n\t\t\tcovar_module.raw_outputscale...................... 5.12e-01\n\t\t\tcovar_module.base_kernel.raw_lengthscale.......... 1.83e-01\nbatch 1\n\tAR sampling with efficiency 2.5e-03, expect 25281 draws: 17536, 19180, \nbatch 2\n\tAR sampling with efficiency 2.5e-03, expect 25500 draws: 17664, 25944, 27600, \nbatch 3\n\tAR sampling with efficiency 2.0e-03, expect 31766 draws: 22016, 30272, 31992, 33368, 34056, 34744, 35088, 35432, 35776, 36120, 36464, 36808, \nbatch 4\n\tAR sampling with efficiency 1.7e-03, expect 36697 draws: 25472, 31840, 34626, 35024, 35422, \nbatch 5\n\tAR sampling with efficiency 1.5e-03, expect 42097 draws: 29184, 42408, 46056, \nbatch 6\n\tAR sampling with efficiency 1.3e-03, expect 50094 draws: 34752, 45612, 46155, 46698, \nbatch 7\n\tAR sampling with efficiency 1.1e-03, expect 55935 draws: 38784, 49086, 53328, \nbatch 8\n\tAR sampling with efficiency 1.0e-03, expect 63192 draws: 43840, 56855, \nbatch 9\n\tAR sampling with efficiency 9.4e-04, expect 68419 draws: 47424, 57798, 63726, 65949, 68172, \nbatch 10\n\tAR sampling with efficiency 8.8e-04, expect 72820 draws: 50496, 65487, 67854, 69432, 71010, 71799, 72588, \nbatch 11\n\tAR sampling with efficiency 8.1e-04, expect 79266 draws: 54976, 65284, 69579, 70438, 71297, \nbatch 12\n\tAR sampling with efficiency 7.6e-04, expect 84428 draws: 58560, 61305, 63135, 64050, 64965, \nbatch 13\n\tAR sampling with efficiency 7.0e-04, expect 91332 draws: 63296, 74175, 75164, 76153, 77142, 78131, \nbatch 14\n\tAR sampling with efficiency 6.7e-04, expect 95110 draws: 65920, 79310, 83430, \nbatch 15\n\tAR sampling with efficiency 6.4e-04, expect 99354 draws: 68864, 88232, 90384, 92536, 94688, \nbatch 16\n\tAR sampling with efficiency 6.2e-04, expect 103811 draws: 71936, 104532, 115772, 116896, 118020, \nbatch 17\n\tAR sampling with efficiency 6.0e-04, expect 106125 draws: 73600, 83950, 86250, \nbatch 18\n\tAR sampling with efficiency 5.7e-04, expect 111408 draws: 77248, 90525, 95353, \nbatch 19\n\tAR sampling with efficiency 5.5e-04, expect 116498 draws: 80768, 99698, 112318, 116104, 118628, \nbatch 20\n\tAR sampling with efficiency 5.3e-04, expect 120838 draws: 83776, 102102, 111265, 112574, \nbatch 21\n\tAR sampling with efficiency 5.1e-04, expect 124745 draws: 86464, 114835, 131047, 133749, 135100, 136451, \nPFGPCIData (Data)\n    solution        0.007\n    error_bound     0.002\n    bound_low       0.005\n    bound_high      0.010\n    n_total         1856\n    time_integrate  114.951\nPFGPCI (AbstractStoppingCriterion)\n    abs_tol         0.003\n    n_init          2^(9)\n    n_limit         2500\n    n_batch         2^(6)\nHartmann6d (AbstractIntegrand)\nUniform (AbstractTrueMeasure)\n    lower_bound     0\n    upper_bound     1\nDigitalNetB2 (AbstractLDDiscreteDistribution)\n    d               6\n    replications    1\n    randomize       LMS DS\n    gen_mats_source joe_kuo.6.21201.txt\n    order           RADICAL INVERSE\n    t               63\n    alpha           1\n    n_limit         2^(32)\n    entropy         17\n\nIteration Summary\n    iter  n_sum  n_batch  error_bounds  ci_low  ci_high  solutions  solutions_ref  error_ref  in_ci\n0     0    512     512      1.3e-02    0.0e+00 1.6e-02   3.3e-03      7.4e-03      4.1e-03    True \n1     1    576      64      1.3e-02    0.0e+00 1.9e-02   6.9e-03      7.4e-03      5.1e-04    True \n2     2    640      64      1.0e-02    0.0e+00 1.7e-02   7.2e-03      7.4e-03      1.5e-04    True \n3     3    704      64      8.7e-03    0.0e+00 1.6e-02   7.4e-03      7.4e-03      8.2e-06    True \n4     4    768      64      7.6e-03    0.0e+00 1.5e-02   7.4e-03      7.4e-03      4.4e-06    True \n5     5    832      64      6.4e-03    1.0e-03 1.4e-02   7.4e-03      7.4e-03      4.1e-05    True \n6     6    896      64      5.7e-03    1.7e-03 1.3e-02   7.5e-03      7.4e-03      8.0e-05    True \n7     7    960      64      5.1e-03    2.4e-03 1.3e-02   7.5e-03      7.4e-03      8.0e-05    True \n8     8   1024      64      4.7e-03    2.7e-03 1.2e-02   7.4e-03      7.4e-03      2.2e-05    True \n9     9   1088      64      4.4e-03    3.0e-03 1.2e-02   7.4e-03      7.4e-03      2.2e-05    True \n10   10   1152      64      4.0e-03    3.4e-03 1.1e-02   7.4e-03      7.4e-03      3.2e-06    True \n11   11   1216      64      3.8e-03    3.6e-03 1.1e-02   7.4e-03      7.4e-03      1.8e-05    True \n12   12   1280      64      3.5e-03    3.9e-03 1.1e-02   7.4e-03      7.4e-03      1.5e-05    True \n13   13   1344      64      3.4e-03    4.0e-03 1.1e-02   7.4e-03      7.4e-03      1.8e-05    True \n14   14   1408      64      3.2e-03    4.2e-03 1.1e-02   7.4e-03      7.4e-03      1.5e-05    True \n15   15   1472      64      3.1e-03    4.3e-03 1.0e-02   7.4e-03      7.4e-03      6.0e-07    True \n16   16   1536      64      3.0e-03    4.4e-03 1.0e-02   7.4e-03      7.4e-03      7.0e-06    True \n17   17   1600      64      2.9e-03    4.5e-03 1.0e-02   7.4e-03      7.4e-03      1.8e-05    True \n18   18   1664      64      2.7e-03    4.6e-03 1.0e-02   7.4e-03      7.4e-03      6.0e-07    True \n19   19   1728      64      2.6e-03    4.7e-03 1.0e-02   7.4e-03      7.4e-03      6.0e-07    True \n20   20   1792      64      2.6e-03    4.8e-03 9.9e-03   7.4e-03      7.4e-03      2.0e-05    True \n21   21   1856      64      2.5e-03    4.9e-03 9.9e-03   7.4e-03      7.4e-03      2.0e-05    True \n</pre> In\u00a0[3]: Copied! <pre>import umbridge\n</pre> import umbridge In\u00a0[4]: Copied! <pre>!docker run --name tsunami -d -it -p 4242:4242 linusseelinger/model-exahype-tsunami:latest # https://um-bridge-benchmarks.readthedocs.io/en/docs/models/exahype-tsunami.html\n</pre> !docker run --name tsunami -d -it -p 4242:4242 linusseelinger/model-exahype-tsunami:latest # https://um-bridge-benchmarks.readthedocs.io/en/docs/models/exahype-tsunami.html <pre>211181b555f7cfc0b32abb7e147d8dea2321e9d4c958c6f9746dbfeacd95ec06\n</pre> In\u00a0[5]: Copied! <pre>ld_sampler = qp.DigitalNetB2(2,seed=7)\norigin_distrib = qp.Uniform(ld_sampler,lower_bound=[-239,-339],upper_bound=[739,339])\numbridge_tsu_model = umbridge.HTTPModel('http://localhost:4242','forward')\numbridge_config = {'d': origin_distrib.d, 'level':1}\nqmcpy_umbridge_tsu_model = qp.UMBridgeWrapper(origin_distrib,umbridge_tsu_model,umbridge_config,parallel=True)\n</pre> ld_sampler = qp.DigitalNetB2(2,seed=7) origin_distrib = qp.Uniform(ld_sampler,lower_bound=[-239,-339],upper_bound=[739,339]) umbridge_tsu_model = umbridge.HTTPModel('http://localhost:4242','forward') umbridge_config = {'d': origin_distrib.d, 'level':1} qmcpy_umbridge_tsu_model = qp.UMBridgeWrapper(origin_distrib,umbridge_tsu_model,umbridge_config,parallel=True) In\u00a0[6]: Copied! <pre>class TsunamiMaxWaveHeightBouy1(qp.Integrand):\n    # https://um-bridge-benchmarks.readthedocs.io/en/docs/models/exahype-tsunami.html#\n    def __init__(self, qmcpy_umbridge_tsu_model):\n        self.tsu_model = qmcpy_umbridge_tsu_model\n        self.true_measure = self.sampler = self.tsu_model.true_measure\n        assert self.tsu_model.d == 2\n        super(TsunamiMaxWaveHeightBouy1,self).__init__(\n            dimension_indv = (),\n            dimension_comb = (),\n            parallel = False,\n            threadpool = False)\n    def g(self, t):\n        assert t.ndim==2 and t.shape[1]==self.d\n        y = self.tsu_model.g(t)\n        y_buoy1_height = 1000*y[1] # max wave height at buoy (meters SSHA)\n        height_above_thresh = y_buoy1_height\n        return height_above_thresh\ntmwhb1 = TsunamiMaxWaveHeightBouy1(qmcpy_umbridge_tsu_model)\n</pre> class TsunamiMaxWaveHeightBouy1(qp.Integrand):     # https://um-bridge-benchmarks.readthedocs.io/en/docs/models/exahype-tsunami.html#     def __init__(self, qmcpy_umbridge_tsu_model):         self.tsu_model = qmcpy_umbridge_tsu_model         self.true_measure = self.sampler = self.tsu_model.true_measure         assert self.tsu_model.d == 2         super(TsunamiMaxWaveHeightBouy1,self).__init__(             dimension_indv = (),             dimension_comb = (),             parallel = False,             threadpool = False)     def g(self, t):         assert t.ndim==2 and t.shape[1]==self.d         y = self.tsu_model.g(t)         y_buoy1_height = 1000*y[1] # max wave height at buoy (meters SSHA)         height_above_thresh = y_buoy1_height         return height_above_thresh tmwhb1 = TsunamiMaxWaveHeightBouy1(qmcpy_umbridge_tsu_model) In\u00a0[7]: Copied! <pre>mcispfgp = qp.PFGPCI(\n    integrand = tmwhb1,\n    failure_threshold = 3,\n    failure_above_threshold=True,\n    abs_tol = 1e-3,\n    alpha = 1e-1,\n    n_init = 64,\n    init_samples = None,\n    batch_sampler = qp.PFSampleErrorDensityAR(verbose=True),\n    n_batch = 8,\n    n_limit = 128,\n    n_approx = 2**20,\n    gpytorch_prior_mean = gpytorch.means.ConstantMean(),\n    gpytorch_prior_cov = gpytorch.kernels.ScaleKernel(\n        gpytorch.kernels.MaternKernel(nu=2.5,\n            lengthscale_constraint = gpytorch.constraints.Interval(.1,1)),\n        outputscale_constraint = gpytorch.constraints.Interval(1e-8,1)),\n    gpytorch_likelihood = gpytorch.likelihoods.GaussianLikelihood(noise_constraint = gpytorch.constraints.Interval(1e-12,1e-10)),\n    gpytorch_marginal_log_likelihood_func = lambda likelihood,gpyt_model: gpytorch.mlls.ExactMarginalLogLikelihood(likelihood,gpyt_model),\n    torch_optimizer_func = lambda gpyt_model: torch.optim.Adam(gpyt_model.parameters(),lr=0.1),\n    gpytorch_train_iter = 1000,\n    gpytorch_use_gpu = gpytorch_use_gpu,\n    verbose = 1000,\n    n_ref_approx = 0)\nsolution,data = mcispfgp.integrate(seed=11,refit=False)\nprint(data)\ndf = pd.DataFrame(data.get_results_dict())\nprint(\"\\nIteration Summary\")\nprint(df)\npyplot.rcParams['font.size'] = 25\npyplot.rcParams['legend.fontsize'] = 25\nfig = data.plot()\ndf.to_csv(\"outputs/Tsunami2D_df.csv\",index=False)\nfig.savefig(\"outputs/Tsunami2D_fig.png\",dpi=256,transparent=True,bbox_inches=\"tight\")\n</pre> mcispfgp = qp.PFGPCI(     integrand = tmwhb1,     failure_threshold = 3,     failure_above_threshold=True,     abs_tol = 1e-3,     alpha = 1e-1,     n_init = 64,     init_samples = None,     batch_sampler = qp.PFSampleErrorDensityAR(verbose=True),     n_batch = 8,     n_limit = 128,     n_approx = 2**20,     gpytorch_prior_mean = gpytorch.means.ConstantMean(),     gpytorch_prior_cov = gpytorch.kernels.ScaleKernel(         gpytorch.kernels.MaternKernel(nu=2.5,             lengthscale_constraint = gpytorch.constraints.Interval(.1,1)),         outputscale_constraint = gpytorch.constraints.Interval(1e-8,1)),     gpytorch_likelihood = gpytorch.likelihoods.GaussianLikelihood(noise_constraint = gpytorch.constraints.Interval(1e-12,1e-10)),     gpytorch_marginal_log_likelihood_func = lambda likelihood,gpyt_model: gpytorch.mlls.ExactMarginalLogLikelihood(likelihood,gpyt_model),     torch_optimizer_func = lambda gpyt_model: torch.optim.Adam(gpyt_model.parameters(),lr=0.1),     gpytorch_train_iter = 1000,     gpytorch_use_gpu = gpytorch_use_gpu,     verbose = 1000,     n_ref_approx = 0) solution,data = mcispfgp.integrate(seed=11,refit=False) print(data) df = pd.DataFrame(data.get_results_dict()) print(\"\\nIteration Summary\") print(df) pyplot.rcParams['font.size'] = 25 pyplot.rcParams['legend.fontsize'] = 25 fig = data.plot() df.to_csv(\"outputs/Tsunami2D_df.csv\",index=False) fig.savefig(\"outputs/Tsunami2D_fig.png\",dpi=256,transparent=True,bbox_inches=\"tight\") <pre>batch 0\n\tgpytorch model fitting\n\t\titer 1000 of 1000\n\t\t\tlikelihood.noise_covar.raw_noise.................. 1.17e+00\n\t\t\tmean_module.raw_constant.......................... -2.25e+00\n\t\t\tcovar_module.raw_outputscale...................... 1.99e+00\n\t\t\tcovar_module.base_kernel.raw_lengthscale.......... -3.02e+00\nbatch 1\n\tAR sampling with efficiency 1.9e-02, expect 426 draws: 296, 444, 555, \nbatch 2\n\tAR sampling with efficiency 5.5e-03, expect 1450 draws: 1008, 1512, 1638, \nbatch 3\n\tAR sampling with efficiency 1.1e-02, expect 757 draws: 528, 924, 990, \nbatch 4\n\tAR sampling with efficiency 3.2e-03, expect 2489 draws: 1728, 3024, \nbatch 5\n\tAR sampling with efficiency 2.5e-03, expect 3227 draws: 2240, 2800, 3360, 3640, \nbatch 6\n\tAR sampling with efficiency 1.4e-03, expect 5698 draws: 3952, \nbatch 7\n\tAR sampling with efficiency 1.7e-03, expect 4841 draws: 3360, 3780, \nbatch 8\n\tAR sampling with efficiency 1.6e-03, expect 4905 draws: 3400, 4675, 5100, \nPFGPCIData (Data)\n    solution        0.057\n    error_bound     0.005\n    bound_low       0.053\n    bound_high      0.062\n    n_total         2^(7)\n    time_integrate  1857.892\nPFGPCI (AbstractStoppingCriterion)\n    abs_tol         0.001\n    n_init          2^(6)\n    n_limit         2^(7)\n    n_batch         2^(3)\nTsunamiMaxWaveHeightBouy1 (AbstractIntegrand)\nUniform (AbstractTrueMeasure)\n    lower_bound     [-239 -339]\n    upper_bound     [739 339]\nDigitalNetB2 (AbstractLDDiscreteDistribution)\n    d               2^(1)\n    replications    1\n    randomize       LMS DS\n    gen_mats_source joe_kuo.6.21201.txt\n    order           RADICAL INVERSE\n    t               63\n    alpha           1\n    n_limit         2^(32)\n    entropy         7\n\nIteration Summary\n   iter  n_sum  n_batch  error_bounds  ci_low  ci_high  solutions\n0    0     64     64       9.4e-02    0.0e+00 1.5e-01   5.3e-02  \n1    1     72      8       2.8e-02    3.5e-02 9.0e-02   6.2e-02  \n2    2     80      8       5.3e-02    1.9e-02 1.2e-01   7.1e-02  \n3    3     88      8       1.6e-02    5.0e-02 8.2e-02   6.6e-02  \n4    4     96      8       1.2e-02    4.9e-02 7.4e-02   6.2e-02  \n5    5    104      8       7.0e-03    5.4e-02 6.8e-02   6.1e-02  \n6    6    112      8       8.3e-03    5.1e-02 6.8e-02   5.9e-02  \n7    7    120      8       8.2e-03    5.2e-02 6.8e-02   6.0e-02  \n8    8    128      8       4.5e-03    5.3e-02 6.2e-02   5.7e-02  \n</pre> In\u00a0[30]: Copied! <pre>!docker rm -f tsunami\n</pre> !docker rm -f tsunami <pre>python(15698) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n</pre> <pre>tsunami\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"demos/talk_paper_demos/ProbFailureSorokinRao/prob_failure_gp_ci/#probability-of-failure-estimation-with-gaussian-processes","title":"Probability of Failure Estimation with Gaussian Processes\u00b6","text":""},{"location":"demos/talk_paper_demos/ProbFailureSorokinRao/prob_failure_gp_ci/#method-visual","title":"Method Visual\u00b6","text":""},{"location":"demos/talk_paper_demos/ProbFailureSorokinRao/prob_failure_gp_ci/#sin-1d-problem","title":"Sin 1d Problem\u00b6","text":""},{"location":"demos/talk_paper_demos/ProbFailureSorokinRao/prob_failure_gp_ci/#multimodal-2d-problem","title":"Multimodal 2d Problem\u00b6","text":""},{"location":"demos/talk_paper_demos/ProbFailureSorokinRao/prob_failure_gp_ci/#four-branch-2d-problem","title":"Four Branch 2d Problem\u00b6","text":""},{"location":"demos/talk_paper_demos/ProbFailureSorokinRao/prob_failure_gp_ci/#ishigami-3d-problem","title":"Ishigami 3d Problem\u00b6","text":""},{"location":"demos/talk_paper_demos/ProbFailureSorokinRao/prob_failure_gp_ci/#hartmann-6d-problem","title":"Hartmann 6d Problem\u00b6","text":""},{"location":"demos/talk_paper_demos/ProbFailureSorokinRao/prob_failure_gp_ci/#tsunami","title":"Tsunami\u00b6","text":""},{"location":"demos/talk_paper_demos/Purdue_Talk_2023_March/Purdue_Talk_Figures/","title":"2023 Purdue Talk","text":"In\u00a0[1]: Copied! <pre>import matplotlib.pyplot as plt\nimport numpy as np\nimport qmcpy as qp\nimport time  #timing routines\nimport warnings  #to suppress warnings when needed\nimport pickle  #write output to a file and load it back in\nfrom copy import deepcopy\n\nplt.rc('font', size=16)  #set defaults so that the plots are readable\nplt.rc('axes', titlesize=16)\nplt.rc('axes', labelsize=16)\nplt.rc('xtick', labelsize=16)\nplt.rc('ytick', labelsize=16)\nplt.rc('legend', fontsize=16)\nplt.rc('figure', titlesize=16)\n\n#a helpful plotting method to show increasing numbers of points\ndef plot_successive_points(distrib,ld_name,first_n=64,n_cols=1,\n                           pt_clr=['tab:blue', 'tab:green', 'k', 'tab:cyan', 'tab:purple', 'tab:orange'],\n                           xlim=[0,1],ylim=[0,1]):\n  fig,ax = plt.subplots(nrows=1,ncols=n_cols,figsize=(5*n_cols,5.5))\n  if n_cols==1: ax = [ax]\n  last_n = first_n*(2**n_cols)\n  points = distrib.gen_samples(n=last_n)\n  for i in range(n_cols):\n    n = first_n\n    nstart = 0\n    for j in range(i+1):\n      n = first_n*(2**j)\n      ax[i].scatter(points[nstart:n,0],points[nstart:n,1],color=pt_clr[j])\n      nstart = n\n    ax[i].set_title('n = %d'%n)\n    ax[i].set_xlim(xlim); ax[i].set_xticks(xlim); ax[i].set_xlabel('$x_{i,1}$')\n    ax[i].set_ylim(ylim); ax[i].set_yticks(ylim); ax[i].set_ylabel('$x_{i,2}$')\n    ax[i].set_aspect((xlim[1]-xlim[0])/(ylim[1]-ylim[0]))\n  fig.suptitle('%s Points'%ld_name, y=0.87)\n  return fig\n\nprint('QMCPy Version',qp.__version__)\n</pre> import matplotlib.pyplot as plt import numpy as np import qmcpy as qp import time  #timing routines import warnings  #to suppress warnings when needed import pickle  #write output to a file and load it back in from copy import deepcopy  plt.rc('font', size=16)  #set defaults so that the plots are readable plt.rc('axes', titlesize=16) plt.rc('axes', labelsize=16) plt.rc('xtick', labelsize=16) plt.rc('ytick', labelsize=16) plt.rc('legend', fontsize=16) plt.rc('figure', titlesize=16)  #a helpful plotting method to show increasing numbers of points def plot_successive_points(distrib,ld_name,first_n=64,n_cols=1,                            pt_clr=['tab:blue', 'tab:green', 'k', 'tab:cyan', 'tab:purple', 'tab:orange'],                            xlim=[0,1],ylim=[0,1]):   fig,ax = plt.subplots(nrows=1,ncols=n_cols,figsize=(5*n_cols,5.5))   if n_cols==1: ax = [ax]   last_n = first_n*(2**n_cols)   points = distrib.gen_samples(n=last_n)   for i in range(n_cols):     n = first_n     nstart = 0     for j in range(i+1):       n = first_n*(2**j)       ax[i].scatter(points[nstart:n,0],points[nstart:n,1],color=pt_clr[j])       nstart = n     ax[i].set_title('n = %d'%n)     ax[i].set_xlim(xlim); ax[i].set_xticks(xlim); ax[i].set_xlabel('$x_{i,1}$')     ax[i].set_ylim(ylim); ax[i].set_yticks(ylim); ax[i].set_ylabel('$x_{i,2}$')     ax[i].set_aspect((xlim[1]-xlim[0])/(ylim[1]-ylim[0]))   fig.suptitle('%s Points'%ld_name, y=0.87)   return fig  print('QMCPy Version',qp.__version__) <pre>QMCPy Version 1.6.3c\n</pre> In\u00a0[2]: Copied! <pre>figpath = '' #this path sends the figures to the directory that you want\n</pre> figpath = '' #this path sends the figures to the directory that you want In\u00a0[3]: Copied! <pre>d = 5 #dimension\nn = 16 #number of points\nld = qp.Lattice(d) #define the generator\nxpts = ld.gen_samples(n) #generate points\nprint(xpts)\nfig = plot_successive_points(ld,'Lattice',first_n=n,n_cols=4)\nfig.savefig(figpath+'latticepts.eps',format='eps')\n</pre> d = 5 #dimension n = 16 #number of points ld = qp.Lattice(d) #define the generator xpts = ld.gen_samples(n) #generate points print(xpts) fig = plot_successive_points(ld,'Lattice',first_n=n,n_cols=4) fig.savefig(figpath+'latticepts.eps',format='eps') <pre>[[0.40132838 0.98330345 0.11124065 0.76370523 0.74520191]\n [0.90132838 0.48330345 0.61124065 0.26370523 0.24520191]\n [0.65132838 0.73330345 0.86124065 0.51370523 0.99520191]\n [0.15132838 0.23330345 0.36124065 0.01370523 0.49520191]\n [0.52632838 0.35830345 0.48624065 0.63870523 0.37020191]\n [0.02632838 0.85830345 0.98624065 0.13870523 0.87020191]\n [0.77632838 0.10830345 0.23624065 0.38870523 0.62020191]\n [0.27632838 0.60830345 0.73624065 0.88870523 0.12020191]\n [0.46382838 0.67080345 0.29874065 0.20120523 0.55770191]\n [0.96382838 0.17080345 0.79874065 0.70120523 0.05770191]\n [0.71382838 0.42080345 0.04874065 0.95120523 0.80770191]\n [0.21382838 0.92080345 0.54874065 0.45120523 0.30770191]\n [0.58882838 0.04580345 0.67374065 0.07620523 0.18270191]\n [0.08882838 0.54580345 0.17374065 0.57620523 0.68270191]\n [0.83882838 0.79580345 0.42374065 0.82620523 0.43270191]\n [0.33882838 0.29580345 0.92374065 0.32620523 0.93270191]]\n</pre> In\u00a0[4]: Copied! <pre>ld = qp.Sobol(d) #define the generator\nxpts_Sobol = ld.gen_samples(n) #generate points\nfig = plot_successive_points(ld,'Sobol\\'',first_n=n,n_cols=4)\nfig.savefig(figpath+'sobolpts.eps',format='eps')\n</pre> ld = qp.Sobol(d) #define the generator xpts_Sobol = ld.gen_samples(n) #generate points fig = plot_successive_points(ld,'Sobol\\'',first_n=n,n_cols=4) fig.savefig(figpath+'sobolpts.eps',format='eps') In\u00a0[5]: Copied! <pre>iid = qp.IIDStdUniform(d) #define the generator\nxpts = ld.gen_samples(n) #generate points\nxpts\nfig = plot_successive_points(iid,'IID',first_n=n,n_cols=4)\nfig.savefig(figpath+'iidpts.eps',format='eps')\n</pre> iid = qp.IIDStdUniform(d) #define the generator xpts = ld.gen_samples(n) #generate points xpts fig = plot_successive_points(iid,'IID',first_n=n,n_cols=4) fig.savefig(figpath+'iidpts.eps',format='eps') In\u00a0[6]: Copied! <pre>with open(figpath+'iid_ld.pkl','rb') as myfile: tol_vec,n_tol,ii_iid,ld_time,ld_n,iid_time,iid_n,best_solution_i = pickle.load(myfile)\nprint(best_solution_i)\nfig,ax = plt.subplots(nrows=1,ncols=2,figsize=(13,5.5))\nax[0].scatter(tol_vec[0:n_tol],ld_time[0:n_tol],color='b'); \nax[0].plot(tol_vec[0:n_tol],[(ld_time[0]*tol_vec[0])/tol_vec[jj] for jj in range(n_tol)],color='b')\nax[0].scatter(tol_vec[ii_iid:n_tol],iid_time[ii_iid:n_tol],color='g'); \nax[0].plot(tol_vec[ii_iid:n_tol],[(iid_time[ii_iid]*(tol_vec[ii_iid]**2))/(tol_vec[jj]**2) for jj in range(ii_iid,n_tol)],color='g')\nax[0].set_ylim([0.001,1000]); ax[0].set_ylabel('Time (s)')\nax[1].scatter(tol_vec[0:n_tol],ld_n[0:n_tol],color='b'); \nax[1].plot(tol_vec[0:n_tol],[(ld_n[0]*tol_vec[0])/tol_vec[jj] for jj in range(n_tol)],color='b')\nax[1].scatter(tol_vec[ii_iid:n_tol],iid_n[ii_iid:n_tol],color='g'); \nax[1].plot(tol_vec[ii_iid:n_tol],[(iid_n[ii_iid]*(tol_vec[ii_iid]**2))/(tol_vec[jj]**2) for jj in range(ii_iid,n_tol)],color='g')\nax[1].set_ylim([1e2,1e8]); ax[1].set_ylabel('n')\nfor ii in range(2):\n  ax[ii].set_xlim([0.007,100]); ax[ii].set_xlabel('Tolerance, '+r'$\\varepsilon$')\n  ax[ii].set_xscale('log'); ax[ii].set_yscale('log')\n  ax[ii].legend([r'$\\mathcal{O}(\\varepsilon^{-1})$',r'$\\mathcal{O}(\\varepsilon^{-2})$','LD','IID'],frameon=False)\n  ax[ii].set_aspect(0.65)\nax[0].set_yticks([1, 60, 3600], labels = ['1 sec', '1 min', '1 hr'])    \nfig.savefig(figpath+'iidldbeam.eps',format='eps',bbox_inches='tight')\n</pre> with open(figpath+'iid_ld.pkl','rb') as myfile: tol_vec,n_tol,ii_iid,ld_time,ld_n,iid_time,iid_n,best_solution_i = pickle.load(myfile) print(best_solution_i) fig,ax = plt.subplots(nrows=1,ncols=2,figsize=(13,5.5)) ax[0].scatter(tol_vec[0:n_tol],ld_time[0:n_tol],color='b');  ax[0].plot(tol_vec[0:n_tol],[(ld_time[0]*tol_vec[0])/tol_vec[jj] for jj in range(n_tol)],color='b') ax[0].scatter(tol_vec[ii_iid:n_tol],iid_time[ii_iid:n_tol],color='g');  ax[0].plot(tol_vec[ii_iid:n_tol],[(iid_time[ii_iid]*(tol_vec[ii_iid]**2))/(tol_vec[jj]**2) for jj in range(ii_iid,n_tol)],color='g') ax[0].set_ylim([0.001,1000]); ax[0].set_ylabel('Time (s)') ax[1].scatter(tol_vec[0:n_tol],ld_n[0:n_tol],color='b');  ax[1].plot(tol_vec[0:n_tol],[(ld_n[0]*tol_vec[0])/tol_vec[jj] for jj in range(n_tol)],color='b') ax[1].scatter(tol_vec[ii_iid:n_tol],iid_n[ii_iid:n_tol],color='g');  ax[1].plot(tol_vec[ii_iid:n_tol],[(iid_n[ii_iid]*(tol_vec[ii_iid]**2))/(tol_vec[jj]**2) for jj in range(ii_iid,n_tol)],color='g') ax[1].set_ylim([1e2,1e8]); ax[1].set_ylabel('n') for ii in range(2):   ax[ii].set_xlim([0.007,100]); ax[ii].set_xlabel('Tolerance, '+r'$\\varepsilon$')   ax[ii].set_xscale('log'); ax[ii].set_yscale('log')   ax[ii].legend([r'$\\mathcal{O}(\\varepsilon^{-1})$',r'$\\mathcal{O}(\\varepsilon^{-2})$','LD','IID'],frameon=False)   ax[ii].set_aspect(0.65) ax[0].set_yticks([1, 60, 3600], labels = ['1 sec', '1 min', '1 hr'])     fig.savefig(figpath+'iidldbeam.eps',format='eps',bbox_inches='tight') <pre>[1037.12106673]\n</pre> In\u00a0[7]: Copied! <pre>with open(figpath+'ld_parallel.pkl','rb') as myfile: tol_vec,n_tol,ld_time,ld_n,ld_p_time,ld_p_n,best_solution = pickle.load(myfile)\nprint(best_solution_i)\nfig,ax = plt.subplots(nrows=1,ncols=2,figsize=(11,4))\nax[0].scatter(tol_vec[0:n_tol],ld_time[0:n_tol],color='tab:blue'); \nax[0].plot(tol_vec[0:n_tol],[(ld_time[0]*tol_vec[0])/tol_vec[jj] for jj in range(n_tol)],color='tab:blue')\nax[0].scatter(tol_vec[0:n_tol],ld_p_time[0:n_tol],color='tab:orange',marker = '+',s=100,linewidths=2); \n#ax[0].plot(tol_vec[0:n_tol],[(ld_p_time[0]*tol_vec[0])/tol_vec[jj] for jj in range(n_tol)],color='tab:orange')\nax[0].set_ylim([0.05,500]); ax[0].set_ylabel('Time')\nax[1].scatter(tol_vec[0:n_tol],ld_n[0:n_tol],color='tab:blue'); \nax[1].plot(tol_vec[0:n_tol],[(ld_n[0]*tol_vec[0])/tol_vec[jj] for jj in range(n_tol)],color='tab:blue')\nax[1].scatter(tol_vec[0:n_tol],ld_p_n[0:n_tol],color='tab:orange',marker = '+',s=100,linewidths=2); \n#ax[1].plot(tol_vec[0:n_tol],[(ld_p_n[0]*tol_vec[0])/tol_vec[jj] for jj in range(n_tol)],color='tab:orange')\nax[1].set_ylim([10,1e5]); ax[1].set_ylabel('n')\nfor ii in range(2):\n  ax[ii].set_xlim([0.007,4]); ax[ii].set_xlabel('Tolerance, '+r'$\\varepsilon$')\n  ax[ii].set_xscale('log'); ax[ii].set_yscale('log')\n  ax[ii].legend(['Lattice',r'$\\mathcal{O}(\\varepsilon^{-1})$','Lattice Parallel',r'$\\mathcal{O}(\\varepsilon^{-1})$'],frameon=False)\n  ax[ii].set_aspect(0.6)\nax[0].set_yticks([1, 60], labels = ['1 sec', '1 min'])\nfig.savefig(figpath+'ldparallelbeam.eps',format='eps',bbox_inches='tight')\n</pre> with open(figpath+'ld_parallel.pkl','rb') as myfile: tol_vec,n_tol,ld_time,ld_n,ld_p_time,ld_p_n,best_solution = pickle.load(myfile) print(best_solution_i) fig,ax = plt.subplots(nrows=1,ncols=2,figsize=(11,4)) ax[0].scatter(tol_vec[0:n_tol],ld_time[0:n_tol],color='tab:blue');  ax[0].plot(tol_vec[0:n_tol],[(ld_time[0]*tol_vec[0])/tol_vec[jj] for jj in range(n_tol)],color='tab:blue') ax[0].scatter(tol_vec[0:n_tol],ld_p_time[0:n_tol],color='tab:orange',marker = '+',s=100,linewidths=2);  #ax[0].plot(tol_vec[0:n_tol],[(ld_p_time[0]*tol_vec[0])/tol_vec[jj] for jj in range(n_tol)],color='tab:orange') ax[0].set_ylim([0.05,500]); ax[0].set_ylabel('Time') ax[1].scatter(tol_vec[0:n_tol],ld_n[0:n_tol],color='tab:blue');  ax[1].plot(tol_vec[0:n_tol],[(ld_n[0]*tol_vec[0])/tol_vec[jj] for jj in range(n_tol)],color='tab:blue') ax[1].scatter(tol_vec[0:n_tol],ld_p_n[0:n_tol],color='tab:orange',marker = '+',s=100,linewidths=2);  #ax[1].plot(tol_vec[0:n_tol],[(ld_p_n[0]*tol_vec[0])/tol_vec[jj] for jj in range(n_tol)],color='tab:orange') ax[1].set_ylim([10,1e5]); ax[1].set_ylabel('n') for ii in range(2):   ax[ii].set_xlim([0.007,4]); ax[ii].set_xlabel('Tolerance, '+r'$\\varepsilon$')   ax[ii].set_xscale('log'); ax[ii].set_yscale('log')   ax[ii].legend(['Lattice',r'$\\mathcal{O}(\\varepsilon^{-1})$','Lattice Parallel',r'$\\mathcal{O}(\\varepsilon^{-1})$'],frameon=False)   ax[ii].set_aspect(0.6) ax[0].set_yticks([1, 60], labels = ['1 sec', '1 min']) fig.savefig(figpath+'ldparallelbeam.eps',format='eps',bbox_inches='tight') <pre>[1037.12106673]\n</pre> In\u00a0[8]: Copied! <pre>fig,ax = plt.subplots(figsize=(6,3))\nax.plot(best_solution,'-')\nax.set_xlim([0,len(best_solution)-1]); ax.set_xlabel('Position')\nax.set_ylim([1050,-50]);  ax.set_ylabel('Mean Deflection');\nax.set_aspect(0.02)\nfig.suptitle('Cantilevered Beam')\nfig.savefig(figpath+'cantileveredbeamwords.eps',format='eps',bbox_inches='tight')\n</pre> fig,ax = plt.subplots(figsize=(6,3)) ax.plot(best_solution,'-') ax.set_xlim([0,len(best_solution)-1]); ax.set_xlabel('Position') ax.set_ylim([1050,-50]);  ax.set_ylabel('Mean Deflection'); ax.set_aspect(0.02) fig.suptitle('Cantilevered Beam') fig.savefig(figpath+'cantileveredbeamwords.eps',format='eps',bbox_inches='tight') In\u00a0[9]: Copied! <pre>qp.util.stop_notebook()\n</pre> qp.util.stop_notebook() <p>Below is long-running code, that we rarely wish to run</p> In\u00a0[11]: Copied! <pre>import umbridge #this is the connector\n!docker run --name muqbp -d -it -p 4243:4243 linusseelinger/benchmark-muq-beam-propagation:latest #get beam example\nd = 3 #dimension of the randomness\nlb = 1 #lower bound on randomness\nub = 1.2 #upper bound on randomness\numbridge_config = {\"d\": d}\nmodel = umbridge.HTTPModel('http://localhost:4243','forward') #this is the original model\noutindex = -1 #choose last element of the vector of beam deflections\nmodeli = deepcopy(model) #and construct a model for just that deflection\nmodeli.get_output_sizes = lambda *args : [1]\nmodeli.get_output_sizes()\nmodeli.__call__ = lambda *args,**kwargs: [[model.__call__(*args,**kwargs)[0][outindex]]]\n</pre> import umbridge #this is the connector !docker run --name muqbp -d -it -p 4243:4243 linusseelinger/benchmark-muq-beam-propagation:latest #get beam example d = 3 #dimension of the randomness lb = 1 #lower bound on randomness ub = 1.2 #upper bound on randomness umbridge_config = {\"d\": d} model = umbridge.HTTPModel('http://localhost:4243','forward') #this is the original model outindex = -1 #choose last element of the vector of beam deflections modeli = deepcopy(model) #and construct a model for just that deflection modeli.get_output_sizes = lambda *args : [1] modeli.get_output_sizes() modeli.__call__ = lambda *args,**kwargs: [[model.__call__(*args,**kwargs)[0][outindex]]] <pre>docker: Error response from daemon: Conflict. The container name \"/muqbp\" is already in use by container \"3bf2d2f6046f7ed0b096ce6945a8c3ee4662fb05f481c81b0e68b3345c125125\". You have to remove (or rename) that container to be able to reuse that name.\nSee 'docker run --help'.\n</pre> In\u00a0[\u00a0]: Copied! <pre>ld = qp.Uniform(qp.Lattice(d,seed=7),lower_bound=lb,upper_bound=ub) #lattice points for this problem\nld_integ = qp.UMBridgeWrapper(ld,modeli,umbridge_config,parallel=False) #integrand\niid = qp.Uniform(qp.IIDStdUniform(d),lower_bound=lb,upper_bound=ub) #iid points for this problem\niid_integ = qp.UMBridgeWrapper(iid,modeli,umbridge_config,parallel=False) #integrand\ntol = 0.01  #smallest tolerance\n\nn_tol = 14  #number of different tolerances\nii_iid = 9  #make this larger to reduce the time required by not running all cases for IID\ntol_vec = [tol*(2**ii) for ii in range(n_tol)]  #initialize vector of tolerances\nld_time = [0]*n_tol; ld_n = [0]*n_tol  #low discrepancy time and number of function values\niid_time = [0]*n_tol; iid_n = [0]*n_tol  #IID time and number of function values\nprint(f'\\nCantilever Beam\\n')\nprint('iteration ', end = '')\nfor ii in range(n_tol):\n  solution, data = qp.CubQMCLatticeG(ld_integ, abs_tol = tol_vec[ii]).integrate()\n  if ii == 0:\n    best_solution_i = solution\n  ld_time[ii] = data.time_integrate\n  ld_n[ii] = data.n_total\n  if ii &gt;= ii_iid:\n    solution, data = qp.CubMCG(iid_integ, abs_tol = tol_vec[ii]).integrate()\n    iid_time[ii] = data.time_integrate\n    iid_n[ii] = data.n_total\n  print(ii, end = ' ')\nwith open(figpath+'iid_ld.pkl','wb') as myfile:pickle.dump([tol_vec,n_tol,ii_iid,ld_time,ld_n,iid_time,iid_n,best_solution_i],myfile)\n</pre> ld = qp.Uniform(qp.Lattice(d,seed=7),lower_bound=lb,upper_bound=ub) #lattice points for this problem ld_integ = qp.UMBridgeWrapper(ld,modeli,umbridge_config,parallel=False) #integrand iid = qp.Uniform(qp.IIDStdUniform(d),lower_bound=lb,upper_bound=ub) #iid points for this problem iid_integ = qp.UMBridgeWrapper(iid,modeli,umbridge_config,parallel=False) #integrand tol = 0.01  #smallest tolerance  n_tol = 14  #number of different tolerances ii_iid = 9  #make this larger to reduce the time required by not running all cases for IID tol_vec = [tol*(2**ii) for ii in range(n_tol)]  #initialize vector of tolerances ld_time = [0]*n_tol; ld_n = [0]*n_tol  #low discrepancy time and number of function values iid_time = [0]*n_tol; iid_n = [0]*n_tol  #IID time and number of function values print(f'\\nCantilever Beam\\n') print('iteration ', end = '') for ii in range(n_tol):   solution, data = qp.CubQMCLatticeG(ld_integ, abs_tol = tol_vec[ii]).integrate()   if ii == 0:     best_solution_i = solution   ld_time[ii] = data.time_integrate   ld_n[ii] = data.n_total   if ii &gt;= ii_iid:     solution, data = qp.CubMCG(iid_integ, abs_tol = tol_vec[ii]).integrate()     iid_time[ii] = data.time_integrate     iid_n[ii] = data.n_total   print(ii, end = ' ') with open(figpath+'iid_ld.pkl','wb') as myfile:pickle.dump([tol_vec,n_tol,ii_iid,ld_time,ld_n,iid_time,iid_n,best_solution_i],myfile) In\u00a0[\u00a0]: Copied! <pre>ld_integ = qp.UMBridgeWrapper(ld,model,umbridge_config,parallel=False) #integrand\nld_integ_p = qp.UMBridgeWrapper(ld,model,umbridge_config,parallel=8) #integrand with parallel processing\n\ntol = 0.01\nn_tol = 9  #number of different tolerances\ntol_vec = [tol*(2**ii) for ii in range(n_tol)]  #initialize vector of tolerances\nld_time = [0]*n_tol; ld_n = [0]*n_tol  #low discrepancy time and number of function values\nld_p_time = [0]*n_tol; ld_p_n = [0]*n_tol  #low discrepancy time and number of function values with parallel\nprint(f'\\nCantilever Beam\\n')\nprint('iteration ', end = '')\nfor ii in range(n_tol):\n  solution, data = qp.CubQMCLatticeG(ld_integ, abs_tol = tol_vec[ii]).integrate()\n  if ii == 0:\n    best_solution = solution\n  ld_time[ii] = data.time_integrate\n  ld_n[ii] = data.n_total\n  solution, data = qp.CubQMCLatticeG(ld_integ_p, abs_tol = tol_vec[ii]).integrate()\n  ld_p_time[ii] = data.time_integrate\n  ld_p_n[ii] = data.n_total\n  print(ii, end = ' ') \nwith open(figpath+'ld_parallel.pkl','wb') as myfile:pickle.dump([tol_vec,n_tol,ld_time,ld_n,ld_p_time,ld_p_n,best_solution],myfile)\n</pre> ld_integ = qp.UMBridgeWrapper(ld,model,umbridge_config,parallel=False) #integrand ld_integ_p = qp.UMBridgeWrapper(ld,model,umbridge_config,parallel=8) #integrand with parallel processing  tol = 0.01 n_tol = 9  #number of different tolerances tol_vec = [tol*(2**ii) for ii in range(n_tol)]  #initialize vector of tolerances ld_time = [0]*n_tol; ld_n = [0]*n_tol  #low discrepancy time and number of function values ld_p_time = [0]*n_tol; ld_p_n = [0]*n_tol  #low discrepancy time and number of function values with parallel print(f'\\nCantilever Beam\\n') print('iteration ', end = '') for ii in range(n_tol):   solution, data = qp.CubQMCLatticeG(ld_integ, abs_tol = tol_vec[ii]).integrate()   if ii == 0:     best_solution = solution   ld_time[ii] = data.time_integrate   ld_n[ii] = data.n_total   solution, data = qp.CubQMCLatticeG(ld_integ_p, abs_tol = tol_vec[ii]).integrate()   ld_p_time[ii] = data.time_integrate   ld_p_n[ii] = data.n_total   print(ii, end = ' ')  with open(figpath+'ld_parallel.pkl','wb') as myfile:pickle.dump([tol_vec,n_tol,ld_time,ld_n,ld_p_time,ld_p_n,best_solution],myfile) In\u00a0[\u00a0]: Copied! <pre>!docker rm -f muqbp #shut down docker image\n</pre> !docker rm -f muqbp #shut down docker image"},{"location":"demos/talk_paper_demos/Purdue_Talk_2023_March/Purdue_Talk_Figures/#purdue-university-colloquium-talk","title":"Purdue University Colloquium Talk\u00b6","text":"<p>Computations and Figures for Department of Statistics Colloquium at Purdue University</p> <p>presented on Friday, March 3, 2023, slides here</p>"},{"location":"demos/talk_paper_demos/Purdue_Talk_2023_March/Purdue_Talk_Figures/#import-the-necessary-packages-and-set-up-plotting-routines","title":"Import the necessary packages and set up plotting routines\u00b6","text":""},{"location":"demos/talk_paper_demos/Purdue_Talk_2023_March/Purdue_Talk_Figures/#set-the-path-to-save-the-figures-here","title":"Set the path to save the figures here\u00b6","text":""},{"location":"demos/talk_paper_demos/Purdue_Talk_2023_March/Purdue_Talk_Figures/#here-are-some-plots-of-iid-and-low-discrepancy-ld-points","title":"Here are some plots of IID and Low Discrepancy (LD) Points\u00b6","text":""},{"location":"demos/talk_paper_demos/Purdue_Talk_2023_March/Purdue_Talk_Figures/#lattice-points-first","title":"Lattice points first\u00b6","text":""},{"location":"demos/talk_paper_demos/Purdue_Talk_2023_March/Purdue_Talk_Figures/#next-sobol-points","title":"Next Sobol' points\u00b6","text":""},{"location":"demos/talk_paper_demos/Purdue_Talk_2023_March/Purdue_Talk_Figures/#compare-to-iid","title":"Compare to IID\u00b6","text":"<p>Note that there are more gaps and clusters</p>"},{"location":"demos/talk_paper_demos/Purdue_Talk_2023_March/Purdue_Talk_Figures/#beam-example-figures","title":"Beam Example Figures\u00b6","text":"<p>Using computations done below</p>"},{"location":"demos/talk_paper_demos/Purdue_Talk_2023_March/Purdue_Talk_Figures/#plot-the-time-and-sample-size-required-to-solve-for-the-deflection-of-the-end-point-using-iid-and-low-discrepancy","title":"Plot the time and sample size required to solve for the deflection of the end point using IID and low discrepancy\u00b6","text":""},{"location":"demos/talk_paper_demos/Purdue_Talk_2023_March/Purdue_Talk_Figures/#plot-the-time-and-sample-size-required-to-solve-for-the-deflection-of-the-whole-beam-using-low-discrepancy-with-and-without-parallel","title":"Plot the time and sample size required to solve for the deflection of the whole beam using low discrepancy with and without parallel\u00b6","text":""},{"location":"demos/talk_paper_demos/Purdue_Talk_2023_March/Purdue_Talk_Figures/#plot-of-beam-solution","title":"Plot of beam solution\u00b6","text":""},{"location":"demos/talk_paper_demos/Purdue_Talk_2023_March/Purdue_Talk_Figures/#beam-example-computations","title":"Beam Example Computations\u00b6","text":""},{"location":"demos/talk_paper_demos/Purdue_Talk_2023_March/Purdue_Talk_Figures/#set-up-the-problem-using-a-docker-container-to-solve-the-ode","title":"Set up the problem using a docker container to solve the ODE\u00b6","text":"<p>To run this, you need to be running the docker application, https://www.docker.com/products/docker-desktop/</p>"},{"location":"demos/talk_paper_demos/Purdue_Talk_2023_March/Purdue_Talk_Figures/#first-we-compute-the-time-required-to-solve-for-the-deflection-of-the-end-point-using-iid-and-low-discrepancy","title":"First we compute the time required to solve for the deflection of the end point using IID and low discrepancy\u00b6","text":""},{"location":"demos/talk_paper_demos/Purdue_Talk_2023_March/Purdue_Talk_Figures/#next-we-compute-the-time-required-to-solve-for-the-deflection-of-the-whole-beam-using-low-discrepancy-with-and-without-parallel","title":"Next, we compute the time required to solve for the deflection of the whole beam using low discrepancy with and without parallel\u00b6","text":""},{"location":"demos/talk_paper_demos/Purdue_Talk_2023_March/Purdue_Talk_Figures/#shut-down-docker","title":"Shut down docker\u00b6","text":""},{"location":"demos/talk_paper_demos/SorokinThesis2025/sorokin_thesis_2025/","title":"2025 Sorokin Thesis","text":"In\u00a0[1]: Copied! <pre>import qmcpy as qp\nimport numpy as np\n</pre> import qmcpy as qp import numpy as np In\u00a0[2]: Copied! <pre>%%time \nlattice = qp.Lattice(\n    dimension = 52,\n    randomize = \"shift\", # for unrandomized lattice set randomize = None\n    replications = 16, # R\n    order = \"radical inverse\", # also supports \"linear\"\n    seed = None, # pass integer seed for reproducibility\n    generating_vector = \"mps.exod2_base2_m20_CKN.txt\")\nx = lattice(2**16) # a numpy.ndarray with shape 16 x 65536 x 52\n</pre> %%time  lattice = qp.Lattice(     dimension = 52,     randomize = \"shift\", # for unrandomized lattice set randomize = None     replications = 16, # R     order = \"radical inverse\", # also supports \"linear\"     seed = None, # pass integer seed for reproducibility     generating_vector = \"mps.exod2_base2_m20_CKN.txt\") x = lattice(2**16) # a numpy.ndarray with shape 16 x 65536 x 52 <pre>CPU times: user 59.7 ms, sys: 28.5 ms, total: 88.2 ms\nWall time: 459 ms\n</pre> In\u00a0[3]: Copied! <pre>%%time\ndnb2 = qp.DigitalNetB2(\n    dimension = 52, \n    randomize = \"LMS DS\", # Matousek's LMS then a digital shift\n    # other options [\"NUS\", \"DS\", \"LMS\", None]\n    t = 64, # number of LMS bits i.e. number of rows in S_j\n    alpha = 2, # interlacing factor for higher order digital nets\n    replications = 16, # R\n    order = \"radical inverse\", # also supports \"Gray code\"\n    seed = None, # pass integer seed for reproducibility\n    generating_matrices = \"joe_kuo.6.21201.txt\")\nx = dnb2(2**16) # a numpy.ndarray with shape 16 x 65536 x 52\n</pre> %%time dnb2 = qp.DigitalNetB2(     dimension = 52,      randomize = \"LMS DS\", # Matousek's LMS then a digital shift     # other options [\"NUS\", \"DS\", \"LMS\", None]     t = 64, # number of LMS bits i.e. number of rows in S_j     alpha = 2, # interlacing factor for higher order digital nets     replications = 16, # R     order = \"radical inverse\", # also supports \"Gray code\"     seed = None, # pass integer seed for reproducibility     generating_matrices = \"joe_kuo.6.21201.txt\") x = dnb2(2**16) # a numpy.ndarray with shape 16 x 65536 x 52 <pre>CPU times: user 377 ms, sys: 101 ms, total: 478 ms\nWall time: 479 ms\n</pre> In\u00a0[4]: Copied! <pre>%%time \nhalton = qp.Halton(\n    dimension = 52, \n    randomize = \"LMS DP\", # Matousek's LMS then a digital permutation\n    # other options [\"LMS DS\", \"LMS\", \"DP\", \"DS\", \"NUS\", \"QRNG\", None]\n    t = 64, # number of LMS digits i.e. number of rows in S_j\n    replications = 16, # R\n    seed = None) # pass integer seed for reproducibility\nx = halton(2**10) # a numpy.ndarray with shape 16 x 1024 x 52\n</pre> %%time  halton = qp.Halton(     dimension = 52,      randomize = \"LMS DP\", # Matousek's LMS then a digital permutation     # other options [\"LMS DS\", \"LMS\", \"DP\", \"DS\", \"NUS\", \"QRNG\", None]     t = 64, # number of LMS digits i.e. number of rows in S_j     replications = 16, # R     seed = None) # pass integer seed for reproducibility x = halton(2**10) # a numpy.ndarray with shape 16 x 1024 x 52 <pre>CPU times: user 341 ms, sys: 61.9 ms, total: 403 ms\nWall time: 402 ms\n</pre> In\u00a0[5]: Copied! <pre>d = 3 # dimension \nm = 10 # will generate 2^m points\nn = 2**m # number of points\nlattice = qp.Lattice(d) # default to radical inverse order\nkernel = qp.KernelShiftInvar(\n    d, # dimension \n    alpha = [1,2,3], # per dimension smoothness parameters\n    lengthscales = [1, 1/2, 1/4]) # per dimension product weights\nx = lattice(n_min=0,n_max=n) # shape=(n,d) lattice points\ny = np.random.rand(n) # shape=(n,) random uniforms\n# fast matrix multiplication and linear system solve\nk1 = kernel(x,x[0]) # shape=(n,) first column of Gram matrix\nlam = np.sqrt(n)*qp.fftbr(k1) # vector of eigenvalues\nyt = qp.fftbr(y)\nu = qp.ifftbr(yt*lam) # fast matrix multiplication \nv = qp.ifftbr(yt/lam) # fast linear system solve\n# efficient fast transform updates\nynew = np.random.rand(n) # shape=(n,) new random uniforms\nomega = qp.omega_fftbr(m) # shape=(n,)\nytnew = qp.fftbr(ynew) # shape=(n,)\nytfull = np.concatenate([yt+omega*ytnew,yt-omega*ytnew])/np.sqrt(2) # shape=(2n,)\n</pre> d = 3 # dimension  m = 10 # will generate 2^m points n = 2**m # number of points lattice = qp.Lattice(d) # default to radical inverse order kernel = qp.KernelShiftInvar(     d, # dimension      alpha = [1,2,3], # per dimension smoothness parameters     lengthscales = [1, 1/2, 1/4]) # per dimension product weights x = lattice(n_min=0,n_max=n) # shape=(n,d) lattice points y = np.random.rand(n) # shape=(n,) random uniforms # fast matrix multiplication and linear system solve k1 = kernel(x,x[0]) # shape=(n,) first column of Gram matrix lam = np.sqrt(n)*qp.fftbr(k1) # vector of eigenvalues yt = qp.fftbr(y) u = qp.ifftbr(yt*lam) # fast matrix multiplication  v = qp.ifftbr(yt/lam) # fast linear system solve # efficient fast transform updates ynew = np.random.rand(n) # shape=(n,) new random uniforms omega = qp.omega_fftbr(m) # shape=(n,) ytnew = qp.fftbr(ynew) # shape=(n,) ytfull = np.concatenate([yt+omega*ytnew,yt-omega*ytnew])/np.sqrt(2) # shape=(2n,) In\u00a0[6]: Copied! <pre># slow matrix multiplication and linear system solve\nkmat = kernel(x[:,None,:],x[None,:,:]) # shape=(n,n)\nu_slow = kmat@y # matrix multiplication\nv_slow = np.linalg.solve(kmat,y) # solve a linear system\n# verify correctness\nassert np.allclose(u,u_slow)\nassert np.allclose(v,v_slow)\n# get next samples \nxnew = lattice(n_min=n,n_max=2*n) # shape=(n,d) new lattice points\nk1new = kernel(xnew,x[0]) # shape=(n,) new values in the first column\n# inefficient fast transform update \nk1full = np.concatenate([k1,k1new]) # shape=(2*n,) full first column\nlamfull_inefficient = np.sqrt(2*n)*qp.fftbr(k1full) # shape=(2*n,) full eigenvalues\nyfull = np.concatenate([y,ynew]) # shape=(2*n,) full random values\nytfull_inefficient = qp.fftbr(yfull) # shape=(2*n,) full transformed points\n# efficient fast transform updates\nlamnew = np.sqrt(n)*qp.fftbr(k1new) # shape=(n,) new eigenvalues\nlamfull = np.hstack([lam+omega*lamnew,lam-omega*lamnew])\n# verify correctness\nassert np.allclose(lamfull,lamfull_inefficient)\nassert np.allclose(ytfull,ytfull_inefficient)\n</pre> # slow matrix multiplication and linear system solve kmat = kernel(x[:,None,:],x[None,:,:]) # shape=(n,n) u_slow = kmat@y # matrix multiplication v_slow = np.linalg.solve(kmat,y) # solve a linear system # verify correctness assert np.allclose(u,u_slow) assert np.allclose(v,v_slow) # get next samples  xnew = lattice(n_min=n,n_max=2*n) # shape=(n,d) new lattice points k1new = kernel(xnew,x[0]) # shape=(n,) new values in the first column # inefficient fast transform update  k1full = np.concatenate([k1,k1new]) # shape=(2*n,) full first column lamfull_inefficient = np.sqrt(2*n)*qp.fftbr(k1full) # shape=(2*n,) full eigenvalues yfull = np.concatenate([y,ynew]) # shape=(2*n,) full random values ytfull_inefficient = qp.fftbr(yfull) # shape=(2*n,) full transformed points # efficient fast transform updates lamnew = np.sqrt(n)*qp.fftbr(k1new) # shape=(n,) new eigenvalues lamfull = np.hstack([lam+omega*lamnew,lam-omega*lamnew]) # verify correctness assert np.allclose(lamfull,lamfull_inefficient) assert np.allclose(ytfull,ytfull_inefficient) In\u00a0[7]: Copied! <pre>d = 3 # dimension \nm = 10 # will generate 2^m points\nn = 2**m # number of points\ndnb2 = qp.DigitalNetB2(d) # default to radical inverse order\nkernel = qp.KernelDigShiftInvar(\n    d, # dimension \n    t = dnb2.t, # number of bits in integer representation of points\n    alpha = [1,2,3], # per dimension smoothness parameters\n    lengthscales = [1, 1/2, 1/4]) # per dimension product weights\nx = dnb2(n_min=0,n_max=n) # shape=(n,d) digital net\ny = np.random.rand(n) # shape=(n,) random uniforms\n# fast matrix multiplication and linear system solve\nk1 = kernel(x,x[0]) # shape=(n,) first column of Gram matrix\nlam = np.sqrt(n)*qp.fwht(k1) # vector of eigenvalues\nyt = qp.fwht(y)\nu = qp.fwht(yt*lam) # fast matrix multiplication \nv = qp.fwht(yt/lam) # fast linear system solve\n# efficient fast transform updates\nynew = np.random.rand(n) # shape=(n,) new random uniforms\nomega = qp.omega_fwht(m) # shape=(n,)\nytnew = qp.fwht(ynew) # shape=(n,)\nytfull = np.concatenate([yt+omega*ytnew,yt-omega*ytnew])/np.sqrt(2) # shape=(2n,)\n</pre> d = 3 # dimension  m = 10 # will generate 2^m points n = 2**m # number of points dnb2 = qp.DigitalNetB2(d) # default to radical inverse order kernel = qp.KernelDigShiftInvar(     d, # dimension      t = dnb2.t, # number of bits in integer representation of points     alpha = [1,2,3], # per dimension smoothness parameters     lengthscales = [1, 1/2, 1/4]) # per dimension product weights x = dnb2(n_min=0,n_max=n) # shape=(n,d) digital net y = np.random.rand(n) # shape=(n,) random uniforms # fast matrix multiplication and linear system solve k1 = kernel(x,x[0]) # shape=(n,) first column of Gram matrix lam = np.sqrt(n)*qp.fwht(k1) # vector of eigenvalues yt = qp.fwht(y) u = qp.fwht(yt*lam) # fast matrix multiplication  v = qp.fwht(yt/lam) # fast linear system solve # efficient fast transform updates ynew = np.random.rand(n) # shape=(n,) new random uniforms omega = qp.omega_fwht(m) # shape=(n,) ytnew = qp.fwht(ynew) # shape=(n,) ytfull = np.concatenate([yt+omega*ytnew,yt-omega*ytnew])/np.sqrt(2) # shape=(2n,) In\u00a0[8]: Copied! <pre># slow matrix multiplication and linear system solve\nkmat = kernel(x[:,None,:],x[None,:,:]) # shape=(n,n)\nu_slow = kmat@y # matrix multiplication\nv_slow = np.linalg.solve(kmat,y) # solve a linear system\n# verify correctness\nassert np.allclose(u,u_slow)\nassert np.allclose(v,v_slow)\n# get next samples \nxnew = dnb2(n_min=n,n_max=2*n) # shape=(n,d) new digital net points\nk1new = kernel(xnew,x[0]) # shape=(n,) new values in the first column\n# inefficient fast transform update \nk1full = np.concatenate([k1,k1new]) # shape=(2*n,) full first column\nlamfull_inefficient = np.sqrt(2*n)*qp.fwht(k1full) # shape=(2*n,) full eigenvalues\nyfull = np.concatenate([y,ynew]) # shape=(2*n,) full random values\nytfull_inefficient = qp.fwht(yfull) # shape=(2*n,) full transformed points\n# efficient fast transform updates\nlamnew = np.sqrt(n)*qp.fwht(k1new) # shape=(n,) new eigenvalues\nlamfull = np.hstack([lam+omega*lamnew,lam-omega*lamnew])\n# verify correctness\nassert np.allclose(lamfull,lamfull_inefficient)\nassert np.allclose(ytfull,ytfull_inefficient)\n</pre> # slow matrix multiplication and linear system solve kmat = kernel(x[:,None,:],x[None,:,:]) # shape=(n,n) u_slow = kmat@y # matrix multiplication v_slow = np.linalg.solve(kmat,y) # solve a linear system # verify correctness assert np.allclose(u,u_slow) assert np.allclose(v,v_slow) # get next samples  xnew = dnb2(n_min=n,n_max=2*n) # shape=(n,d) new digital net points k1new = kernel(xnew,x[0]) # shape=(n,) new values in the first column # inefficient fast transform update  k1full = np.concatenate([k1,k1new]) # shape=(2*n,) full first column lamfull_inefficient = np.sqrt(2*n)*qp.fwht(k1full) # shape=(2*n,) full eigenvalues yfull = np.concatenate([y,ynew]) # shape=(2*n,) full random values ytfull_inefficient = qp.fwht(yfull) # shape=(2*n,) full transformed points # efficient fast transform updates lamnew = np.sqrt(n)*qp.fwht(k1new) # shape=(n,) new eigenvalues lamfull = np.hstack([lam+omega*lamnew,lam-omega*lamnew]) # verify correctness assert np.allclose(lamfull,lamfull_inefficient) assert np.allclose(ytfull,ytfull_inefficient) In\u00a0[\u00a0]: Copied! <pre>import scipy.stats\ndef gen_corner_peak_2(x):\n    d = x.shape[-1] # x.shape=(...,n,d), e.g., (n,d) or (R,n,d) \n    c_tilde = 1/np.arange(1,d+1)**2\n    c = 0.25*c_tilde/np.sum(c_tilde)\n    y = (1+np.sum(c*x,axis=-1))**(-(d+1)) \n    return y # y.shape=(...,n), e.g., (n,) or (R,n)\nR = 10 # number of randomizations\nn = 2**15 # number of points \nd = 50 # dimension\ndnb2 = qp.DigitalNetB2(dimension=d, replications=R, seed=7, alpha=3)\nx = dnb2(n) # x.shape=(R,n,d)\ny = gen_corner_peak_2(x) # y.shape=(R,n) \nmuhats = np.mean(y,axis=1) # muhats.shape=(R,)\nmuhat_aggregate = np.mean(muhats) # muhat_aggregate is a scalar \nprint(muhat_aggregate)\n\"\"\" 0.014936813948394042 \"\"\"\nalpha = 0.01 # uncertainty level\nt_star = -scipy.stats.t.ppf(alpha/2,df=R-1) # quantile of Student's t \nstdhat = np.std(muhats,ddof=1) # unbiased estimate of standard deviation\nstd_error = t_star*stdhat/np.sqrt(R)\nprint(std_error)\n\"\"\" 5.247445301861484e-07 \"\"\"\nconf_int = [muhat_aggregate-std_error,muhat_aggregate+std_error]\n</pre> import scipy.stats def gen_corner_peak_2(x):     d = x.shape[-1] # x.shape=(...,n,d), e.g., (n,d) or (R,n,d)      c_tilde = 1/np.arange(1,d+1)**2     c = 0.25*c_tilde/np.sum(c_tilde)     y = (1+np.sum(c*x,axis=-1))**(-(d+1))      return y # y.shape=(...,n), e.g., (n,) or (R,n) R = 10 # number of randomizations n = 2**15 # number of points  d = 50 # dimension dnb2 = qp.DigitalNetB2(dimension=d, replications=R, seed=7, alpha=3) x = dnb2(n) # x.shape=(R,n,d) y = gen_corner_peak_2(x) # y.shape=(R,n)  muhats = np.mean(y,axis=1) # muhats.shape=(R,) muhat_aggregate = np.mean(muhats) # muhat_aggregate is a scalar  print(muhat_aggregate) \"\"\" 0.014936813948394042 \"\"\" alpha = 0.01 # uncertainty level t_star = -scipy.stats.t.ppf(alpha/2,df=R-1) # quantile of Student's t  stdhat = np.std(muhats,ddof=1) # unbiased estimate of standard deviation std_error = t_star*stdhat/np.sqrt(R) print(std_error) \"\"\" 5.247445301861484e-07 \"\"\" conf_int = [muhat_aggregate-std_error,muhat_aggregate+std_error] <pre>0.014936813948394042\n5.247445301861484e-07\n</pre> In\u00a0[\u00a0]: Copied! <pre>def gen_corner_peak_2(x):\n    d = x.shape[-1] # x.shape=(...,n,d), e.g., (n,d) or (R,n,d) \n    c_tilde = 1/np.arange(1,d+1)**2\n    c = 0.25*c_tilde/np.sum(c_tilde)\n    y = (1+np.sum(c*x,axis=-1))**(-(d+1)) \n    return y # y.shape=(...,n), e.g., (n,) or (R,n)\nR = 10\nd = 50 \ndnb2 = qp.DigitalNetB2(dimension=d, replications=R, seed=7, alpha=3)\ntrue_measure = qp.Uniform(dnb2, lower_bound=0, upper_bound=1)\nintegrand = qp.CustomFun(true_measure=true_measure, g=gen_corner_peak_2)\n# equivalent to \n# integrand = qp.Genz(dnb2, kind_func=\"CORNER PEAK\", kind_coeff=2)\nqmc_algo = qp.CubQMCRepStudentT(integrand, abs_tol=1e-4)\nsolution,data = qmc_algo.integrate() # run adaptive QMC algorithm \nprint(solution)\n\"\"\" 0.014950908095474802 \"\"\"\nconf_int = [data.comb_bound_low,data.comb_bound_high]\nstd_error = (conf_int[1]-conf_int[0])/2\nprint(std_error)\n\"\"\" 2.7968149935497788e-05 \"\"\"\nprint(data)\n\"\"\"\nData (Data)\n    solution        0.015\n    comb_bound_low  0.015\n    comb_bound_high 0.015\n    comb_bound_diff 5.59e-05\n    comb_flags      1\n    n_total         10240\n    n               10240\n    n_rep           2^(10)\n    time_integrate  0.019\nCubQMCRepStudentT (AbstractStoppingCriterion)\n    inflate         1\n    alpha           0.010\n    abs_tol         1.00e-04\n    rel_tol         0\n    n_init          2^(8)\n    n_limit         2^(30)\nCustomFun (AbstractIntegrand)\nUniform (AbstractTrueMeasure)\n    lower_bound     0\n    upper_bound     1\nDigitalNetB2 (AbstractLDDiscreteDistribution)\n    d               50\n    replications    10\n    randomize       LMS DS\n    gen_mats_source joe_kuo.6.21201.txt\n    order           RADICAL INVERSE\n    t               63\n    alpha           3\n    n_limit         2^(32)\n    entropy         7\n\"\"\"\n</pre> def gen_corner_peak_2(x):     d = x.shape[-1] # x.shape=(...,n,d), e.g., (n,d) or (R,n,d)      c_tilde = 1/np.arange(1,d+1)**2     c = 0.25*c_tilde/np.sum(c_tilde)     y = (1+np.sum(c*x,axis=-1))**(-(d+1))      return y # y.shape=(...,n), e.g., (n,) or (R,n) R = 10 d = 50  dnb2 = qp.DigitalNetB2(dimension=d, replications=R, seed=7, alpha=3) true_measure = qp.Uniform(dnb2, lower_bound=0, upper_bound=1) integrand = qp.CustomFun(true_measure=true_measure, g=gen_corner_peak_2) # equivalent to  # integrand = qp.Genz(dnb2, kind_func=\"CORNER PEAK\", kind_coeff=2) qmc_algo = qp.CubQMCRepStudentT(integrand, abs_tol=1e-4) solution,data = qmc_algo.integrate() # run adaptive QMC algorithm  print(solution) \"\"\" 0.014950908095474802 \"\"\" conf_int = [data.comb_bound_low,data.comb_bound_high] std_error = (conf_int[1]-conf_int[0])/2 print(std_error) \"\"\" 2.7968149935497788e-05 \"\"\" print(data) \"\"\" Data (Data)     solution        0.015     comb_bound_low  0.015     comb_bound_high 0.015     comb_bound_diff 5.59e-05     comb_flags      1     n_total         10240     n               10240     n_rep           2^(10)     time_integrate  0.019 CubQMCRepStudentT (AbstractStoppingCriterion)     inflate         1     alpha           0.010     abs_tol         1.00e-04     rel_tol         0     n_init          2^(8)     n_limit         2^(30) CustomFun (AbstractIntegrand) Uniform (AbstractTrueMeasure)     lower_bound     0     upper_bound     1 DigitalNetB2 (AbstractLDDiscreteDistribution)     d               50     replications    10     randomize       LMS DS     gen_mats_source joe_kuo.6.21201.txt     order           RADICAL INVERSE     t               63     alpha           3     n_limit         2^(32)     entropy         7 \"\"\" <pre>0.014950908095474802\n2.7968149935497788e-05\nData (Data)\n    solution        0.015\n    comb_bound_low  0.015\n    comb_bound_high 0.015\n    comb_bound_diff 5.59e-05\n    comb_flags      1\n    n_total         10240\n    n               10240\n    n_rep           2^(10)\n    time_integrate  0.004\nCubQMCRepStudentT (AbstractStoppingCriterion)\n    inflate         1\n    alpha           0.010\n    abs_tol         1.00e-04\n    rel_tol         0\n    n_init          2^(8)\n    n_limit         2^(30)\nCustomFun (AbstractIntegrand)\nUniform (AbstractTrueMeasure)\n    lower_bound     0\n    upper_bound     1\nDigitalNetB2 (AbstractLDDiscreteDistribution)\n    d               50\n    replications    10\n    randomize       LMS DS\n    gen_mats_source joe_kuo.6.21201.txt\n    order           RADICAL INVERSE\n    t               63\n    alpha           3\n    n_limit         2^(32)\n    entropy         7\n</pre> Out[\u00a0]: <pre>'\\nData (Data)\\n    solution        0.015\\n    comb_bound_low  0.015\\n    comb_bound_high 0.015\\n    comb_bound_diff 5.59e-05\\n    comb_flags      1\\n    n_total         10240\\n    n               10240\\n    n_rep           2^(10)\\n    time_integrate  0.019\\nCubQMCRepStudentT (AbstractStoppingCriterion)\\n    inflate         1\\n    alpha           0.010\\n    abs_tol         1.00e-04\\n    rel_tol         0\\n    n_init          2^(8)\\n    n_limit         2^(30)\\nCustomFun (AbstractIntegrand)\\nUniform (AbstractTrueMeasure)\\n    lower_bound     0\\n    upper_bound     1\\nDigitalNetB2 (AbstractLDDiscreteDistribution)\\n    d               50\\n    replications    10\\n    randomize       LMS DS\\n    gen_mats_source joe_kuo.6.21201.txt\\n    order           RADICAL INVERSE\\n    t               63\\n    alpha           3\\n    n_limit         2^(32)\\n    entropy         7\\n'</pre> In\u00a0[11]: Copied! <pre>def cantilever_beam_function(T,compute_flags): # T is (n x 3)\n    Y = np.zeros((2,len(T)),dtype=float) # (n x 2)\n    l,w,t = 100,4,2\n    T1,T2,T3 = T[:,0],T[:,1],T[:,2] # Python is indexed from 0\n    if compute_flags[0]: # compute D. x^2 is \"x**2\" in Python\n        Y[0] = 4*l**3/(T1*w*t)*np.sqrt(T2**2/t**4+T3**2/w**4)\n    if compute_flags[1]: # compute S\n        Y[1] = 600*(T2/(w*t**2)+T3/(w**2*t))\n    return Y\ntrue_measure = qp.Gaussian(\n    sampler = qp.DigitalNetB2(dimension=3,seed=7),\n    mean = [2.9e7,500,1000],\n    covariance = np.diag([(1.45e6)**2,(100)**2,(100)**2]))\nintegrand = qp.CustomFun(true_measure,\n    g = cantilever_beam_function,\n    dimension_indv = 2)\nqmc_stop_crit = qp.CubQMCNetG(integrand,\n    abs_tol = 1e-3,\n    rel_tol = 1e-6)\nsolution,data = qmc_stop_crit.integrate()\nprint(data)\n</pre> def cantilever_beam_function(T,compute_flags): # T is (n x 3)     Y = np.zeros((2,len(T)),dtype=float) # (n x 2)     l,w,t = 100,4,2     T1,T2,T3 = T[:,0],T[:,1],T[:,2] # Python is indexed from 0     if compute_flags[0]: # compute D. x^2 is \"x**2\" in Python         Y[0] = 4*l**3/(T1*w*t)*np.sqrt(T2**2/t**4+T3**2/w**4)     if compute_flags[1]: # compute S         Y[1] = 600*(T2/(w*t**2)+T3/(w**2*t))     return Y true_measure = qp.Gaussian(     sampler = qp.DigitalNetB2(dimension=3,seed=7),     mean = [2.9e7,500,1000],     covariance = np.diag([(1.45e6)**2,(100)**2,(100)**2])) integrand = qp.CustomFun(true_measure,     g = cantilever_beam_function,     dimension_indv = 2) qmc_stop_crit = qp.CubQMCNetG(integrand,     abs_tol = 1e-3,     rel_tol = 1e-6) solution,data = qmc_stop_crit.integrate() print(data) <pre>Data (Data)\n    solution        [2.426e+00 3.750e+04]\n    comb_bound_low  [2.425e+00 3.750e+04]\n    comb_bound_high [2.427e+00 3.750e+04]\n    comb_bound_diff [0.002 0.041]\n    comb_flags      [ True  True]\n    n_total         2^(18)\n    n               [  1024 262144]\n    time_integrate  0.053\nCubQMCNetG (AbstractStoppingCriterion)\n    abs_tol         0.001\n    rel_tol         1.00e-06\n    n_init          2^(10)\n    n_limit         2^(35)\nCustomFun (AbstractIntegrand)\nGaussian (AbstractTrueMeasure)\n    mean            [2.9e+07 5.0e+02 1.0e+03]\n    covariance      [[2.102e+12 0.000e+00 0.000e+00]\n                     [0.000e+00 1.000e+04 0.000e+00]\n                     [0.000e+00 0.000e+00 1.000e+04]]\n    decomp_type     PCA\nDigitalNetB2 (AbstractLDDiscreteDistribution)\n    d               3\n    replications    1\n    randomize       LMS DS\n    gen_mats_source joe_kuo.6.21201.txt\n    order           RADICAL INVERSE\n    t               63\n    alpha           1\n    n_limit         2^(32)\n    entropy         7\n</pre> In\u00a0[12]: Copied! <pre>import pandas as pd\nfrom sklearn.model_selection import train_test_split\ndf = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/haberman/haberman.data',header=None)\ndf.columns = ['Age','1900 Year','Axillary Nodes','Survival Status']\ndf.loc[df['Survival Status']==2,'Survival Status'] = 0\nx,y = df[['Age','1900 Year','Axillary Nodes']],df['Survival Status']\nxt,xv,yt,yv = train_test_split(x,y,test_size=.33,random_state=7)\n</pre> import pandas as pd from sklearn.model_selection import train_test_split df = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/haberman/haberman.data',header=None) df.columns = ['Age','1900 Year','Axillary Nodes','Survival Status'] df.loc[df['Survival Status']==2,'Survival Status'] = 0 x,y = df[['Age','1900 Year','Axillary Nodes']],df['Survival Status'] xt,xv,yt,yv = train_test_split(x,y,test_size=.33,random_state=7) In\u00a0[13]: Copied! <pre>print(df.head(),'\\n')\nprint(df[['Age','1900 Year','Axillary Nodes']].describe(),'\\n')\nprint(df['Survival Status'].astype(str).describe())\nprint('\\ntrain samples: %d test samples: %d\\n'%(len(xt),len(xv)))\nprint('train positives %d   train negatives: %d'%(np.sum(yt==1),np.sum(yt==0)))\nprint(' test positives %d    test negatives: %d'%(np.sum(yv==1),np.sum(yv==0)))\nxt.head()\n</pre> print(df.head(),'\\n') print(df[['Age','1900 Year','Axillary Nodes']].describe(),'\\n') print(df['Survival Status'].astype(str).describe()) print('\\ntrain samples: %d test samples: %d\\n'%(len(xt),len(xv))) print('train positives %d   train negatives: %d'%(np.sum(yt==1),np.sum(yt==0))) print(' test positives %d    test negatives: %d'%(np.sum(yv==1),np.sum(yv==0))) xt.head() <pre>   Age  1900 Year  Axillary Nodes  Survival Status\n0   30         64               1                1\n1   30         62               3                1\n2   30         65               0                1\n3   31         59               2                1\n4   31         65               4                1 \n\n              Age   1900 Year  Axillary Nodes\ncount  306.000000  306.000000      306.000000\nmean    52.457516   62.852941        4.026144\nstd     10.803452    3.249405        7.189654\nmin     30.000000   58.000000        0.000000\n25%     44.000000   60.000000        0.000000\n50%     52.000000   63.000000        1.000000\n75%     60.750000   65.750000        4.000000\nmax     83.000000   69.000000       52.000000 \n\ncount     306\nunique      2\ntop         1\nfreq      225\nName: Survival Status, dtype: object\n\ntrain samples: 205 test samples: 101\n\ntrain positives 151   train negatives: 54\n test positives 74    test negatives: 27\n</pre> Out[13]: Age 1900 Year Axillary Nodes 46 41 58 0 199 57 64 1 115 49 64 10 128 50 61 0 249 63 63 0 In\u00a0[14]: Copied! <pre>blr = qp.BayesianLRCoeffs(\n    sampler = qp.DigitalNetB2(4,seed=1),\n    feature_array = xt, # np.ndarray of shape (n,d-1)\n    response_vector = yt, # np.ndarray of shape (n,)\n    prior_mean = 0, # normal prior mean = (0,0,...,0)\n    prior_covariance = 5) # normal prior covariance = 5I\nqmc_sc = qp.CubQMCNetG(blr,\n    abs_tol = .1,\n    rel_tol = .5,\n    error_fun = \"BOTH\",\n    n_limit=2**18)\nblr_coefs,blr_data = qmc_sc.integrate()\nprint(blr_data)\n\"\"\"\nData (Data)\nsolution        [-0.128  0.084 -0.091  0.09 ]\ncomb_bound_low  [-0.198  0.069 -0.14   0.074]\ncomb_bound_high [-0.105  0.129 -0.075  0.138]\ncomb_bound_diff [0.092 0.06  0.065 0.065]\ncomb_flags      [ True  True  True  True]\nn_total         2^(17)\nn               [[  1024   1024   2048 131072]\n                    [  1024   1024   2048 131072]]\ntime_integrate  0.351\n\"\"\"\n</pre> blr = qp.BayesianLRCoeffs(     sampler = qp.DigitalNetB2(4,seed=1),     feature_array = xt, # np.ndarray of shape (n,d-1)     response_vector = yt, # np.ndarray of shape (n,)     prior_mean = 0, # normal prior mean = (0,0,...,0)     prior_covariance = 5) # normal prior covariance = 5I qmc_sc = qp.CubQMCNetG(blr,     abs_tol = .1,     rel_tol = .5,     error_fun = \"BOTH\",     n_limit=2**18) blr_coefs,blr_data = qmc_sc.integrate() print(blr_data) \"\"\" Data (Data) solution        [-0.128  0.084 -0.091  0.09 ] comb_bound_low  [-0.198  0.069 -0.14   0.074] comb_bound_high [-0.105  0.129 -0.075  0.138] comb_bound_diff [0.092 0.06  0.065 0.065] comb_flags      [ True  True  True  True] n_total         2^(17) n               [[  1024   1024   2048 131072]                     [  1024   1024   2048 131072]] time_integrate  0.351 \"\"\" <pre>Data (Data)\n    solution        [-0.128  0.084 -0.091  0.09 ]\n    comb_bound_low  [-0.198  0.069 -0.14   0.074]\n    comb_bound_high [-0.105  0.129 -0.075  0.138]\n    comb_bound_diff [0.092 0.06  0.065 0.065]\n    comb_flags      [ True  True  True  True]\n    n_total         2^(17)\n    n               [[  1024   1024   2048 131072]\n                     [  1024   1024   2048 131072]]\n    time_integrate  0.350\nCubQMCNetG (AbstractStoppingCriterion)\n    abs_tol         0.100\n    rel_tol         2^(-1)\n    n_init          2^(10)\n    n_limit         2^(18)\nBayesianLRCoeffs (AbstractIntegrand)\nGaussian (AbstractTrueMeasure)\n    mean            0\n    covariance      5\n    decomp_type     PCA\nDigitalNetB2 (AbstractLDDiscreteDistribution)\n    d               2^(2)\n    replications    1\n    randomize       LMS DS\n    gen_mats_source joe_kuo.6.21201.txt\n    order           RADICAL INVERSE\n    t               63\n    alpha           1\n    n_limit         2^(32)\n    entropy         1\n</pre> Out[14]: <pre>'\\nData (Data)\\nsolution        [-0.128  0.084 -0.091  0.09 ]\\ncomb_bound_low  [-0.198  0.069 -0.14   0.074]\\ncomb_bound_high [-0.105  0.129 -0.075  0.138]\\ncomb_bound_diff [0.092 0.06  0.065 0.065]\\ncomb_flags      [ True  True  True  True]\\nn_total         2^(17)\\nn               [[  1024   1024   2048 131072]\\n                    [  1024   1024   2048 131072]]\\ntime_integrate  0.351\\n'</pre> In\u00a0[15]: Copied! <pre>from sklearn.linear_model import LogisticRegression\ndef metrics(y,yhat):\n    y,yhat = np.array(y),np.array(yhat)\n    tp = np.sum((y==1)*(yhat==1))\n    tn = np.sum((y==0)*(yhat==0))\n    fp = np.sum((y==0)*(yhat==1))\n    fn = np.sum((y==1)*(yhat==0))\n    accuracy = (tp+tn)/(len(y))\n    precision = tp/(tp+fp)\n    recall = tp/(tp+fn)\n    return [accuracy,precision,recall]\n\nresults = pd.DataFrame({name:[] for name in ['method','Age','1900 Year','Axillary Nodes','Intercept','Accuracy','Precision','Recall']})\nfor i,l1_ratio in enumerate([0,.5,1]):\n    lr = LogisticRegression(random_state=7,penalty=\"elasticnet\",solver='saga',l1_ratio=l1_ratio).fit(xt,yt)\n    results.loc[i] = [r'Elastic-Net \\lambda=%.1f'%l1_ratio]+lr.coef_.squeeze().tolist()+[lr.intercept_.item()]+metrics(yv,lr.predict(xv))\n\nblr_predict = lambda x: 1/(1+np.exp(-np.array(x)@blr_coefs[:-1]-blr_coefs[-1]))&gt;=.5\nblr_train_accuracy = np.mean(blr_predict(xt)==yt)\nblr_test_accuracy = np.mean(blr_predict(xv)==yv)\nresults.loc[len(results)] = ['Bayesian']+blr_coefs.squeeze().tolist()+metrics(yv,blr_predict(xv))\n\nimport warnings\nwarnings.simplefilter('ignore',FutureWarning)\nresults.set_index('method',inplace=True)\nprint(results.head())\n#root: results.to_latex(root+'lr_table.tex',formatters={'%s'%tt:lambda v:'%.1f'%(100*v) for tt in ['accuracy','precision','recall']},float_format=\"%.2e\")\n</pre> from sklearn.linear_model import LogisticRegression def metrics(y,yhat):     y,yhat = np.array(y),np.array(yhat)     tp = np.sum((y==1)*(yhat==1))     tn = np.sum((y==0)*(yhat==0))     fp = np.sum((y==0)*(yhat==1))     fn = np.sum((y==1)*(yhat==0))     accuracy = (tp+tn)/(len(y))     precision = tp/(tp+fp)     recall = tp/(tp+fn)     return [accuracy,precision,recall]  results = pd.DataFrame({name:[] for name in ['method','Age','1900 Year','Axillary Nodes','Intercept','Accuracy','Precision','Recall']}) for i,l1_ratio in enumerate([0,.5,1]):     lr = LogisticRegression(random_state=7,penalty=\"elasticnet\",solver='saga',l1_ratio=l1_ratio).fit(xt,yt)     results.loc[i] = [r'Elastic-Net \\lambda=%.1f'%l1_ratio]+lr.coef_.squeeze().tolist()+[lr.intercept_.item()]+metrics(yv,lr.predict(xv))  blr_predict = lambda x: 1/(1+np.exp(-np.array(x)@blr_coefs[:-1]-blr_coefs[-1]))&gt;=.5 blr_train_accuracy = np.mean(blr_predict(xt)==yt) blr_test_accuracy = np.mean(blr_predict(xv)==yv) results.loc[len(results)] = ['Bayesian']+blr_coefs.squeeze().tolist()+metrics(yv,blr_predict(xv))  import warnings warnings.simplefilter('ignore',FutureWarning) results.set_index('method',inplace=True) print(results.head()) #root: results.to_latex(root+'lr_table.tex',formatters={'%s'%tt:lambda v:'%.1f'%(100*v) for tt in ['accuracy','precision','recall']},float_format=\"%.2e\") <pre>                              Age  1900 Year  Axillary Nodes  Intercept  \\\nmethod                                                                    \nElastic-Net \\lambda=0.0 -0.012279   0.034401       -0.115153   0.001990   \nElastic-Net \\lambda=0.5 -0.012041   0.034170       -0.114770   0.002025   \nElastic-Net \\lambda=1.0 -0.011803   0.033940       -0.114387   0.002061   \nBayesian                -0.128270   0.083826       -0.090906   0.089903   \n\n                         Accuracy  Precision    Recall  \nmethod                                                  \nElastic-Net \\lambda=0.0  0.742574   0.766667  0.932432  \nElastic-Net \\lambda=0.5  0.742574   0.766667  0.932432  \nElastic-Net \\lambda=1.0  0.742574   0.766667  0.932432  \nBayesian                 0.326733   1.000000  0.081081  \n</pre> In\u00a0[16]: Copied! <pre>a,b = 7,0.1\ndnb2 = qp.DigitalNetB2(3,seed=7)\nishigami = qp.Ishigami(dnb2,a,b)\nidxs = np.array([\n    [True,False,False],\n    [False,True,False],\n    [False,False,True],\n    [True,True,False],\n    [True,False,True],\n    [False,True,True]],dtype=bool)\nishigami_si = qp.SensitivityIndices(ishigami,idxs)\nqmc_algo = qp.CubQMCNetG(ishigami_si,abs_tol=.05)\nsolution,data = qmc_algo.integrate()\nprint(data)\nsi_closed = solution[0].squeeze()\nsi_total = solution[1].squeeze()\nci_comb_low_closed = data.comb_bound_low[0].squeeze()\nci_comb_high_closed = data.comb_bound_high[0].squeeze()\nci_comb_low_total = data.comb_bound_low[1].squeeze()\nci_comb_high_total = data.comb_bound_high[1].squeeze()\nprint(\"\\nApprox took %.1f sec and n = 2^(%d)\"%\n    (data.time_integrate,np.log2(data.n_total)))\nprint('\\t si_closed:',si_closed)\nprint('\\t si_total:',si_total)\nprint('\\t ci_comb_low_closed:',ci_comb_low_closed)\nprint('\\t ci_comb_high_closed:',ci_comb_high_closed)\nprint('\\t ci_comb_low_total:',ci_comb_low_total)\nprint('\\t ci_comb_high_total:',ci_comb_high_total)\n\ntrue_indices = qp.Ishigami._exact_sensitivity_indices(idxs,a,b)\nsi_closed_true = true_indices[0]\nsi_total_true = true_indices[1]\n</pre> a,b = 7,0.1 dnb2 = qp.DigitalNetB2(3,seed=7) ishigami = qp.Ishigami(dnb2,a,b) idxs = np.array([     [True,False,False],     [False,True,False],     [False,False,True],     [True,True,False],     [True,False,True],     [False,True,True]],dtype=bool) ishigami_si = qp.SensitivityIndices(ishigami,idxs) qmc_algo = qp.CubQMCNetG(ishigami_si,abs_tol=.05) solution,data = qmc_algo.integrate() print(data) si_closed = solution[0].squeeze() si_total = solution[1].squeeze() ci_comb_low_closed = data.comb_bound_low[0].squeeze() ci_comb_high_closed = data.comb_bound_high[0].squeeze() ci_comb_low_total = data.comb_bound_low[1].squeeze() ci_comb_high_total = data.comb_bound_high[1].squeeze() print(\"\\nApprox took %.1f sec and n = 2^(%d)\"%     (data.time_integrate,np.log2(data.n_total))) print('\\t si_closed:',si_closed) print('\\t si_total:',si_total) print('\\t ci_comb_low_closed:',ci_comb_low_closed) print('\\t ci_comb_high_closed:',ci_comb_high_closed) print('\\t ci_comb_low_total:',ci_comb_low_total) print('\\t ci_comb_high_total:',ci_comb_high_total)  true_indices = qp.Ishigami._exact_sensitivity_indices(idxs,a,b) si_closed_true = true_indices[0] si_total_true = true_indices[1] <pre>Data (Data)\n    solution        [[0.315 0.421 0.004 0.735 0.558 0.419]\n                     [0.565 0.444 0.244 0.987 0.558 0.688]]\n    comb_bound_low  [[0.293 0.409 0.    0.706 0.534 0.399]\n                     [0.541 0.435 0.234 0.974 0.539 0.667]]\n    comb_bound_high [[0.336 0.432 0.008 0.765 0.582 0.438]\n                     [0.59  0.452 0.253 1.    0.577 0.708]]\n    comb_bound_diff [[0.043 0.023 0.008 0.059 0.047 0.039]\n                     [0.049 0.017 0.02  0.026 0.038 0.042]]\n    comb_flags      [[ True  True  True  True  True  True]\n                     [ True  True  True  True  True  True]]\n    n_total         2^(10)\n    n               [[[1024 1024 1024 1024 1024 1024]\n                      [1024 1024 1024 1024 1024 1024]\n                      [1024 1024 1024 1024 1024 1024]]\n                    \n                     [[1024 1024 1024 1024 1024 1024]\n                      [1024 1024 1024 1024 1024 1024]\n                      [1024 1024 1024 1024 1024 1024]]]\n    time_integrate  0.005\nCubQMCNetG (AbstractStoppingCriterion)\n    abs_tol         0.050\n    rel_tol         0\n    n_init          2^(10)\n    n_limit         2^(35)\nSensitivityIndices (AbstractIntegrand)\n    indices         [[ True False False]\n                     [False  True False]\n                     [False False  True]\n                     [ True  True False]\n                     [ True False  True]\n                     [False  True  True]]\nUniform (AbstractTrueMeasure)\n    lower_bound     -3.142\n    upper_bound     3.142\nDigitalNetB2 (AbstractLDDiscreteDistribution)\n    d               3\n    replications    1\n    randomize       LMS DS\n    gen_mats_source joe_kuo.6.21201.txt\n    order           RADICAL INVERSE\n    t               63\n    alpha           1\n    n_limit         2^(32)\n    entropy         7\n\nApprox took 0.0 sec and n = 2^(10)\n\t si_closed: [0.31495301 0.42054985 0.00403061 0.73545156 0.5580266  0.41868287]\n\t si_total: [0.56524058 0.4435145  0.24364672 0.98722305 0.557734   0.68753131]\n\t ci_comb_low_closed: [0.2934895  0.4092381  0.         0.70577578 0.53435058 0.39922204]\n\t ci_comb_high_closed: [0.33641652 0.4318616  0.00806122 0.76512734 0.58170261 0.4381437 ]\n\t ci_comb_low_total: [0.54066449 0.43508219 0.23389506 0.9744461  0.53864679 0.66669245]\n\t ci_comb_high_total: [0.58981667 0.45194681 0.25339839 1.         0.57682122 0.70837017]\n</pre> In\u00a0[17]: Copied! <pre>from sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neural_network import MLPClassifier\ndata = load_iris()\nfeature_names = data[\"feature_names\"]\nfeature_names = [fn.replace('sepal ','S')\\\n    .replace('length ','L')\\\n    .replace('petal ','P')\\\n    .replace('width ','W')\\\n    .replace('(cm)','') for fn in feature_names]\ntarget_names = data[\"target_names\"]\nxt,xv,yt,yv = train_test_split(data[\"data\"],data[\"target\"],\n    test_size = 1/3,\n    random_state = 7)\nmlpc = MLPClassifier(random_state=7,max_iter=1024).fit(xt,yt)\nyhat = mlpc.predict(xv)\nprint(\"accuracy: %.1f%%\"%(100*(yv==yhat).mean()))\n# accuracy: 98.0%\nsampler = qp.DigitalNetB2(4,seed=7)\ntrue_measure =  qp.Uniform(sampler,\n    lower_bound = xt.min(0),\n    upper_bound = xt.max(0))\nfun = qp.CustomFun(\n    true_measure = true_measure,\n    g = lambda x: mlpc.predict_proba(x).T,\n    dimension_indv = 3)\nsi_fun = qp.SensitivityIndices(fun,indices=\"all\")\nqmc_algo = qp.CubQMCNetG(si_fun,abs_tol=.005)\nnn_sis,nn_sis_data = qmc_algo.integrate()\nnn_sis.shape\n\"\"\"\n(2, 14, 3)\n\"\"\"\n#print(nn_sis_data.flags_indv.shape)\n#print(nn_sis_data.flags_comb.shape)\nprint('samples: 2^(%d)'%np.log2(nn_sis_data.n_total))\nprint('time: %.1e'%nn_sis_data.time_integrate)\nprint('indices:\\n%s'%nn_sis_data.integrand.indices)\n\nimport pandas as pd\n\ndf_closed = pd.DataFrame(nn_sis[0],columns=target_names,index=[str(np.where(idx)[0]) for idx in nn_sis_data.integrand.indices])\nprint('\\nClosed Indices')\nprint(df_closed)\ndf_total = pd.DataFrame(nn_sis[1],columns=target_names,index=[str(np.where(idx)[0]) for idx in nn_sis_data.integrand.indices])\nprint('\\nTotal Indices')\ndf_closed_singletons = df_closed.loc[['[%d]'%i for i in range(4)]].T\ndf_closed_singletons['sum singletons'] = df_closed_singletons[['[%d]'%i for i in range(4)]].sum(1)\ndf_closed_singletons.columns = data['feature_names']+['sum']\ndf_closed_singletons = df_closed_singletons*100\ndf_closed_singletons\n</pre> from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split from sklearn.neural_network import MLPClassifier data = load_iris() feature_names = data[\"feature_names\"] feature_names = [fn.replace('sepal ','S')\\     .replace('length ','L')\\     .replace('petal ','P')\\     .replace('width ','W')\\     .replace('(cm)','') for fn in feature_names] target_names = data[\"target_names\"] xt,xv,yt,yv = train_test_split(data[\"data\"],data[\"target\"],     test_size = 1/3,     random_state = 7) mlpc = MLPClassifier(random_state=7,max_iter=1024).fit(xt,yt) yhat = mlpc.predict(xv) print(\"accuracy: %.1f%%\"%(100*(yv==yhat).mean())) # accuracy: 98.0% sampler = qp.DigitalNetB2(4,seed=7) true_measure =  qp.Uniform(sampler,     lower_bound = xt.min(0),     upper_bound = xt.max(0)) fun = qp.CustomFun(     true_measure = true_measure,     g = lambda x: mlpc.predict_proba(x).T,     dimension_indv = 3) si_fun = qp.SensitivityIndices(fun,indices=\"all\") qmc_algo = qp.CubQMCNetG(si_fun,abs_tol=.005) nn_sis,nn_sis_data = qmc_algo.integrate() nn_sis.shape \"\"\" (2, 14, 3) \"\"\" #print(nn_sis_data.flags_indv.shape) #print(nn_sis_data.flags_comb.shape) print('samples: 2^(%d)'%np.log2(nn_sis_data.n_total)) print('time: %.1e'%nn_sis_data.time_integrate) print('indices:\\n%s'%nn_sis_data.integrand.indices)  import pandas as pd  df_closed = pd.DataFrame(nn_sis[0],columns=target_names,index=[str(np.where(idx)[0]) for idx in nn_sis_data.integrand.indices]) print('\\nClosed Indices') print(df_closed) df_total = pd.DataFrame(nn_sis[1],columns=target_names,index=[str(np.where(idx)[0]) for idx in nn_sis_data.integrand.indices]) print('\\nTotal Indices') df_closed_singletons = df_closed.loc[['[%d]'%i for i in range(4)]].T df_closed_singletons['sum singletons'] = df_closed_singletons[['[%d]'%i for i in range(4)]].sum(1) df_closed_singletons.columns = data['feature_names']+['sum'] df_closed_singletons = df_closed_singletons*100 df_closed_singletons <pre>accuracy: 98.0%\nsamples: 2^(15)\ntime: 6.4e-01\nindices:\n[[ True False False False]\n [False  True False False]\n [False False  True False]\n [False False False  True]\n [ True  True False False]\n [ True False  True False]\n [ True False False  True]\n [False  True  True False]\n [False  True False  True]\n [False False  True  True]\n [ True  True  True False]\n [ True  True False  True]\n [ True False  True  True]\n [False  True  True  True]]\n\nClosed Indices\n           setosa  versicolor  virginica\n[0]      0.001323    0.068645   0.077325\n[1]      0.063749    0.026565   0.004784\n[2]      0.713825    0.325072   0.497800\n[3]      0.052967    0.025579   0.120317\n[0 1]    0.063925    0.091300   0.085151\n[0 2]    0.715316    0.460314   0.637738\n[0 3]    0.053469    0.092601   0.205639\n[1 2]    0.841655    0.431035   0.513277\n[1 3]    0.110739    0.039410   0.131264\n[2 3]    0.822910    0.583282   0.703142\n[0 1 2]  0.843726    0.570076   0.658272\n[0 1 3]  0.112798    0.104804   0.215817\n[0 2 3]  0.825330    0.815263   0.945267\n[1 2 3]  0.995864    0.739588   0.728499\n\nTotal Indices\n</pre> Out[17]: sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) sum setosa 0.132343 6.374860 71.382498 5.296674 83.186375 versicolor 6.864536 2.656530 32.507230 2.557911 44.586208 virginica 7.732521 0.478364 49.779978 12.031725 70.022587 In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"demos/talk_paper_demos/SorokinThesis2025/sorokin_thesis_2025/#aleksei-sorokin-phd-thesis-2025-quasi-monte-carlo-codes","title":"Aleksei Sorokin PhD Thesis 2025: Quasi-Monte Carlo Codes\u00b6","text":"<p>https://www.arxiv.org/abs/2511.21915</p>"},{"location":"demos/talk_paper_demos/SorokinThesis2025/sorokin_thesis_2025/#point-sets","title":"Point Sets\u00b6","text":""},{"location":"demos/talk_paper_demos/SorokinThesis2025/sorokin_thesis_2025/#kernel-methods","title":"Kernel Methods\u00b6","text":""},{"location":"demos/talk_paper_demos/SorokinThesis2025/sorokin_thesis_2025/#lattice-fftbr-ifftbr","title":"Lattice + FFTBR + IFFTBR\u00b6","text":""},{"location":"demos/talk_paper_demos/SorokinThesis2025/sorokin_thesis_2025/#digital-net-fwht","title":"Digital Net + FWHT\u00b6","text":""},{"location":"demos/talk_paper_demos/SorokinThesis2025/sorokin_thesis_2025/#integration","title":"Integration\u00b6","text":""},{"location":"demos/talk_paper_demos/SorokinThesis2025/sorokin_thesis_2025/#cantilever-beam","title":"Cantilever Beam\u00b6","text":""},{"location":"demos/talk_paper_demos/SorokinThesis2025/sorokin_thesis_2025/#bayesian-logistic-regression","title":"Bayesian Logistic Regression\u00b6","text":""},{"location":"demos/talk_paper_demos/SorokinThesis2025/sorokin_thesis_2025/#ishigami-sensitivity-indices","title":"Ishigami Sensitivity Indices\u00b6","text":""},{"location":"demos/talk_paper_demos/SorokinThesis2025/sorokin_thesis_2025/#neural-network-classifier-sensitiviy-indices","title":"Neural Network Classifier Sensitiviy Indices\u00b6","text":""},{"location":"demos/talk_paper_demos/why_add_q_to_mc_blog/why_add_q_to_mc_blog/","title":"2020 Why Add Q to MC?","text":"In\u00a0[1]: Copied! <pre>from qmcpy import *\nfrom matplotlib import pyplot\n\npyplot.rc('font', size=16)          # controls default text sizes\npyplot.rc('axes', titlesize=16)     # fontsize of the axes title\npyplot.rc('axes', labelsize=16)    # fontsize of the x and y labels\npyplot.rc('xtick', labelsize=16)    # fontsize of the tick labels\npyplot.rc('ytick', labelsize=16)    # fontsize of the tick labels\npyplot.rc('legend', fontsize=16)    # legend fontsize\npyplot.rc('figure', titlesize=16)  # fontsize of the figure title\n\nn = 64\n\npts_sets = [\n    IIDStdUniform(2,seed=7).gen_samples(n),\n    Lattice(2,seed=7).gen_samples(n)]\ntitles = ['$U[0,1]^2$','Shifted Lattice']\nsymbols = ['T','X']\noutput_files = ['iid_uniform_pts','lattice_pts']\n\nfor pts,title,symbol,out_f in zip(pts_sets,titles,symbols,output_files):\n    fig,ax = pyplot.subplots(nrows=1, ncols=1, figsize=(5,5))\n    ax.scatter(pts[:,0],pts[:,1],color='b')\n    ax.set_xlabel('$%s_{i1}$'%symbol)\n    ax.set_xlim([0,1])\n    ax.set_xticks([0,1])\n    ax.set_ylabel('$%s_{i2}$'%symbol)\n    ax.set_ylim([0,1])\n    ax.set_yticks([0,1])\n    ax.set_title(title)\n    ax.set_aspect('equal')\n    pyplot.tight_layout()\n    fig.savefig('./outputs/%s.png'%out_f,dpi=200)\n</pre> from qmcpy import * from matplotlib import pyplot  pyplot.rc('font', size=16)          # controls default text sizes pyplot.rc('axes', titlesize=16)     # fontsize of the axes title pyplot.rc('axes', labelsize=16)    # fontsize of the x and y labels pyplot.rc('xtick', labelsize=16)    # fontsize of the tick labels pyplot.rc('ytick', labelsize=16)    # fontsize of the tick labels pyplot.rc('legend', fontsize=16)    # legend fontsize pyplot.rc('figure', titlesize=16)  # fontsize of the figure title  n = 64  pts_sets = [     IIDStdUniform(2,seed=7).gen_samples(n),     Lattice(2,seed=7).gen_samples(n)] titles = ['$U[0,1]^2$','Shifted Lattice'] symbols = ['T','X'] output_files = ['iid_uniform_pts','lattice_pts']  for pts,title,symbol,out_f in zip(pts_sets,titles,symbols,output_files):     fig,ax = pyplot.subplots(nrows=1, ncols=1, figsize=(5,5))     ax.scatter(pts[:,0],pts[:,1],color='b')     ax.set_xlabel('$%s_{i1}$'%symbol)     ax.set_xlim([0,1])     ax.set_xticks([0,1])     ax.set_ylabel('$%s_{i2}$'%symbol)     ax.set_ylim([0,1])     ax.set_yticks([0,1])     ax.set_title(title)     ax.set_aspect('equal')     pyplot.tight_layout()     fig.savefig('./outputs/%s.png'%out_f,dpi=200)  In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"paper/","title":"QMCPy JOSS Paper","text":""},{"location":"paper/#qmcpy-joss-paper","title":"QMCPy JOSS Paper","text":"<p>To render <code>paper.md</code> as a PDF, </p> <ul> <li> <p>Install DockerHub and login. See also exec: \"docker-credential-desktop.exe\": executable file not found in $PATH. </p> </li> <li> <p>Run the following from the <code>QMCSoftware/</code> directory. </p> </li> </ul> <pre><code>docker run --rm     --volume $PWD/paper:/data     --user $(id -u):$(id -g)     --env JOURNAL=joss     openjournals/inara\n</code></pre> <ul> <li>To count number of words run <code>wc -w paper/paper.md</code>. To count the number of words without header and Acknowledgements, run the following from the repository root (QMCSoftware/) in Terminal:</li> </ul> <pre><code>start=$(grep -n '^# Summary' paper/paper.md | cut -d: -f1) &amp;&amp; \\\nend=$(grep -n '^# Acknowledgements' paper/paper.md | cut -d: -f1) &amp;&amp; \\\necho Words between lines $start and $end &amp;&amp; \\\nawk -v s=$start -v e=$end 'NR&gt;s &amp;&amp; NR&lt;e{print}' paper/paper.md | wc -w\n</code></pre>"},{"location":"paper/paper/","title":"QMCPy: A Python Framework for (Quasi-)Monte Carlo Algorithms","text":"","tags":["(quasi-)Monte Carlo numerical integration","randomized low-discrepancy sequences","automatic error estimation","object oriented Python framework"]},{"location":"paper/paper/#summary","title":"Summary","text":"<p>Monte Carlo (MC) methods estimate high-dimensional integrals by computing sample averages at independent and identically distributed (IID) random points. Quasi-Monte Carlo (QMC) methods replace IID samples with low-discrepancy (LD) sequences which more uniformly cover the integration domain, leading to faster convergence and reduced computational requirements. \\autoref{fig:points} visualizes IID and LD sequences. </p> <p><code>QMCPy</code> (https://qmcsoftware.github.io/QMCSoftware) [@QMCPy2026] is our Python package for high-dimensional numerical integration using MC and QMC methods, collectively \"(Q)MC.\" Its object-oriented design enables researchers to easily implement novel (Q)MC algorithms. The framework offers user-friendly APIs, diverse (Q)MC algorithms, adaptive error estimation techniques, and integration with scientific libraries following reproducible research practices [@Cho14a2;@ChoEtal22a]. Compared to previous versions, <code>QMCPy</code> v2.2 (which is easily installed with <code>pip install -U qmcpy</code>) includes</p> <ul> <li>improved documentation,</li> <li>strengthened tests and demos, and </li> <li>expanded support for randomized LD sequences.</li> </ul> <p></p>","tags":["(quasi-)Monte Carlo numerical integration","randomized low-discrepancy sequences","automatic error estimation","object oriented Python framework"]},{"location":"paper/paper/#statement-of-need","title":"Statement of Need","text":"<p>(Q)MC methods are essential for computational finance [@Lem04a;@wangsloan05;@giles2009multilevel;@zhang2021sentiment], uncertainty quantification [@Seelinger2023;@MUQ;@parno2021muq;@Marzouk2016;@KaaEtal21], machine learning [@DICK2021101587;@pmlr-v80-chen18f], and physics [@AB02;@LanBin14;@bernhard2015quantifying]. While (Q)MC methods are well established [@DicPil10a;@dick2013high], practical implementation demands numerical and algorithmic expertise. <code>QMCPy</code> follows MATLAB's Guaranteed Automatic Integration Library (GAIL) [@ChoEtal21a2;@TonEtAl22a] in consolidating a broad range of cutting-edge (Q)MC algorithms into a unified framework [@ChoEtal22a;@ChoEtal24a2;@sorokin2022bounding;@sorokin2025unified;@HicKirSor26a]. <code>QMCPy</code> features</p> <ul> <li>intuitive APIs for (Q)MC components,</li> <li>flexible integrations with <code>NumPy</code> [@harris2020array], <code>SciPy</code> [@2020SciPy-NMeth], and <code>PyTorch</code> [@NEURIPS2019_9015],</li> <li>robust and adaptive sampling with theoretically grounded error estimation, and</li> <li>extensible (Q)MC components enabling researchers to implement and test new algorithms.</li> </ul> <p>While popular modules like <code>scipy.stats.qmc</code> [@Roy2023] and <code>torch.quasirandom</code> [@NEURIPS2019_9015] provide basic (Q)MC sequences such as Sobol' and Halton, <code>QMCPy</code> provides (Q)MC researchers and practitioners an end-to-end research framework with additional capabilities to enable state-of-the-art (Q)MC techniques. Advanced features unique to <code>QMCPy</code> include</p> <ul> <li>customizable LD sequences with diverse randomization techniques,</li> <li>efficient generators of LD sequences with multiple independent randomizations,</li> <li>automatic variable transformations for (Q)MC compatibility, and</li> <li>rigorous adaptive error estimation algorithms.</li> </ul>","tags":["(quasi-)Monte Carlo numerical integration","randomized low-discrepancy sequences","automatic error estimation","object oriented Python framework"]},{"location":"paper/paper/#components","title":"Components","text":"<p>(Q)MC methods approximate the multivariate integral \\begin{equation}\\label{eq:mu-general}   \\mu := \\mathbb{E}[g(\\mathbf{T})] = \\int_{\\mathcal{T}} g(\\mathbf{t}) \\, \\lambda(\\mathbf{t}) \\, d\\mathbf{t}, \\qquad \\mathbf{T} \\sim \\lambda, \\end{equation} where \\(g\\) is the integrand and \\(\\lambda\\) is the probability density of a random variable \\(\\mathbf{T}\\) whose distribution we call the true measure. To accommodate LD samples (approximately uniform on \\([0,1]^d\\)), a transformation \\(\\boldsymbol{\\psi}\\) is performed to rewrite \\(\\mu\\) as \\begin{equation}\\label{eq:mu-uniform}   \\mu = \\mathbb{E}[f(\\mathbf{X})] = \\int_{[0,1]^d} f(\\mathbf{x}) \\, d\\mathbf{x}, \\qquad \\mathbf{X} \\sim \\mathcal{U}[0,1]^d. \\end{equation} If \\(\\mathbf{T} \\sim \\boldsymbol{\\psi}(\\mathbf{X})\\), then \\(f = g \\circ \\boldsymbol{\\psi}\\). </p> <p>(Q)MC methods estimate the population mean \\(\\mu\\) in \\eqref{eq:mu-uniform} via the sample mean \\begin{equation}\\label{eq:mu-hat}   \\widehat{\\mu} := \\frac{1}{n} \\sum_{i=1}^{n} f(\\mathbf{X}_i). \\end{equation} MC methods use IID \\(\\mathbf{X}_1,\\dots,\\mathbf{X}_n\\) and have error \\(|\\widehat{\\mu}-\\mu|\\) like \\(\\mathcal{O}(n^{-1/2})\\) [@Nie78]. QMC methods choose dependent LD nodes that fill \\([0,1]^d\\) more evenly, i.e., the discrepancy between the discrete distribution of \\(\\mathbf{X}_1,\\dots,\\mathbf{X}_n\\) and the uniform distribution is small. QMC methods can achieve errors like \\(\\mathcal{O}(n^{-1+\\delta})\\) where \\(\\delta&gt;0\\) is arbitrarily small [@WanHic00b;@Wan03a]. A key feature of <code>QMCPy</code> is stopping criteria that automatically determine \\(n\\) so \\(|\\mu - \\widehat{\\mu}| \\le \\varepsilon\\) for a user-specified tolerance \\(\\varepsilon&gt;0\\), deterministically or with high probability.</p> <p><code>QMCPy</code> contains four main abstract classes:</p> <ol> <li> <p>Discrete Distributions generate IID or randomized LD sequences [@sorokin2025unified] including</p> <ul> <li>Lattices with random shifts [@CraPat76;@HicEtal03;@Ric51;@coveyou1967fourier;@WanHic02a].</li> <li>Digital Sequences (including Sobol' and Faure constructions) with digital shifts (DS), linear matrix scrambling (LMS), or nested uniform scrambling (NUS, also called Owen scrambling) [@Sob67;@dick2005multivariate; @Mat98;@Owe95;@owen2003variance;@dick2011higher;@Nie87;@Nie92;@DicPil10a]. Higher-order digital sequences are available to enable QMC convergence like \\(\\mathcal{O}(n^{-\\alpha+\\delta})\\) when \\(f\\) has \\(\\alpha\\) degrees of smoothness [@dick2011higher]. </li> <li>Halton Sequences with digital permutations, DS, LMS, or NUS [@Hal60;@WanHic00;@Mat98;@owen2024gain;@MorCaf94].</li> </ul> <p>Internally, <code>QMCPy</code>'s LD generators call our C package <code>QMCToolsCL</code> [@QMCToolsCL]. We also integrate with the <code>LDData</code> repository [@LDData] which collects lattice generating vectors and digital sequence generating matrices from Kuo's websites [@cools2006constructing;@nuyens2006fast;@KuoGenerators;@JoeKuo03;@joe2008constructing;@SobolDirection], the <code>Magic Point Shop</code> [@KuoNuy16a], and <code>LatNet Builder</code> [@LatNetBuilder.software].</p> </li> <li> <p>True Measures come with default transformations \\(\\boldsymbol{\\psi}\\) satisfying \\(\\boldsymbol{\\psi}(\\mathbf{X}) \\sim \\mathbf{T}\\). For example, if \\(\\mathbf{T} \\sim \\mathcal{N}(\\mathbf{m},\\Sigma= \\mathbf{A}\\mathbf{A}^T)\\) is a \\(d\\)-dimensional Gaussian, then \\(\\boldsymbol{\\psi}(\\mathbf{X}) = \\mathbf{A} \\Phi^{-1}(\\mathbf{X}) + \\mathbf{m}\\) where \\(\\Phi^{-1}\\) is the inverse Gaussian distribution function applied elementwise. We support the broad range of measures included in <code>scipy.stats</code> [@2020SciPy-NMeth].</p> </li> <li> <p>Integrands \\(g\\), given a transformation \\(\\boldsymbol{\\psi}\\), automatically set \\(f = g \\circ \\boldsymbol{\\psi}\\) so that \\(\\mu = \\mathbb{E}[g(\\mathbf{T})] = \\mathbb{E}[f(\\mathbf{X})]\\).</p> </li> <li> <p>Stopping Criteria (SC) adaptively increase the sample size \\(n\\) until (Q)MC estimates satisfy user-defined error tolerances [@HicEtal18a;@TonEtAl22a;@owen2024error]. SC include guaranteed MC algorithms [@HicEtal14a] and QMC algorithms based on:</p> <ul> <li>multiple randomizations of LD sequences [@l2023confidence],</li> <li>quickly tracking the decay of Fourier coefficients [@HicJim16a;@JimHic16a;@HicEtal17a;@DinHic20a], or</li> <li>fast Bayesian cubature [@Jag19a;@RatHic19a;@JagHic22a].</li> </ul> <p><code>QMCPy</code> is also capable of simultaneously approximating functions of multiple integrands [@sorokin2022bounding], and we are actively expanding support for multilevel (Q)MC algorithms following Julia's <code>MultilevelEstimators.jl</code> [@MultilevelEstimators].</p> <p>\\autoref{fig:stopping_crit} compares (Q)MC SC for Asian option pricing with 100 independent trials per error tolerance \\(\\varepsilon\\). The left and middle plots show median lines and shaded regions for 10%--90% quantiles. While MC SC require \\(n = \\mathcal{O}(1/\\varepsilon^2)\\) samples (and time), QMC SC require only \\(n = \\mathcal{O}(1/\\varepsilon)\\). (Q)MC SC consistently meet tolerances, with the right plot showing distributions of errors for a single error tolerance. </p> </li> </ol> <p></p>","tags":["(quasi-)Monte Carlo numerical integration","randomized low-discrepancy sequences","automatic error estimation","object oriented Python framework"]},{"location":"paper/paper/#acknowledgements","title":"Acknowledgements","text":"<p>The authors acknowledge support from the U.S. National Science Foundation grant DMS-2316011 and the Department of Energy Office of Science Graduate Student Research Program. We thank the international (Q)MC research community as well as JOSS reviewers and editors for invaluable and timely feedback and support.</p> <p>This article has been co-authored by employees of National Technology and Engineering Solutions of Sandia, LLC under Contract No. DE-NA0003525 with the U.S. Department of Energy (DOE). The employees co-own right, title and interest in and to the article and are responsible for its contents. The United States Government retains and the publisher, by accepting the article for publication, acknowledges that the United States Government retains a non-exclusive, paid-up, irrevocable, world-wide license to publish or reproduce the published form of this article or allow others to do so, for United States Government purposes. The DOE will provide public access to these results of federally sponsored research in accordance with the DOE Public Access Plan (https://www.energy.gov/downloads/doe-public-access-plan).</p>","tags":["(quasi-)Monte Carlo numerical integration","randomized low-discrepancy sequences","automatic error estimation","object oriented Python framework"]},{"location":"paper/paper/#references","title":"References","text":"","tags":["(quasi-)Monte Carlo numerical integration","randomized low-discrepancy sequences","automatic error estimation","object oriented Python framework"]}]}