{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2eefbe5",
   "metadata": {},
   "source": [
    "# How Much Accuracy Do You Need?\n",
    "\n",
    "Sou-Cheng Choi (with some edits by Fred Hickernell)\n",
    "\n",
    "Jun 16, 2025\n",
    "\n",
    "## Art Owen's Reflections on Lyness and the Accuracy Question\n",
    "\n",
    "This notebook explores a fascinating discussion about the accuracy requirements for numerical integration, particularly in the context of automatic quadrature routines. It's based on a conversation between Art Owen and Fred, highlighting the challenges of translating scientific needs into quantifiable accuracy targets.\n",
    "\n",
    "The central theme revolves around how a scientist determines the desired accuracy of a numerical integration result.  Let's examine three common responses:\n",
    "\n",
    "**Case A: The \"Plenty of Time\" Response**\n",
    "\n",
    "*   **Response:** \"I would like 8-figure accuracy. I have quite enough computer time available for this.\"\n",
    "*   **Explanation:** This is a relatively rare response. It indicates a situation where the scientist is primarily concerned with the *result* itself, rather than the computational cost.  It's suitable for small problems where the time spent on achieving high accuracy isn't a significant constraint.  Automatic quadrature routines are designed to handle such scenarios.\n",
    "\n",
    "**Case B: The \"Time-Constrained\" Response**\n",
    "\n",
    "*   **Response:** \"I need at least 4-figure accuracy. But I don’t want to use more than 2 seconds CPU time. If this can’t be done, I shall abandon this problem. If it can be done, I should prefer 6- or 7-figure accuracy. But if the marginal cost for more figures is really small let’s go to 12 figures.\"\n",
    "*   **Explanation:** This is a much more typical response. It reflects a realistic situation where the scientist is operating under time and resource constraints. They need a solution, but they also have a limited amount of CPU time.  Automatic quadrature routines with restart facilities (which are becoming more common) can be useful here, allowing the routine to refine its solution if it initially falls short of the desired accuracy.  The \"marginal cost\" refers to the additional time and effort required to increase the number of digits of accuracy.\n",
    "\n",
    "**Case C: The \"I Don't Know\" Response**\n",
    "\n",
    "*   **Response:** \"I really don’t know. Let me explain...\"\n",
    "*   **Explanation:** This is the most common response, and it highlights the fundamental challenge. The scientist may not have a clear understanding of the required accuracy, perhaps because the problem is complex, the underlying physics is poorly understood, or the desired application doesn't demand extremely high precision.  This is where the numerical analyst's role becomes crucial – to guide the scientist in understanding the implications of accuracy for their specific problem.\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0412d13",
   "metadata": {},
   "source": [
    "## The Problem\n",
    "Automatic quadrature routines (like those in QMCPy) require the user to specify a target accuracy. But in practice, scientists often don't know what accuracy they need, or their needs may change as they see results or as computational budgets shift.\n",
    "\n",
    "## The Solution: Resumable Integration in QMCPy\n",
    "With the new `resume` feature in QMCPy, you can start an integration with a loose tolerance, inspect the results, and then _resume_ the computation with a tighter tolerance—without starting over. This enables a flexible, iterative, and cost-effective approach to scientific computing.\n",
    "\n",
    "### How it Works\n",
    "- **Start with a loose tolerance**: Get a quick, rough answer.\n",
    "- **Save the computation state**: The integration data can be saved to disk.\n",
    "- **Resume with a tighter tolerance**: Continue from where you left off, using all previous samples.\n",
    "- **Repeat as needed**: You can keep tightening the tolerance, or even pause and resume across sessions or machines.\n",
    "\n",
    "This is especially useful for Case B and Case C scientists: you can explore, adapt, and only pay for more accuracy if you need it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e7e351",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "The following implementation will be carried out in the subclasses of StoppingCriterion:\n",
    "1. Add a `resume` parameter to the `integrate()` method \n",
    "2. Implement logic to restore the previous state when `resume=<AccumulateData instance>`.\n",
    "    * Ensure `n_min` is set correctly based on the resumed state.\n",
    "    * Support restoration of all relevant data (e.g., sample points, posterior state).\n",
    "3. Write tests to verify correct resumption behavior.\n",
    " \n",
    "**Bayesian Classes:**\n",
    "- `CubBayesLatticeG` \n",
    "- `CubBayesNetG` \n",
    "\n",
    "**Monte Carlo Classes:**\n",
    "- `CubMCCLT`\n",
    "- `CubMCCLT`\n",
    "- `CubMC`\n",
    "- Multilevel methods\n",
    "  - `CubMCML`\n",
    "  - `CubMCMLCont`\n",
    "\n",
    "**Quasi Monte Carlo Classes:**\n",
    "- `CubQMCCLT`\n",
    "- `CubQMCLatticeG`\n",
    "- `CubQMCNetG`\n",
    "- Multilevel methods\n",
    "  - `CubQMCML`\n",
    "  - `CubQMCMLCont`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13289d64",
   "metadata": {},
   "source": [
    "\n",
    "Let's see how this works in code. We will use a Genz oscillatory integrand and QMCPy's `CubQMCLatticeG` routine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da88901b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qmcpy import *\n",
    "import os\n",
    "\n",
    "# Define integrand and measure\n",
    "dimension = 3\n",
    "discrete_distrib = Lattice(dimension=dimension)\n",
    "true_measure = Gaussian(discrete_distrib, mean=0, covariance=1)\n",
    "integrand = Genz(discrete_distrib, kind_func='oscillatory', kind_coeff=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d57105",
   "metadata": {},
   "source": [
    "### Step 1: Quick Estimate\n",
    "Suppose you want a quick answer, so you set a loose tolerance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a1e6526",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loose tolerance solution: -0.428931, time: 0.003 s, # samples: 4096, error bound: 9.787e-05\n"
     ]
    }
   ],
   "source": [
    "abs_tol = 1e-4\n",
    "rel_tol = 0\n",
    "solver = CubQMCLatticeG(integrand, abs_tol=abs_tol, rel_tol=rel_tol)\n",
    "solution1, data1 = solver.integrate()\n",
    "print(f'Loose tolerance solution: {solution1[0]:.6f}, time: {data1.time_integrate:.3f} s, # samples: {data1.n_total:.0f}, error bound: {(data1.comb_bound_high[0] - data1.comb_bound_low[0])/2:.3e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7b31379",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LDTransformData (AccumulateData Object)\n",
       "    solution        -0.429\n",
       "    comb_bound_low  -0.429\n",
       "    comb_bound_high -0.429\n",
       "    comb_flags      1\n",
       "    n_total         2^(12)\n",
       "    n               2^(12)\n",
       "    time_integrate  0.003\n",
       "CubQMCLatticeG (StoppingCriterion Object)\n",
       "    abs_tol         1.00e-04\n",
       "    rel_tol         0\n",
       "    n_init          2^(10)\n",
       "    n_max           2^(35)\n",
       "Genz (Integrand Object)\n",
       "    kind_func       oscillatory\n",
       "    kind_coeff      1\n",
       "Uniform (TrueMeasure Object)\n",
       "    lower_bound     0\n",
       "    upper_bound     1\n",
       "Lattice (DiscreteDistribution Object)\n",
       "    d               3\n",
       "    dvec            [0 1 2]\n",
       "    randomize       SHIFT\n",
       "    order           NATURAL\n",
       "    gen_vec         [     1 182667 213731]\n",
       "    entropy         325005213987133239155911009368926147147\n",
       "    spawn_key       ()"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e6b4dca",
   "metadata": {},
   "source": [
    "The `data1` object contains all the diagnostic information from the first integration run. It includes the estimated solution, error bounds, number of samples used, and other useful statistics. This lets you see how close you are to your initial (loose) tolerance and how much work was done so far."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "334b832e",
   "metadata": {},
   "source": [
    "### Step 2: Save the State (Optional)\n",
    "You can save the integration state to disk for later resumption."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6d70ea1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved integration state to demo_resume_data/demo_resume_data.pkl\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "output_dir = Path(\"demo_resume_data\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "save_path = output_dir / 'demo_resume_data.pkl'\n",
    "data1.save(save_path)\n",
    "print(f'Saved integration state to {save_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36100d98",
   "metadata": {},
   "source": [
    "**Bonus: File Size Optimization with Compression**\n",
    "\n",
    "QMCPy's `AccumulateData.save()` method supports compression to reduce file sizes. This is especially useful for large integration problems or when storage space is limited.  When `compress=True`, the `.gz` extension is automatically appended to maintain consistency with standard compression conventions.\n",
    "\n",
    "**Example Usage:**\n",
    "```python\n",
    "# Save compressed - automatically creates 'data.pkl.gz'\n",
    "data.save('data.pkl', compress=True)  \n",
    "\n",
    "# Load compressed - auto-detection works\n",
    "loaded_data = AccumulateData.load('data.pkl.gz')\n",
    "```\n",
    "\n",
    "Let's demonstrate the file size difference and automatic naming behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a6c213a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files created:\n",
      "  Uncompressed: demo_resume_data_uncompressed.pkl\n",
      "  Compressed:   demo_resume_data_compressed.pkl.gz\n",
      "  Without compression: 339,578 bytes\n",
      "  With compression:    197,262 bytes (41.9% reduction)\n",
      "  Uncompressed data samples: 4096\n",
      "  Compressed data samples:   4096\n"
     ]
    }
   ],
   "source": [
    "# Save without compression (default)\n",
    "save_path_uncompressed = output_dir / 'demo_resume_data_uncompressed.pkl'\n",
    "data1.save(save_path_uncompressed, compress=False)\n",
    "\n",
    "# Save with compression (note: .gz extension will be automatically appended)\n",
    "save_path_compressed_base = output_dir / 'demo_resume_data_compressed.pkl'\n",
    "data1.save(save_path_compressed_base, compress=True)\n",
    "save_path_compressed = save_path_compressed_base.with_suffix('.pkl.gz')  \n",
    "\n",
    "print(f\"Files created:\")\n",
    "print(f\"  Uncompressed: {save_path_uncompressed.name}\")\n",
    "print(f\"  Compressed:   {save_path_compressed.name}\")\n",
    "\n",
    "# Compare file sizes\n",
    "size_uncompressed = os.path.getsize(save_path_uncompressed)\n",
    "size_compressed = os.path.getsize(save_path_compressed)\n",
    "print(f\"  Without compression: {size_uncompressed:,} bytes\")\n",
    "print(f\"  With compression:    {size_compressed:,} bytes ({100*(1-size_compressed/size_uncompressed):.1f}% reduction)\")\n",
    "\n",
    "# Verify that both files load correctly and produce the same data\n",
    "data_uncompressed = data1.__class__.load(save_path_uncompressed) \n",
    "data_compressed = data1.__class__.load(save_path_compressed)     \n",
    "print(f\"  Uncompressed data samples: {data_uncompressed.n_total:.0f}\")\n",
    "print(f\"  Compressed data samples:   {data_compressed.n_total:.0f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d175590d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test if TqdmWarning is resolved\n",
    "import warnings\n",
    "warnings.filterwarnings('error')  # Convert warnings to errors to catch them\n",
    "\n",
    "try:\n",
    "    from tqdm.auto import tqdm\n",
    "    import numpy as np\n",
    "    # Test tqdm with a simple progress bar\n",
    "    for i in tqdm(range(10), desc=\"Testing tqdm\"):\n",
    "        pass\n",
    "    print(\"✓ TqdmWarning resolved - ipywidgets working correctly\")\n",
    "except Exception as e:\n",
    "    print(f\"⚠️  Issue still exists: {e}\")\n",
    "finally:\n",
    "    warnings.resetwarnings()  # Reset warning filters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "933dbd5a",
   "metadata": {},
   "source": [
    "### Step 3: Resume with Tighter Tolerance\n",
    "Now suppose you want more accuracy. You can resume from the saved state, using all previous samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "58221b82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resumed with tighter tolerance solution: -0.4289321, time: 0.250 s, # samples: 1048576, error bound: 4.570e-08\n"
     ]
    }
   ],
   "source": [
    "# data1 = AccumulateData.load(save_path)  # optional\n",
    "abs_tol = 1e-7\n",
    "solver.set_tolerance(abs_tol=abs_tol)\n",
    "solution2, data2 = solver.integrate(resume=data1)\n",
    "print(f'Resumed with tighter tolerance solution: {solution2[0]:.7f}, time: {data2.time_integrate:.3f} s, # samples: {data2.n_total:.0f}, error bound: {(data2.comb_bound_high[0] - data2.comb_bound_low[0])/2:.3e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3d46fab0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LDTransformData (AccumulateData Object)\n",
       "    solution        -0.429\n",
       "    comb_bound_low  -0.429\n",
       "    comb_bound_high -0.429\n",
       "    comb_flags      1\n",
       "    n_total         2^(20)\n",
       "    n               2^(20)\n",
       "    time_integrate  0.250\n",
       "CubQMCLatticeG (StoppingCriterion Object)\n",
       "    abs_tol         1.00e-07\n",
       "    rel_tol         0\n",
       "    n_init          2^(10)\n",
       "    n_max           2^(35)\n",
       "Genz (Integrand Object)\n",
       "    kind_func       oscillatory\n",
       "    kind_coeff      1\n",
       "Uniform (TrueMeasure Object)\n",
       "    lower_bound     0\n",
       "    upper_bound     1\n",
       "Lattice (DiscreteDistribution Object)\n",
       "    d               3\n",
       "    dvec            [0 1 2]\n",
       "    randomize       SHIFT\n",
       "    order           NATURAL\n",
       "    gen_vec         [     1 182667 213731]\n",
       "    entropy         325005213987133239155911009368926147147\n",
       "    spawn_key       ()"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30addfbe",
   "metadata": {},
   "source": [
    "\n",
    "After resuming with a tighter tolerance, `data2` shows the updated integration state. You can compare this to `data1` to see how the error bounds have tightened, how many more samples were needed, and how the solution has changed. This demonstrates the efficiency of the resume feature: you don't lose any previous work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9517e844",
   "metadata": {},
   "source": [
    "### Step 4: Compare to Starting from Scratch\n",
    "For reference, let's see how long it takes to get the same accuracy if we start from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f3db5aef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start from scratch solution: -0.4289321, time: 0.260 s, # samples: 1048576, error bound: 4.570e-08\n",
      "Compared to the two-step process of time: 0.250\n"
     ]
    }
   ],
   "source": [
    "solver2 = CubQMCLatticeG(integrand, abs_tol=abs_tol, rel_tol=rel_tol)\n",
    "solution3, data3 = solver2.integrate()\n",
    "print(f'Start from scratch solution: {solution3[0]:.7f}, time: {data3.time_integrate:.3f} s, # samples: {data3.n_total:.0f}, error bound: {(data3.comb_bound_high[0] - data3.comb_bound_low[0])/2:.3e}')\n",
    "print(f'Compared to the two-step process of time: {data2.time_integrate :.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "21f52a6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LDTransformData (AccumulateData Object)\n",
       "    solution        -0.429\n",
       "    comb_bound_low  -0.429\n",
       "    comb_bound_high -0.429\n",
       "    comb_flags      1\n",
       "    n_total         2^(20)\n",
       "    n               2^(20)\n",
       "    time_integrate  0.260\n",
       "CubQMCLatticeG (StoppingCriterion Object)\n",
       "    abs_tol         1.00e-07\n",
       "    rel_tol         0\n",
       "    n_init          2^(10)\n",
       "    n_max           2^(35)\n",
       "Genz (Integrand Object)\n",
       "    kind_func       oscillatory\n",
       "    kind_coeff      1\n",
       "Uniform (TrueMeasure Object)\n",
       "    lower_bound     0\n",
       "    upper_bound     1\n",
       "Lattice (DiscreteDistribution Object)\n",
       "    d               3\n",
       "    dvec            [0 1 2]\n",
       "    randomize       SHIFT\n",
       "    order           NATURAL\n",
       "    gen_vec         [     1 182667 213731]\n",
       "    entropy         325005213987133239155911009368926147147\n",
       "    spawn_key       ()"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eebdbfb4",
   "metadata": {},
   "source": [
    "\n",
    "This is the diagnostic output from starting the integration from scratch with the tight tolerance. Compare this to `data2` (the resumed run), you should see that the time required is greater. This highlights the practical benefit of QMCPy's resume feature."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c0ff889",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "- With the resume feature, you can adaptively decide how much accuracy you need, and only pay for more if you want it.\n",
    "- You can pause, checkpoint, and resume long computations.\n",
    "- This is a practical answer to Lyness's Case B and Case C: you don't have to know your accuracy in advance!\n",
    "\n",
    "Try it yourself: change the tolerances, or resume from a saved file in a new session."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qmcpy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
