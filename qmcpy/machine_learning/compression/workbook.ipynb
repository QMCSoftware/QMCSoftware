{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sklearn in /home/r2q2/anaconda3/envs/qmcpy/lib/python3.9/site-packages (0.0)\n",
      "Requirement already satisfied: scikit-learn in /home/r2q2/anaconda3/envs/qmcpy/lib/python3.9/site-packages (from sklearn) (1.1.2)\n",
      "Requirement already satisfied: joblib>=1.0.0 in /home/r2q2/anaconda3/envs/qmcpy/lib/python3.9/site-packages (from scikit-learn->sklearn) (1.1.0)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /home/r2q2/anaconda3/envs/qmcpy/lib/python3.9/site-packages (from scikit-learn->sklearn) (1.23.0)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /home/r2q2/anaconda3/envs/qmcpy/lib/python3.9/site-packages (from scikit-learn->sklearn) (1.8.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/r2q2/anaconda3/envs/qmcpy/lib/python3.9/site-packages (from scikit-learn->sklearn) (3.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in /Users/r2q2/opt/anaconda3/envs/qmcpy/lib/python3.9/site-packages (2.9.1)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /Users/r2q2/opt/anaconda3/envs/qmcpy/lib/python3.9/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /Users/r2q2/opt/anaconda3/envs/qmcpy/lib/python3.9/site-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in /Users/r2q2/opt/anaconda3/envs/qmcpy/lib/python3.9/site-packages (from tensorflow) (1.1.2)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /Users/r2q2/opt/anaconda3/envs/qmcpy/lib/python3.9/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: packaging in /Users/r2q2/opt/anaconda3/envs/qmcpy/lib/python3.9/site-packages (from tensorflow) (21.3)\n",
      "Requirement already satisfied: six>=1.12.0 in /Users/r2q2/opt/anaconda3/envs/qmcpy/lib/python3.9/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /Users/r2q2/opt/anaconda3/envs/qmcpy/lib/python3.9/site-packages (from tensorflow) (4.3.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /Users/r2q2/opt/anaconda3/envs/qmcpy/lib/python3.9/site-packages (from tensorflow) (1.14.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /Users/r2q2/opt/anaconda3/envs/qmcpy/lib/python3.9/site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /Users/r2q2/opt/anaconda3/envs/qmcpy/lib/python3.9/site-packages (from tensorflow) (1.2.0)\n",
      "Requirement already satisfied: setuptools in /Users/r2q2/opt/anaconda3/envs/qmcpy/lib/python3.9/site-packages (from tensorflow) (65.3.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /Users/r2q2/opt/anaconda3/envs/qmcpy/lib/python3.9/site-packages (from tensorflow) (3.7.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /Users/r2q2/opt/anaconda3/envs/qmcpy/lib/python3.9/site-packages (from tensorflow) (1.46.4)\n",
      "Requirement already satisfied: tensorflow-estimator<2.10.0,>=2.9.0rc0 in /Users/r2q2/opt/anaconda3/envs/qmcpy/lib/python3.9/site-packages (from tensorflow) (2.9.0)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /Users/r2q2/opt/anaconda3/envs/qmcpy/lib/python3.9/site-packages (from tensorflow) (0.4.0)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in /Users/r2q2/opt/anaconda3/envs/qmcpy/lib/python3.9/site-packages (from tensorflow) (3.20.1)\n",
      "Requirement already satisfied: flatbuffers<2,>=1.12 in /Users/r2q2/opt/anaconda3/envs/qmcpy/lib/python3.9/site-packages (from tensorflow) (1.12)\n",
      "Requirement already satisfied: tensorboard<2.10,>=2.9 in /Users/r2q2/opt/anaconda3/envs/qmcpy/lib/python3.9/site-packages (from tensorflow) (2.9.0)\n",
      "Requirement already satisfied: keras<2.10.0,>=2.9.0rc0 in /Users/r2q2/opt/anaconda3/envs/qmcpy/lib/python3.9/site-packages (from tensorflow) (2.9.0)\n",
      "Requirement already satisfied: numpy>=1.20 in /Users/r2q2/opt/anaconda3/envs/qmcpy/lib/python3.9/site-packages (from tensorflow) (1.23.2)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /Users/r2q2/opt/anaconda3/envs/qmcpy/lib/python3.9/site-packages (from astunparse>=1.6.0->tensorflow) (0.37.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /Users/r2q2/opt/anaconda3/envs/qmcpy/lib/python3.9/site-packages (from tensorboard<2.10,>=2.9->tensorflow) (2.11.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /Users/r2q2/opt/anaconda3/envs/qmcpy/lib/python3.9/site-packages (from tensorboard<2.10,>=2.9->tensorflow) (0.6.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Users/r2q2/opt/anaconda3/envs/qmcpy/lib/python3.9/site-packages (from tensorboard<2.10,>=2.9->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /Users/r2q2/opt/anaconda3/envs/qmcpy/lib/python3.9/site-packages (from tensorboard<2.10,>=2.9->tensorflow) (0.4.6)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /Users/r2q2/opt/anaconda3/envs/qmcpy/lib/python3.9/site-packages (from tensorboard<2.10,>=2.9->tensorflow) (1.8.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /Users/r2q2/opt/anaconda3/envs/qmcpy/lib/python3.9/site-packages (from tensorboard<2.10,>=2.9->tensorflow) (2.2.2)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /Users/r2q2/opt/anaconda3/envs/qmcpy/lib/python3.9/site-packages (from tensorboard<2.10,>=2.9->tensorflow) (2.28.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/r2q2/opt/anaconda3/envs/qmcpy/lib/python3.9/site-packages (from packaging->tensorflow) (3.0.9)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /Users/r2q2/opt/anaconda3/envs/qmcpy/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (5.2.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /Users/r2q2/opt/anaconda3/envs/qmcpy/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (4.9)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/r2q2/opt/anaconda3/envs/qmcpy/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (0.2.7)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /Users/r2q2/opt/anaconda3/envs/qmcpy/lib/python3.9/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /Users/r2q2/opt/anaconda3/envs/qmcpy/lib/python3.9/site-packages (from markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow) (4.11.4)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /Users/r2q2/opt/anaconda3/envs/qmcpy/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/r2q2/opt/anaconda3/envs/qmcpy/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/r2q2/opt/anaconda3/envs/qmcpy/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (1.26.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/r2q2/opt/anaconda3/envs/qmcpy/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (2022.6.15)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /Users/r2q2/opt/anaconda3/envs/qmcpy/lib/python3.9/site-packages (from werkzeug>=1.0.1->tensorboard<2.10,>=2.9->tensorflow) (2.1.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/r2q2/opt/anaconda3/envs/qmcpy/lib/python3.9/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow) (3.8.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /Users/r2q2/opt/anaconda3/envs/qmcpy/lib/python3.9/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /Users/r2q2/opt/anaconda3/envs/qmcpy/lib/python3.9/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow) (3.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Recognizing hand-written digits\n",
    "\n",
    "This example shows how scikit-learn can be used to recognize images of\n",
    "hand-written digits, from 0-9.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Import datasets, classifiers and performance metrics\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m datasets, svm, metrics\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "# Author: Gael Varoquaux <gael dot varoquaux at normalesup dot org>\n",
    "# License: BSD 3 clause\n",
    "\n",
    "# Standard scientific Python imports\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import datasets, classifiers and performance metrics\n",
    "from sklearn import datasets, svm, metrics\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-24 10:55:13.999758: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-09-24 10:55:13.999780: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "from ctypes import *\n",
    "from numpy.ctypeslib import ndpointer\n",
    "import tensorflow as tf\n",
    "from qmcpy import *\n",
    "\n",
    "m = 10\n",
    "nu = 3\n",
    "\n",
    "Ndata = 60000\n",
    "Nqmc = 2**m;\n",
    "\n",
    "# load mnist data and rescale to 10x10\n",
    "mnist = tf.keras.datasets.mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_images, train_labels), (_, _) = tf.keras.datasets.mnist.load_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_labels' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain_labels\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_labels' is not defined"
     ]
    }
   ],
   "source": [
    "train_labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* What was discussed0\n",
    "    * Figured out how they propose to change the RMSE to change the data computations\n",
    "    * Actually computes the neural network in C….\n",
    "    * Steps to compile and reproduce Examples 1 and 2 in QMCPy’s ml_train_and_compress branch (SC’s request)\n",
    "* Next meeting: 10am Oct 15, Sat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1024, 100)\n",
      "(100, 60000)\n",
      "Weights loaded\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "from ctypes import *\n",
    "from numpy.ctypeslib import ndpointer\n",
    "import tensorflow as tf\n",
    "from qmcpy import *\n",
    "\n",
    "m = 10\n",
    "nu = 3\n",
    "\n",
    "Ndata = 60000\n",
    "Nqmc = 2**m;\n",
    "\n",
    "# load mnist data and rescale to 10x10\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "y_train =  np.float64(tf.one_hot(y_train,10).numpy())\n",
    "\n",
    "\n",
    "x_train= x_train[:Ndata,:,:]\n",
    "y_train = y_train[:Ndata]\n",
    "\n",
    "x_train = x_train[...,tf.newaxis]\n",
    "x_train = tf.image.resize(x_train,[10,10])\n",
    "x_train = x_train.numpy()[:,:,:,0]\n",
    "\n",
    "\n",
    "# data dimension\n",
    "Ndata=x_train.shape[0]\n",
    "s=x_train.shape[1]*x_train.shape[2]\n",
    "outs = y_train.shape[1]\n",
    "\n",
    "# flatten 10x10 to 100x1 for weight computation\n",
    "x_train_flat = np.float64(0.99*np.transpose(x_train.reshape(x_train.shape[0],s)))\n",
    "\n",
    "# load qmc points\n",
    "#dig_net = DigitalNetB2(2,seed=6)\n",
    "#qmc_points_gen = np.array(dig_net.gen_samples(Nqmc), dtype=np.ndarray)\n",
    "qmc_points = np.loadtxt('sobol.dat')\n",
    "#breakpoint()\n",
    "qmc_points = qmc_points[0:Nqmc,0:s]\n",
    "\n",
    "\n",
    "print(qmc_points.shape)\n",
    "print(x_train_flat.shape)\n",
    "\n",
    "# load c functions \n",
    "lib = cdll.LoadLibrary(\"/Users/r2q2/Projects/QMCSoftware/qmcpy/machine_learning/c_lib/computeMXY.so\")\n",
    "computeWeights = lib.computeWeights\n",
    "computeWeights.restype=ndpointer(dtype=c_double,shape=(1+outs,Nqmc))\n",
    "print('Weights loaded')\n",
    "\n",
    "# compute weights\n",
    "weights = computeWeights(c_int(nu),\n",
    "                         c_int(m),\n",
    "                         c_int(s),\n",
    "                         c_int(Ndata),\n",
    "                         c_int(Nqmc),\n",
    "                         c_int(outs),\n",
    "                         c_void_p(x_train_flat.ctypes.data),\n",
    "                         c_void_p(qmc_points.ctypes.data),c_void_p(y_train.ctypes.data))\n",
    "weights = np.transpose(weights)\n",
    "\n",
    "#breakpoint()\n",
    "print(weights.shape)\n",
    "with open('weights.pkl', 'wb') as handle:\n",
    "    pickle.dump(weights, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    tf.keras.layers.Dense(128, activation='sigmoid'),\n",
    "    tf.keras.layers.Dense(10)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.04247682, 0.00419723, 0.00481732, 0.00424134, 0.00429414,\n",
       "       0.0041096 , 0.00387474, 0.00418268, 0.00445003, 0.00414186,\n",
       "       0.00416787])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 4.24768229e-02,  4.19723307e-03,  4.81731771e-03, ...,\n",
       "         4.45003255e-03,  4.14186198e-03,  4.16787109e-03],\n",
       "       [ 5.15690104e-04,  4.71354167e-05,  6.29557292e-05, ...,\n",
       "         4.78515625e-05,  5.50455729e-05,  4.45312500e-05],\n",
       "       [-5.51979167e-03, -5.43587240e-04, -6.30794271e-04, ...,\n",
       "        -5.81933594e-04, -5.44856771e-04, -5.38509115e-04],\n",
       "       ...,\n",
       "       [ 1.36093750e-03,  1.37955729e-04,  1.52929688e-04, ...,\n",
       "         1.41731771e-04,  1.38313802e-04,  1.35872396e-04],\n",
       "       [-2.21803385e-03, -2.18326823e-04, -2.69856771e-04, ...,\n",
       "        -2.38769531e-04, -2.12695312e-04, -2.16894531e-04],\n",
       "       [ 2.44114583e-03,  2.51106771e-04,  2.73437500e-04, ...,\n",
       "         2.50716146e-04,  2.41634115e-04,  2.43229167e-04]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 5.62449511e-04,  5.51458865e-05,  6.32687606e-05,\n",
       "         5.61572598e-05,  5.80993569e-05,  5.44386426e-05,\n",
       "         5.08993676e-05,  5.53369473e-05,  5.88913690e-05,\n",
       "         5.45778858e-05,  5.56340349e-05],\n",
       "       [ 5.21740885e-03,  5.14790251e-04,  5.90131341e-04,\n",
       "         5.19727129e-04,  5.27989697e-04,  5.05833616e-04,\n",
       "         4.73468637e-04,  5.14485960e-04,  5.46916044e-04,\n",
       "         5.10293252e-04,  5.13772928e-04],\n",
       "       [ 2.33680013e-02,  2.31141764e-03,  2.65014648e-03,\n",
       "         2.33517253e-03,  2.36344401e-03,  2.26146647e-03,\n",
       "         2.13494466e-03,  2.30260417e-03,  2.44449056e-03,\n",
       "         2.27423503e-03,  2.29007975e-03],\n",
       "       [-7.43334993e-03, -7.34199474e-04, -8.45186121e-04,\n",
       "        -7.41203278e-04, -7.51012306e-04, -7.17591529e-04,\n",
       "        -6.77937347e-04, -7.30644914e-04, -7.80934053e-04,\n",
       "        -7.26115707e-04, -7.28525199e-04],\n",
       "       [ 1.59628635e-02,  1.57664388e-03,  1.81056586e-03,\n",
       "         1.59500868e-03,  1.61448568e-03,  1.54403483e-03,\n",
       "         1.45664605e-03,  1.57137316e-03,  1.67023926e-03,\n",
       "         1.55515951e-03,  1.56870660e-03],\n",
       "       [-4.54011429e-03, -4.48545766e-04, -5.15277296e-04,\n",
       "        -4.54349923e-04, -4.57705922e-04, -4.39099392e-04,\n",
       "        -4.15027006e-04, -4.47105517e-04, -4.76137876e-04,\n",
       "        -4.42676987e-04, -4.44188609e-04],\n",
       "       [ 2.77377398e-03,  2.73800569e-04,  3.13637389e-04,\n",
       "         2.75748053e-04,  2.81328480e-04,  2.68982699e-04,\n",
       "         2.52002598e-04,  2.73731436e-04,  2.91291029e-04,\n",
       "         2.70390013e-04,  2.72861715e-04],\n",
       "       [ 9.54046897e-03,  9.44287671e-04,  1.07923626e-03,\n",
       "         9.51868939e-04,  9.65456627e-04,  9.25042654e-04,\n",
       "         8.69550332e-04,  9.39168014e-04,  9.98125449e-04,\n",
       "         9.30528242e-04,  9.37204786e-04],\n",
       "       [ 4.24768229e-02,  4.19723307e-03,  4.81731771e-03,\n",
       "         4.24134115e-03,  4.29414063e-03,  4.10960286e-03,\n",
       "         3.87473958e-03,  4.18268229e-03,  4.45003255e-03,\n",
       "         4.14186198e-03,  4.16787109e-03],\n",
       "       [-1.25069484e-02, -1.23538707e-03, -1.42231297e-03,\n",
       "        -1.25092330e-03, -1.25889264e-03, -1.20539477e-03,\n",
       "        -1.14348662e-03, -1.22999527e-03, -1.31270715e-03,\n",
       "        -1.22106120e-03, -1.22678741e-03],\n",
       "       [-1.92177323e-03, -1.89725702e-04, -2.19019771e-04,\n",
       "        -1.92188622e-04, -1.93127956e-04, -1.85355678e-04,\n",
       "        -1.76090121e-04, -1.89770040e-04, -2.02211484e-04,\n",
       "        -1.87141740e-04, -1.87142114e-04]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "#X = np.array([[1, 2], [1, 4], [1, 0],              [10, 2], [10, 4], [10, 0]])\n",
    "kmeans = KMeans(n_clusters=11, random_state=0).fit(weights)\n",
    "kmeans.labels_\n",
    "\n",
    "#kmeans.predict([[0, 0], [12, 3]])\n",
    "\n",
    "kmeans.cluster_centers_\n",
    "\n",
    "#kmeans.labels_array([1, 2, 3, 4, 5, 6, 7 ,8 , 9 , 0], dtype=int32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(kmeans.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'module' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [14]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m to_predict \u001b[38;5;241m=\u001b[39m \u001b[43marray\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0.04247682\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.00419723\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.00481732\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.00424134\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.00429414\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m       \u001b[49m\u001b[38;5;241;43m0.0041096\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.00387474\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.00418268\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.00445003\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.00414186\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m       \u001b[49m\u001b[38;5;241;43m0.00416787\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      4\u001b[0m kmeans\u001b[38;5;241m.\u001b[39mpredict(to_predict)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'module' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "to_predict = array[0.04247682, 0.00419723, 0.00481732, 0.00424134, 0.00429414,\n",
    "       0.0041096 , 0.00387474, 0.00418268, 0.00445003, 0.00414186,\n",
    "       0.00416787].reshape(-1, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#kmeans.predict([[0, 0], [12, 3]]) array([1, 0], dtype=int32)\n",
    "#kmeans.cluster_centers_array([[10.,  2.],[ 1.,  2.]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "X = np.array([[-1, -1], [-2, -1], [1, 1], [2, 1]])\n",
    "y = np.array([1, 1, 2, 2])\n",
    "from sklearn.svm import SVC\n",
    "clf = make_pipeline(StandardScaler(), SVC(gamma='auto'))\n",
    "clf.fit(X, y)\n",
    "Pipeline(steps=[('standardscaler', StandardScaler()),\n",
    "                ('svc', SVC(gamma='auto'))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 4.24768229e-02,  4.19723307e-03,  4.81731771e-03, ...,\n",
       "         4.45003255e-03,  4.14186198e-03,  4.16787109e-03],\n",
       "       [ 5.15690104e-04,  4.71354167e-05,  6.29557292e-05, ...,\n",
       "         4.78515625e-05,  5.50455729e-05,  4.45312500e-05],\n",
       "       [-5.51979167e-03, -5.43587240e-04, -6.30794271e-04, ...,\n",
       "        -5.81933594e-04, -5.44856771e-04, -5.38509115e-04],\n",
       "       ...,\n",
       "       [ 1.36093750e-03,  1.37955729e-04,  1.52929688e-04, ...,\n",
       "         1.41731771e-04,  1.38313802e-04,  1.35872396e-04],\n",
       "       [-2.21803385e-03, -2.18326823e-04, -2.69856771e-04, ...,\n",
       "        -2.38769531e-04, -2.12695312e-04, -2.16894531e-04],\n",
       "       [ 2.44114583e-03,  2.51106771e-04,  2.73437500e-04, ...,\n",
       "         2.50716146e-04,  2.41634115e-04,  2.43229167e-04]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a classifier: a support vector classifier\n",
    "clf = svm.SVC(gamma=0.001)\n",
    "\n",
    "# Split data into 50% train and 50% test subsets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data, digits.target, test_size=0.5, shuffle=False\n",
    ")\n",
    "\n",
    "# Learn the digits on the train subset\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict the value of the digit on the test subset\n",
    "predicted = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0.,  0.,  5., ...,  1.,  0.,  0.],\n",
       "        [ 0.,  0., 13., ..., 15.,  5.,  0.],\n",
       "        [ 0.,  3., 15., ..., 11.,  8.,  0.],\n",
       "        ...,\n",
       "        [ 0.,  4., 11., ..., 12.,  7.,  0.],\n",
       "        [ 0.,  2., 14., ..., 12.,  0.,  0.],\n",
       "        [ 0.,  0.,  6., ...,  0.,  0.,  0.]],\n",
       "\n",
       "       [[ 0.,  0.,  0., ...,  5.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ...,  9.,  0.,  0.],\n",
       "        [ 0.,  0.,  3., ...,  6.,  0.,  0.],\n",
       "        ...,\n",
       "        [ 0.,  0.,  1., ...,  6.,  0.,  0.],\n",
       "        [ 0.,  0.,  1., ...,  6.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ..., 10.,  0.,  0.]],\n",
       "\n",
       "       [[ 0.,  0.,  0., ..., 12.,  0.,  0.],\n",
       "        [ 0.,  0.,  3., ..., 14.,  0.,  0.],\n",
       "        [ 0.,  0.,  8., ..., 16.,  0.,  0.],\n",
       "        ...,\n",
       "        [ 0.,  9., 16., ...,  0.,  0.,  0.],\n",
       "        [ 0.,  3., 13., ..., 11.,  5.,  0.],\n",
       "        [ 0.,  0.,  0., ..., 16.,  9.,  0.]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 0.,  0.,  1., ...,  1.,  0.,  0.],\n",
       "        [ 0.,  0., 13., ...,  2.,  1.,  0.],\n",
       "        [ 0.,  0., 16., ..., 16.,  5.,  0.],\n",
       "        ...,\n",
       "        [ 0.,  0., 16., ..., 15.,  0.,  0.],\n",
       "        [ 0.,  0., 15., ..., 16.,  0.,  0.],\n",
       "        [ 0.,  0.,  2., ...,  6.,  0.,  0.]],\n",
       "\n",
       "       [[ 0.,  0.,  2., ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0., 14., ..., 15.,  1.,  0.],\n",
       "        [ 0.,  4., 16., ..., 16.,  7.,  0.],\n",
       "        ...,\n",
       "        [ 0.,  0.,  0., ..., 16.,  2.,  0.],\n",
       "        [ 0.,  0.,  4., ..., 16.,  2.,  0.],\n",
       "        [ 0.,  0.,  5., ..., 12.,  0.,  0.]],\n",
       "\n",
       "       [[ 0.,  0., 10., ...,  1.,  0.,  0.],\n",
       "        [ 0.,  2., 16., ...,  1.,  0.,  0.],\n",
       "        [ 0.,  0., 15., ..., 15.,  0.,  0.],\n",
       "        ...,\n",
       "        [ 0.,  4., 16., ..., 16.,  6.,  0.],\n",
       "        [ 0.,  8., 16., ..., 16.,  8.,  0.],\n",
       "        [ 0.,  1.,  8., ..., 12.,  1.,  0.]]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "digits.images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1000x300 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#lenweights[0]\n",
    "_, axes = plt.subplots(nrows=1, ncols=4, figsize=(10, 3))\n",
    "for ax, image, label in zip(axes, digits.images, digits.target):\n",
    "    ax.set_axis_off()\n",
    "    ax.imshow(image, cmap=plt.cm.gray_r, interpolation=\"nearest\")\n",
    "    ax.set_title(\"Training: %i\" % label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a classifier: a support vector classifier\n",
    "clf = svm.SVC(gamma=0.001)\n",
    "\n",
    "# Split data into 50% train and 50% test subsets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    weights, digits.target, test_size=0.5, shuffle=False\n",
    ")\n",
    "\n",
    "# Learn the digits on the train subset\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict the value of the digit on the test subset\n",
    "predicted = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Digits dataset\n",
    "\n",
    "The digits dataset consists of 8x8\n",
    "pixel images of digits. The ``images`` attribute of the dataset stores\n",
    "8x8 arrays of grayscale values for each image. We will use these arrays to\n",
    "visualize the first 4 images. The ``target`` attribute of the dataset stores\n",
    "the digit each image represents and this is included in the title of the 4\n",
    "plots below.\n",
    "\n",
    "Note: if we were working from image files (e.g., 'png' files), we would load\n",
    "them using :func:`matplotlib.pyplot.imread`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00],\n",
       "       [0.00000000e+00, 0.00000000e+00, 4.80392165e-02, 6.00784361e-01,\n",
       "        8.45686197e-01, 9.92156863e-01, 7.32941628e-01, 6.20588243e-01,\n",
       "        8.78432393e-02, 0.00000000e+00],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 3.99764597e-01,\n",
       "        9.59804058e-01, 3.01961787e-03, 1.26823992e-01, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        7.14627624e-01, 8.99997428e-02, 1.17646836e-04, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        3.17646563e-03, 7.05411673e-01, 6.59333587e-01, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 3.33803326e-01, 9.90745068e-01, 4.03922945e-02,\n",
       "        0.00000000e+00, 0.00000000e+00],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 2.30000079e-01,\n",
       "        9.35686231e-01, 8.95098090e-01, 2.26470873e-01, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00],\n",
       "       [0.00000000e+00, 2.17686057e-01, 9.55097973e-01, 8.52274895e-01,\n",
       "        3.31608415e-01, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00]], dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1000x300 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "digits = datasets.load_digits()\n",
    "\n",
    "_, axes = plt.subplots(nrows=1, ncols=4, figsize=(10, 3))\n",
    "for ax, image, label in zip(axes, digits.images, digits.target):\n",
    "    ax.set_axis_off()\n",
    "    ax.imshow(image, cmap=plt.cm.gray_r, interpolation=\"nearest\")\n",
    "    ax.set_title(\"Training: %i\" % label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Input \u001b[0;32mIn [15]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdigits\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "\u001b[0;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "digits[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification\n",
    "\n",
    "To apply a classifier on this data, we need to flatten the images, turning\n",
    "each 2-D array of grayscale values from shape ``(8, 8)`` into shape\n",
    "``(64,)``. Subsequently, the entire dataset will be of shape\n",
    "``(n_samples, n_features)``, where ``n_samples`` is the number of images and\n",
    "``n_features`` is the total number of pixels in each image.\n",
    "\n",
    "We can then split the data into train and test subsets and fit a support\n",
    "vector classifier on the train samples. The fitted classifier can\n",
    "subsequently be used to predict the value of the digit for the samples\n",
    "in the test subset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flatten the images\n",
    "n_samples = len(digits.images)\n",
    "data = digits.images.reshape((n_samples, -1))\n",
    "\n",
    "# Create a classifier: a support vector classifier\n",
    "clf = svm.SVC(gamma=0.001)\n",
    "\n",
    "# Split data into 50% train and 50% test subsets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data, digits.target, test_size=0.5, shuffle=False\n",
    ")\n",
    "\n",
    "# Learn the digits on the train subset\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict the value of the digit on the test subset\n",
    "predicted = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we visualize the first 4 test samples and show their predicted\n",
    "digit value in the title.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, axes = plt.subplots(nrows=1, ncols=4, figsize=(10, 3))\n",
    "for ax, image, prediction in zip(axes, X_test, predicted):\n",
    "    ax.set_axis_off()\n",
    "    image = image.reshape(8, 8)\n",
    "    ax.imshow(image, cmap=plt.cm.gray_r, interpolation=\"nearest\")\n",
    "    ax.set_title(f\"Prediction: {prediction}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":func:`~sklearn.metrics.classification_report` builds a text report showing\n",
    "the main classification metrics.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    f\"Classification report for classifier {clf}:\\n\"\n",
    "    f\"{metrics.classification_report(y_test, predicted)}\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also plot a `confusion matrix <confusion_matrix>` of the\n",
    "true digit values and the predicted digit values.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disp = metrics.ConfusionMatrixDisplay.from_predictions(y_test, predicted)\n",
    "disp.figure_.suptitle(\"Confusion Matrix\")\n",
    "print(f\"Confusion matrix:\\n{disp.confusion_matrix}\")\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
