name: All Tests
on: [push]

jobs:
  #============================================================
  # 3 Test Jobs on 3 OSes: Windows, macOS, Ubuntu
  #============================================================
  tests:
    name: All Tests on ${{ matrix.os }}
    runs-on: ${{ matrix.os }}
    strategy:
      matrix: # https://docs.github.com/en/actions/reference/runners/github-hosted-runners
        os: ["windows-latest", "macos-latest", "ubuntu-latest"]

    # Ensure each matrix runner writes a distinct coverage data file (.coverage.<os>)
    # so we can safely combine them later.
    env:
      COVERAGE_FILE: .coverage.${{ matrix.os }}

    steps:
      - uses: actions/checkout@v4

      - uses: conda-incubator/setup-miniconda@v3
        with:
          miniconda-version: "latest"
          auto-activate-base: true
          conda-remove-defaults: true
          use-only-tar-bz2: true

      # -----------------------------------------------------------
      # Clean old coverage files
      # -----------------------------------------------------------
      - name: Remove old coverage data (Unix)
        if: runner.os != 'Windows'
        run: |
          rm -f .coverage* coverage.json || true

      - name: Remove old coverage data (Windows)
        if: runner.os == 'Windows'
        shell: pwsh
        run: |
          Remove-Item -Path .coverage* -Force -ErrorAction SilentlyContinue -Confirm:$false
          Remove-Item -Path coverage.json -Force -ErrorAction SilentlyContinue -Confirm:$false

      # -----------------------------------------------------------
      # Add 12GB swap, i.e., virtual memory (OS-specific)
      # -----------------------------------------------------------
      - name: Add 12GB swap (Linux)
        if: runner.os == 'Linux'
        run: |
          sudo fallocate -l 12G /swapfile
          sudo chmod 600 /swapfile
          sudo mkswap /swapfile
          sudo swapon /swapfile
          free -h

      - name: Add 12GB swap (macOS)
        if: runner.os == 'macOS'
        run: |
          # Create and mount a temporary swapfile
          sudo mkfile 12g /private/var/vm/tempswapfile
          sudo chmod 600 /private/var/vm/tempswapfile

      - name: Configure pagefile (Windows) via CIM (12GB)
        if: runner.os == 'Windows'
        shell: pwsh
        run: |
          $ErrorActionPreference = "Stop"

          $drive     = "D:"
          $initialMB = 12288
          $maxMB     = 12288
          $pagefile  = "$drive\pagefile.sys"

          Write-Host "Configuring pagefile: $pagefile (Initial=${initialMB}MB, Max=${maxMB}MB)"

          # Disable automatic management
          $cs = Get-CimInstance Win32_ComputerSystem
          if ($cs.AutomaticManagedPagefile) {
            Set-CimInstance -InputObject $cs -Property @{ AutomaticManagedPagefile = $false } | Out-Null
          }

          # Create or update the pagefile setting
          $escaped = $pagefile.Replace('','\')
          $pfs = Get-CimInstance Win32_PageFileSetting -Filter "Name='$escaped'" -ErrorAction SilentlyContinue

          if ($null -eq $pfs) {
            New-CimInstance -ClassName Win32_PageFileSetting -Property @{
              Name        = $pagefile
              InitialSize = $initialMB
              MaximumSize = $maxMB
            } | Out-Null
          } else {
            Set-CimInstance -InputObject $pfs -Property @{
              InitialSize = $initialMB
              MaximumSize = $maxMB
            } | Out-Null
          }

          "=== Pagefile settings ==="
          Get-CimInstance Win32_PageFileSetting | Format-Table Name,InitialSize,MaximumSize -Auto

      # -----------------------------------------------------------
      # Free disk + conda cache early (OS-specific)
      # -----------------------------------------------------------
      - name: Free space (Linux)
        if: runner.os == 'Linux'
        run: |
          df -h
          sudo apt-get clean
          sudo rm -rf /usr/share/dotnet /usr/local/lib/android /opt/ghc /opt/hostedtoolcache
          conda clean --all --yes
          pip cache purge || true
          df -h

      - name: Free space (macOS)
        if: runner.os == 'macOS'
        run: |
          df -h
          conda clean --all --yes
          pip cache purge || true
          df -h

      - name: Free space (Windows)
        if: runner.os == 'Windows'
        shell: pwsh
        run: |
          Get-Volume | Select-Object DriveLetter, Size, SizeRemaining | Format-Table
          conda clean --all --yes || true
          pip cache purge || true
          Get-Volume | Select-Object DriveLetter, Size, SizeRemaining | Format-Table

      # -----------------------------------------------------------
      # Show resources (OS-specific)
      # -----------------------------------------------------------
      - name: Show runner resources (Unix)
        if: runner.os != 'Windows'
        run: |
          echo "CPUs: $(nproc || sysctl -n hw.ncpu)"
          free -h || vm_stat
          df -h

      - name: Show runner resources (Windows)
        if: runner.os == 'Windows'
        shell: pwsh
        run: |
          # Check current pagefile settings
          Get-WmiObject Win32_PageFileUsage | Select-Object Name, AllocatedBaseSize
          # Display system memory info
          Get-ComputerInfo -Property CsTotalPhysicalMemory

      # -----------------------------------------------------------
      # Install Python dependencies
      # -----------------------------------------------------------
      - name: Cache pip packages
        uses: actions/cache@v4
        with:
          path: |
            ~/.cache/pip
            ~/.cache/pip/wheels
            ~/Library/Caches/pip
            ~/Library/Caches/pip/wheels
            ~\AppData\Local\pip\Cache
            ~\AppData\Local\pip\Cache\wheels
          key: ${{ runner.os }}-pip-${{ hashFiles('pyproject.toml') }}
          restore-keys: |
            ${{ runner.os }}-pip-

      - name: Cache wheel files
        uses: actions/cache@v4
        with:
          path: ./.wheels
          key: ${{ runner.os }}-wheels-${{ hashFiles('pyproject.toml') }}
          restore-keys: |
            ${{ runner.os }}-wheels-

      - name: Build and cache wheels (Linux)
        if: runner.os == 'Linux'
        run: |
          python -m pip wheel -w ./.wheels .[test,test_torch,test_gpytorch,test_botorch,test_umbridge] || true

      - name: Install Python dependencies (Linux)
        if: runner.os == 'Linux'
        run: |
          pip install --find-links ./.wheels -e ".[test,test_torch,test_gpytorch,test_botorch,test_umbridge]"

      - name: Build and cache wheels (macOS)
        if: runner.os == 'macOS'
        run: |
          python -m pip wheel -w ./.wheels .[test,test_torch,test_gpytorch,test_botorch] || true

      - name: Install Python dependencies (macOS)
        if: runner.os == 'macOS'
        run: |
          pip install --find-links ./.wheels -e ".[test,test_torch,test_gpytorch,test_botorch]"

      - name: Build and cache wheels (Windows)
        if: runner.os == 'Windows'
        shell: pwsh
        run: |
          python -m pip wheel -w ./.wheels .[test,test_torch,test_gpytorch,test_botorch] || Write-Host 'wheel build step completed'

      - name: Install Python dependencies (Windows)
        if: runner.os == 'Windows'
        shell: pwsh
        run: |
          pip install --find-links ./.wheels -e '.[test,test_torch,test_gpytorch,test_botorch]'

      # -----------------------------------------------------------
      # Install minimal LaTeX required by Jupyter notebooks (OS-specific)
      # -----------------------------------------------------------
      - name: Install minimal LaTeX (Linux)
        if: runner.os == 'Linux'
        run: |
          sudo apt update
          sudo apt install -y texlive-latex-base texlive-latex-extra texlive-latex-recommended latexmk dvipng cm-super

      - name: Install minimal LaTeX (macOS)
        if: runner.os == 'macOS'
        run: |
          brew install --cask basictex
          eval "$(/usr/libexec/path_helper)"
          sudo tlmgr update --self
          sudo tlmgr install latexmk dvipng collection-fontsrecommended type1cm

      - name: Cache MiKTeX (Windows)
        if: runner.os == 'Windows'
        uses: actions/cache@v4
        with:
          path: C:\Program Files\MiKTeX
          key: ${{ runner.os }}-miktex-${{ hashFiles('.github/workflows/alltests.yml') }}
          restore-keys: |
            ${{ runner.os }}-miktex-

      - name: Install MiKTeX (retry if needed)
        if: runner.os == 'Windows'
        shell: pwsh
        run: |
          choco install miktex --no-progress --limit-output -y || choco install miktex --no-progress --limit-output -y
          # Put MiKTeX on PATH for all subsequent steps
          echo "C:\Program Files\MiKTeX\miktex\bin\x64" | Out-File -FilePath $env:GITHUB_PATH -Encoding utf8 -Append
          $env:PATH = "C:\Program Files\MiKTeX\miktex\bin\x64;$env:PATH"

      - name: Install required MiKTeX packages
        if: runner.os == 'Windows'
        shell: pwsh
        run: |
          miktexsetup finish
          mpm --admin --verbose --require=cm,cm-super,cmex10,luxi,times,thailatex,latexmk,dvipng

      - name: Update MiKTeX font map
        if: runner.os == 'Windows'
        shell: pwsh
        run: |
          initexmf --admin --update-fndb
          initexmf --admin --mkmaps
          latex --version
          latexmk -v
          dvipng --version

      # -----------------------------------------------------------
      # Run doctests (OS-specific)
      # -----------------------------------------------------------
      - run: pip freeze
      - run: make doctests_minimal
      - run: make doctests_torch
      - run: make doctests_gpytorch
      - run: make doctests_botorch
      - run: make doctests_markdown

      - name: run umbridge doctests on Linux only
        shell: bash -l {0}
        run: |
          if [ "$RUNNER_OS" == "Linux" ]; then
            make doctests_umbridge
          else
            echo "umbridge doctests only run on Linux"
          fi

      # -----------------------------------------------------------
      # Run unittests for Python source files
      # -----------------------------------------------------------
      - name: Run unittests (parallel)
        run: make unittests

      # -----------------------------------------------------------
      # Free disk + conda cache before booktests (OS-specific)
      # -----------------------------------------------------------
      - name: Free space (Linux)
        if: runner.os == 'Linux'
        run: |
          df -h
          sudo apt-get clean
          sudo rm -rf /usr/share/dotnet /usr/local/lib/android /opt/ghc /opt/hostedtoolcache
          docker system prune -af || true
          conda clean --all --yes
          pip cache purge || true
          df -h

      - name: Free space (macOS)
        if: runner.os == 'macOS'
        run: |
          df -h
          conda clean --all --yes
          pip cache purge || true
          df -h

      - name: Free space (Windows)
        if: runner.os == 'Windows'
        shell: pwsh
        run: |
          Get-Volume | Select-Object DriveLetter, Size, SizeRemaining | Format-Table
          conda clean --all --yes || true
          pip cache purge || true
          Get-Volume | Select-Object DriveLetter, Size, SizeRemaining | Format-Table

      # -----------------------------------------------------------
      # Run unittests for Jupyter notebooks (OS-specific)
      # -----------------------------------------------------------
      - name: Run booktests (parallel) on Linux/macOS
        if: runner.os != 'Windows'
        shell: bash -l {0}
        run: |
          make booktests_parallel_no_docker # Parsl not supported on Windows

      - name: Run booktests (parallel) on Windows
        if: runner.os == 'Windows'
        shell: pwsh
        run: |
          make booktests_parallel_pytest PYTEST_XDIST="-n 4"

      # -----------------------------------------------------------
      # Display final accumulated coverage from all test stages
      # -----------------------------------------------------------
      - name: Display final coverage report
        if: runner.os != 'Windows'
        run: |
          coverage report -m
          coverage json

      # -----------------------------------------------------------
      # Upload coverage data for later combination
      # -----------------------------------------------------------
      - name: Upload coverage artifacts
        uses: actions/upload-artifact@v4
        with:
          name: coverage-${{ matrix.os }}
          path: |
            .coverage
            .coverage.*
            coverage.json
          if-no-files-found: warn
          retention-days: 1

  #============================================================
  # Coverage Summary Job - Runs after all OS tests complete
  #============================================================
  coverage-summary:
    name: Generate Coverage Summary
    needs: tests
    runs-on: ubuntu-latest
    if: always()
    
    steps:
      - uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
      
      - name: Install coverage
        run: pip install coverage
      
      # Download all coverage artifacts
      - name: Download all coverage artifacts
        uses: actions/download-artifact@v4
        with:
          pattern: coverage-*
          path: coverage-artifacts
          merge-multiple: false
      
      - name: Generate OS-specific coverage summary
        run: |
          python3 << 'EOF'
          import json
          import os
          from pathlib import Path
          
          # OS platforms from the workflow
          os_mapping = {
              "windows-latest": "Windows",
              "macos-latest": "macOS",
              "ubuntu-latest": "Ubuntu"
          }
          
          coverage_data = {}
          artifacts_dir = Path("coverage-artifacts")
          
          # Read coverage from each OS artifact
          for os_key in os_mapping.keys():
              artifact_path = artifacts_dir / f"coverage-{os_key}" / "coverage.json"
              if artifact_path.exists():
                  with open(artifact_path, 'r') as f:
                      data = json.load(f)
                      coverage_pct = data.get('totals', {}).get('percent_covered', 0)
                      coverage_data[os_key] = coverage_pct
              else:
                  coverage_data[os_key] = None
          
          # Generate summary table
          print("\n" + "="*60)
          print("OS and Coverage Summary")
          print("="*60)
          print(f"{'Operating System':<25} {'Coverage %':>20}")
          print("-"*60)
          
          total_coverage = 0
          valid_count = 0
          
          for os_key, os_name in os_mapping.items():
              coverage_pct = coverage_data.get(os_key)
              if coverage_pct is not None:
                  print(f"{os_name:<25} {coverage_pct:>19.2f}%")
                  total_coverage += coverage_pct
                  valid_count += 1
              else:
                  print(f"{os_name:<25} {'N/A':>20}")
          
          print("-"*60)
          if valid_count > 0:
              avg_coverage = total_coverage / valid_count
              print(f"{'Average Coverage':<25} {avg_coverage:>19.2f}%")
          print("="*60)
          print()
          EOF
      
      - name: Comment coverage summary on PR (if applicable)
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            const path = require('path');
            
            const osMapping = {
              'windows-latest': 'Windows',
              'macos-latest': 'macOS',
              'ubuntu-latest': 'Ubuntu'
            };
            
            let coverageData = {};
            const artifactsDir = 'coverage-artifacts';
            
            // Read coverage from each OS artifact
            for (const [osKey, osName] of Object.entries(osMapping)) {
              const artifactPath = path.join(artifactsDir, `coverage-${osKey}`, 'coverage.json');
              if (fs.existsSync(artifactPath)) {
                const data = JSON.parse(fs.readFileSync(artifactPath, 'utf8'));
                const coveragePct = data.totals?.percent_covered || 0;
                coverageData[osKey] = coveragePct;
              } else {
                coverageData[osKey] = null;
              }
            }
            
            // Generate markdown table
            let comment = '## ðŸ“Š Coverage Summary by OS\n\n';
            comment += '| Operating System | Coverage % |\n';
            comment += '|-----------------|------------|\n';
            
            let totalCoverage = 0;
            let validCount = 0;
            
            for (const [osKey, osName] of Object.entries(osMapping)) {
              const coverage = coverageData[osKey];
              if (coverage !== null) {
                comment += `| ${osName} | ${coverage.toFixed(2)}% |\n`;
                totalCoverage += coverage;
                validCount++;
              } else {
                comment += `| ${osName} | N/A |\n`;
              }
            }
            
            if (validCount > 0) {
              const avgCoverage = totalCoverage / validCount;
              comment += `| **Average** | **${avgCoverage.toFixed(2)}%** |\n`;
            }
            
            // Post comment
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: comment
            });

  
# For success reports, see https://github.com/QMCSoftware/QMCSoftware/actions?query=is%3Asuccess+workflow%3AAll+Tests
