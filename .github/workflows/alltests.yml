name: Run all tests

on:
  push:
    branches: [ main, develop, booktests_choi ]
  pull_request:
    branches: [ main, develop ]
  workflow_dispatch:

jobs:
  test:
    runs-on: ${{ matrix.os }}
    strategy:
      fail-fast: false
      matrix:
        os: [ubuntu-latest, macos-latest, windows-latest]
        python-version: ['3.8', '3.9', '3.10', '3.11']
        exclude:
          # Exclude some combinations to reduce build time
          - os: macos-latest
            python-version: '3.8'
          - os: windows-latest
            python-version: '3.8'

    steps:
      - uses: actions/checkout@v4
      - uses: conda-incubator/setup-miniconda@v3
        with:
          miniconda-version: "latest"
          auto-activate-base: true
          conda-remove-defaults: true
          use-only-tar-bz2: true
      
      # -----------------------------------------------------------
      # Clean old coverage files
      # -----------------------------------------------------------
      - name: Remove old coverage data (Unix)
        if: runner.os != 'Windows'
        run: |
          rm -f .coverage* coverage.json || true
      - name: Remove old coverage data (Windows)
        if: runner.os == 'Windows'
        shell: pwsh
        run: |
          Remove-Item -Path .coverage* -Force -ErrorAction SilentlyContinue -Confirm:$false
          Remove-Item -Path coverage.json -Force -ErrorAction SilentlyContinue -Confirm:$false
      # -----------------------------------------------------------
      # Add 12GB swap, i.e., virtual memory (OS-specific)
      # -----------------------------------------------------------
      - name: Add 12GB swap (Linux)
        if: runner.os == 'Linux'
        run: |
          sudo fallocate -l 12G /swapfile
          sudo chmod 600 /swapfile
          sudo mkswap /swapfile
          sudo swapon /swapfile
          free -h
      - name: Add 12GB swap (macOS)
        if: runner.os == 'macOS'
        run: |
          # Create and mount a temporary swapfile
          sudo mkfile 12g /private/var/vm/tempswapfile
          sudo chmod 600 /private/var/vm/tempswapfile
      - name: Configure pagefile (Windows) via CIM (12GB)
        if: runner.os == 'Windows'
        shell: pwsh
        run: |
          $ErrorActionPreference = "Stop"
          $drive    = "D:"
          $initialMB = 12288
          $maxMB     = 12288
          $pagefile  = "$drive\pagefile.sys"
          Write-Host "Configuring pagefile: $pagefile (Initial=${initialMB}MB, Max=${maxMB}MB)"
          # Disable automatic management
          $cs = Get-CimInstance Win32_ComputerSystem
          if ($cs.AutomaticManagedPagefile) {
            Set-CimInstance -InputObject $cs -Property @{ AutomaticManagedPagefile = $false } | Out-Null
          }
          # Create or update the pagefile setting
          $escaped = $pagefile.Replace('\','\\')
          $pfs = Get-CimInstance Win32_PageFileSetting -Filter "Name='$escaped'" -ErrorAction SilentlyContinue
          if ($null -eq $pfs) {
            New-CimInstance -ClassName Win32_PageFileSetting -Property @{
              Name        = $pagefile
              InitialSize = $initialMB
              MaximumSize = $maxMB
            } | Out-Null
          } else {
            Set-CimInstance -InputObject $pfs -Property @{
              InitialSize = $initialMB
              MaximumSize = $maxMB
            } | Out-Null
          }
          "=== Pagefile settings ==="
          Get-CimInstance Win32_PageFileSetting | Format-Table Name,InitialSize,MaximumSize -Auto
      # -----------------------------------------------------------
      # Free disk + conda cache early (OS-specific)
      # -----------------------------------------------------------
      - name: Free space (Linux)
        if: runner.os == 'Linux'
        run: |
          df -h
          sudo apt-get clean
          sudo rm -rf /usr/share/dotnet /usr/local/lib/android /opt/ghc /opt/hostedtoolcache
          conda clean --all --yes
          pip cache purge || true
          df -h
      - name: Free space (macOS)
        if: runner.os == 'macOS'
        run: |
          df -h
          conda clean --all --yes
          pip cache purge || true
          df -h
      - name: Free space (Windows)
        if: runner.os == 'Windows'
        shell: pwsh
        run: |
          Get-Volume | Select-Object DriveLetter, Size, SizeRemaining | Format-Table
          conda clean --all --yes || true
          pip cache purge || true
          Get-Volume | Select-Object DriveLetter, Size, SizeRemaining | Format-Table
      # -----------------------------------------------------------
      # Show resources (OS-specific)
      # -----------------------------------------------------------
      - name: Show runner resources
        if: runner.os != 'Windows'
        run: |
          echo "CPUs: $(nproc || sysctl -n hw.ncpu)"
          free -h || vm_stat
          df -h
      - name: Show runner resources
        if: runner.os == 'Windows'
        run: |
          # Check current pagefile settings
          Get-WmiObject Win32_PageFileUsage | Select-Object Name, AllocatedBaseSize
          # Display system memory info
          Get-ComputerInfo -Property CsTotalPhysicalMemory
      # -----------------------------------------------------------
      # Install Python dependencies
      # -----------------------------------------------------------
      - name: Cache pip packages
        uses: actions/cache@v4
        with:
          path: |
            ~/.cache/pip
            ~/.cache/pip/wheels
            ~/Library/Caches/pip
            ~/Library/Caches/pip/wheels
            ~\AppData\Local\pip\Cache
            ~\AppData\Local\pip\Cache\wheels
          key: ${{ runner.os }}-pip-${{ hashFiles('pyproject.toml') }}
          restore-keys: |
            ${{ runner.os }}-pip-
      - name: Cache wheel files
        uses: actions/cache@v4
        with:
          path: ./.wheels
          key: ${{ runner.os }}-wheels-${{ hashFiles('pyproject.toml') }}
          restore-keys: |
            ${{ runner.os }}-wheels-
          
      - name: Build and cache wheels (Linux)
        if: runner.os == 'Linux'
        run: |
          python -m pip wheel -w ./.wheels .[test,test_torch,test_gpytorch,test_botorch,test_umbridge] || true
      - name: Install Python dependencies (Linux)
        if: runner.os == 'Linux'
        run: |
          pip install --find-links ./.wheels -e ".[test,test_torch,test_gpytorch,test_botorch,test_umbridge]"
      - name: Build and cache wheels (macOS)
        if: runner.os == 'macOS'
        run: |
          python -m pip wheel -w ./.wheels .[test,test_torch,test_gpytorch,test_botorch] || true
      - name: Install Python dependencies (macOS)
        if: runner.os == 'macOS'
        run: |
          pip install --find-links ./.wheels -e ".[test,test_torch,test_gpytorch,test_botorch]"
      - name: Build and cache wheels (Windows)
        if: runner.os == 'Windows'
        shell: pwsh
        run: |
          python -m pip wheel -w ./.wheels .[test,test_torch,test_gpytorch,test_botorch] || Write-Host 'wheel build step completed'
      - name: Install Python dependencies (Windows)
        if: runner.os == 'Windows'
        shell: pwsh
        run: |
          pip install --find-links ./.wheels -e '.[test,test_torch,test_gpytorch,test_botorch]'
      # -----------------------------------------------------------
      # Install minimal LaTeX required by Jupyter notebooks (OS-specific)
      # -----------------------------------------------------------
      - name: Install minimal LaTeX (Linux)
        if: runner.os == 'Linux'
        run: |
          sudo apt update
          sudo apt install -y texlive-latex-base texlive-latex-extra texlive-latex-recommended latexmk dvipng cm-super
      - name: Install minimal LaTeX (macOS)
        if: runner.os == 'macOS'
        run: |
          brew install --cask basictex
          eval "$(/usr/libexec/path_helper)"
          sudo tlmgr update --self
          sudo tlmgr install latexmk dvipng collection-fontsrecommended type1cm
      - name: Cache MiKTeX (Windows)
        if: runner.os == 'Windows'
        uses: actions/cache@v4
        with:
          path: C:\Program Files\MiKTeX
          key: ${{ runner.os }}-miktex-${{ hashFiles('.github/workflows/alltests.yml') }}
          restore-keys: |
            ${{ runner.os }}-miktex-
          
      - name: Install MiKTeX (retry if needed)
        if: runner.os == 'Windows'
        shell: pwsh
        run: |
          choco install miktex --no-progress --limit-output -y || choco install miktex --no-progress --limit-output -y
          # Put MiKTeX on PATH for all subsequent steps
          echo "C:\Program Files\MiKTeX\miktex\bin\x64" | Out-File -FilePath $env:GITHUB_PATH -Encoding utf8 -Append
          $env:PATH = "C:\Program Files\MiKTeX\miktex\bin\x64;$env:PATH"
      
      - name: Install required MiKTeX packages
        if: runner.os == 'Windows'
        shell: pwsh
        run: |
          miktexsetup finish
          mpm --admin --verbose --require=cm,cm-super,cmex10,luxi,times,thailatex,latexmk,dvipng
      
      - name: Update MiKTeX font map
        if: runner.os == 'Windows'
        shell: pwsh
        run: |
          initexmf --admin --update-fndb
          initexmf --admin --mkmaps
          
          latex --version
          latexmk -v
          dvipng --version
      
      # -----------------------------------------------------------
      # Run doctests (OS-specific)
      # -----------------------------------------------------------      
      - run: pip freeze
      - run: make doctests_minimal
      - run: make doctests_torch
      - run: make doctests_gpytorch
      - run: make doctests_botorch
      - run: make doctests_markdown
      - name: run umbridge doctests on Linux only
        shell: bash -l {0}
        run: |
          if [ "$RUNNER_OS" == "Linux" ]; then
            make doctests_umbridge
          else
            echo "umbridge doctests only run on Linux"
          fi
      # -----------------------------------------------------------
      # Run unittests for Python source files
      # -----------------------------------------------------------   
      - name: Run unittests (parallel)
        run: make unittests
      
      # -----------------------------------------------------------
      # Free disk + conda cache before booktests (OS-specific)
      # -----------------------------------------------------------
      - name: Free space (Linux)
        if: runner.os == 'Linux'
        run: |
          df -h
          sudo apt-get clean
          sudo rm -rf /usr/share/dotnet /usr/local/lib/android /opt/ghc /opt/hostedtoolcache
          docker system prune -af || true
          conda clean --all --yes
          pip cache purge || true
          df -h
      - name: Free space (macOS)
        if: runner.os == 'macOS'
        run: |
          df -h
          conda clean --all --yes
          pip cache purge || true
          df -h
      - name: Free space (Windows)
        if: runner.os == 'Windows'
        shell: pwsh
        run: |
          Get-Volume | Select-Object DriveLetter, Size, SizeRemaining | Format-Table
          conda clean --all --yes || true
          pip cache purge || true
          Get-Volume | Select-Object DriveLetter, Size, SizeRemaining | Format-Table
      # -----------------------------------------------------------
      # Run unittests for Jupyter notebooks (OS-specific)
      # ----------------------------------------------------------- 
      - name: Run booktests (parallel) on Linux/macOS
        if: runner.os != 'Windows'
        shell: bash -l {0}
        run: |
          make booktests_parallel_no_docker    # Parsl not supported on Windows
      - name: Run booktests (parallel) on Windows
        if: runner.os == 'Windows'
        shell: pwsh
        run: |
          make booktests_parallel_pytest PYTEST_XDIST="-n 4"
      # -----------------------------------------------------------
      # Display final accumulated coverage from all test stages
      # -----------------------------------------------------------
      - name: Display final coverage report
        if: runner.os != 'Windows'
        run: |
          coverage report -m
          coverage json
      
      # -----------------------------------------------------------
      # Debug: List coverage files before upload
      # -----------------------------------------------------------
      - name: List coverage files before upload
        run: |
          echo "=== Current directory ==="
          pwd
          echo "=== All coverage files ==="
          ls -la .coverage* coverage.json 2>/dev/null || echo "No coverage files found"
          echo "=== Find all .coverage files ==="
          find . -maxdepth 1 -name '.coverage*' -o -name 'coverage.json' | head -20
        shell: bash
        
      # -----------------------------------------------------------
      # Upload coverage data for later combination
      # -----------------------------------------------------------
      - name: Upload coverage artifacts
        uses: actions/upload-artifact@v4
        with:
          name: coverage-${{ matrix.os }}
          path: |
            .coverage
            .coverage.*
            coverage.json
          retention-days: 1
          if-no-files-found: warn
          include-hidden-files: true

  # =============================================================
  # Combine coverage from all OS runners and produce final report
  # =============================================================
  combine-coverage:
    runs-on: ubuntu-latest
    needs: test
    
    steps:
    - uses: actions/checkout@v3
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e .[dev]
    
    - name: Download all coverage artifacts
      uses: actions/download-artifact@v3
      with:
        path: coverage-data
    
    - name: Combine coverage data
      run: |
        coverage combine coverage-data/**/.coverage*
        coverage xml
        coverage report
    
    - name: Upload combined coverage
      uses: codecov/codecov-action@v3
      with:
        file: ./coverage.xml
        flags: combined
        name: codecov-combined

  test-book:
    runs-on: ubuntu-latest
    strategy:
      fail-fast: false
      matrix:
        book-section: [
          'preface',
          'chapter1',
          'chapter2', 
          'chapter3',
          'chapter4',
          'chapter5',
          'chapter6',
          'chapter7',
          'chapter8',
          'appendix'
        ]

    steps:
    - uses: actions/checkout@v3
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
    
    - name: Install system dependencies
      run: |
        sudo apt-get update
        sudo apt-get install -y texlive-xetex texlive-fonts-recommended texlive-plain-generic pandoc
    
    - name: Install Python dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e .[dev]
        pip install jupyter-book sphinx-book-theme myst-nb
    
    - name: List book files
      run: |
        echo "Checking for book files in section: ${{ matrix.book-section }}"
        if [ -d "book/${{ matrix.book-section }}" ]; then
          echo "Found directory: book/${{ matrix.book-section }}"
          ls -la book/${{ matrix.book-section }}
        else
          echo "Directory not found: book/${{ matrix.book-section }}"
        fi
    
    - name: Test book section - ${{ matrix.book-section }}
      run: |
        section="${{ matrix.book-section }}"
        
        # Check if section directory exists
        if [ ! -d "book/$section" ]; then
          echo "Section directory book/$section does not exist, skipping..."
          exit 0
        fi
        
        # Find all .md and .ipynb files in the section
        files=$(find "book/$section" -type f \( -name "*.md" -o -name "*.ipynb" \) 2>/dev/null || true)
        
        if [ -z "$files" ]; then
          echo "No markdown or notebook files found in book/$section, skipping..."
          exit 0
        fi
        
        echo "Found files to test:"
        echo "$files"
        
        # Test each file
        failed=0
        for file in $files; do
          echo "Testing: $file"
          
          if [[ "$file" == *.ipynb ]]; then
            # For notebooks, execute them
            if jupyter nbconvert --to notebook --execute --inplace "$file"; then
              echo "✓ Passed: $file"
            else
              echo "✗ Failed: $file"
              failed=$((failed + 1))
            fi
          else
            # For markdown files, just check if they're valid
            if [ -r "$file" ]; then
              echo "✓ Readable: $file"
            else
              echo "✗ Not readable: $file"
              failed=$((failed + 1))
            fi
          fi
        done
        
        if [ $failed -gt 0 ]; then
          echo "$failed file(s) failed in section $section"
          exit 1
        fi
    
    - name: Build book section
      run: |
        section="${{ matrix.book-section }}"
        
        # Only try to build if section exists
        if [ ! -d "book/$section" ]; then
          echo "Section directory book/$section does not exist, skipping build..."
          exit 0
        fi
        
        # Create a minimal _config.yml for testing if it doesn't exist
        if [ ! -f "book/_config.yml" ]; then
          cat > book/_config.yml << 'EOF'
        title: QMC Software Book
        author: QMC Software Team
        logo: ''
        
        execute:
          execute_notebooks: cache
          timeout: 300
        
        parse:
          myst_enable_extensions:
            - dollarmath
            - linkify
            - substitution
            - colon_fence
        
        sphinx:
          config:
            nb_execution_mode: cache
        EOF
        fi
        
        # Create a minimal _toc.yml for this section if needed
        if [ ! -f "book/_toc.yml" ]; then
          cat > book/_toc.yml << EOF
        format: jb-book
        root: README
        chapters:
        - file: $section/README
        EOF
        fi
        
        # Try to build just this section
        echo "Building book section: $section"
        cd book
        if ls $section/*.md $section/*.ipynb 2>/dev/null | grep -q .; then
          if ls .coverage .coverage.* 2>/dev/null | grep -v '.coveragerc' | grep -q .; then
            echo "Found coverage files, combining..."
            coverage combine
          fi
          # Build the whole book (jupyter-book doesn't support partial builds easily)
          jupyter-book build . || echo "Build had warnings but continuing..."
        else
          echo "No content files found in $section/"
        fi

  build-full-book:
    runs-on: ubuntu-latest
    needs: test-book
    if: github.event_name == 'push' && (github.ref == 'refs/heads/main' || github.ref == 'refs/heads/develop' || github.ref == 'refs/heads/booktests_choi')
    
    steps:
    - uses: actions/checkout@v3
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
    
    - name: Install system dependencies
      run: |
        sudo apt-get update
        sudo apt-get install -y texlive-xetex texlive-fonts-recommended texlive-plain-generic pandoc
    
    - name: Install Python dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e .[dev]
        pip install jupyter-book sphinx-book-theme myst-nb
    
    - name: Build full book
      run: |
        cd book
        jupyter-book build .
    
    - name: Deploy to GitHub Pages
      if: github.ref == 'refs/heads/main'
      uses: peaceiris/actions-gh-pages@v3
      with:
        github_token: ${{ secrets.GITHUB_TOKEN }}
        publish_dir: ./book/_build/html
        publish_branch: gh-pages
        force_orphan: true
